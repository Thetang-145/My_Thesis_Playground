,paper_id,input,output_bestRouge1,output_bestRougeAvg
0,SP:b19df5243359791fbaad005d6f13d7e9fdb0ff63,"Role - based learning USED-FOR scalable multi - agent learning. Role - based learning USED-FOR decomposing complex tasks. decomposing complex tasks USED-FOR scalable multi - agent learning. roles USED-FOR decomposing complex tasks. role selector USED-FOR role discovery. role selector USED-FOR role space. role space CONJUNCTION temporal resolution. temporal resolution CONJUNCTION role space. it USED-FOR bi - level learning hierarchy. primitive action - observation spaces FEATURE-OF role policies. action effects USED-FOR role selector. learning efficiency CONJUNCTION policy generalization. policy generalization CONJUNCTION learning efficiency. method COMPARE MARL algorithms. MARL algorithms COMPARE method. OtherScientificTerm are joint action spaces, and restricted role action spaces. Material is StarCraft II micromanagement benchmark. ","This paper proposes a new multi-agent multi-task RL algorithm that uses role-based learning to tackle the problem of decomposing complex tasks into roles. The authors propose to use a role selector to learn a role space and temporal resolution, and then use a bi-level learning hierarchy to learn the role selector for role discovery. The role policies are learned in primitive action-observation spaces, where joint action spaces are not available. The proposed method is evaluated on the StarCraft II micromanagement benchmark, and it is shown to outperform existing MARL algorithms in terms of learning efficiency and policy generalization in restricted role action spaces.","This paper proposes a new multi-agent multi-task RL algorithm that uses role-based learning to tackle the problem of decomposing complex tasks into roles. The authors propose to use a role selector to learn a role space and temporal resolution, and then use a bi-level learning hierarchy to learn the role selector for role discovery. The role policies are learned in primitive action-observation spaces, where joint action spaces are not available. The proposed method is evaluated on the StarCraft II micromanagement benchmark, and it is shown to outperform existing MARL algorithms in terms of learning efficiency and policy generalization in restricted role action spaces."
9,SP:7deb61890d97422a0fe141ca807f968c70ab239a,"stochastic subgradient descent ( SSGD ) method USED-FOR over - parameterized nonsmooth optimization problems. interpolation condition FEATURE-OF over - parameterized nonsmooth optimization problems. composite structure USED-FOR SSGD. composite structure USED-FOR empirical risk minimization problems. stochastic gradient descent ( SGD ) method USED-FOR smooth problems. rates COMPARE rates. rates COMPARE rates. rates USED-FOR stochastic gradient descent ( SGD ) method. SGD USED-FOR smooth and nonsmooth machine learning models. SSGD USED-FOR smooth and nonsmooth machine learning models. SGD CONJUNCTION SSGD. SSGD CONJUNCTION SGD. subgradient method USED-FOR convex and interpolation setting. OtherScientificTerm are convex and strongly - convex objectives, and interpolation. ","This paper proposes a stochastic subgradient descent (SSGD) method for solving over-parameterized nonsmooth optimization problems with an interpolation condition. The authors show that the composite structure of SSGD can be used to solve empirical risk minimization problems with convex and strongly-convex objectives. The paper also shows that the rates of the SSGD outperforms rates of SGD for smooth problems.   The authors also show that SGD and SSGD are equivalent to each other in the case of smooth and nonsmoothed machine learning models. The main contribution of the paper is that the subgradient method can be applied to both the smooth and interpolation setting, and that the interpolation is a special case of the convex case.","This paper proposes a stochastic subgradient descent (SSGD) method for solving over-parameterized nonsmooth optimization problems with an interpolation condition. The authors show that the composite structure of SSGD can be used to solve empirical risk minimization problems with convex and strongly-convex objectives. The paper also shows that the rates of the SSGD outperforms rates of SGD for smooth problems.   The authors also show that SGD and SSGD are equivalent to each other in the case of smooth and nonsmoothed machine learning models. The main contribution of the paper is that the subgradient method can be applied to both the smooth and interpolation setting, and that the interpolation is a special case of the convex case."
18,SP:c7e0b3fedc0d0409d662dd612b529fdacad2b03e,"non - linear “ reservoir ” layers CONJUNCTION regular transformer layers. regular transformer layers CONJUNCTION non - linear “ reservoir ” layers. Method are transformers, and machine learning. OtherScientificTerm is layers. Metric is wall - clock compute time. Task is machine translation. ",This paper proposes to use non-linear “reservoir” layers instead of regular transformer layers in order to reduce the computational cost of transformers. The authors show that the proposed layers can be used to reduce wall-clock compute time and achieve state-of-the-art performance in machine learning. The paper is well-written and well-motivated. Experiments on machine translation are conducted to validate the effectiveness of the proposed method. ,This paper proposes to use non-linear “reservoir” layers instead of regular transformer layers in order to reduce the computational cost of transformers. The authors show that the proposed layers can be used to reduce wall-clock compute time and achieve state-of-the-art performance in machine learning. The paper is well-written and well-motivated. Experiments on machine translation are conducted to validate the effectiveness of the proposed method. 
27,SP:ba9f1d4738ec67a440346f3ac6c4cf35f7232077,"transformation invariance CONJUNCTION equivariance. equivariance CONJUNCTION transformation invariance. transformation invariance FEATURE-OF network architecture. equivariance FEATURE-OF network architecture. geometry transformation of data FEATURE-OF network robustness. Filter transform USED-FOR steerable CNN. group representation theory USED-FOR steerable CNN. group representation theory USED-FOR function space structure. group representation theory USED-FOR steerable kernel function. function space structure FEATURE-OF steerable kernel function. theory COMPARE filter transform technique. filter transform technique COMPARE theory. group representation theory USED-FOR kernel. filter transform USED-FOR kernel. filter transformed kernels USED-FOR group representation. approach USED-FOR steerable convolution operators. Method are Steerable CNN, and steerable CNN theory. OtherScientificTerm is overfitting. ","This paper proposes a steerable CNN based on the group representation theory. The authors show that steerable convolutional kernels are invariant to transformation invariance and equivariant to equivariance. They also show that the steerable kernel function can be represented as a group representation based on filter transformed kernels.    Steerable CNN is an important topic in the field of neural network robustness to geometry transformation of data. Filter transform has been proposed as a simple and effective way to represent steerable neural networks. However, this paper shows that this theory is more general than the standard filter transform technique, and can be used to represent any kernel based on a filter transform. This paper also shows that the proposed approach can be applied to steerable convex and non-convex convolution operators to avoid overfitting.","This paper proposes a steerable CNN based on the group representation theory. The authors show that steerable convolutional kernels are invariant to transformation invariance and equivariant to equivariance. They also show that the steerable kernel function can be represented as a group representation based on filter transformed kernels.    Steerable CNN is an important topic in the field of neural network robustness to geometry transformation of data. Filter transform has been proposed as a simple and effective way to represent steerable neural networks. However, this paper shows that this theory is more general than the standard filter transform technique, and can be used to represent any kernel based on a filter transform. This paper also shows that the proposed approach can be applied to steerable convex and non-convex convolution operators to avoid overfitting."
36,SP:c1116fbb4d058eb6be195b5d13d19a55ba86b602,"Multimodal program synthesis USED-FOR program. user input USED-FOR program. Multimodal program synthesis USED-FOR program synthesis. user input USED-FOR Multimodal program synthesis. noisy signals USED-FOR it. natural language HYPONYM-OF noisy signals. neural model USED-FOR program ’s score. natural language ( NL ) CONJUNCTION input - output examples. input - output examples CONJUNCTION natural language ( NL ). user intent FEATURE-OF multimodal synthesis tasks. input - output examples USED-FOR multimodal synthesis tasks. input - output examples USED-FOR user intent. top - down recurrent neural model PART-OF method. automated program analysis techniques USED-FOR search space. user ’s constraints FEATURE-OF infeasibility of partial programs. automated program analysis techniques USED-FOR it. multimodal synthesis dataset ( STRUCTUREDREGEX ) EVALUATE-FOR method. method COMPARE techniques. techniques COMPARE method. multimodal synthesis dataset ( STRUCTUREDREGEX ) EVALUATE-FOR techniques. accuracy EVALUATE-FOR techniques. accuracy EVALUATE-FOR method. Method is optimal neural synthesis approach. OtherScientificTerm are user - provided constraints, abstract syntax trees, NL input, and syntactically valid programs. Generic is model. ","Multimodal program synthesis is an important problem in program synthesis with user input, where the goal is to generate a program that satisfies a set of user-provided constraints. In this paper, the authors propose an optimal neural synthesis approach, where a program synthesis model is trained to generate programs that satisfy the user’s constraints. The authors propose to use a top-down recurrent neural model to learn the program synthesis score, and then use it to search for the optimal program in a search space with noisy signals (e.g., natural language, input-output examples). The authors show that the optimal neural model can be trained to learn to generate the program with a large number of different types of noisy signals, and that it can be used to synthesize programs with different user intent (i.e., the user intent for different multimodal synthesis tasks based on user intent, natural language (NL) and input-Output examples).  The authors also show that their method can be combined with existing automated program analysis techniques to search the search space, and it outperforms existing techniques in terms of accuracy and accuracy on the STRUCTUREDREGEX program synthesis dataset.    The main contribution of the paper is that the authors introduce abstract syntax trees, which is a way to represent abstract programs as abstract syntactically valid programs, and a neural model that learns to predict the program's score from the abstract syntax tree. The proposed model is also able to generate partial programs that are infeasible in the presence of the user's constraints, and is able to generalize to infeasibility of partial programs in the absence of a certain set of constraints. ","Multimodal program synthesis is an important problem in program synthesis with user input, where the goal is to generate a program that satisfies a set of user-provided constraints. In this paper, the authors propose an optimal neural synthesis approach, where a program synthesis model is trained to generate programs that satisfy the user’s constraints. The authors propose to use a top-down recurrent neural model to learn the program synthesis score, and then use it to search for the optimal program in a search space with noisy signals (e.g., natural language, input-output examples). The authors show that the optimal neural model can be trained to learn to generate the program with a large number of different types of noisy signals, and that it can be used to synthesize programs with different user intent (i.e., the user intent for different multimodal synthesis tasks based on user intent, natural language (NL) and input-Output examples).  The authors also show that their method can be combined with existing automated program analysis techniques to search the search space, and it outperforms existing techniques in terms of accuracy and accuracy on the STRUCTUREDREGEX program synthesis dataset.    The main contribution of the paper is that the authors introduce abstract syntax trees, which is a way to represent abstract programs as abstract syntactically valid programs, and a neural model that learns to predict the program's score from the abstract syntax tree. The proposed model is also able to generate partial programs that are infeasible in the presence of the user's constraints, and is able to generalize to infeasibility of partial programs in the absence of a certain set of constraints. "
45,SP:55e02d79146bbb42f1ab6d4fafa2db5ddbe599b0,"protease enzymes HYPONYM-OF proteins. substrate specificity landscape FEATURE-OF protease enzyme. sequence motifs PART-OF substrate specificity landscape. methods USED-FOR predicting protease specificity landscapes. sequence patterns USED-FOR methods. protein graph convolutional neural network ( PGCN ) USED-FOR substrate specificity. Rosetta energy function USED-FOR topology and energetic features. structure - based molecular interaction graph USED-FOR substrate specificity. Rosetta energy function USED-FOR structure - based molecular interaction graph. structure - based molecular interaction graph USED-FOR protein graph convolutional neural network ( PGCN ). PGCN USED-FOR specificity. specificity FEATURE-OF NS3/4 protease. Hepatitic C virus FEATURE-OF NS3/4 protease. PGCN COMPARE machine learning models. machine learning models COMPARE PGCN. classification tasks EVALUATE-FOR PGCN. feature importance USED-FOR sub - graph patterns. sub - graph patterns USED-FOR molecular recognition. physical interactions USED-FOR PGCN. PGCN model USED-FOR enzymes. Task is robustness of key life processes. OtherScientificTerm are protease specificity landscapes, mutational changes, and molecular interactions. ","This paper proposes a protein graph convolutional neural network (PGCN) based on the structure-based molecular interaction graph (SIREN) to predict the substrate specificity landscape of a specific type of proteins (protease enzymes). Previous methods for predicting protease specificity landscapes have been based on sequence patterns, but the authors show that the sequence motifs of a given protein are not necessarily the same across all proteins. The authors propose to use the structure of a protein to model the topology and energetic features of the protein, and then use the Rosetta energy function to map the protein's topology to the topological and energy features of a particular protein. The paper shows that the proposed PGCN (Protein Graph Convolutional Neural Network) is able to predict a protein’s specificity landscape, and that the specificity of NS3/4 protease on the Hepatitic C virus is similar to that of a typical NS3-type enzyme.   The authors also demonstrate that the sub-graph patterns learned by the proposed method are highly correlated with feature importance, which is a measure of the robustness of key life processes to mutational changes.  The paper also shows that, in classification tasks, the proposed model outperforms state-of-the-art machine learning models. The main contribution of the paper is that the authors propose a PGCNN model that can be applied to a wide range of proteins, including NS3 and NS4 proteases. The proposed model is based on physical interactions between two proteins, and is trained to predict whether a protein is active or not active, and the subgraphs of two proteins are active or inactive. The subgraph patterns are then used for molecular recognition based on feature importance for sub-based sub- graph patterns.","This paper proposes a protein graph convolutional neural network (PGCN) based on the structure-based molecular interaction graph (SIREN) to predict the substrate specificity landscape of a specific type of proteins (protease enzymes). Previous methods for predicting protease specificity landscapes have been based on sequence patterns, but the authors show that the sequence motifs of a given protein are not necessarily the same across all proteins. The authors propose to use the structure of a protein to model the topology and energetic features of the protein, and then use the Rosetta energy function to map the protein's topology to the topological and energy features of a particular protein. The paper shows that the proposed PGCN (Protein Graph Convolutional Neural Network) is able to predict a protein’s specificity landscape, and that the specificity of NS3/4 protease on the Hepatitic C virus is similar to that of a typical NS3-type enzyme.   The authors also demonstrate that the sub-graph patterns learned by the proposed method are highly correlated with feature importance, which is a measure of the robustness of key life processes to mutational changes.  The paper also shows that, in classification tasks, the proposed model outperforms state-of-the-art machine learning models. The main contribution of the paper is that the authors propose a PGCNN model that can be applied to a wide range of proteins, including NS3 and NS4 proteases. The proposed model is based on physical interactions between two proteins, and is trained to predict whether a protein is active or not active, and the subgraphs of two proteins are active or inactive. The subgraph patterns are then used for molecular recognition based on feature importance for sub-based sub- graph patterns."
54,SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"method USED-FOR overestimation bias. Double Q - learning USED-FOR overestimation bias. Double Q - learning HYPONYM-OF method. approximate Bellman operator USED-FOR non - optimal fixed points. underestimation bias PART-OF double Q - learning. approximate dynamic programming USED-FOR approach. method COMPARE baseline algorithms. baseline algorithms COMPARE method. Atari benchmark tasks EVALUATE-FOR baseline algorithms. Atari benchmark tasks EVALUATE-FOR method. Method are Bellman operation, and deep Q - learning paradigm. Task are value prediction, and learning. OtherScientificTerm is non - optimal stationary solutions. ","This paper proposes Double Q-learning, a method to mitigate the overestimation bias in Double Q - learning, which is a popular method to tackle the problem of value prediction. The authors propose to use the Bellman operation as a regularizer to avoid non-optimal stationary solutions, which has been a common problem in the recent years in the deep Q-Learning paradigm. They show that the approximate Bellman operator can be used to avoid the existence of non-optimistic fixed points, and that the underestimation bias is alleviated in the case of double Q- learning. The approach is based on approximate dynamic programming, and the authors show that their method outperforms several baseline algorithms on several Atari benchmark tasks. ","This paper proposes Double Q-learning, a method to mitigate the overestimation bias in Double Q - learning, which is a popular method to tackle the problem of value prediction. The authors propose to use the Bellman operation as a regularizer to avoid non-optimal stationary solutions, which has been a common problem in the recent years in the deep Q-Learning paradigm. They show that the approximate Bellman operator can be used to avoid the existence of non-optimistic fixed points, and that the underestimation bias is alleviated in the case of double Q- learning. The approach is based on approximate dynamic programming, and the authors show that their method outperforms several baseline algorithms on several Atari benchmark tasks. "
63,SP:1d630b69f95392a5ef3d7d580b523e077a3555a8,"models USED-FOR high - resolution image generation. BigGAN CONJUNCTION VQVAE-2. VQVAE-2 CONJUNCTION BigGAN. compute resources USED-FOR models. VQVAE-2 HYPONYM-OF models. BigGAN HYPONYM-OF models. ESRGAN HYPONYM-OF GAN - based image super - resolution models. two - step training framework USED-FOR deep generative models ( DGMs ). high - dimensional natural images USED-FOR deep generative models ( DGMs ). wavelet domain USED-FOR sampler. wavelet super - resolution decoder network USED-FOR images. Wavelet - based down - sampling method COMPARE pixel - based methods. pixel - based methods COMPARE Wavelet - based down - sampling method. generative quality EVALUATE-FOR low - resolution sampler. Wavelet - based down - sampling method USED-FOR structural information. generative quality EVALUATE-FOR Wavelet - based down - sampling method. generative quality EVALUATE-FOR pixel - based methods. sampler CONJUNCTION decoder. decoder CONJUNCTION sampler. ImageNet EVALUATE-FOR model. model COMPARE BigGAN model. BigGAN model COMPARE model. Fréchet Inception Distance ( FID ) EVALUATE-FOR BigGAN model. Fréchet Inception Distance ( FID ) EVALUATE-FOR model. OtherScientificTerm are low - frequency bands, pixel - space, and dimensional spaces. Method is end - to - end models. Metric is training cost. ","This paper proposes a two-step training framework for deep generative models (DGMs) trained on high-dimensional natural images. The authors propose two models for high-resolution image generation, BigGAN and VQVAE-2, which are GAN-based image super-resolution models (e.g., ESRGAN). The models are trained with different compute resources, and the authors propose to use end-to-end models.    The authors first train a sampler in the wavelet domain, and then a wavelet super-resolution decoder network is used to generate images in the low-frequency bands.  Wavelet-based down-sampling method is shown to improve the generative quality of a low-resolution sampler and to preserve structural information in the pixel-space. The paper also shows that the proposed Wavelet - based down-sampling method preserves the structural information of the images, and that the training cost is much lower than that of pixel-based methods.  The proposed model is evaluated on ImageNet, where the authors show that their model achieves better Fréchet Inception Distance (FID) compared to the original BigGAN model. They also show that the sampler, decoder, and decoder can be trained in parallel. ","This paper proposes a two-step training framework for deep generative models (DGMs) trained on high-dimensional natural images. The authors propose two models for high-resolution image generation, BigGAN and VQVAE-2, which are GAN-based image super-resolution models (e.g., ESRGAN). The models are trained with different compute resources, and the authors propose to use end-to-end models.    The authors first train a sampler in the wavelet domain, and then a wavelet super-resolution decoder network is used to generate images in the low-frequency bands.  Wavelet-based down-sampling method is shown to improve the generative quality of a low-resolution sampler and to preserve structural information in the pixel-space. The paper also shows that the proposed Wavelet - based down-sampling method preserves the structural information of the images, and that the training cost is much lower than that of pixel-based methods.  The proposed model is evaluated on ImageNet, where the authors show that their model achieves better Fréchet Inception Distance (FID) compared to the original BigGAN model. They also show that the sampler, decoder, and decoder can be trained in parallel. "
72,SP:b943a73b1ec34867371325748dc3a91ff4011947,"self - supervised learning ( SSL ) algorithms USED-FOR Fewshot learning(FSL ). pre - trained embedding network USED-FOR downstream FSL tasks. self - supervised training USED-FOR pre - trained embedding network. SSL USED-FOR FSL. self - supervised training USED-FOR FSL. supervised training USED-FOR FSL. self - supervised loss CONJUNCTION supervised loss. supervised loss CONJUNCTION self - supervised loss. supervised training CONJUNCTION self - supervised training. self - supervised training CONJUNCTION supervised training. test accuracy EVALUATE-FOR self - supervised FSL. Material are large - scale labeled data, and labeled data. Method are embedding network, and supervised FSL methods. ","This paper studies self-supervised learning (SSL) algorithms for Fewshot learning(FSL) with large-scale labeled data. In particular, the authors propose to use SSL to improve the performance of FSL by using a pre-trained embedding network for downstream FSL tasks. The authors show that self -supervised training is able to learn a good pre-trainable pre-training set for FSL, and that SSL can be used to improve FSL performance when supervised training is not available.  The authors also show that, when the number of labeled data is large enough, the embedding networks can be trained in a supervised manner.   The main contribution of this paper is that the authors provide a theoretical analysis of the trade-off between self-trained loss and supervised loss in FSL. They show that the tradeoff between supervised training and self-substituted training improves the test accuracy, and the authors also provide empirical evidence that the self supervised FSL methods outperform the supervised ones. ","This paper studies self-supervised learning (SSL) algorithms for Fewshot learning(FSL) with large-scale labeled data. In particular, the authors propose to use SSL to improve the performance of FSL by using a pre-trained embedding network for downstream FSL tasks. The authors show that self -supervised training is able to learn a good pre-trainable pre-training set for FSL, and that SSL can be used to improve FSL performance when supervised training is not available.  The authors also show that, when the number of labeled data is large enough, the embedding networks can be trained in a supervised manner.   The main contribution of this paper is that the authors provide a theoretical analysis of the trade-off between self-trained loss and supervised loss in FSL. They show that the tradeoff between supervised training and self-substituted training improves the test accuracy, and the authors also provide empirical evidence that the self supervised FSL methods outperform the supervised ones. "
81,SP:bd552f98e6a447cefa6b1a9bbdf40bc6539fb643,"first order methods USED-FOR ultra - wide neural networks. finite width FEATURE-OF neural networks. OtherScientificTerm are global minima, initialization, teacher neurons, local minima, student neurons, and Angular Distance ( AD ) function. Method is two - layer teacher - student networks. Generic is methodology. ","This paper studies the problem of finding global minima of two-layer teacher-student networks. The authors propose to use first order methods to find ultra-wide neural networks with finite width. The key idea is to use the initialization of the teacher neurons to find the local minima, and then train the student neurons so that the student networks are as wide as possible. This is achieved by using the Angular Distance (AD) function. The methodology is shown to be computationally efficient. ","This paper studies the problem of finding global minima of two-layer teacher-student networks. The authors propose to use first order methods to find ultra-wide neural networks with finite width. The key idea is to use the initialization of the teacher neurons to find the local minima, and then train the student neurons so that the student networks are as wide as possible. This is achieved by using the Angular Distance (AD) function. The methodology is shown to be computationally efficient. "
90,SP:0f62846913ec10b44ed32845770da0565479dc75,"framework USED-FOR deep neural networks. user - provided formal knowledge USED-FOR learning from data. Deep Adaptive Semantic Logic ( DASL ) USED-FOR deep neural networks. Deep Adaptive Semantic Logic ( DASL ) HYPONYM-OF framework. knowledge representation USED-FOR first order logic. finite sampling USED-FOR truth values. infinite domains FEATURE-OF finite sampling. prior neuro - symbolic work USED-FOR DASL ’s representation. structure PART-OF image classification task. DASL USED-FOR visual relationship detection task. OtherScientificTerm are formal semantics, vanishing gradients, deeper logical structure, data requirements, commonsense knowledge, and data scarcity. ","This paper proposes a framework called Deep Adaptive Semantic Logic (DASL) to train deep neural networks with user-provided formal knowledge for learning from data. The key idea is to learn a knowledge representation for first order logic, where the formal semantics is learned in an adaptive way. The paper shows that vanishing gradients can be achieved by learning a deeper logical structure, and that finite sampling in infinite domains can be used to learn truth values. DASL’s representation is based on prior neuro-symbolic work, and is shown to be robust to data requirements, data scarcity, and lack of commonsense knowledge. Experiments show that the structure in an image classification task can be incorporated into a visual relationship detection task, and the results are shown to improve performance. ","This paper proposes a framework called Deep Adaptive Semantic Logic (DASL) to train deep neural networks with user-provided formal knowledge for learning from data. The key idea is to learn a knowledge representation for first order logic, where the formal semantics is learned in an adaptive way. The paper shows that vanishing gradients can be achieved by learning a deeper logical structure, and that finite sampling in infinite domains can be used to learn truth values. DASL’s representation is based on prior neuro-symbolic work, and is shown to be robust to data requirements, data scarcity, and lack of commonsense knowledge. Experiments show that the structure in an image classification task can be incorporated into a visual relationship detection task, and the results are shown to improve performance. "
99,SP:2f19259d65fab904c1b771244da3dcb2f8aa0c26,"feedforward residual neural networks ( ResNets ) USED-FOR iterative recurrent computations. they USED-FOR neural networks. regularization approach USED-FOR learning of iterative solutions. ResNets USED-FOR iterative solutions. iteration CONJUNCTION convergence. convergence CONJUNCTION iteration. ResNets USED-FOR iterative solutions. regularizations USED-FOR iterative convergent computation. this USED-FOR inductive bias. regularizations USED-FOR inductive bias. ResNet CONJUNCTION recurrent ” ResNet. recurrent ” ResNet CONJUNCTION ResNet. method USED-FOR recurrence regularization. recurrent network USED-FOR one. one HYPONYM-OF recurrent ” ResNet. Lipschitz constraint FEATURE-OF residual functions. spectral normalization USED-FOR Lipschitz constraint. gradient coupling CONJUNCTION Lipschitz constraint. Lipschitz constraint CONJUNCTION gradient coupling. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. recurrence regularization CONJUNCTION spectral normalization. spectral normalization CONJUNCTION recurrence regularization. visual recognition tasks EVALUATE-FOR classification accuracy. Digitclutter HYPONYM-OF recognition tasks. MNIST HYPONYM-OF visual recognition tasks. classification accuracy EVALUATE-FOR spectral normalization. classification accuracy EVALUATE-FOR recurrence regularization. CIFAR-10 HYPONYM-OF visual recognition tasks. Iterative convergent computation USED-FOR tasks. inductive bias FEATURE-OF ResNets. Task are Iterative computations, and computer vision tasks. Method are Iterative methods, and soft gradient coupling. Metric is iterative convergence. Generic are them, and networks. ","This paper proposes a new regularization approach for the learning of iterative solutions in feedforward residual neural networks (ResNets) for iterative recurrent computations. Iterative computations are an important problem in many computer vision tasks, and they have been used to train neural networks.    The authors argue that ResNets have the potential to learn iterative problems that are computationally expensive to compute, and that this inductive bias can be alleviated by introducing regularizations to the iterative convergent computation. The authors propose a method for recurrence regularization that combines the benefits of gradient coupling, Lipschitz constraint, and spectral normalization. They also propose a “recurrent” ResNet, which is a combination of a ResNet and a recurrent “resnet”, where the one is a recurrent network, and the other one is the one that is a residual network. They show that their method is more computationally efficient than existing methods, and shows that iterative convergence converges to a solution that is asymptotically similar to the original solution. They further show that this is a generalization of soft gradient coupling.  They also show that the Lipsichitz constraint on the residual functions is satisfied by spectral normalisation.  Experiments are conducted on several visual recognition tasks (MNIST, CIFAR-10, CifAR-100, and Digitclutter) where they show that recurrence normalization improves classification accuracy and improves the classification accuracy in terms of classification accuracy on a variety of tasks. In addition, the authors show that they are able to improve the performance of their method on a number of tasks that rely heavily on iterative converge (e.g., MNIST, MNIST).   Finally, they also show how their method can be applied to a range of existing methods and show that it can be used to improve their method. ","This paper proposes a new regularization approach for the learning of iterative solutions in feedforward residual neural networks (ResNets) for iterative recurrent computations. Iterative computations are an important problem in many computer vision tasks, and they have been used to train neural networks.    The authors argue that ResNets have the potential to learn iterative problems that are computationally expensive to compute, and that this inductive bias can be alleviated by introducing regularizations to the iterative convergent computation. The authors propose a method for recurrence regularization that combines the benefits of gradient coupling, Lipschitz constraint, and spectral normalization. They also propose a “recurrent” ResNet, which is a combination of a ResNet and a recurrent “resnet”, where the one is a recurrent network, and the other one is the one that is a residual network. They show that their method is more computationally efficient than existing methods, and shows that iterative convergence converges to a solution that is asymptotically similar to the original solution. They further show that this is a generalization of soft gradient coupling.  They also show that the Lipsichitz constraint on the residual functions is satisfied by spectral normalisation.  Experiments are conducted on several visual recognition tasks (MNIST, CIFAR-10, CifAR-100, and Digitclutter) where they show that recurrence normalization improves classification accuracy and improves the classification accuracy in terms of classification accuracy on a variety of tasks. In addition, the authors show that they are able to improve the performance of their method on a number of tasks that rely heavily on iterative converge (e.g., MNIST, MNIST).   Finally, they also show how their method can be applied to a range of existing methods and show that it can be used to improve their method. "
108,SP:6c14506b8b2b06043409d912e6bf877651aaa665,"Normalization techniques USED-FOR deep neural networks. they USED-FOR independent and identically distributed ( IID ) data. normalization methods USED-FOR OOD generalization. SelfNorm CONJUNCTION CrossNorm. CrossNorm CONJUNCTION SelfNorm. CrossNorm USED-FOR OOD generalization. SelfNorm USED-FOR OOD generalization. SelfNorm HYPONYM-OF normalization methods. CrossNorm HYPONYM-OF normalization methods. SelfNorm COMPARE CrossNorm. CrossNorm COMPARE SelfNorm. attention USED-FOR SelfNorm. SelfNorm USED-FOR OOD generalization. CrossNorm USED-FOR OOD generalization. SelfNorm CONJUNCTION CrossNorm. CrossNorm CONJUNCTION SelfNorm. OtherScientificTerm are channel - wise mean and variance, feature maps, and statistics usage. Task is classification and segmentation. ","Normalization techniques for deep neural networks are well-known to be effective for improving OOD generalization. However, they are typically applied to both independent and identically distributed (IID) data. This paper shows that standard normalization methods such as SelfNorm and CrossNorm can be used to improve the OOD performance of standard deep learning models. The main contribution of this paper is to show that SelfNorm, which is based on attention, is more robust to channel-wise mean and variance than CrossNorm. The authors also show that the performance of SelfNorm is comparable to that of CrossNorm in terms of out-of-distribution (OOD) generalization, and that CrossNorm is also more robust than SelfNorm when the feature maps are IID. The paper also shows that the self-attention used by SelfNorm for OOD classification and segmentation is more sensitive to the statistics usage. ","Normalization techniques for deep neural networks are well-known to be effective for improving OOD generalization. However, they are typically applied to both independent and identically distributed (IID) data. This paper shows that standard normalization methods such as SelfNorm and CrossNorm can be used to improve the OOD performance of standard deep learning models. The main contribution of this paper is to show that SelfNorm, which is based on attention, is more robust to channel-wise mean and variance than CrossNorm. The authors also show that the performance of SelfNorm is comparable to that of CrossNorm in terms of out-of-distribution (OOD) generalization, and that CrossNorm is also more robust than SelfNorm when the feature maps are IID. The paper also shows that the self-attention used by SelfNorm for OOD classification and segmentation is more sensitive to the statistics usage. "
117,SP:2774abdc11917321dd4994af0f0da1ff824bea03,language CONJUNCTION speech. speech CONJUNCTION language. vision CONJUNCTION language. language CONJUNCTION vision. unsupervised pre - training CONJUNCTION generative modeling. generative modeling CONJUNCTION unsupervised pre - training. supervised learning CONJUNCTION unsupervised pre - training. unsupervised pre - training CONJUNCTION supervised learning. generative modeling USED-FOR multiple domains. Attention mechanisms HYPONYM-OF inductive biases. unsupervised pre - training USED-FOR multiple domains. vision HYPONYM-OF multiple domains. speech HYPONYM-OF multiple domains. language HYPONYM-OF multiple domains. neural network architectures USED-FOR reinforcement learning ( RL ). they USED-FOR neural network architectures. high dimensional inputs USED-FOR neural network architectures. pixels HYPONYM-OF high dimensional inputs. attention module PART-OF convolutional encoder. attention module PART-OF RL agent. convolutional encoder PART-OF RL agent. data augmentations CONJUNCTION contrastive losses. contrastive losses CONJUNCTION data augmentations. module USED-FOR interpretable task - relevant information. DeepMind Control Suite environments EVALUATE-FOR module. sampleefficiency EVALUATE-FOR agents. module USED-FOR agents. sampleefficiency EVALUATE-FOR module. attention mechanisms USED-FOR reinforcement learning and control. Generic is approach. ,"This paper proposes a new approach to learn a neural network architecture for reinforcement learning (RL) using neural network architectures with high dimensional inputs (e.g. pixels). The authors propose to use inductive biases (i.e. Attention mechanisms) in reinforcement learning and control, where the goal is to learn multiple domains: vision, language, speech, and generative modeling, and unsupervised pre-training in multiple domains.    The key idea is that the attention module in a convolutional encoder in an RL agent can be seen as an inductive bias, and that they can be used to learn neural networks that are robust to different types of high-dimensional inputs. The authors show that the proposed approach can be combined with data augmentations and contrastive losses to improve the sample efficiency of the RL agent. The proposed module is evaluated on the DeepMind Control Suite environments and shows that the module is able to extract interpretable task-relevant information from the data. ","This paper proposes a new approach to learn a neural network architecture for reinforcement learning (RL) using neural network architectures with high dimensional inputs (e.g. pixels). The authors propose to use inductive biases (i.e. Attention mechanisms) in reinforcement learning and control, where the goal is to learn multiple domains: vision, language, speech, and generative modeling, and unsupervised pre-training in multiple domains.    The key idea is that the attention module in a convolutional encoder in an RL agent can be seen as an inductive bias, and that they can be used to learn neural networks that are robust to different types of high-dimensional inputs. The authors show that the proposed approach can be combined with data augmentations and contrastive losses to improve the sample efficiency of the RL agent. The proposed module is evaluated on the DeepMind Control Suite environments and shows that the module is able to extract interpretable task-relevant information from the data. "
126,SP:31a7051d08d19c01e11f1fac2f3041ed2fa28f15,"gradient - based approach USED-FOR multitask networks. GradNorm HYPONYM-OF gradient - based approach. extension USED-FOR GradNorm. game theory USED-FOR Rotograd. Rotograd COMPARE approaches. approaches COMPARE Rotograd. approaches USED-FOR multitask learning. Rotograd USED-FOR multitask learning. real - world datasets EVALUATE-FOR Rotograd. real - world datasets CONJUNCTION network architectures. network architectures CONJUNCTION real - world datasets. network architectures EVALUATE-FOR Rotograd. Task is learning. OtherScientificTerm are network parameters, gradient magnitude, gradient magnitudes, and task gradients. Generic is it. Metric is convergence. ","This paper proposes a gradient-based approach, called GradNorm, to train multitask networks. This extension to GradNorm is based on the observation that the learning is more efficient when the gradient magnitude of the task gradients is smaller than the gradient magnitudes of the other tasks. The authors propose Rotograd, an extension of the game theory to the problem of learning, and show that it converges to the optimal solution when the task magnitudes are small enough. They also show that the convergence is faster than the convergence of gradient-free gradient descent. The paper also shows that the proposed RotogRad outperforms existing approaches for multitask learning on several real-world datasets and network architectures. ","This paper proposes a gradient-based approach, called GradNorm, to train multitask networks. This extension to GradNorm is based on the observation that the learning is more efficient when the gradient magnitude of the task gradients is smaller than the gradient magnitudes of the other tasks. The authors propose Rotograd, an extension of the game theory to the problem of learning, and show that it converges to the optimal solution when the task magnitudes are small enough. They also show that the convergence is faster than the convergence of gradient-free gradient descent. The paper also shows that the proposed RotogRad outperforms existing approaches for multitask learning on several real-world datasets and network architectures. "
135,SP:ac9ebd027b92527d9a87b13ad11d002d99a2b0f6,"geometry distortion problem FEATURE-OF methods. randomness of color transformation FEATURE-OF translation process. unwanted distortions FEATURE-OF translation. Minimal Geometry - Distortion Constraint ( MGC ) HYPONYM-OF I2I translation constraint. approximate representation of mutual information USED-FOR estimation and maximization of MGC. MGC COMPARE state - of - the - art methods. state - of - the - art methods COMPARE MGC. benchmark datasets EVALUATE-FOR state - of - the - art methods. benchmark datasets EVALUATE-FOR MGC. OtherScientificTerm are domain mapping function, mapping function, geometry structure, and consistency of geometry structures. Material are paired data, and translated images. Generic is function. ","This paper studies the geometry distortion problem of existing methods. The authors propose a new domain mapping function, called Minimal Geometry-Distortion Constraint (MGC), which is an I2I translation constraint. They show that existing methods suffer from geometry distortion issue in the case of paired data, which is caused by the randomness of color transformation in the translation process. They also show that the mapping function is invariant to the geometry structure of the source and target domains, and that the resulting translation suffers from unwanted distortions. To address this problem, the authors propose the Minimal geometry-distortion constraint, i.e., a function that minimizes the mutual information between a pair of translated images and the target images. The estimation and maximization of MGC is based on the approximate representation of mutual information. They demonstrate that MGC outperforms state-of-the-art methods on several benchmark datasets. ","This paper studies the geometry distortion problem of existing methods. The authors propose a new domain mapping function, called Minimal Geometry-Distortion Constraint (MGC), which is an I2I translation constraint. They show that existing methods suffer from geometry distortion issue in the case of paired data, which is caused by the randomness of color transformation in the translation process. They also show that the mapping function is invariant to the geometry structure of the source and target domains, and that the resulting translation suffers from unwanted distortions. To address this problem, the authors propose the Minimal geometry-distortion constraint, i.e., a function that minimizes the mutual information between a pair of translated images and the target images. The estimation and maximization of MGC is based on the approximate representation of mutual information. They demonstrate that MGC outperforms state-of-the-art methods on several benchmark datasets. "
144,SP:92a38d7d18f07f68b8f93c61180e2cc1dddd21de,point sampling patterns USED-FOR point cloud GANs. DGCNN CONJUNCTION PointConv. PointConv CONJUNCTION DGCNN. PointConv CONJUNCTION KPConv. KPConv CONJUNCTION PointConv. sampling - oversensitive discriminators USED-FOR valid shape generation. PointNet++ CONJUNCTION DGCNN. DGCNN CONJUNCTION PointNet++. sampling - insensitive discriminators USED-FOR shape point clouds. point clustering artifacts FEATURE-OF shape point clouds. KPConv HYPONYM-OF sampling - oversensitive discriminators. PointNet - Max HYPONYM-OF sampling - insensitive discriminators. PointNet++ HYPONYM-OF sampling - oversensitive discriminators. PointConv HYPONYM-OF sampling - oversensitive discriminators. DGCNN HYPONYM-OF sampling - oversensitive discriminators. evaluation metrics EVALUATE-FOR sampling pattern. perceptual metrics PART-OF sampling spectrum of metrics. sampling pattern COMPARE geometry. geometry COMPARE sampling pattern. sampling spectrum USED-FOR middle - point sampling - aware baseline discriminator. PointNet - Mix HYPONYM-OF point cloud generators. sampling - related metrics EVALUATE-FOR point cloud generators. PointNet - Mix HYPONYM-OF middle - point sampling - aware baseline discriminator. Task is generator design. Method is discriminator design. Generic is discriminators. ,"This paper studies the problem of point sampling patterns in point cloud GANs. It shows that sampling-oversensitive discriminators (e.g., PointNet++, DGCNN, PointConv, PointNet-Max, and KPConv) are the most common sampling-sensitive discriminators for valid shape generation. The paper also shows that the sampling-insensitive discriminator design is sensitive to point clustering artifacts in shape point clouds, which is a common problem in generator design. The authors also show that the discriminators are sensitive to the sampling pattern of the point cloud generated by the generator, and propose a middle-point sampling-aware baseline discriminator (PointNet-Mix), which uses a sampling spectrum of metrics that includes perceptual metrics. They show that this sampling pattern is more robust to sampling-related metrics compared to geometry. ","This paper studies the problem of point sampling patterns in point cloud GANs. It shows that sampling-oversensitive discriminators (e.g., PointNet++, DGCNN, PointConv, PointNet-Max, and KPConv) are the most common sampling-sensitive discriminators for valid shape generation. The paper also shows that the sampling-insensitive discriminator design is sensitive to point clustering artifacts in shape point clouds, which is a common problem in generator design. The authors also show that the discriminators are sensitive to the sampling pattern of the point cloud generated by the generator, and propose a middle-point sampling-aware baseline discriminator (PointNet-Mix), which uses a sampling spectrum of metrics that includes perceptual metrics. They show that this sampling pattern is more robust to sampling-related metrics compared to geometry. "
153,SP:16c4be3eb162bc81cb3343c2fc115eb8e926a5b5,"images USED-FOR Convolutional Neural Networks ( CNNs ). small quasi - imperceptible artificial perturbations USED-FOR Convolutional Neural Networks ( CNNs ). Capsule Networks ( CapsNets ) COMPARE CNNs. CNNs COMPARE Capsule Networks ( CapsNets ). CNNs COMPARE Capsule Networks ( CapsNets ). Capsule Networks ( CapsNets ) COMPARE CNNs. Capsule Networks ( CapsNets ) USED-FOR white - box attacks. attack protocols USED-FOR CNNs. CapsNets USED-FOR adversarial examples. adversarial robustness FEATURE-OF CapsNets. multi - step attack methods USED-FOR CapsNets. multi - step attack methods USED-FOR CNNs. routing process USED-FOR vote attack. vote attack PART-OF detection - aware attack paradigm. vote attack USED-FOR CapsNets. OtherScientificTerm are votes, computationally expensive routing mechanism, and votes of CapsNets. Metric is computational cost. Method is class - conditional reconstruction based detection method. ","This paper studies the problem of white-box attacks on Convolutional Neural Networks (CNNs) trained on images with small quasi-imperceptible artificial perturbations. The authors show that Capsule Networks (CapsNets) are more robust than CNNs to small quasi imperceptible adversarial attacks. They also show that the robustness of Capsule networks can be improved by learning a computationally expensive routing mechanism, which can be used to attack the votes of CapsNets.    The paper also shows that the attack protocols for CNNs can be adapted to attack Capsule NNs.  The authors further show that multi-step attack methods can be applied to attack CNNs, and that the performance of Capsules is comparable to that of CNNs.  Finally, the authors propose a class-conditional reconstruction based detection method to detect the presence of vote attacks on Capsules. They show that a simple modification to the routing process of the vote attack in the detection-aware attack paradigm is sufficient to detect a vote attack on a Capsule Network.  They also demonstrate that Capsules can be trained to generate adversarial examples that are more likely to be detected by the adversarial robustness. ","This paper studies the problem of white-box attacks on Convolutional Neural Networks (CNNs) trained on images with small quasi-imperceptible artificial perturbations. The authors show that Capsule Networks (CapsNets) are more robust than CNNs to small quasi imperceptible adversarial attacks. They also show that the robustness of Capsule networks can be improved by learning a computationally expensive routing mechanism, which can be used to attack the votes of CapsNets.    The paper also shows that the attack protocols for CNNs can be adapted to attack Capsule NNs.  The authors further show that multi-step attack methods can be applied to attack CNNs, and that the performance of Capsules is comparable to that of CNNs.  Finally, the authors propose a class-conditional reconstruction based detection method to detect the presence of vote attacks on Capsules. They show that a simple modification to the routing process of the vote attack in the detection-aware attack paradigm is sufficient to detect a vote attack on a Capsule Network.  They also demonstrate that Capsules can be trained to generate adversarial examples that are more likely to be detected by the adversarial robustness. "
162,SP:dbd093dff7a38ba8882bb8119c34623ddaaf4cc6,"Meta - reinforcement learning USED-FOR policy. recurrent neural networks USED-FOR policies. algorithm USED-FOR learning of recurrent policies. privileged information USED-FOR learning of recurrent policies. task descriptor FEATURE-OF privileged information. privileged information USED-FOR algorithm. parameters sharing CONJUNCTION auxiliary objective. auxiliary objective CONJUNCTION parameters sharing. method USED-FOR informed policy. informed policy USED-FOR task embeddings. policy HYPONYM-OF informed policy. parameters sharing USED-FOR recurrent policy. descriptors USED-FOR task embeddings. auxiliary objective USED-FOR recurrent policy. learning sample complexity EVALUATE-FOR approach. task - inference approaches USED-FOR meta - reinforcement learning. Thompson sampling CONJUNCTION task - inference approaches. task - inference approaches CONJUNCTION Thompson sampling. vanilla RNNs CONJUNCTION Thompson sampling. Thompson sampling CONJUNCTION vanilla RNNs. it COMPARE vanilla RNNs. vanilla RNNs COMPARE it. it COMPARE Thompson sampling. Thompson sampling COMPARE it. it COMPARE task - inference approaches. task - inference approaches COMPARE it. it USED-FOR meta - reinforcement learning. Thompson sampling USED-FOR meta - reinforcement learning. exploration / exploitation strategies USED-FOR algorithm. Generic are information, them, and they. Task is online adaptation setting. OtherScientificTerm is behaviour. Method is RNNs. ","Meta-reinforcement learning is an important problem in reinforcement learning, where the goal is to learn a policy that can adapt to a new task in an online adaptation setting. The authors propose an algorithm that leverages privileged information from a task descriptor to improve the learning of recurrent policies. In particular, the authors propose to use recurrent neural networks to learn policies that are able to adapt to new tasks in a way that preserves the information about the previous tasks. The algorithm is based on the idea that the privileged information in the task descriptor is related to the behaviour of the RNNs. The proposed method learns an informed policy that is able to learn the task embeddings from the new task descriptors. This informed policy is then used to train a recurrent policy using parameters sharing and an auxiliary objective that encourages the learned policy to learn to perform well on new tasks. This approach is shown to reduce the learning sample complexity by a factor of $O(\sqrt{T})$. The authors show that the proposed method outperforms Thompson sampling, task-inference approaches for meta-rewarding, and other task-interference approaches. They also show that it is more efficient than the previous state-of-the-art meta-regression learning algorithms, and that it outperforms the previous work on meta-REINFORCE learning in the online setting with exploration/exploitation strategies. ","Meta-reinforcement learning is an important problem in reinforcement learning, where the goal is to learn a policy that can adapt to a new task in an online adaptation setting. The authors propose an algorithm that leverages privileged information from a task descriptor to improve the learning of recurrent policies. In particular, the authors propose to use recurrent neural networks to learn policies that are able to adapt to new tasks in a way that preserves the information about the previous tasks. The algorithm is based on the idea that the privileged information in the task descriptor is related to the behaviour of the RNNs. The proposed method learns an informed policy that is able to learn the task embeddings from the new task descriptors. This informed policy is then used to train a recurrent policy using parameters sharing and an auxiliary objective that encourages the learned policy to learn to perform well on new tasks. This approach is shown to reduce the learning sample complexity by a factor of $O(\sqrt{T})$. The authors show that the proposed method outperforms Thompson sampling, task-inference approaches for meta-rewarding, and other task-interference approaches. They also show that it is more efficient than the previous state-of-the-art meta-regression learning algorithms, and that it outperforms the previous work on meta-REINFORCE learning in the online setting with exploration/exploitation strategies. "
171,SP:bd89d254fbf31db61db237d08ab42981e27c52df,"trial - and - errors USED-FOR realworld applications. trial - and - errors USED-FOR RL. simulator USED-FOR optimal policies. dataset USED-FOR simulator. offline dataset USED-FOR policy. paradigm USED-FOR RL policy. model learning technique USED-FOR paradigm. offline data USED-FOR paradigm. offline data USED-FOR RL policy. models USED-FOR policy learning. adaptive policy USED-FOR real - world environments. stochasticity FEATURE-OF dynamics. synthetic environments CONJUNCTION real - world ride - hailing platform. real - world ride - hailing platform CONJUNCTION synthetic environments. method USED-FOR robust recommendations. method USED-FOR distortion problem. Generic is approach. OtherScientificTerm are fidelity of the simulator, and online sampling. Method is learning. ","This paper proposes a new approach to tackle the problem of trial-and-errors in RL for realworld applications. The authors propose a new paradigm to learn an RL policy from offline data using a model learning technique, where a simulator is used to learn optimal policies and a dataset is used for training a policy from an offline dataset. The RL policy is learned from this offline data, and the fidelity of the simulator is then used to train a new RL policy on the new dataset. In this way, the RL policy can be trained on offline data without the need for online sampling. The paper also proposes a method to learn robust recommendations that can be used for policy learning using existing models. In addition, the paper proposes an adaptive policy for real-world environments, which is able to adapt to the stochasticity of the dynamics. Experiments are conducted on two synthetic environments and a real-life ride-hailing platform. The method is shown to be able to overcome the distortion problem caused by online sampling, and to be more robust to changes in the dynamics during learning.","This paper proposes a new approach to tackle the problem of trial-and-errors in RL for realworld applications. The authors propose a new paradigm to learn an RL policy from offline data using a model learning technique, where a simulator is used to learn optimal policies and a dataset is used for training a policy from an offline dataset. The RL policy is learned from this offline data, and the fidelity of the simulator is then used to train a new RL policy on the new dataset. In this way, the RL policy can be trained on offline data without the need for online sampling. The paper also proposes a method to learn robust recommendations that can be used for policy learning using existing models. In addition, the paper proposes an adaptive policy for real-world environments, which is able to adapt to the stochasticity of the dynamics. Experiments are conducted on two synthetic environments and a real-life ride-hailing platform. The method is shown to be able to overcome the distortion problem caused by online sampling, and to be more robust to changes in the dynamics during learning."
180,SP:1a166b28cf684e0d5759bd629f6a53370d2bf11c,"sparse rewards USED-FOR goal - reaching behaviors. expert demonstrations CONJUNCTION value function. value function CONJUNCTION expert demonstrations. RL algorithms USED-FOR goal reaching policies. imitation learning USED-FOR goal reaching policies. imitation learning USED-FOR RL algorithms. algorithm USED-FOR goal - reaching behaviors. goal - reaching performance CONJUNCTION robustness. robustness CONJUNCTION goal - reaching performance. robustness EVALUATE-FOR RL algorithms. goal - reaching performance EVALUATE-FOR RL algorithms. iterated supervised learning procedure USED-FOR RL objective. benchmark tasks EVALUATE-FOR RL algorithms. Method are reinforcement learning ( RL ) algorithms, and supervised imitation learning. Generic is it. OtherScientificTerm are demonstrations, policy, and performance bounds. ","This paper studies the problem of learning goal-reaching behaviors with sparse rewards in reinforcement learning (RL) algorithms. The authors show that RL algorithms trained with imitation learning can learn goal reaching policies from expert demonstrations and a learned value function, but it is not guaranteed that the learned policy will reach the goal. To this end, the authors propose supervised imitation learning, where an agent is encouraged to learn a policy that maximizes the expected return from the demonstrations. The algorithm is shown to be able to learn to learn goal -reaching behaviors in the presence of sparse rewards. The paper also shows that the RL objective can be learned through an iterated supervised learning procedure, where the policy is trained to reach goals in a supervised manner, and that the algorithm is able to generalize well to new environments.  The authors also provide performance bounds for RL algorithms on several benchmark tasks, and show that their algorithm can generalize to a variety of environments and learn to reach new goals. They also show that the proposed algorithm can be used to improve the goal-reaching performance and robustness of RL algorithms. ","This paper studies the problem of learning goal-reaching behaviors with sparse rewards in reinforcement learning (RL) algorithms. The authors show that RL algorithms trained with imitation learning can learn goal reaching policies from expert demonstrations and a learned value function, but it is not guaranteed that the learned policy will reach the goal. To this end, the authors propose supervised imitation learning, where an agent is encouraged to learn a policy that maximizes the expected return from the demonstrations. The algorithm is shown to be able to learn to learn goal -reaching behaviors in the presence of sparse rewards. The paper also shows that the RL objective can be learned through an iterated supervised learning procedure, where the policy is trained to reach goals in a supervised manner, and that the algorithm is able to generalize well to new environments.  The authors also provide performance bounds for RL algorithms on several benchmark tasks, and show that their algorithm can generalize to a variety of environments and learn to reach new goals. They also show that the proposed algorithm can be used to improve the goal-reaching performance and robustness of RL algorithms. "
189,SP:c306530164d677e670554eeba8203c66bb3d9f7a,autoregressive models USED-FOR speech. autoregressive teacher model USED-FOR duration prediction. one - to - many mapping problem PART-OF TTS. knowledge distillation USED-FOR one - to - many mapping problem. autoregressive teacher model USED-FOR FastSpeech model. teacher model USED-FOR mel - spectrograms. teacher model USED-FOR duration. information loss FEATURE-OF mel - spectrograms. pitch CONJUNCTION energy. energy CONJUNCTION pitch. energy CONJUNCTION duration. duration CONJUNCTION energy. FastSpeech 2 USED-FOR FastSpeech. FastSpeech 2 USED-FOR one - to - many mapping problem. variation information of speech USED-FOR conditional inputs. one - to - many mapping problem PART-OF TTS. duration HYPONYM-OF variation information of speech. energy HYPONYM-OF variation information of speech. pitch HYPONYM-OF variation information of speech. pitch CONJUNCTION energy. energy CONJUNCTION pitch. duration CONJUNCTION pitch. pitch CONJUNCTION duration. predicted values USED-FOR inference. conditional inputs USED-FOR training. speech waveform USED-FOR pitch. speech waveform USED-FOR energy. FastSpeech 2s USED-FOR speech waveform. end - to - end inference USED-FOR FastSpeech 2s. FastSpeech 2 COMPARE FastSpeech. FastSpeech COMPARE FastSpeech 2. FastSpeech 2 COMPARE FastSpeech. FastSpeech COMPARE FastSpeech 2. FastSpeech 2 COMPARE autoregressive models. autoregressive models COMPARE FastSpeech 2. FastSpeech 2 COMPARE FastSpeech 2s. FastSpeech 2s COMPARE FastSpeech 2. training speed - up EVALUATE-FOR FastSpeech. training speed - up EVALUATE-FOR FastSpeech 2. voice quality EVALUATE-FOR FastSpeech 2. voice quality EVALUATE-FOR FastSpeech. Method is teacher - student distillation pipeline. Task is data simplification. Generic is model. ,"This paper proposes a teacher-student distillation pipeline to improve the performance of autoregressive models for speech. The authors propose FastSpeech 2, which is a variant of the autorgressive teacher model for duration prediction. The key idea is to use knowledge distillation to tackle the one-to-many mapping problem in TTS, where the teacher model is trained on mel-spectrograms with information loss, and the student is trained to learn to predict the duration of the input signal. The teacher model learns to map the variation information of speech, such as pitch, energy, duration, etc., to conditional inputs for training.   The authors show that the proposed model, Fastspeech 2s, is able to learn a speech waveform from the conditional inputs to the predicted values for inference. They also demonstrate that the model can learn to learn the pitch and energy based on the learned conditional inputs, and that the predicted signal can be used for data simplification. Finally, the authors show the training speed-up in terms of the number of samples and training speed - up of the model compared to the original teacher model.  Experiments are conducted on a number of datasets, and show that, in general, the proposed method achieves better performance than the autore progressive models, and outperforms the previous state-of-the-art.  The main contribution of the paper is that the authors propose to use the predicted signals from the teacher and student models as conditional inputs in the training process, and to use end-to -to-end inference to train the speech waveforms from the two models. The paper also shows that the resulting model achieves better voice quality than the original model, and shows that it is more robust than the previous model.","This paper proposes a teacher-student distillation pipeline to improve the performance of autoregressive models for speech. The authors propose FastSpeech 2, which is a variant of the autorgressive teacher model for duration prediction. The key idea is to use knowledge distillation to tackle the one-to-many mapping problem in TTS, where the teacher model is trained on mel-spectrograms with information loss, and the student is trained to learn to predict the duration of the input signal. The teacher model learns to map the variation information of speech, such as pitch, energy, duration, etc., to conditional inputs for training.   The authors show that the proposed model, Fastspeech 2s, is able to learn a speech waveform from the conditional inputs to the predicted values for inference. They also demonstrate that the model can learn to learn the pitch and energy based on the learned conditional inputs, and that the predicted signal can be used for data simplification. Finally, the authors show the training speed-up in terms of the number of samples and training speed - up of the model compared to the original teacher model.  Experiments are conducted on a number of datasets, and show that, in general, the proposed method achieves better performance than the autore progressive models, and outperforms the previous state-of-the-art.  The main contribution of the paper is that the authors propose to use the predicted signals from the teacher and student models as conditional inputs in the training process, and to use end-to -to-end inference to train the speech waveforms from the two models. The paper also shows that the resulting model achieves better voice quality than the original model, and shows that it is more robust than the previous model."
198,SP:79e9fb20d383816f54738ce70d137131ebc10290,"k - dimensional subspace FEATURE-OF tempered distribution q(x ). tempered distributions USED-FOR unsupervised dimension reduction problem ( UDR ). tempered distribution q(x ) USED-FOR empirical probability density function. q CONJUNCTION pemp. pemp CONJUNCTION q. minimization of the distance USED-FOR problem. generalized functions USED-FOR minimization of the distance. sufficient dimension reduction problem ( SDR ) HYPONYM-OF data science. algorithm USED-FOR problem. algorithm USED-FOR second. algorithm USED-FOR problem. optimization problem USED-FOR optimization problem. distributions USED-FOR optimization problem. ordinary functions USED-FOR optimization problem. algorithm USED-FOR minimization of I(f ) + λR(f ). two - step iterative computation USED-FOR algorithm. two - step iterative computation USED-FOR minimization of I(f ) + λR(f ). synthetic data CONJUNCTION datasets. datasets CONJUNCTION synthetic data. examples EVALUATE-FOR method. datasets USED-FOR method. synthetic data USED-FOR method. datasets USED-FOR examples. synthetic data USED-FOR examples. UDR HYPONYM-OF examples. Method is infinite - dimensional formulation. OtherScientificTerm are nonnegative penalty function R(f ), and λR(f ). Material is real data. ","This paper studies the unsupervised dimension reduction problem (UDR) with tempered distributions in the k-dimensional subspace of a tempered distribution q(x) over the empirical probability density function. The authors propose an infinite-dimensional formulation, where the problem is formulated as the minimization of the distance between a nonnegative penalty function R(f, x) and the true distribution Q(f). The authors show that this problem can be solved using generalized functions, which is a well-studied problem in data science, such as the so-called sufficient dimension reduction task (SDR). They also propose an algorithm to solve this problem. The first algorithm solves the problem using a two-step iterative computation, and the second algorithm uses a single-step iteration to solve the problem. They show that the optimization problem of this optimization problem is equivalent to solving an optimization problem over distributions over ordinary functions. They also show that their algorithm converges to an algorithm that minimizes I(f) + λR(f), which is an algorithm based on the maximization of I(F(f)) +  (1) and (2) of a non-asymptotic minimizer of the objective function.  The authors evaluate their method on synthetic data, two synthetic data and two datasets, and two real data sets. They find that the proposed method outperforms existing methods on all three examples of UDR.   ","This paper studies the unsupervised dimension reduction problem (UDR) with tempered distributions in the k-dimensional subspace of a tempered distribution q(x) over the empirical probability density function. The authors propose an infinite-dimensional formulation, where the problem is formulated as the minimization of the distance between a nonnegative penalty function R(f, x) and the true distribution Q(f). The authors show that this problem can be solved using generalized functions, which is a well-studied problem in data science, such as the so-called sufficient dimension reduction task (SDR). They also propose an algorithm to solve this problem. The first algorithm solves the problem using a two-step iterative computation, and the second algorithm uses a single-step iteration to solve the problem. They show that the optimization problem of this optimization problem is equivalent to solving an optimization problem over distributions over ordinary functions. They also show that their algorithm converges to an algorithm that minimizes I(f) + λR(f), which is an algorithm based on the maximization of I(F(f)) +  (1) and (2) of a non-asymptotic minimizer of the objective function.  The authors evaluate their method on synthetic data, two synthetic data and two datasets, and two real data sets. They find that the proposed method outperforms existing methods on all three examples of UDR.   "
207,SP:93e54522e6c2b805905d21fc968fc40866f2898b,methods USED-FOR model. methods USED-FOR robustness. rare or underrepresented patterns FEATURE-OF model. contextual feature utility CONJUNCTION contextual feature sensitivity. contextual feature sensitivity CONJUNCTION contextual feature utility. robustness CONJUNCTION sensitivity. sensitivity CONJUNCTION robustness. Feature Contrastive Learning ( FCL ) USED-FOR model. contextual utility FEATURE-OF features. robustness CONJUNCTION sensitivity. sensitivity CONJUNCTION robustness. noise FEATURE-OF generalization. sensitivity EVALUATE-FOR models. robustness EVALUATE-FOR models. generalization EVALUATE-FOR models. FCL USED-FOR models. Task is real - world applications. ,"This paper proposes two methods to improve the robustness of a model trained on rare or underrepresented patterns. The main idea is to use Feature Contrastive Learning (FCL) to learn a model that is robust to noise and sensitive to changes in the contextual utility of the features. Experiments on real-world applications show that models trained with FCL show improved robustness and sensitivity to noise, and improved generalization to noise.","This paper proposes two methods to improve the robustness of a model trained on rare or underrepresented patterns. The main idea is to use Feature Contrastive Learning (FCL) to learn a model that is robust to noise and sensitive to changes in the contextual utility of the features. Experiments on real-world applications show that models trained with FCL show improved robustness and sensitivity to noise, and improved generalization to noise."
216,SP:f03c50f15022c4f56ac2b3085354ffed38ad1145,"algorithm USED-FOR autonomous agents. latent representation PART-OF discriminator network. latent representation USED-FOR adversarial learning. adversarial learning USED-FOR algorithm. high dimensional observations USED-FOR autonomous agents. adversarial learning USED-FOR autonomous agents. mutual information constraints USED-FOR latent representation. shared feature space USED-FOR imitation. environment appearance CONJUNCTION agent embodiment. agent embodiment CONJUNCTION environment appearance. balancing CONJUNCTION manipulation and locomotive tasks. manipulation and locomotive tasks CONJUNCTION balancing. algorithm USED-FOR control problems. agent embodiment FEATURE-OF domain differences. environment appearance FEATURE-OF domain differences. manipulation and locomotive tasks HYPONYM-OF control problems. balancing HYPONYM-OF control problems. Method is Imitation learning methods. Generic are they, and constraints. OtherScientificTerm are optimal states, and features. ","This paper proposes a new algorithm for learning autonomous agents from high dimensional observations. The algorithm is based on adversarial learning on the latent representation of a discriminator network. Imitation learning methods have been proposed in the literature, but they are usually based on the assumption that the optimal states are shared across all agents. This paper proposes to use mutual information constraints to learn a latent representation that maximizes the mutual information between the features of the agent and the discriminator. The authors show that this imitation learning method learns a shared feature space that is invariant to the constraints. The proposed algorithm is applied to several control problems, including balancing, manipulation and locomotive tasks, and is shown to be able to generalize to domain differences in terms of environment appearance and agent embodiment. ","This paper proposes a new algorithm for learning autonomous agents from high dimensional observations. The algorithm is based on adversarial learning on the latent representation of a discriminator network. Imitation learning methods have been proposed in the literature, but they are usually based on the assumption that the optimal states are shared across all agents. This paper proposes to use mutual information constraints to learn a latent representation that maximizes the mutual information between the features of the agent and the discriminator. The authors show that this imitation learning method learns a shared feature space that is invariant to the constraints. The proposed algorithm is applied to several control problems, including balancing, manipulation and locomotive tasks, and is shown to be able to generalize to domain differences in terms of environment appearance and agent embodiment. "
225,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"test accuracy EVALUATE-FOR unpruned network. pruned network COMPARE unpruned network. unpruned network COMPARE pruned network. computer vision CONJUNCTION natural language processing. natural language processing CONJUNCTION computer vision. deep neural network ( DNN ) USED-FOR applications. LTH PART-OF deep neural network ( DNN ). natural language processing HYPONYM-OF applications. computer vision HYPONYM-OF applications. objective function CONJUNCTION sample complexity. sample complexity CONJUNCTION objective function. guaranteed generalization EVALUATE-FOR model. algorithm USED-FOR pruned neural network. non - pruned weights PART-OF hidden layer. accelerated ) stochastic gradient descent algorithm USED-FOR algorithm. pruned neural network USED-FOR model. pruned neural network COMPARE unpruned one. unpruned one COMPARE pruned neural network. Task are lottery ticket hypothesis ( LTH ), and pruning multi - layer neural networks. Metric are generalization, and generalization of the winning ticket. OtherScientificTerm are zero generalization error, and convex region. Method is neural network model. ","This paper studies the lottery ticket hypothesis (LTH), which claims that the generalization of a neural network trained with pruned weights is guaranteed to have zero generalization error, and that the test accuracy of an unpruned network is asymptotically the same as that of a pruned network. The authors propose to use the LTH in a deep neural network (DNN) for two applications: computer vision and natural language processing. They show that pruning multi-layer neural networks is equivalent to pruning the weights of a single hidden layer, and show that under certain assumptions on the objective function, sample complexity, and the number of weights, the pruned neural network has the same test accuracy as an unplunged network. They also provide an algorithm to prune the weights at each hidden layer to achieve the same guaranteed generalization. The algorithm is based on the (adversarial) stochastic gradient descent algorithm, where the algorithm first prunes the weights in the first hidden layer of the neural network model, and then uses the algorithm to train a pruning neural network that has a similar objective function and sample complexity to the one that was used to train the original neural network. Finally, the authors show that the model trained with the prune neural network is able to generalize as well as the original model with the same objective function.    The authors also provide a theoretical analysis that shows that the pruning of the winning ticket leads to a convex region where the non-pruned weights in a hidden layer have non-zero generalization performance. ","This paper studies the lottery ticket hypothesis (LTH), which claims that the generalization of a neural network trained with pruned weights is guaranteed to have zero generalization error, and that the test accuracy of an unpruned network is asymptotically the same as that of a pruned network. The authors propose to use the LTH in a deep neural network (DNN) for two applications: computer vision and natural language processing. They show that pruning multi-layer neural networks is equivalent to pruning the weights of a single hidden layer, and show that under certain assumptions on the objective function, sample complexity, and the number of weights, the pruned neural network has the same test accuracy as an unplunged network. They also provide an algorithm to prune the weights at each hidden layer to achieve the same guaranteed generalization. The algorithm is based on the (adversarial) stochastic gradient descent algorithm, where the algorithm first prunes the weights in the first hidden layer of the neural network model, and then uses the algorithm to train a pruning neural network that has a similar objective function and sample complexity to the one that was used to train the original neural network. Finally, the authors show that the model trained with the prune neural network is able to generalize as well as the original model with the same objective function.    The authors also provide a theoretical analysis that shows that the pruning of the winning ticket leads to a convex region where the non-pruned weights in a hidden layer have non-zero generalization performance. "
234,SP:eed6cb2f8caed39f8295f4aeb6e044c2ac981c4d,generalization EVALUATE-FOR neural networks. accuracy CONJUNCTION generalization. generalization CONJUNCTION accuracy. data augmentation approaches USED-FOR neural networks. accuracy EVALUATE-FOR neural networks. generalization EVALUATE-FOR data augmentation approaches. accuracy EVALUATE-FOR data augmentation approaches. augmented data COMPARE clean data. clean data COMPARE augmented data. AutoLabel USED-FOR augmented data. clean distribution CONJUNCTION augmented distribution. augmented distribution CONJUNCTION clean distribution. hold - out validation set USED-FOR calibration - performance. calibration - performance USED-FOR AutoLabel. hold - out validation set USED-FOR AutoLabel. label smoothing USED-FOR AutoLabel. mixup CONJUNCTION adversarial training. adversarial training CONJUNCTION mixup. AugMix CONJUNCTION mixup. mixup CONJUNCTION AugMix. AutoLabel USED-FOR data augmentation methods. adversarial training HYPONYM-OF data augmentation methods. AugMix HYPONYM-OF data augmentation methods. mixup HYPONYM-OF data augmentation methods. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. AutoLabel USED-FOR models. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR AutoLabel. calibration EVALUATE-FOR AutoLabel. accuracy EVALUATE-FOR AutoLabel. AutoLabel USED-FOR adversarial training. clean accuracy CONJUNCTION adversarial robustness. adversarial robustness CONJUNCTION clean accuracy. OtherScientificTerm is distributional shift. ,"This paper proposes AutoLabel, a data augmentation method to improve the generalization performance of neural networks. The authors show that AutoLabel improves the accuracy and generalization of a number of existing methods on CIFAR-10/CIFAR100/ImageNet and ImageNet. AutoLabel is based on label smoothing, and the authors propose to use a hold-out validation set to improve calibration-performance of AutoLabel on augmented data, which is more robust to distributional shift between the clean distribution and the augmented distribution.  AutoLabel can be applied to several existing methods, such as AugMix, mixup, and adversarial training, and is shown to improve accuracy and calibration.  The authors also show that the models trained with AutoLabel are robust to adversarial attacks.   ","This paper proposes AutoLabel, a data augmentation method to improve the generalization performance of neural networks. The authors show that AutoLabel improves the accuracy and generalization of a number of existing methods on CIFAR-10/CIFAR100/ImageNet and ImageNet. AutoLabel is based on label smoothing, and the authors propose to use a hold-out validation set to improve calibration-performance of AutoLabel on augmented data, which is more robust to distributional shift between the clean distribution and the augmented distribution.  AutoLabel can be applied to several existing methods, such as AugMix, mixup, and adversarial training, and is shown to improve accuracy and calibration.  The authors also show that the models trained with AutoLabel are robust to adversarial attacks.   "
243,SP:0d5017e1a405bf86e3bac40e6e59886d4bf48450,"heuristic proxy classification tasks CONJUNCTION data augmentations. data augmentations CONJUNCTION heuristic proxy classification tasks. methods USED-FOR heuristic proxy classification tasks. data augmentations USED-FOR methods. causal framework USED-FOR self - supervised representation learning. proxy classifiers USED-FOR pretraining. invariance constraints USED-FOR proxy classifiers. invariance constraints USED-FOR data augmentations. selfsupervised objective, Representation Learning USED-FOR invariant prediction of proxy targets. invariance regularizer USED-FOR generalization guarantees. invariance regularizer USED-FOR invariant prediction of proxy targets. Invariant Causal Mechanisms ( RELIC ) USED-FOR selfsupervised objective, Representation Learning. causality USED-FOR contrastive learning. contrastive learning HYPONYM-OF self - supervised method. RELIC COMPARE methods. methods COMPARE RELIC. robustness CONJUNCTION out - of - distribution generalization. out - of - distribution generalization CONJUNCTION robustness. RELIC COMPARE methods. methods COMPARE RELIC. out - of - distribution generalization FEATURE-OF ImageNet. human - level performance EVALUATE-FOR methods. out - of - distribution generalization EVALUATE-FOR RELIC. Atari EVALUATE-FOR methods. robustness EVALUATE-FOR RELIC. out - of - distribution generalization EVALUATE-FOR methods. robustness EVALUATE-FOR methods. Method is Self - supervised learning. OtherScientificTerm are supervised signals, and augmentations. Material is unlabeled data. ","Self-supervised learning is an important problem in machine learning, especially in settings where there are no supervised signals and there is a large amount of unlabeled data. In this paper, the authors propose two methods to combine heuristic proxy classification tasks and data augmentations to improve the generalization performance of existing methods. The authors propose a causal framework for self-supervision in which the proxy classifiers are trained with invariance constraints on the augmentations used during pretraining. They propose a selfsupervised objective, Representation Learning based on Invariant Causal Mechanisms (RELIC) to learn an invariant prediction of proxy targets using an invariance regularizer to improve generalization guarantees. They also propose a contrastive method, called contrastive learning based on causality, which is an extension of a previous work (Zhang et al., 2017). Experiments show that RELIC outperforms existing methods in terms of robustness, out-of-distribution generalization on ImageNet, and human-level performance on Atari.","Self-supervised learning is an important problem in machine learning, especially in settings where there are no supervised signals and there is a large amount of unlabeled data. In this paper, the authors propose two methods to combine heuristic proxy classification tasks and data augmentations to improve the generalization performance of existing methods. The authors propose a causal framework for self-supervision in which the proxy classifiers are trained with invariance constraints on the augmentations used during pretraining. They propose a selfsupervised objective, Representation Learning based on Invariant Causal Mechanisms (RELIC) to learn an invariant prediction of proxy targets using an invariance regularizer to improve generalization guarantees. They also propose a contrastive method, called contrastive learning based on causality, which is an extension of a previous work (Zhang et al., 2017). Experiments show that RELIC outperforms existing methods in terms of robustness, out-of-distribution generalization on ImageNet, and human-level performance on Atari."
252,SP:8f80a6f79f78c6421857f392c9a5e98061d7eb60,visual representations of the observed scene USED-FOR navigation actions. Visual Transformer Network ( VTNet ) USED-FOR informative visual representation in navigation. spatial locations of objects CONJUNCTION image regions. image regions CONJUNCTION spatial locations of objects. VTNet HYPONYM-OF structure. structure USED-FOR visual representations. pre - training scheme USED-FOR navigation policy learning. pre - training scheme USED-FOR visual representations. navigation signals USED-FOR visual representations. informative representation USED-FOR navigation. attention operations USED-FOR informative representation. descriptors USED-FOR informative representation. VTNet USED-FOR informative representation. object and region features USED-FOR spatial - aware descriptors. spatial - aware descriptors USED-FOR VTNet. object and region features USED-FOR VTNet. location cues FEATURE-OF object and region features. attention operations USED-FOR descriptors. artificial environment AI2 - Thor EVALUATE-FOR VTNet. VTNet COMPARE methods. methods COMPARE VTNet. artificial environment AI2 - Thor EVALUATE-FOR methods. Task is Object goal navigation. OtherScientificTerm is directional navigation signals. Method is visual representation. ,"Object goal navigation is a challenging problem in which the goal is to navigate to a goal using only directional navigation signals. In this paper, the authors propose Visual Transformer Network (VTNet) to learn an informative visual representation in navigation. The visual representations of the observed scene are used to guide the navigation actions. The authors propose a new structure, called VTNet, to learn visual representations based on the structure of spatial locations of objects and image regions. A pre-training scheme is used to pre-train the visual representations for navigation policy learning. The proposed VTNet uses spatial-aware descriptors based on object and region features to learn the informative representation for navigation. These descriptors are learned using attention operations, and the visual representation is trained to capture the location cues in the object/region features. Experiments on the artificial environment AI2-Thor show that VTNet outperforms existing methods in terms of performance. ","Object goal navigation is a challenging problem in which the goal is to navigate to a goal using only directional navigation signals. In this paper, the authors propose Visual Transformer Network (VTNet) to learn an informative visual representation in navigation. The visual representations of the observed scene are used to guide the navigation actions. The authors propose a new structure, called VTNet, to learn visual representations based on the structure of spatial locations of objects and image regions. A pre-training scheme is used to pre-train the visual representations for navigation policy learning. The proposed VTNet uses spatial-aware descriptors based on object and region features to learn the informative representation for navigation. These descriptors are learned using attention operations, and the visual representation is trained to capture the location cues in the object/region features. Experiments on the artificial environment AI2-Thor show that VTNet outperforms existing methods in terms of performance. "
261,SP:3e7cbe3dff592ef371e48dd86be7719fc5343f17,Federated learning USED-FOR neural network models. model parameters USED-FOR federated learning. solution USED-FOR privacy - preserving federated learning. secure aggregation primitive USED-FOR privacy - preserving federated learning. secure aggregation primitive USED-FOR solution. communication - computation efficient secure aggregation COMPARE secure solution. secure solution COMPARE communication - computation efficient secure aggregation. communication - computation efficient secure aggregation USED-FOR communication / computational resources. scheme USED-FOR topology. sparse random graphs COMPARE complete graph. complete graph COMPARE sparse random graphs. topology FEATURE-OF secret - sharing nodes. sparse random graphs USED-FOR topology. Erdős - Rényi graph USED-FOR G. reliability / privacy EVALUATE-FOR scheme. reliability CONJUNCTION data privacy. data privacy CONJUNCTION reliability. data privacy FEATURE-OF federated learning systems. scheme USED-FOR scheme. federated learning systems EVALUATE-FOR scheme. data privacy EVALUATE-FOR scheme. reliability EVALUATE-FOR scheme. OtherScientificTerm is local data. ,This paper studies the problem of federated learning for neural network models. The authors propose a solution to privacy-preserving federated federatedlearning based on the secure aggregation primitive. They show that the communication-computation efficient secure aggregation saves communication/computational resources compared to the secure solution. They also propose a scheme to learn the topology of secret-sharing nodes based on sparse random graphs instead of a complete graph. They use the Erdős-Rényi graph to represent G. They evaluate the reliability/privacy of the proposed scheme on a number of standard federated learners and show that their scheme achieves state-of-the-art reliability and data privacy. ,This paper studies the problem of federated learning for neural network models. The authors propose a solution to privacy-preserving federated federatedlearning based on the secure aggregation primitive. They show that the communication-computation efficient secure aggregation saves communication/computational resources compared to the secure solution. They also propose a scheme to learn the topology of secret-sharing nodes based on sparse random graphs instead of a complete graph. They use the Erdős-Rényi graph to represent G. They evaluate the reliability/privacy of the proposed scheme on a number of standard federated learners and show that their scheme achieves state-of-the-art reliability and data privacy. 
270,SP:00fae41e0eca0a1575cd7b2dcfabf0dc5c9c8b8a,"incentive compatible auction PART-OF Auction Design. theoretical approaches USED-FOR problem. neural network architectures USED-FOR optimal auctions. theoretical auction design USED-FOR time - independent Lagrangian. inner maximization loop USED-FOR optimal misreports. inner maximization loop USED-FOR optimization procedure. stationary utility functions FEATURE-OF two - player game. two - player game USED-FOR Auction Design. Generic is approach. Method are hyper - parameter search, and neural network. OtherScientificTerm is auctions. ","This paper considers the problem of finding an incentive compatible auction in Auction Design, which is an extension of previous theoretical approaches to this problem. The approach is based on the observation that neural network architectures can be used to design optimal auctions, and the authors propose a theoretical auction design that allows for a time-independent Lagrangian. The optimization procedure is then based on an inner maximization loop that maximizes the optimal misreports. Auction Design is then formulated as a two-player game with stationary utility functions, where the hyper-parameter search is performed by a neural network. ","This paper considers the problem of finding an incentive compatible auction in Auction Design, which is an extension of previous theoretical approaches to this problem. The approach is based on the observation that neural network architectures can be used to design optimal auctions, and the authors propose a theoretical auction design that allows for a time-independent Lagrangian. The optimization procedure is then based on an inner maximization loop that maximizes the optimal misreports. Auction Design is then formulated as a two-player game with stationary utility functions, where the hyper-parameter search is performed by a neural network. "
279,SP:a0e8061beb5e9a6c631419861559d22b8d645cb4,pre - trained model USED-FOR downstream task. large - scale dataset USED-FOR deep neural network. supervised and unsupervised pre - training approaches USED-FOR learning representations. discriminative knowledge of labels CONJUNCTION intrinsic structure of data. intrinsic structure of data CONJUNCTION discriminative knowledge of labels. discriminative knowledge USED-FOR fine - tuning. former USED-FOR fine - tuning methods. intrinsic structure of data USED-FOR boosting fine - tuning. general learning approach USED-FOR supervised and unsupervised pre - trained representations. Bi - tuning HYPONYM-OF general learning approach. supervised and unsupervised pre - trained representations USED-FOR downstream tasks. classifier head CONJUNCTION projector head. projector head CONJUNCTION classifier head. projector head USED-FOR intrinsic structure of data. contrastive cross - entropy loss USED-FOR label information. classifier head USED-FOR label information. Bi - tuning USED-FOR vanilla fine - tuning. instancecontrast way FEATURE-OF label information. contrastive cross - entropy loss FEATURE-OF classifier head. categorical contrastive learning loss USED-FOR projector head. Bi - tuning USED-FOR fine - tuning tasks. fine - tuning tasks EVALUATE-FOR supervised and unsupervised pre - trained models. low - data regime FEATURE-OF accuracy. Generic is latter. OtherScientificTerm is pre - trained representations. ,"This paper studies the problem of fine-tuning a pre-trained model for a downstream task on a large-scale dataset. The authors consider both supervised and unsupervised pre-training approaches for learning representations. The former has been widely used in fine tuning methods, while the latter has not. In this paper, the authors propose to use the discriminative knowledge of labels and the intrinsic structure of data to improve the performance of both. They propose a general learning approach, called Bi-tunning, to fine-tune both the classifier head and the projector head, which is based on a categorical contrastive learning loss. They show that the intrinsic structures of a classifier and a projector head can be used to boost the performance on a number of downstream tasks. They also show that a contrastive cross-entropy loss is used to enhance the label information of a particular classifier in an instancecontrast way, and that this improves the accuracy in the low-data regime.   The authors also propose a variant of their approach, which they call Bi-tuning, which can be applied to vanilla fine tuning, and can be seen as an extension of the recent work of [1].  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15]","This paper studies the problem of fine-tuning a pre-trained model for a downstream task on a large-scale dataset. The authors consider both supervised and unsupervised pre-training approaches for learning representations. The former has been widely used in fine tuning methods, while the latter has not. In this paper, the authors propose to use the discriminative knowledge of labels and the intrinsic structure of data to improve the performance of both. They propose a general learning approach, called Bi-tunning, to fine-tune both the classifier head and the projector head, which is based on a categorical contrastive learning loss. They show that the intrinsic structures of a classifier and a projector head can be used to boost the performance on a number of downstream tasks. They also show that a contrastive cross-entropy loss is used to enhance the label information of a particular classifier in an instancecontrast way, and that this improves the accuracy in the low-data regime.   The authors also propose a variant of their approach, which they call Bi-tuning, which can be applied to vanilla fine tuning, and can be seen as an extension of the recent work of [1].  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15]"
288,SP:87e5b552c13d73bd85249062a152c6c140e594a9,"adversarial accuracy CONJUNCTION adversarial training. adversarial training CONJUNCTION adversarial accuracy. robustness EVALUATE-FOR classifiers. adversarial accuracy USED-FOR robustness. adversarial accuracy EVALUATE-FOR classifiers. measure USED-FOR robustness. measure USED-FOR classifiers. robustness EVALUATE-FOR classifiers. accuracy CONJUNCTION accuracy. accuracy CONJUNCTION accuracy. It USED-FOR adversarial robustness. adversarial robustness EVALUATE-FOR classifiers. It USED-FOR classifiers. accuracy FEATURE-OF adversarially perturbed samples. invariance - based adversarial examples USED-FOR model. genuine adversarial accuracy EVALUATE-FOR classifier. test accuracy CONJUNCTION lp norm - based test adversarial robustness. lp norm - based test adversarial robustness CONJUNCTION test accuracy. OtherScientificTerm are generalization, predicted classes, and perceptual classes. Material is clean data. Generic are it, and distance metrics. Method is norm - based distance metric. ","This paper proposes a new measure to measure the robustness of classifiers to adversarial accuracy and adversarial training. It shows that classifiers trained on adversarial perturbations are more robust to the perturbation than trained on clean data. The authors also show that adversarial robustness is correlated with generalization.    The main contribution of the paper is that the proposed measure is a measure that can be used to measure robustness for classifiers. It measures the distance between the predicted classes of a classifier trained on the clean and adversarially perturbed data, and it is shown that the distance metrics are invariance-based, which means that it is invariant to the class of the adversarial examples.  The authors further show that the invariance of adversarial attacks to the perceptual classes is a norm-based distance metric, and that the test accuracy of the classifier is correlated to the accuracy of perceptual classes.  Finally, the authors show that a model trained on invariant-based adversarial adversaries can be trained on a set of invariant examples, and the model can be further trained on more robust examples that are not invariant.  Experiments are conducted on a number of datasets, showing that the robust classifier can achieve a high level of accuracy on the adversary examples, but can also achieve high levels of accuracy for the original classifier on the original adversarial samples.  In addition, they show that test accuracy and test accuracy are correlated with test accuracy, and show that there is a trade-off between test accuracy on test and test adversarial instances.","This paper proposes a new measure to measure the robustness of classifiers to adversarial accuracy and adversarial training. It shows that classifiers trained on adversarial perturbations are more robust to the perturbation than trained on clean data. The authors also show that adversarial robustness is correlated with generalization.    The main contribution of the paper is that the proposed measure is a measure that can be used to measure robustness for classifiers. It measures the distance between the predicted classes of a classifier trained on the clean and adversarially perturbed data, and it is shown that the distance metrics are invariance-based, which means that it is invariant to the class of the adversarial examples.  The authors further show that the invariance of adversarial attacks to the perceptual classes is a norm-based distance metric, and that the test accuracy of the classifier is correlated to the accuracy of perceptual classes.  Finally, the authors show that a model trained on invariant-based adversarial adversaries can be trained on a set of invariant examples, and the model can be further trained on more robust examples that are not invariant.  Experiments are conducted on a number of datasets, showing that the robust classifier can achieve a high level of accuracy on the adversary examples, but can also achieve high levels of accuracy for the original classifier on the original adversarial samples.  In addition, they show that test accuracy and test accuracy are correlated with test accuracy, and show that there is a trade-off between test accuracy on test and test adversarial instances."
297,SP:2fda410b9281c5e253d385bc4382ec168bc161f3,"fairness PART-OF algorithmic designs. graph - structured data USED-FOR disparate impact. fairness concept FEATURE-OF dyadic fairness. edges PART-OF graph. graph connections USED-FOR dyadic fairness. dyadic fairness FEATURE-OF link predictive scores. algorithm USED-FOR fair adjacency matrix. fair adjacency matrix USED-FOR fair link prediction. graph structural constraints USED-FOR fair link prediction. FairAdj USED-FOR fair adjacency matrix. graph structural constraints FEATURE-OF fair adjacency matrix. method USED-FOR dyadic fairness. fairness - utility tradeoff EVALUATE-FOR method. Task are Disparate impact, machine learning applications, and mitigating discrimination. OtherScientificTerm is predictive relationship. Method is graph neural networks. Metric is predictive accuracy. ","Disparate impact is an important problem in many machine learning applications. This paper studies the problem of disparate impact in graph-structured data. The authors introduce a new fairness concept called dyadic fairness, which is the notion of fairness in algorithmic designs. They show that the dyadic fairness of link predictive scores depends on the number of edges in a graph and the predictive relationship between the edges in the graph. In particular, the authors show that for a certain class of graph connections, the fair adjacency matrix of a pair of edges is equal to that of all edges in that graph. They then propose an algorithm called FairAdj to learn a fair admissible matrix for the fair link prediction under certain graph structural constraints. The proposed method is shown to achieve a fair-utility tradeoff between the predictive accuracy of the algorithm and the fairness of the final link prediction, mitigating discrimination.   ","Disparate impact is an important problem in many machine learning applications. This paper studies the problem of disparate impact in graph-structured data. The authors introduce a new fairness concept called dyadic fairness, which is the notion of fairness in algorithmic designs. They show that the dyadic fairness of link predictive scores depends on the number of edges in a graph and the predictive relationship between the edges in the graph. In particular, the authors show that for a certain class of graph connections, the fair adjacency matrix of a pair of edges is equal to that of all edges in that graph. They then propose an algorithm called FairAdj to learn a fair admissible matrix for the fair link prediction under certain graph structural constraints. The proposed method is shown to achieve a fair-utility tradeoff between the predictive accuracy of the algorithm and the fairness of the final link prediction, mitigating discrimination.   "
306,SP:b614e9fbec58e9efa7722d2ec4a60fc93d210f92,"Autoencoders HYPONYM-OF information compression framework. generative ability FEATURE-OF it. generative ability FEATURE-OF autoencoder. Gaussian prior knowledge USED-FOR synthesis. Gaussian prior knowledge USED-FOR VAE. interpolation HYPONYM-OF exploration in latent space. disentangled representation CONJUNCTION regularization. regularization CONJUNCTION disentangled representation. regularization USED-FOR exploration in latent space. Disentangled Exploration Autoencoder ( DEAE ) USED-FOR controllable synthesis. regularization USED-FOR controllable synthesis. regularization USED-FOR Disentangled Exploration Autoencoder ( DEAE ). disentangled representation USED-FOR Disentangled Exploration Autoencoder ( DEAE ). encoder USED-FOR DEAE. encoder USED-FOR latent code space. directed interpolation USED-FOR encoder. directed interpolation USED-FOR latent code space. encoder USED-FOR latent representation. disentanglement FEATURE-OF latent representation. disentanglement CONJUNCTION exploration. exploration CONJUNCTION disentanglement. positive loop USED-FOR DEAE. exploration USED-FOR positive loop. disentanglement USED-FOR positive loop. DEAE USED-FOR attribute - controllable augmented samples. DEAE USED-FOR dataset bias. DEAE USED-FOR fairness problems. Method are GAN - based adversarial training, and decoder. OtherScientificTerm are latent code, disentangled latent code, and interpolated latent code. Generic is method. ","Autoencoders are an important information compression framework, and it is well known that the generative ability of an autoencoder is highly dependent on the disentangledness of the latent code. This paper proposes a new method called Disentangled Exploration Autoencoding (DEAE) that combines disentangling representation and regularization to improve the exploration in latent space (i.e. interpolation). The key idea is to use GAN-based adversarial training to train a VAE with Gaussian prior knowledge for the synthesis, and then to use disentanglement and exploration in the positive loop of the DEAE. DEAE uses an encoder to disentangle a latent code space from the encoder and a decoder to learn a latent representation. The encoder is trained with directed interpolation, and the decoder is used to generate a disentanged latent code, which is then used as a positive loop for the positive loops. The authors show that DEAE is able to generate attribute-controllable augmented samples, and that disentangler and exploration are used to improve fairness problems, and DEAE can be used to mitigate dataset bias. The method is evaluated on a number of datasets, and is shown to be effective.","Autoencoders are an important information compression framework, and it is well known that the generative ability of an autoencoder is highly dependent on the disentangledness of the latent code. This paper proposes a new method called Disentangled Exploration Autoencoding (DEAE) that combines disentangling representation and regularization to improve the exploration in latent space (i.e. interpolation). The key idea is to use GAN-based adversarial training to train a VAE with Gaussian prior knowledge for the synthesis, and then to use disentanglement and exploration in the positive loop of the DEAE. DEAE uses an encoder to disentangle a latent code space from the encoder and a decoder to learn a latent representation. The encoder is trained with directed interpolation, and the decoder is used to generate a disentanged latent code, which is then used as a positive loop for the positive loops. The authors show that DEAE is able to generate attribute-controllable augmented samples, and that disentangler and exploration are used to improve fairness problems, and DEAE can be used to mitigate dataset bias. The method is evaluated on a number of datasets, and is shown to be effective."
315,SP:c934adb14926a00ef9c73c9773cb0b3a2669921e,"Episodic and semantic memory PART-OF human memory model. serial event ( episodic memory ) USED-FOR compressed representation. Bayesian memory allocation scheme USED-FOR episodic and semantic memory. hierarchical latent variable model USED-FOR Bayesian memory allocation scheme. locally contiguous memory USED-FOR differentiable block allocated latent memory. locally contiguous memory USED-FOR Kanerva Machine. feed forward deterministic process USED-FOR it. binarized MNIST CONJUNCTION binarized Omniglot. binarized Omniglot CONJUNCTION binarized MNIST. allocation scheme USED-FOR memory conditional image generation. binarized MNIST FEATURE-OF conditional likelihood values. DMLab Mazes CONJUNCTION Celeb - A. Celeb - A CONJUNCTION DMLab Mazes. CIFAR10 CONJUNCTION DMLab Mazes. DMLab Mazes CONJUNCTION CIFAR10. Celeb - A CONJUNCTION ImageNet32×32. ImageNet32×32 CONJUNCTION Celeb - A. Method are complementary learning systems, and heap allocation. Task is memory writing. OtherScientificTerm is read key distribution. ","This paper proposes a Bayesian memory allocation scheme for the episodic and semantic memory in the human memory model, which is based on a hierarchical latent variable model. The authors propose to use a serial event (episodic memory) and a compressed representation (semantic memory) as complementary learning systems. The Kanerva Machine uses a locally contiguous memory to store the differentiable block allocated latent memory, and then uses it as part of a feed forward deterministic process. The allocation scheme is applied to the problem of memory conditional image generation. The conditional likelihood values of the conditional likelihood on binarized MNIST and binarised Omniglot are used, and the authors show that the allocation scheme works well for CIFAR10, DMLab Mazes, Celeb-A, and ImageNet32×32. They also show that memory writing is more efficient than previous work, and that the read key distribution is similar to the one used in previous work.  ","This paper proposes a Bayesian memory allocation scheme for the episodic and semantic memory in the human memory model, which is based on a hierarchical latent variable model. The authors propose to use a serial event (episodic memory) and a compressed representation (semantic memory) as complementary learning systems. The Kanerva Machine uses a locally contiguous memory to store the differentiable block allocated latent memory, and then uses it as part of a feed forward deterministic process. The allocation scheme is applied to the problem of memory conditional image generation. The conditional likelihood values of the conditional likelihood on binarized MNIST and binarised Omniglot are used, and the authors show that the allocation scheme works well for CIFAR10, DMLab Mazes, Celeb-A, and ImageNet32×32. They also show that memory writing is more efficient than previous work, and that the read key distribution is similar to the one used in previous work.  "
324,SP:e63d7d8c581019e17585fb9c0eac33d6836e187d,deep learning models USED-FOR machine learning tasks. Attention mechanisms CONJUNCTION deep learning models. deep learning models CONJUNCTION Attention mechanisms. sample complexity CONJUNCTION loss landscape. loss landscape CONJUNCTION sample complexity. loss landscape FEATURE-OF attention - based neural networks. sample complexity FEATURE-OF attention - based neural networks. attention models COMPARE models. models COMPARE attention models. local minimum PART-OF attention model. sample complexity EVALUATE-FOR models. prediction error EVALUATE-FOR local minimum. sample complexity EVALUATE-FOR attention models. OtherScientificTerm is attention. Method is self - attention. ,This paper studies the sample complexity and loss landscape of attention-based neural networks. Attention mechanisms and deep learning models are well-studied in machine learning tasks. This paper shows that attention mechanisms are more efficient than other deep learning approaches. The authors also show that attention models with self-attention have a lower sample complexity than other models. The paper also shows that the local minimum of the attention model has a lower prediction error.,This paper studies the sample complexity and loss landscape of attention-based neural networks. Attention mechanisms and deep learning models are well-studied in machine learning tasks. This paper shows that attention mechanisms are more efficient than other deep learning approaches. The authors also show that attention models with self-attention have a lower sample complexity than other models. The paper also shows that the local minimum of the attention model has a lower prediction error.
333,SP:f739d199fdee26f09994e3f9487aec1eab0f2e89,Bayesian modeling USED-FOR Active inference. biologically plausible model USED-FOR Bayesian modeling. free energy principle CONJUNCTION prior preference. prior preference CONJUNCTION free energy principle. reinforcement learning ( RL ) algorithms USED-FOR active inference. negative value function USED-FOR EFE. method USED-FOR prior preference. prior preference CONJUNCTION theoretical connection. theoretical connection CONJUNCTION prior preference. theoretical connection USED-FOR method. active inference USED-FOR inverse RL. prior preference learning USED-FOR active inference. active inference USED-FOR inverse RL problem. prior preference learning USED-FOR inverse RL problem. EFE - based rewards USED-FOR active inference. OtherScientificTerm is expected free energy ( EFE ). ,"Active inference is an important problem in Bayesian modeling based on a biologically plausible model. In this paper, the authors propose to use reinforcement learning (RL) algorithms to solve the problem of active inference using the expected free energy (EFE). The EFE is modeled as a negative value function, and the authors use the free energy principle, prior preference, and a theoretical connection between the EFE and the prior preference to propose a method to learn a prior preference for active inference in inverse RL. The authors show that active inference with EFE-based rewards is equivalent to prior preference learning for the inverse RL problem. ","Active inference is an important problem in Bayesian modeling based on a biologically plausible model. In this paper, the authors propose to use reinforcement learning (RL) algorithms to solve the problem of active inference using the expected free energy (EFE). The EFE is modeled as a negative value function, and the authors use the free energy principle, prior preference, and a theoretical connection between the EFE and the prior preference to propose a method to learn a prior preference for active inference in inverse RL. The authors show that active inference with EFE-based rewards is equivalent to prior preference learning for the inverse RL problem. "
342,SP:5592b79e49eba95c15103a3348f2bde57b60f2ab,"data augmentation method USED-FOR generalization. data augmentation method USED-FOR adversarial and standard learning. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-100. OOD data USED-FOR learning scenario. method COMPARE data augmentation methods. data augmentation methods COMPARE method. method COMPARE adversarial training. adversarial training COMPARE method. Method is neural networks. Generic is methods. Material are UID data, and image data. OtherScientificTerm are pseudo - labels, and undesirable features. ","This paper proposes a new data augmentation method for improving generalization of neural networks. The authors argue that existing methods do not generalize well to out-of-distribution (OOD) data, which is a common problem in adversarial and standard learning. The paper proposes to use pseudo-labels as pseudo-label for OOD data to avoid undesirable features. Experiments are conducted on CIFAR-10, CifAR-100, and ImageNet, and show that the proposed method outperforms existing data augmentation methods, and is competitive with adversarial training. The main contribution of the paper is that the learning scenario is not limited to out of distribution data, but can be applied to any learning scenario where there is a large amount of OOD training data.   ","This paper proposes a new data augmentation method for improving generalization of neural networks. The authors argue that existing methods do not generalize well to out-of-distribution (OOD) data, which is a common problem in adversarial and standard learning. The paper proposes to use pseudo-labels as pseudo-label for OOD data to avoid undesirable features. Experiments are conducted on CIFAR-10, CifAR-100, and ImageNet, and show that the proposed method outperforms existing data augmentation methods, and is competitive with adversarial training. The main contribution of the paper is that the learning scenario is not limited to out of distribution data, but can be applied to any learning scenario where there is a large amount of OOD training data.   "
351,SP:3cac7a2c310165ed0de46d8e5546c3bfbd639158,"Fast Linearized Adaptive Policy ( FLAP ) HYPONYM-OF metareinforcement learning ( meta - RL ) method. shared linear representation of the policy USED-FOR FLAP. adapter network USED-FOR linear weights. adapter network USED-FOR policy. MAML HYPONYM-OF prior meta - RL methods. gradient descent USED-FOR meta - policy. adaptation run - time EVALUATE-FOR separate feed - forward network. FLAP COMPARE prior methods. prior methods COMPARE FLAP. continuous - control meta - RL benchmarks EVALUATE-FOR FLAP. adaptation run - time speeds EVALUATE-FOR prior methods. average return CONJUNCTION adaptation run - time speeds. adaptation run - time speeds CONJUNCTION average return. out - of - distribution tasks EVALUATE-FOR FLAP. average return EVALUATE-FOR FLAP. adaptation run - time speeds EVALUATE-FOR FLAP. Task are outof - distribution tasks, and adaptation. Generic are task, and tasks. Method is prior MetaRL methods. ",This paper proposes a metareinforcement learning (meta-RL) method called Fast Linearized Adaptive Policy (FLAP) that learns a shared linear representation of the policy for out-of-distribution tasks. The policy is learned using an adapter network that learns linear weights for each task and adapts the policy to the new task using gradient descent. The adaptation run-time of a separate feed-forward network is also improved. The authors show that FLAP outperforms prior meta-RL methods such as MAML in terms of average return and adaptation run time. FLAP is also shown to outperform prior methods on continuous-control meta-reward benchmarks.    The authors also compare FLAP to prior MetaRL methods on a number of tasks where the adaptation is performed in an unsupervised way. They also show FLAP achieves better average return as well as adaptation runtime speeds compared to prior methods. ,This paper proposes a metareinforcement learning (meta-RL) method called Fast Linearized Adaptive Policy (FLAP) that learns a shared linear representation of the policy for out-of-distribution tasks. The policy is learned using an adapter network that learns linear weights for each task and adapts the policy to the new task using gradient descent. The adaptation run-time of a separate feed-forward network is also improved. The authors show that FLAP outperforms prior meta-RL methods such as MAML in terms of average return and adaptation run time. FLAP is also shown to outperform prior methods on continuous-control meta-reward benchmarks.    The authors also compare FLAP to prior MetaRL methods on a number of tasks where the adaptation is performed in an unsupervised way. They also show FLAP achieves better average return as well as adaptation runtime speeds compared to prior methods. 
360,SP:21a1bd4ada0723c96c0dbf7a142a2faf5defa4e3,"communication efficiency FEATURE-OF algorithm. kernel k - means USED-FOR optimization problem. federated settings FEATURE-OF kernel k - means. federated settings USED-FOR optimization problem. communication efficient mech anism ( CEM ) USED-FOR communication cost. feder ated kernelk - means USED-FOR privacy preservation. matrix operations USED-FOR local computational results. federated kernel k - means COMPARE kernel k - means. kernel k - means COMPARE federated kernel k - means. clustering quality EVALUATE-FOR federated kernel k - means. clustering quality EVALUATE-FOR kernel k - means. communication cost EVALUATE-FOR DSPGD. O(1 / T ) rate FEATURE-OF DSPGD. CEM USED-FOR DSPGD. CEM USED-FOR DSPGD. communication cost EVALUATE-FOR federated kerne l k - means. clustering quality EVALUATE-FOR federated kerne l k - means. Method are federated kernel k - means algorithm, and kernelk - means. OtherScientificTerm are approximate solution, and cloud server. ","This paper proposes a new federated kernel k-means algorithm. The algorithm is motivated by the communication efficiency of the algorithm. In particular, the authors consider the optimization problem of kernel k - means in federated settings, and propose a communication efficient mech anism (CEM) to reduce the communication cost. The authors show that feder ated kernelk - means can be used for privacy preservation, and provide an approximate solution. They also show that the local computational results can be obtained via matrix operations, which is similar to kernelk-mean. Finally, they show that DSPGD with CEM can achieve O(1/T) rate with a communication cost O(O(T) and O(T^T) times faster than the original federated Kernel k-Means algorithm, and that federated kerne l k-mean achieves better clustering quality compared to the original kernel k--means.   ","This paper proposes a new federated kernel k-means algorithm. The algorithm is motivated by the communication efficiency of the algorithm. In particular, the authors consider the optimization problem of kernel k - means in federated settings, and propose a communication efficient mech anism (CEM) to reduce the communication cost. The authors show that feder ated kernelk - means can be used for privacy preservation, and provide an approximate solution. They also show that the local computational results can be obtained via matrix operations, which is similar to kernelk-mean. Finally, they show that DSPGD with CEM can achieve O(1/T) rate with a communication cost O(O(T) and O(T^T) times faster than the original federated Kernel k-Means algorithm, and that federated kerne l k-mean achieves better clustering quality compared to the original kernel k--means.   "
369,SP:be568dd3fea51ce33a6d1e4b07dda5aee6342395,"hardware & latency constraints FEATURE-OF architectures. accuracy EVALUATE-FOR architectures. approach USED-FOR models. approach USED-FOR resource - intensive tasks. deployment targets USED-FOR resource - intensive tasks. CompOFA HYPONYM-OF design space. model search / extraction time COMPARE state of the art. state of the art COMPARE model search / extraction time. heuristics COMPARE state of the art. state of the art COMPARE heuristics. design space USED-FOR models. diversity of hardware and latency targets FEATURE-OF models. Method is CNNs. Metric are constant training cost, and complexity. Generic is cost. OtherScientificTerm are combinatorial explosion of sub - optimal model configurations, search space, training budget, search, accuracy - latency Pareto frontier, model dimensions, and Pareto optimality. Material is ImageNet. ","This paper proposes a new approach to train models with hardware & latency constraints to improve the accuracy of such architectures. The authors argue that CNNs have a constant training cost, but that the cost is highly correlated with the cost of the hardware and latency constraints. They propose an approach called Pareto-Pareto frontier (PPF) to reduce the combinatorial explosion of sub-optimal model configurations in the search space. They show that this approach can improve the performance of such models on resource-intensive tasks with different deployment targets. They also show that the model search/extraction time can be much faster than the state of the art in terms of the number of parameters and the training budget, and that their approach can be applied to a variety of models in the design space (e.g. CompOFA). They also demonstrate that their search space can be extended to a diversity of hardware and latencies targets and that the search can be done in a single step.    The authors show that they are able to find the accuracy-latency Pareta frontier in a search space with a search budget of $O(\sqrt{n\log n})$, where $n$ is the training cost and $n \log n$ is a training budget. The search space is defined as $n\times n$ where $N$ is $n^{-1/2}$ and $N\times d$ are the model dimensions, and $d$ is an input to the search. The paper also shows that models trained on a diverse set of hardware/latency targets can be found in this search space, and show that their model search time matches the state-of-the-art.  Finally, the paper shows that their method is able to achieve Paret-optimality on ImageNet, which is a dataset with a large number of configurations. ","This paper proposes a new approach to train models with hardware & latency constraints to improve the accuracy of such architectures. The authors argue that CNNs have a constant training cost, but that the cost is highly correlated with the cost of the hardware and latency constraints. They propose an approach called Pareto-Pareto frontier (PPF) to reduce the combinatorial explosion of sub-optimal model configurations in the search space. They show that this approach can improve the performance of such models on resource-intensive tasks with different deployment targets. They also show that the model search/extraction time can be much faster than the state of the art in terms of the number of parameters and the training budget, and that their approach can be applied to a variety of models in the design space (e.g. CompOFA). They also demonstrate that their search space can be extended to a diversity of hardware and latencies targets and that the search can be done in a single step.    The authors show that they are able to find the accuracy-latency Pareta frontier in a search space with a search budget of $O(\sqrt{n\log n})$, where $n$ is the training cost and $n \log n$ is a training budget. The search space is defined as $n\times n$ where $N$ is $n^{-1/2}$ and $N\times d$ are the model dimensions, and $d$ is an input to the search. The paper also shows that models trained on a diverse set of hardware/latency targets can be found in this search space, and show that their model search time matches the state-of-the-art.  Finally, the paper shows that their method is able to achieve Paret-optimality on ImageNet, which is a dataset with a large number of configurations. "
378,SP:04b84d26cf282dbb753cbf27f14c334f65d3f8ec,Meta - learning USED-FOR model. limited data USED-FOR model. adversarial samples USED-FOR meta - learning. ADML ( ADversarial Meta - Learner ) USED-FOR initialization of a learning model. meta - learning algorithm USED-FOR initialization of a learning model. adversarial manner USED-FOR initialization of a learning model. ADML ( ADversarial Meta - Learner ) HYPONYM-OF meta - learning algorithm. clean and adversarial samples USED-FOR ADML ( ADversarial Meta - Learner ). meta - learning algorithms COMPARE it. it COMPARE meta - learning algorithms. it USED-FOR adversarial samples. it COMPARE meta - learning algorithms. meta - learning algorithms COMPARE it. MiniImageNet CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION MiniImageNet. accuracy CONJUNCTION robustness. robustness CONJUNCTION accuracy. ADML COMPARE representative meta - learning algorithms. representative meta - learning algorithms COMPARE ADML. attack mechanisms USED-FOR adversarial samples. image datasets EVALUATE-FOR ADML. MiniImageNet HYPONYM-OF image datasets. CIFAR100 HYPONYM-OF image datasets. Method is learning model. Material is clean samples. OtherScientificTerm is limited and even contaminated samples. ,"Meta-learning aims to train a model on limited data with the goal of improving the performance of the model when the number of samples is limited. This paper proposes a meta-learning algorithm called ADML (ADversarial Meta-Learner) that uses adversarial samples to improve the performance when the learning model is trained on both clean and adversarial data. The authors show that ADML can improve the initialization of a learning model in an adversarial manner, and that it can also improve the robustness to adversarial attacks. The paper also shows that it is more robust to attack mechanisms that can be applied to the training data.   The authors also show that the proposed ADML algorithm is robust to both limited and even contaminated samples.  The paper evaluates ADML on two image datasets, MiniImageNet and CIFAR100, and shows that the performance improvement of ADML is comparable to the state-of-the-art meta-learners. In addition, it is shown that it outperforms the state of the art for adversarial training and robustness, and it is also shown to be robust to different attack mechanisms.  Finally, the paper shows that in the presence of limited data, ADML outperforms other representative meta learning algorithms in terms of accuracy, robustness and transferability. ","Meta-learning aims to train a model on limited data with the goal of improving the performance of the model when the number of samples is limited. This paper proposes a meta-learning algorithm called ADML (ADversarial Meta-Learner) that uses adversarial samples to improve the performance when the learning model is trained on both clean and adversarial data. The authors show that ADML can improve the initialization of a learning model in an adversarial manner, and that it can also improve the robustness to adversarial attacks. The paper also shows that it is more robust to attack mechanisms that can be applied to the training data.   The authors also show that the proposed ADML algorithm is robust to both limited and even contaminated samples.  The paper evaluates ADML on two image datasets, MiniImageNet and CIFAR100, and shows that the performance improvement of ADML is comparable to the state-of-the-art meta-learners. In addition, it is shown that it outperforms the state of the art for adversarial training and robustness, and it is also shown to be robust to different attack mechanisms.  Finally, the paper shows that in the presence of limited data, ADML outperforms other representative meta learning algorithms in terms of accuracy, robustness and transferability. "
387,SP:dfbaa6b53c4e8328d52666ad4641fc917bf0c0b3,Error correction codes PART-OF communication applications. maximum likelihood rule USED-FOR decoding of transmitted codewords. permutation USED-FOR permutation decoding. data - driven framework USED-FOR permutation selection. node embedding CONJUNCTION self - attention. self - attention CONJUNCTION node embedding. domain knowledge CONJUNCTION machine learning concepts. machine learning concepts CONJUNCTION domain knowledge. domain knowledge PART-OF data - driven framework. machine learning concepts PART-OF data - driven framework. self - attention HYPONYM-OF machine learning concepts. node embedding HYPONYM-OF machine learning concepts. simulated Bose Chaudhuri Hocquenghem ( BCH ) code COMPARE baseline decoders. baseline decoders COMPARE simulated Bose Chaudhuri Hocquenghem ( BCH ) code. bit error rate EVALUATE-FOR simulated Bose Chaudhuri Hocquenghem ( BCH ) code. bit error rate EVALUATE-FOR baseline decoders. self - attention networks USED-FOR physical layer communication systems. Method is suboptimal decoding algorithms. Generic is algorithms. ,"Error correction codes are of great importance in many communication applications. However, there are many suboptimal decoding algorithms, and the decoding of transmitted codewords using a maximum likelihood rule is one of the most commonly used algorithms. In this paper, the authors propose a data-driven framework for permutation selection that combines domain knowledge, machine learning concepts (e.g., node embedding, self-attention, etc.). The authors propose to use permutation decoding as a permutation for each permutation in the input sequence. The authors show that their simulated Bose Chaudhuri Hocquenghem (BCH) code achieves the best bit error rate compared to several baseline decoders. They also show that the proposed algorithms can be applied to physical layer communication systems using self-pay attention networks.","Error correction codes are of great importance in many communication applications. However, there are many suboptimal decoding algorithms, and the decoding of transmitted codewords using a maximum likelihood rule is one of the most commonly used algorithms. In this paper, the authors propose a data-driven framework for permutation selection that combines domain knowledge, machine learning concepts (e.g., node embedding, self-attention, etc.). The authors propose to use permutation decoding as a permutation for each permutation in the input sequence. The authors show that their simulated Bose Chaudhuri Hocquenghem (BCH) code achieves the best bit error rate compared to several baseline decoders. They also show that the proposed algorithms can be applied to physical layer communication systems using self-pay attention networks."
396,SP:c860a7b0952d708e7851c9bc4b63d246f64d1cba,"fine - tuning BERT USED-FOR text classification task. unsupervised classification task USED-FOR task. finetuning USED-FOR task. unsupervised classification task EVALUATE-FOR finetuning. unsupervised clustering USED-FOR intermediate task. labeled examples USED-FOR topical classification tasks. classification step USED-FOR topical classification tasks. classification step USED-FOR labeled examples. Material are labeled data, and data sets. Method is BERT. ","This paper studies fine-tuning BERT for the text classification task. The task is an unsupervised classification task, and finetuning for this task is a well-studied topic in the literature. The authors propose to fine-tune BERT on top of an existing unstructured clustering approach for the intermediate task. They show that this classification step is effective for topical classification tasks, and that the labeled examples from the classification step can be used to improve the performance of the downstream task. In addition, the authors show that the classification performance of BERT can be improved when the labeled data is more diverse. ","This paper studies fine-tuning BERT for the text classification task. The task is an unsupervised classification task, and finetuning for this task is a well-studied topic in the literature. The authors propose to fine-tune BERT on top of an existing unstructured clustering approach for the intermediate task. They show that this classification step is effective for topical classification tasks, and that the labeled examples from the classification step can be used to improve the performance of the downstream task. In addition, the authors show that the classification performance of BERT can be improved when the labeled data is more diverse. "
405,SP:ea37f5882fd98dd4ce233077bb3069517d4ed4ea,"fixed ( random shooting ) control agent USED-FOR generative models. mixture density nets COMPARE models. models COMPARE mixture density nets. they COMPARE probabilistic counterparts. probabilistic counterparts COMPARE they. deterministic models COMPARE probabilistic counterparts. probabilistic counterparts COMPARE deterministic models. heteroscedasticity USED-FOR regularizer. them USED-FOR control problem. sample complexity EVALUATE-FOR MBRL. framework USED-FOR MBRL. sample complexity EVALUATE-FOR framework. Acrobot EVALUATE-FOR MBRL. training schedule USED-FOR MBRL. OtherScientificTerm are multimodal posterior predictives, multimodality, and probabilistic posterior predictives. ","This paper studies the problem of training generative models with a fixed (random shooting) control agent, where multimodal posterior predictives are used. The authors show that the sample complexity of MBRL with a certain training schedule is O(1/\sqrt{T}^T) when the multimodality is high, and O(T/T^2) when multimodalities are low. They also show that MBRL can be trained with the same training schedule as a deterministic model, but with a regularizer that encourages heteroscedasticity. They show that this regularizer can be applied to any probabilistic posterior predictive, and that it can be used to train a generative model that is asymptotically similar to deterministic models as well as to their more generalised counterparts. Finally, they show that, when trained with mixture density nets instead of models that are deterministic, they can achieve a sample complexity that is O(\sqrt{\log T}^2/\log T) times faster than their more standard (probabilistic counterparts). They also demonstrate that this framework can also be used for MBRL on Acrobot, where they train them on a control problem and then use them to solve the control problem.","This paper studies the problem of training generative models with a fixed (random shooting) control agent, where multimodal posterior predictives are used. The authors show that the sample complexity of MBRL with a certain training schedule is O(1/\sqrt{T}^T) when the multimodality is high, and O(T/T^2) when multimodalities are low. They also show that MBRL can be trained with the same training schedule as a deterministic model, but with a regularizer that encourages heteroscedasticity. They show that this regularizer can be applied to any probabilistic posterior predictive, and that it can be used to train a generative model that is asymptotically similar to deterministic models as well as to their more generalised counterparts. Finally, they show that, when trained with mixture density nets instead of models that are deterministic, they can achieve a sample complexity that is O(\sqrt{\log T}^2/\log T) times faster than their more standard (probabilistic counterparts). They also demonstrate that this framework can also be used for MBRL on Acrobot, where they train them on a control problem and then use them to solve the control problem."
414,SP:4e25ba3714d78ba59a0d8efbb65e0ef5201702f8,"Affine Disentangled GAN ( ADIS - GAN ) HYPONYM-OF Generative Adversarial Network. affine regularizer USED-FOR inductive bias. affine transformation properties of images USED-FOR affine regularizer. transformation matrices PART-OF affine matrix. maximum likelihood estimation USED-FOR transformation parameters. horizontal and vertical zoom CONJUNCTION horizontal and vertical skew. horizontal and vertical skew CONJUNCTION horizontal and vertical zoom. horizontal and vertical skew CONJUNCTION horizontal and vertical translation. horizontal and vertical translation CONJUNCTION horizontal and vertical skew. rotation CONJUNCTION horizontal and vertical zoom. horizontal and vertical zoom CONJUNCTION rotation. rotation CONJUNCTION horizontal and vertical skew. horizontal and vertical skew CONJUNCTION rotation. disentangled representations COMPARE features. features COMPARE disentangled representations. ADIS - GAN USED-FOR features. approaches USED-FOR disentangled representations. horizontal and vertical translation HYPONYM-OF transformations. rotation HYPONYM-OF transformations. horizontal and vertical skew HYPONYM-OF transformations. horizontal and vertical zoom HYPONYM-OF transformations. ADIS - GAN USED-FOR features. MNIST, CelebA, and dSprites datasets EVALUATE-FOR ADIS - GAN. MNIST, CelebA, and dSprites datasets EVALUATE-FOR features. OtherScientificTerm is affine transformations. Method is InfoGAN. ","This paper proposes Affine Disentangled GAN (ADIS-GAN), a variant of the Generative Adversarial Network (GAN). The key idea is to use an affine regularizer to mitigate the inductive bias of the affine transformation properties of images. The affine transformations are defined as the transformation matrices of the input image. The authors propose a new affine matrix that is composed of the transformed part of the original image and the transformed parts of the target image, and then use maximum likelihood estimation to optimize the transformation parameters. Experiments are conducted on MNIST, CelebA, and dSprites datasets, and show that ADIS-GAN can learn disentangled representations that are more robust to different types of transformations (rotation, horizontal and vertical zoom, horizontal, vertical skew, horizontal & vertical translation, etc.). The authors also show that the features learned by ADIS - GAN outperform existing approaches on disentangling representations across different transformations. ","This paper proposes Affine Disentangled GAN (ADIS-GAN), a variant of the Generative Adversarial Network (GAN). The key idea is to use an affine regularizer to mitigate the inductive bias of the affine transformation properties of images. The affine transformations are defined as the transformation matrices of the input image. The authors propose a new affine matrix that is composed of the transformed part of the original image and the transformed parts of the target image, and then use maximum likelihood estimation to optimize the transformation parameters. Experiments are conducted on MNIST, CelebA, and dSprites datasets, and show that ADIS-GAN can learn disentangled representations that are more robust to different types of transformations (rotation, horizontal and vertical zoom, horizontal, vertical skew, horizontal & vertical translation, etc.). The authors also show that the features learned by ADIS - GAN outperform existing approaches on disentangling representations across different transformations. "
423,SP:121f8420cfb49c6d80b5ebb4051e85947182594a,"contrastive learning methods COMPARE supervised learning counterparts. supervised learning counterparts COMPARE contrastive learning methods. contrastive learning methods USED-FOR Representation learning. data augmentations USED-FOR methods. augmentations USED-FOR instance discrimination - based contrastive learning. fully supervised upper bound USED-FOR unsupervised learning. distribution divergence USED-FOR retrieval of strongly augmented queries. augmentations USED-FOR contrastive loss. ResNet-50 architecture CONJUNCTION single - layer classifier fine - tuned. single - layer classifier fine - tuned CONJUNCTION ResNet-50 architecture. ImageNet EVALUATE-FOR ResNet-50 architecture. top-1 accuracy EVALUATE-FOR method. ImageNet EVALUATE-FOR method. fully supervised ResNet-50 USED-FOR top-1 accuracy. it COMPARE self - supervised and supervised methods. self - supervised and supervised methods COMPARE it. self - supervised and supervised methods USED-FOR transfer learning and object detection tasks. transfer learning and object detection tasks EVALUATE-FOR it. Metric is generalizability. OtherScientificTerm are distortions, image structures, representation bank, overoptimistic assumption, distorted visual structures, and distributions of weakly augmented counterparts. ","Representation learning is an important problem in many applications, and contrastive learning methods have been shown to be superior to their supervised learning counterparts in terms of generalizability. Representation learning with data augmentations can be problematic when there are distortions in the input image structures. This paper proposes a novel method for instance discrimination-based contrastive loss based on augmentations to the representation bank. The main idea is to use a fully supervised upper bound for unsupervised learning, which is based on an overoptimistic assumption that the distribution of strongly augmented queries is the same as that of the distribution divergence for the retrieval of weakly augmented queries. The augmentations are used to improve the performance of the contrastive losses. The proposed method is evaluated on ImageNet on top-1 accuracy on fully supervised ResNet-50 architecture and a single-layer classifier fine-tuned, and it is shown to outperform self-supervised and supervised methods on transfer learning and object detection tasks. The authors also show that the augmented images are more likely to have distorted visual structures, and that the distributions of the augmented examples are similar to those of their weakly augmentations.","Representation learning is an important problem in many applications, and contrastive learning methods have been shown to be superior to their supervised learning counterparts in terms of generalizability. Representation learning with data augmentations can be problematic when there are distortions in the input image structures. This paper proposes a novel method for instance discrimination-based contrastive loss based on augmentations to the representation bank. The main idea is to use a fully supervised upper bound for unsupervised learning, which is based on an overoptimistic assumption that the distribution of strongly augmented queries is the same as that of the distribution divergence for the retrieval of weakly augmented queries. The augmentations are used to improve the performance of the contrastive losses. The proposed method is evaluated on ImageNet on top-1 accuracy on fully supervised ResNet-50 architecture and a single-layer classifier fine-tuned, and it is shown to outperform self-supervised and supervised methods on transfer learning and object detection tasks. The authors also show that the augmented images are more likely to have distorted visual structures, and that the distributions of the augmented examples are similar to those of their weakly augmentations."
432,SP:af54e542223097c315ecd677d0b968e9a0b2a1d4,"magnetic resonance imagery ( MRI ) USED-FOR De - identification. de - identification methods USED-FOR task. MRI de - identification techniques USED-FOR privacy - sensitive facial features. removal - based techniques COMPARE deep learning framework. deep learning framework COMPARE removal - based techniques. segmentation CONJUNCTION age prediction. age prediction CONJUNCTION segmentation. deep learning framework USED-FOR medical analyses. medical analyses FEATURE-OF brain. age prediction HYPONYM-OF medical analyses. segmentation HYPONYM-OF medical analyses. Material are database, and patient ’s MRI scan. Generic are they, and them. OtherScientificTerm is 3D volume. ","De-identification from magnetic resonance imagery (MRI) is one of the most commonly used methods for identifying a patient’s MRI scan from a database. However, there are several issues with existing de-identifying methods for this task: they are expensive to train, they are sensitive to privacy-sensitive facial features (e.g. 3D volume), and they do not generalize well to medical analyses of the brain. This paper proposes to use MRI de-identified techniques to identify a patient's MRI scan and train a deep learning framework that can be used for medical analyses such as segmentation and age prediction.   ","De-identification from magnetic resonance imagery (MRI) is one of the most commonly used methods for identifying a patient’s MRI scan from a database. However, there are several issues with existing de-identifying methods for this task: they are expensive to train, they are sensitive to privacy-sensitive facial features (e.g. 3D volume), and they do not generalize well to medical analyses of the brain. This paper proposes to use MRI de-identified techniques to identify a patient's MRI scan and train a deep learning framework that can be used for medical analyses such as segmentation and age prediction.   "
441,SP:0ac3964bd2320341488476d60f57b75d2a79f92c,Graph neural networks USED-FOR modeling graph data. Graph neural networks USED-FOR node classification and link prediction tasks. representation USED-FOR graph. pooling function USED-FOR node representations. pooling function USED-FOR compact form. pooling function USED-FOR representation. task relevance CONJUNCTION structural dependencies. structural dependencies CONJUNCTION task relevance. hierarchical graph pooling methods USED-FOR representation. representation USED-FOR graphs. Weisfeiler - Lehman test FEATURE-OF graphs. Graph Multiset Transformer ( GMT ) HYPONYM-OF multi - head attention based global pooling layer. graph structure FEATURE-OF auxiliary information. auxiliary information FEATURE-OF multiset encoding problem. multiset encoding problem USED-FOR graph pooling problem. injectiveness CONJUNCTION permutation invariance. permutation invariance CONJUNCTION injectiveness. it COMPARE Weisfeiler - Lehman graph isomorphism test. Weisfeiler - Lehman graph isomorphism test COMPARE it. injectiveness FEATURE-OF GMT. permutation invariance FEATURE-OF GMT. node clustering approaches USED-FOR hierarchical graph pooling. methods USED-FOR hierarchical graph pooling. methods USED-FOR node clustering approaches. GMT COMPARE graph pooling methods. graph pooling methods COMPARE GMT. graph classification benchmarks EVALUATE-FOR graph pooling methods. memory and time efficiency EVALUATE-FOR graph pooling methods. graph classification benchmarks EVALUATE-FOR GMT. memory and time efficiency EVALUATE-FOR GMT. Material is graph data. OtherScientificTerm is node features. Generic is they. ,"Graph neural networks are an important tool for modeling graph data. Graph neural networks have been widely used in node classification and link prediction tasks, but they are not well-suited for the task of graph pooling. In this paper, the authors focus on the problem of learning a representation of a graph that is invariant to changes in task relevance and structural dependencies. The authors propose a new representation for graphs that is based on hierarchical graph partitioning methods, where each pooling function is used to learn a compact form of the representation for each node in the graph.    The paper proposes a multi-head attention based global pooling layer, called Graph Multiset Transformer (GMT), which is a multiset encoding problem with auxiliary information related to the graph structure. The paper shows that the multiiset encoder and decoder of the pooling problem can be decomposed into a multiscale encoding problem, where the auxiliary information is related to graph structure and task relevance.  The authors show that the proposed GMT achieves better injectiveness, permutation invariance, and memory and time efficiency on several graph classification benchmarks compared to existing graph pooled methods. They also compare the proposed methods to existing node clustering approaches for hierarchical graph poolings, and show that GMT achieves the best performance in terms of injectiveness (i.e., the number of node features per layer) compared to the existing methods. ","Graph neural networks are an important tool for modeling graph data. Graph neural networks have been widely used in node classification and link prediction tasks, but they are not well-suited for the task of graph pooling. In this paper, the authors focus on the problem of learning a representation of a graph that is invariant to changes in task relevance and structural dependencies. The authors propose a new representation for graphs that is based on hierarchical graph partitioning methods, where each pooling function is used to learn a compact form of the representation for each node in the graph.    The paper proposes a multi-head attention based global pooling layer, called Graph Multiset Transformer (GMT), which is a multiset encoding problem with auxiliary information related to the graph structure. The paper shows that the multiiset encoder and decoder of the pooling problem can be decomposed into a multiscale encoding problem, where the auxiliary information is related to graph structure and task relevance.  The authors show that the proposed GMT achieves better injectiveness, permutation invariance, and memory and time efficiency on several graph classification benchmarks compared to existing graph pooled methods. They also compare the proposed methods to existing node clustering approaches for hierarchical graph poolings, and show that GMT achieves the best performance in terms of injectiveness (i.e., the number of node features per layer) compared to the existing methods. "
450,SP:76848e7ac3e6709e92f6a6db60269cb5177495d1,"GNNs USED-FOR prediction task. long - range interaction USED-FOR prediction task. tuning CONJUNCTION weights. weights CONJUNCTION tuning. GCN CONJUNCTION GIN. GIN CONJUNCTION GCN. over - squashing FEATURE-OF GNNs. GNNs COMPARE GAT. GAT COMPARE GNNs. GAT CONJUNCTION GGNN. GGNN CONJUNCTION GAT. GNNs USED-FOR over - squashing. GNNs COMPARE GGNN. GGNN COMPARE GNNs. bottleneck USED-FOR GNNs. GIN HYPONYM-OF GNNs. GCN HYPONYM-OF GNNs. Method are graph neural network ( GNN ), and GNN models of long - range problems. OtherScientificTerm are graph, exponentially growing information, fixed - size vectors, long - range signals, and incoming edges. ","This paper studies the problem of over-squashing in graph neural network (GNN) models. In particular, the authors show that GNNs with long-range interaction can suffer from the problem when the prediction task is based on a single node in a graph with exponentially growing information. They show that the problem can be alleviated by training GNN models with fixed-size vectors that are more robust to long range signals. The paper also shows that the long-term interaction between two nodes in the graph is a bottleneck for GNN training, and that the bottleneck can be removed by tuning and/or changing the weights of the GNN.   The authors also show that existing studies have shown that existing GNN-based models (GCN, GIN, GAT, GGNN) suffer from over-Squashing, which is a phenomenon that happens when the number of nodes in a GNN is large enough that the input to the network is dominated by incoming edges. The authors then propose a new GNN model, called Long-range GNN (LGNN), that is able to overcome this issue. The LGNN model is trained to be robust to this problem.","This paper studies the problem of over-squashing in graph neural network (GNN) models. In particular, the authors show that GNNs with long-range interaction can suffer from the problem when the prediction task is based on a single node in a graph with exponentially growing information. They show that the problem can be alleviated by training GNN models with fixed-size vectors that are more robust to long range signals. The paper also shows that the long-term interaction between two nodes in the graph is a bottleneck for GNN training, and that the bottleneck can be removed by tuning and/or changing the weights of the GNN.   The authors also show that existing studies have shown that existing GNN-based models (GCN, GIN, GAT, GGNN) suffer from over-Squashing, which is a phenomenon that happens when the number of nodes in a GNN is large enough that the input to the network is dominated by incoming edges. The authors then propose a new GNN model, called Long-range GNN (LGNN), that is able to overcome this issue. The LGNN model is trained to be robust to this problem."
459,SP:90d8fa381446923902e42b259392e5e975e6caa1,"cross - domain generalizable classifiers USED-FOR methods. methods USED-FOR domain - agnostic representations. annotated data USED-FOR classifier. embedding space USED-FOR domain - agnostic. data distributions USED-FOR domain - agnostic. Task are Sentiment analysis, and marketing strategies. Method are cross - domain sentiment analysis methods, and domain adaptation method. OtherScientificTerm are data annotation, and prototypical distribution. Generic is method. ","Sentiment analysis is an important problem in marketing and advertising, and there are many existing cross-domain sentiment analysis methods. However, these methods rely on cross-distribution generalizable classifiers, which can be problematic when the data annotation is not available. This paper proposes a novel domain adaptation method, where methods are trained to learn domain-agnostic representations. The key idea is to train a classifier on the annotated data, and then adapt the classifier to the target domain by learning a prototypical distribution over the data distributions. The authors show that this method can be applied to any embedding space, and that the proposed method is transferable to other domains.","Sentiment analysis is an important problem in marketing and advertising, and there are many existing cross-domain sentiment analysis methods. However, these methods rely on cross-distribution generalizable classifiers, which can be problematic when the data annotation is not available. This paper proposes a novel domain adaptation method, where methods are trained to learn domain-agnostic representations. The key idea is to train a classifier on the annotated data, and then adapt the classifier to the target domain by learning a prototypical distribution over the data distributions. The authors show that this method can be applied to any embedding space, and that the proposed method is transferable to other domains."
468,SP:893fd7440b82f5da0d4c0944928810322eaee2f0,Gender - bias stereotypes PART-OF natural language processing. genderbias FEATURE-OF natural language understanding. evaluation of genderbias PART-OF natural language understanding. inference USED-FOR natural language understanding. inference USED-FOR evaluation of genderbias. gender neutral premise COMPARE gender - specific hypothesis. gender - specific hypothesis COMPARE gender neutral premise. NLI models USED-FOR gender stereotypes. challenge task USED-FOR NLI models. occupations USED-FOR NLI models. BERT CONJUNCTION RoBERTa. RoBERTa CONJUNCTION BERT. RoBERTa CONJUNCTION BART. BART CONJUNCTION RoBERTa. models USED-FOR genderinduced prediction errors. BERT CONJUNCTION BART. BART CONJUNCTION BERT. MNLI and SNLI data - sets USED-FOR models. BART HYPONYM-OF models. BERT HYPONYM-OF models. RoBERTa HYPONYM-OF models. Generic is evaluation methodology. Method is debiasing techniques. Material is gender - balanced dataset. ,"Gender-biased stereotypes are prevalent in natural language processing. This paper studies the evaluation of genderbias in the context of natural language understanding through inference. The authors propose a challenge task to evaluate NLI models for gender stereotypes based on occupations. They show that a gender neutral premise is more powerful than a gender-specific hypothesis. They also show that models trained on MNLI and SNLI data-sets (BERT, RoBERTa, BART) are more sensitive to genderinduced prediction errors than models trained using other debiasing techniques. The paper also proposes a new evaluation methodology to evaluate the gender-balanced dataset.","Gender-biased stereotypes are prevalent in natural language processing. This paper studies the evaluation of genderbias in the context of natural language understanding through inference. The authors propose a challenge task to evaluate NLI models for gender stereotypes based on occupations. They show that a gender neutral premise is more powerful than a gender-specific hypothesis. They also show that models trained on MNLI and SNLI data-sets (BERT, RoBERTa, BART) are more sensitive to genderinduced prediction errors than models trained using other debiasing techniques. The paper also proposes a new evaluation methodology to evaluate the gender-balanced dataset."
477,SP:a32ab755bd249c393b70938036ce8e810c0c439f,"variational intrinsic control ( VIC ) HYPONYM-OF unsupervised reinforcement learning method. other HYPONYM-OF VIC algorithms. one HYPONYM-OF VIC algorithms. intrinsic reward USED-FOR latter. transitional probability model CONJUNCTION Gaussian mixture model. Gaussian mixture model CONJUNCTION transitional probability model. transitional probability model USED-FOR methods. Gaussian mixture model USED-FOR methods. OtherScientificTerm are intrinsic options, and stochastic environments. ","This paper proposes a new unsupervised reinforcement learning method called variational intrinsic control (VIC). Two VIC algorithms are proposed: one is based on the notion of intrinsic options, and the other is a combination of the two. The latter uses an intrinsic reward to encourage the agent to explore the intrinsic options. Both methods are based on a transitional probability model and a Gaussian mixture model. Experiments are conducted on a number of stochastic environments.","This paper proposes a new unsupervised reinforcement learning method called variational intrinsic control (VIC). Two VIC algorithms are proposed: one is based on the notion of intrinsic options, and the other is a combination of the two. The latter uses an intrinsic reward to encourage the agent to explore the intrinsic options. Both methods are based on a transitional probability model and a Gaussian mixture model. Experiments are conducted on a number of stochastic environments."
486,SP:b4df2c4627a6d46c5100133e38c4bea20b296dd8,Deep neural networks USED-FOR image classification. low data regime FEATURE-OF sample efficiency. ensemble of relatively small deep networks USED-FOR image classification problems. neural ensembling USED-FOR small data domains. technique COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE technique. deep ensembling COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE deep ensembling. deep ensembling HYPONYM-OF technique. Generic is they. Material is small datasets. Method is ensemble configurations. OtherScientificTerm is losses. ,"This paper studies the problem of sample efficiency in the low data regime of training deep neural networks for image classification. The authors propose to use an ensemble of relatively small deep networks to solve the image classification problems. They show that neural ensembling is effective for small data domains and that they can be used to improve sample efficiency on small datasets. They also show that the proposed technique, called deep ensembleling, outperforms state-of-the-art approaches on a variety of datasets and ensemble configurations. The paper also shows that the losses can be tuned to make the ensemble more robust to changes in the number of samples. ","This paper studies the problem of sample efficiency in the low data regime of training deep neural networks for image classification. The authors propose to use an ensemble of relatively small deep networks to solve the image classification problems. They show that neural ensembling is effective for small data domains and that they can be used to improve sample efficiency on small datasets. They also show that the proposed technique, called deep ensembleling, outperforms state-of-the-art approaches on a variety of datasets and ensemble configurations. The paper also shows that the losses can be tuned to make the ensemble more robust to changes in the number of samples. "
495,SP:4a0ee01f4897efa81659f37ef0468ee8195bbc4f,computational power and storage requirements CONJUNCTION processing speed. processing speed CONJUNCTION computational power and storage requirements. DNN - based applications USED-FOR InternetOf - Things ( IoT ) devices. them USED-FOR DNN - based applications. quantized networks COMPARE Binary Neural Networks ( BNNs ). Binary Neural Networks ( BNNs ) COMPARE quantized networks. speed - up EVALUATE-FOR Binary Neural Networks ( BNNs ). fixed and limited compression factor FEATURE-OF they. positive 0/1 binary weights COMPARE -1/+1 weights. -1/+1 weights COMPARE positive 0/1 binary weights. Sparse Binary Neural Networks HYPONYM-OF model and training scheme. -1/+1 weights COMPARE binary networks. binary networks COMPARE -1/+1 weights. sparsity FEATURE-OF BNNs. positive 0/1 binary weights USED-FOR sparsity. compression factor EVALUATE-FOR method. MNIST and CIFAR-10 datasets USED-FOR linear and convolutional networks. linear and convolutional networks EVALUATE-FOR method. it USED-FOR DNNs. compression rates CONJUNCTION generalization. generalization CONJUNCTION compression rates. generalization EVALUATE-FOR SBNNs. compression rates EVALUATE-FOR SBNNs. Method is Quantized neural networks. Metric is accuracy. Material is limited resources. ,"Quantized neural networks are a popular technique for reducing the computational power and storage requirements and processing speed of DNN-based applications for InternetOf-Things (IoT) devices. However, they have a fixed and limited compression factor, which limits them to be useful for many DNN -based applications with limited resources. This paper shows that quantized networks are more efficient than Binary Neural Networks (BNNs) in terms of speed-up, and that they can be much more powerful than BNNs.   The authors propose a new model and training scheme, called Sparse Binary neural Networks (SBNNs), which is based on the observation that -1/+1 weights are much more sparser than standard binary networks. The authors show that sparsity is achieved by using positive 0/1 binary weights instead of the standard -1/-1 weights. The proposed method is evaluated on linear and convolutional networks on MNIST and CIFAR-10 datasets, and it is shown to outperform the state-of-the-art compression rates and generalization performance of SBNNs with the same compression factor. ","Quantized neural networks are a popular technique for reducing the computational power and storage requirements and processing speed of DNN-based applications for InternetOf-Things (IoT) devices. However, they have a fixed and limited compression factor, which limits them to be useful for many DNN -based applications with limited resources. This paper shows that quantized networks are more efficient than Binary Neural Networks (BNNs) in terms of speed-up, and that they can be much more powerful than BNNs.   The authors propose a new model and training scheme, called Sparse Binary neural Networks (SBNNs), which is based on the observation that -1/+1 weights are much more sparser than standard binary networks. The authors show that sparsity is achieved by using positive 0/1 binary weights instead of the standard -1/-1 weights. The proposed method is evaluated on linear and convolutional networks on MNIST and CIFAR-10 datasets, and it is shown to outperform the state-of-the-art compression rates and generalization performance of SBNNs with the same compression factor. "
504,SP:5be8539ad02595ad3c7a2d7afe8cbb3e9924467d,"OOD data USED-FOR model calibration. outlier exposure USED-FOR model probabilities. outlier exposure USED-FOR method. estimates of class membership probabilities USED-FOR model predictions. baseline method USED-FOR predictive uncertainty. softmax probabilities USED-FOR model. softmax probabilities USED-FOR baseline method. softmax probabilities USED-FOR predictive uncertainty. Stochastic Variational Bayesian Inference ( SVBI ) USED-FOR deep learning. model ensembles CONJUNCTION Stochastic Variational Bayesian Inference ( SVBI ). Stochastic Variational Bayesian Inference ( SVBI ) CONJUNCTION model ensembles. Stochastic Variational Bayesian Inference ( SVBI ) HYPONYM-OF approaches. temperature scaling HYPONYM-OF approaches. model ensembles HYPONYM-OF approaches. predicted error rates CONJUNCTION actual error rates. actual error rates CONJUNCTION predicted error rates. calibration error FEATURE-OF methods. Metric is accuracy. Task are Predictive uncertainty, and PREDICTIVE UNCERTAINTY. Generic are models, and measures. Method are post hoc calibration method, machine learning model, and Uncertainty estimates. Material is corrupted data. OtherScientificTerm are class membership probabilities, model outputs, pmax, and Brier score. ","This paper proposes a post hoc calibration method for model calibration on OOD data. The proposed method is based on outlier exposure to model probabilities on the corrupted data. Predictive uncertainty is defined as the difference between the class membership probabilities of the model outputs and the true model outputs. The paper proposes two measures to measure this uncertainty: (1) the Brier score, which is a measure of the distance between the predicted error rates and the actual error rates of a machine learning model, and (2) the Pareto frontier, which measures the gap between the true and predicted uncertainty of the models.   The paper shows that under certain conditions, a baseline method for measuring predictive uncertainty based on the softmax probabilities of a model can be used to calibrate the model. Uncertainty estimates are obtained by comparing the estimates of class membership probability for a given class to the model predictions. The authors also show that the calibration error of the proposed methods is a function of the number of times that the model has been trained on a corrupted data, and that models trained on the same corrupted data tend to have higher calibration error than models trained with a different number of classes.  Experiments are conducted on three different approaches: model ensembles, Stochastic Variational Bayesian Inference (SVB) for deep learning, and temperature scaling. The results show that PREDICTIVE UNCERTAINTY is the best measure of calibration error, and the authors show that their methods achieve better calibration error.","This paper proposes a post hoc calibration method for model calibration on OOD data. The proposed method is based on outlier exposure to model probabilities on the corrupted data. Predictive uncertainty is defined as the difference between the class membership probabilities of the model outputs and the true model outputs. The paper proposes two measures to measure this uncertainty: (1) the Brier score, which is a measure of the distance between the predicted error rates and the actual error rates of a machine learning model, and (2) the Pareto frontier, which measures the gap between the true and predicted uncertainty of the models.   The paper shows that under certain conditions, a baseline method for measuring predictive uncertainty based on the softmax probabilities of a model can be used to calibrate the model. Uncertainty estimates are obtained by comparing the estimates of class membership probability for a given class to the model predictions. The authors also show that the calibration error of the proposed methods is a function of the number of times that the model has been trained on a corrupted data, and that models trained on the same corrupted data tend to have higher calibration error than models trained with a different number of classes.  Experiments are conducted on three different approaches: model ensembles, Stochastic Variational Bayesian Inference (SVB) for deep learning, and temperature scaling. The results show that PREDICTIVE UNCERTAINTY is the best measure of calibration error, and the authors show that their methods achieve better calibration error."
513,SP:ea503f67e38fce7dee9cc4996b55b8959911f030,Graph neural networks CONJUNCTION graph kernels. graph kernels CONJUNCTION Graph neural networks. graph kernels USED-FOR machine learning problems. Graph neural networks USED-FOR machine learning problems. graphs USED-FOR machine learning problems. graph neural networks CONJUNCTION graph kernels. graph kernels CONJUNCTION graph neural networks. approaches USED-FOR graph properties. approaches USED-FOR non - isomorphic graphs. graph representations USED-FOR similarity / distance of graphs. graph neural networks CONJUNCTION graph kernels. graph kernels CONJUNCTION graph neural networks. expressive power EVALUATE-FOR graph kernels. expressive power FEATURE-OF graph neural networks. algorithms COMPARE those. those COMPARE algorithms. graph representations and similarities COMPARE those. those COMPARE graph representations and similarities. algorithms USED-FOR graph representations and similarities. models CONJUNCTION kernels. kernels CONJUNCTION models. node attributes USED-FOR kernels. node attributes USED-FOR models. graph kernels COMPARE graph neural networks. graph neural networks COMPARE graph kernels. ,"Graph neural networks and graph kernels are commonly used in machine learning problems on graphs. Graph neural networks, graph kernels, and non-isomorphic graphs can be seen as special cases of these two approaches to graph properties.   This paper studies the expressive power of graph neural networks (and graph kernels) on graphs and shows that graph representations for the similarity/distance of graphs are more expressive than those of graph kernels. The paper also shows that existing approaches can be extended to non-Isomorphic graphs.  The authors also show that algorithms for learning graph representations and similarities are more powerful than those for graph kernels and show that models trained on node attributes are able to learn more expressive models and kernels. ","Graph neural networks and graph kernels are commonly used in machine learning problems on graphs. Graph neural networks, graph kernels, and non-isomorphic graphs can be seen as special cases of these two approaches to graph properties.   This paper studies the expressive power of graph neural networks (and graph kernels) on graphs and shows that graph representations for the similarity/distance of graphs are more expressive than those of graph kernels. The paper also shows that existing approaches can be extended to non-Isomorphic graphs.  The authors also show that algorithms for learning graph representations and similarities are more powerful than those for graph kernels and show that models trained on node attributes are able to learn more expressive models and kernels. "
522,SP:0cf7b7d929f50e0b7f4fda5e1f68e5ade2f7c29b,"inpainting USED-FOR warping artifacts. data augmentation techniques USED-FOR regularizing non - warp - based image generation. them USED-FOR image animation. difficulty of inpainting FEATURE-OF warped image. CutMix HYPONYM-OF data augmentation techniques. PriorityCut USED-FOR image animation. augmentation approach USED-FOR image animation. PriorityCut HYPONYM-OF augmentation approach. low - level similarity CONJUNCTION keypoint distance. keypoint distance CONJUNCTION low - level similarity. keypoint distance CONJUNCTION feature embedding distance. feature embedding distance CONJUNCTION keypoint distance. pixel - wise difference CONJUNCTION low - level similarity. low - level similarity CONJUNCTION pixel - wise difference. PriorityCut COMPARE vanilla CutMix. vanilla CutMix COMPARE PriorityCut. PriorityCut COMPARE image animation models. image animation models COMPARE PriorityCut. PriorityCut USED-FOR identity. vanilla CutMix COMPARE image animation models. image animation models COMPARE vanilla CutMix. low - level similarity EVALUATE-FOR image animation models. pixel - wise difference EVALUATE-FOR image animation models. inpainting USED-FOR warping artifacts. PriorityCut USED-FOR regularize discriminator predictions. occlusion information USED-FOR regularize discriminator predictions. regularize discriminator predictions USED-FOR inpainting. occlusion information USED-FOR image animation. occlusion information USED-FOR PriorityCut. Method are Image animation, Self - supervised image animation approaches, self - supervised image animation approaches, and Warp - based image animation. OtherScientificTerm are motion of a driving video, pose references, motion of the driving video, pose differences, guidance, inpainted regions, motion of the driving image, smooth transitions, and mixture of context. Task is learning. ","This paper proposes a new method for self-supervised image-to-image (i.e., i.e. i.i.d. inpainting) image generation. The idea is to regularize the image generation process to avoid warping artifacts.   Image animation is an important problem in many real-world applications where the goal is to capture the motion of a driving video in a way that is consistent with the pose references of the driving vehicle.  Self-supervision is often used to train a generative model that can be used for this purpose.  This paper proposes to use data augmentation techniques such as CutMix for regularizing non-warp-based image generation, and then apply them to the task of image animation. CutMix is an augmentation approach for image animation that augments the original image with an image that has been warped by warping.  The main contribution of this paper is to propose a method called PriorityCut, which augments an image with a warped version of the input image. The key idea of the proposed method is to use the information from the inpainted regions of the image to guide the generation of a new image from the warped image.  In order to achieve this, the authors propose to use a combination of two existing data augmentations: (1) a modification of CutMix, which is based on the idea of using the information of the inpainted regions as guidance, and (2) the use of a modification to CutMix to make it more robust to warping, which can be seen as a way to mitigate the difficulty of inpainter of a warped image due to the warping artifact.  Results show that the proposed PriorityCut outperforms vanilla CutMix and other image animation models in terms of pixel-wise difference, low-level similarity, keypoint distance, feature embedding distance, and the ability to learn smooth transitions. The paper also shows that inpainteing is able to mitigate warping by using occlusion information for regularize discriminator predictions.  Overall, the paper is well-written, well-motivated, and easy to follow. The learning is thorough and the experiments are well-designed. The results are interesting.  However, there are a few concerns:   1. The authors do not provide sufficient analysis of their method.  2. They do not compare to other self supervised image animation approaches.  3. They only compare their method to a number of state-of-","This paper proposes a new method for self-supervised image-to-image (i.e., i.e. i.i.d. inpainting) image generation. The idea is to regularize the image generation process to avoid warping artifacts.   Image animation is an important problem in many real-world applications where the goal is to capture the motion of a driving video in a way that is consistent with the pose references of the driving vehicle.  Self-supervision is often used to train a generative model that can be used for this purpose.  This paper proposes to use data augmentation techniques such as CutMix for regularizing non-warp-based image generation, and then apply them to the task of image animation. CutMix is an augmentation approach for image animation that augments the original image with an image that has been warped by warping.  The main contribution of this paper is to propose a method called PriorityCut, which augments an image with a warped version of the input image. The key idea of the proposed method is to use the information from the inpainted regions of the image to guide the generation of a new image from the warped image.  In order to achieve this, the authors propose to use a combination of two existing data augmentations: (1) a modification of CutMix, which is based on the idea of using the information of the inpainted regions as guidance, and (2) the use of a modification to CutMix to make it more robust to warping, which can be seen as a way to mitigate the difficulty of inpainter of a warped image due to the warping artifact.  Results show that the proposed PriorityCut outperforms vanilla CutMix and other image animation models in terms of pixel-wise difference, low-level similarity, keypoint distance, feature embedding distance, and the ability to learn smooth transitions. The paper also shows that inpainteing is able to mitigate warping by using occlusion information for regularize discriminator predictions.  Overall, the paper is well-written, well-motivated, and easy to follow. The learning is thorough and the experiments are well-designed. The results are interesting.  However, there are a few concerns:   1. The authors do not provide sufficient analysis of their method.  2. They do not compare to other self supervised image animation approaches.  3. They only compare their method to a number of state-of-"
531,SP:60b535fc6cbc1a7a26ad53f706ebb17de346dc4f,"approaches USED-FOR disentangled representations. data generation process USED-FOR independent latent variables. independent causal mechanisms ( ICM ) COMPARE disentangled representations. disentangled representations COMPARE independent causal mechanisms ( ICM ). coarse granularity FEATURE-OF data generation processes ( mechanisms ). observational data USED-FOR groundtruth mechanisms. unconventional mixture prior USED-FOR self - supervised generative model. self - supervised generative model USED-FOR mechanisms. mechanisms PART-OF self - supervised scenario. intervention CONJUNCTION covariant shift. covariant shift CONJUNCTION intervention. covariant shift CONJUNCTION noise. noise CONJUNCTION covariant shift. downstream tasks EVALUATE-FOR approach. approach COMPARE disentangled representations. disentangled representations COMPARE approach. approach USED-FOR intervention. covariant shift USED-FOR approach. noise EVALUATE-FOR approach. downstream tasks EVALUATE-FOR disentangled representations. downstream tasks EVALUATE-FOR approach. Generic are model, and methods. OtherScientificTerm is disentanglement. ","This paper proposes a new approach to learn disentangled representations. The authors propose a self-supervised approach to disentangle independent causal mechanisms (ICM) from disentanglement. The key idea is to learn mechanisms that are independent of the data generation process and independent of independent latent variables in the model.   The authors show that existing approaches for learning disentangling representations do not work well because of the coarse granularity of data generation processes (mechanisms). The authors also show that the groundtruth mechanisms can be learned from observational data.  The proposed approach is based on an unconventional mixture prior that is used to train a self -supervised generative model to learn such mechanisms in a self supervised generative scenario.  Experiments are conducted on several downstream tasks and show that this approach is able to learn an intervention, a covariant shift, and noise to improve the performance of the proposed approach on downstream tasks compared to the state-of-the-art methods. ","This paper proposes a new approach to learn disentangled representations. The authors propose a self-supervised approach to disentangle independent causal mechanisms (ICM) from disentanglement. The key idea is to learn mechanisms that are independent of the data generation process and independent of independent latent variables in the model.   The authors show that existing approaches for learning disentangling representations do not work well because of the coarse granularity of data generation processes (mechanisms). The authors also show that the groundtruth mechanisms can be learned from observational data.  The proposed approach is based on an unconventional mixture prior that is used to train a self -supervised generative model to learn such mechanisms in a self supervised generative scenario.  Experiments are conducted on several downstream tasks and show that this approach is able to learn an intervention, a covariant shift, and noise to improve the performance of the proposed approach on downstream tasks compared to the state-of-the-art methods. "
540,SP:44d4e24428d043a69b40013919cda0e8e7bff99c,"2D image USED-FOR molecular graph structure ( W ). graph aligning approach USED-FOR rich or detailed labels. 2D images USED-FOR chemical compound graphs. domain adaptation COMPARE pretrained model. pretrained model COMPARE domain adaptation. domain adaptation USED-FOR model. Maybridge data set EVALUATE-FOR self - labeling approach. Task are machine learning, and predicting chemical compound graphs. Method are mediating representation V, and machine learning model. OtherScientificTerm are f, normal labels W, fully mediating layer, and mediating layer. ","This paper proposes a self-labeling approach for chemical compound graph representation learning. The authors propose a graph aligning approach to generate rich or detailed labels for each molecule in a molecular graph structure (W) from a 2D image, which is then used to train a mediating representation V, where f is the number of molecules in the molecule, and W is the molecule's chemical graph structure.  The authors show that the proposed approach is able to learn chemical compound graphs from 2D images with normal labels W, and that domain adaptation outperforms a pretrained model. The self-labelling approach is evaluated on the Maybridge data set, and is shown to be able to achieve state-of-the-art performance.    The main contribution of the paper is that the authors propose to use a fully mediating layer, where each molecule is partitioned into a set of molecules, and each molecule’s molecule is represented by a fully-connected layer, and a machine learning model is trained to predict the label of each molecule. ","This paper proposes a self-labeling approach for chemical compound graph representation learning. The authors propose a graph aligning approach to generate rich or detailed labels for each molecule in a molecular graph structure (W) from a 2D image, which is then used to train a mediating representation V, where f is the number of molecules in the molecule, and W is the molecule's chemical graph structure.  The authors show that the proposed approach is able to learn chemical compound graphs from 2D images with normal labels W, and that domain adaptation outperforms a pretrained model. The self-labelling approach is evaluated on the Maybridge data set, and is shown to be able to achieve state-of-the-art performance.    The main contribution of the paper is that the authors propose to use a fully mediating layer, where each molecule is partitioned into a set of molecules, and each molecule’s molecule is represented by a fully-connected layer, and a machine learning model is trained to predict the label of each molecule. "
549,SP:ad906dd9a176cffd283593321ff6b9ad19595528,domain knowledge based deep learning framework USED-FOR chiller plants energy optimization problems. image classification CONJUNCTION NLP. NLP CONJUNCTION image classification. deep network USED-FOR realworld physical systems. NLP HYPONYM-OF deep learning. image classification HYPONYM-OF deep learning. methods USED-FOR complex systems. methods USED-FOR linear model. linear model USED-FOR complex systems. deep network USED-FOR nonlinear model. domain knowledge USED-FOR deep network. domain knowledge USED-FOR nonlinear model. redundancy function space FEATURE-OF nonlinear model. domain knowledge USED-FOR small sample size problem. energy consumption estimation FEATURE-OF chillers. input - output monotonic problem USED-FOR energy consumption estimation. monotonic constraints FEATURE-OF Neural Network. framework COMPARE ones. ones COMPARE framework. framework USED-FOR energy optimization. method COMPARE ones. ones COMPARE method. data center FEATURE-OF cooling system. cooling system EVALUATE-FOR method. ,"This paper proposes a domain knowledge based deep learning framework for solving chiller plants energy optimization problems. The authors show that a deep network trained on realworld physical systems (e.g. image classification, NLP, etc.) can be used to learn a nonlinear model based on domain knowledge in the redundancy function space. They show that the proposed methods can be applied to complex systems, where previous methods can only learn a linear model for complex systems. They also show that domain knowledge can be leveraged to solve a small sample size problem, and that the energy consumption estimation of chillers can be solved as an input-output monotonic problem.    The paper also shows that a Neural Network with monotony constraints can be trained to solve the input-outline monotonicity problem. The proposed method is evaluated on a cooling system in a data center, and compared to existing ones for energy optimization, and the proposed framework outperforms existing ones. ","This paper proposes a domain knowledge based deep learning framework for solving chiller plants energy optimization problems. The authors show that a deep network trained on realworld physical systems (e.g. image classification, NLP, etc.) can be used to learn a nonlinear model based on domain knowledge in the redundancy function space. They show that the proposed methods can be applied to complex systems, where previous methods can only learn a linear model for complex systems. They also show that domain knowledge can be leveraged to solve a small sample size problem, and that the energy consumption estimation of chillers can be solved as an input-output monotonic problem.    The paper also shows that a Neural Network with monotony constraints can be trained to solve the input-outline monotonicity problem. The proposed method is evaluated on a cooling system in a data center, and compared to existing ones for energy optimization, and the proposed framework outperforms existing ones. "
558,SP:6cb65ee5d2926858570601eeeade24fe86c7f32f,retail forecasting CONJUNCTION urban traffic forecasting. urban traffic forecasting CONJUNCTION retail forecasting. weather forecasts CONJUNCTION retail forecasting. retail forecasting CONJUNCTION weather forecasts. spatio - temporal predictions USED-FOR real - world applications. spatio - temporal predictions USED-FOR large - scale systems. weather forecasts HYPONYM-OF real - world applications. urban traffic forecasting HYPONYM-OF real - world applications. retail forecasting HYPONYM-OF real - world applications. methods USED-FOR predicting variables. interpretability EVALUATE-FOR forecasting models. methods USED-FOR forecasting models. collaborative causal spatio - temporal fusion transformer USED-FOR collaborative causal effects of predictors. collaborative causal effects of predictors USED-FOR forecasting targets. CausalTrans HYPONYM-OF collaborative causal spatio - temporal fusion transformer. causal attention USED-FOR causal inference. nodes PART-OF graph. Taylor ’s expansion CONJUNCTION softmax. softmax CONJUNCTION Taylor ’s expansion. time complexity EVALUATE-FOR multi - head attention. softmax USED-FOR multi - head attention. Taylor ’s expansion USED-FOR multi - head attention. time efficiency EVALUATE-FOR CausalTrans. model components USED-FOR CausalTrans. time efficiency EVALUATE-FOR model components. error reduction EVALUATE-FOR baseline methods. CausalTrans framework COMPARE baseline methods. baseline methods COMPARE CausalTrans framework. error reduction EVALUATE-FOR CausalTrans framework. Material is ride - sharing platforms. Method is spatial graph fusion mechanism. ,"This paper proposes a new method for spatio-temporal forecasting for large-scale forecasting models. The authors propose a new model called CausalTrans, which is based on the idea of causal attention. The idea is to use a graph-based attention mechanism to capture the causal effects between two nodes in a graph, and then use a multi-head attention with Taylor’s expansion and softmax to reduce the time complexity of the attention. Experiments show that the proposed method outperforms existing baselines in terms of accuracy and time efficiency.","This paper proposes a new method for spatio-temporal forecasting for large-scale forecasting models. The authors propose a new model called CausalTrans, which is based on the idea of causal attention. The idea is to use a graph-based attention mechanism to capture the causal effects between two nodes in a graph, and then use a multi-head attention with Taylor’s expansion and softmax to reduce the time complexity of the attention. Experiments show that the proposed method outperforms existing baselines in terms of accuracy and time efficiency."
567,SP:223980a1954d626d90ff54d8dc61b5d85a6b349c,"unsupervised framework USED-FOR problem. coupled mixture VAE ( cpl - mixVAE ) HYPONYM-OF unsupervised framework. interacting autoencoding agents USED-FOR unsupervised framework. variational inference problem USED-FOR it. categorical assignments EVALUATE-FOR approach. MNIST and dSprites EVALUATE-FOR approach. approach USED-FOR type - specific, activity - regulated genes. type - specific, activity - regulated genes PART-OF single - cell gene expression dataset. cortical neuron types FEATURE-OF single - cell gene expression dataset. single - cell gene expression dataset EVALUATE-FOR approach. OtherScientificTerm are mixture of discrete and continuous factors of variability, and continuous factors. Method are mixture representations, and multi - agent framework. ","This paper proposes an unsupervised framework called the coupled mixture VAE (cpl-mixVAE) which is an extension of the existing VAE framework to the problem of learning a mixture of discrete and continuous factors of variability. The authors propose to use interacting autoencoding agents to solve this problem. The idea is that the mixture of continuous factors can be represented as a set of mixture representations, and that it can be treated as a variational inference problem.  The proposed approach is evaluated on categorical assignments on MNIST and dSprites, and it is shown that the proposed approach can identify type-specific, activity-regulated genes in a single-cell gene expression dataset with cortical neuron types. The multi-agent framework is also shown to be effective. ","This paper proposes an unsupervised framework called the coupled mixture VAE (cpl-mixVAE) which is an extension of the existing VAE framework to the problem of learning a mixture of discrete and continuous factors of variability. The authors propose to use interacting autoencoding agents to solve this problem. The idea is that the mixture of continuous factors can be represented as a set of mixture representations, and that it can be treated as a variational inference problem.  The proposed approach is evaluated on categorical assignments on MNIST and dSprites, and it is shown that the proposed approach can identify type-specific, activity-regulated genes in a single-cell gene expression dataset with cortical neuron types. The multi-agent framework is also shown to be effective. "
576,SP:c982610ad28662c3bd13132abe1f7307d1a61b68,"Group equivariant convolutional networks ( GCNNs ) USED-FOR convolutional networks. symmetry priors FEATURE-OF convolutional networks. convolutions USED-FOR models. equivariance constraint FEATURE-OF kernels. G - steerable kernels USED-FOR convolutions. G HYPONYM-OF compact group. constraints FEATURE-OF steerable kernels. constraints CONJUNCTION spherical tensor operators. spherical tensor operators CONJUNCTION constraints. steerable kernels CONJUNCTION spherical tensor operators. spherical tensor operators CONJUNCTION steerable kernels. quantum mechanics USED-FOR spherical tensor operators. generalized reduced matrix elements CONJUNCTION ClebschGordan coefficients. ClebschGordan coefficients CONJUNCTION generalized reduced matrix elements. ClebschGordan coefficients CONJUNCTION harmonic basis functions. harmonic basis functions CONJUNCTION ClebschGordan coefficients. Wigner - Eckart theorem USED-FOR spherical tensor operators. homogeneous spaces FEATURE-OF harmonic basis functions. generalized reduced matrix elements USED-FOR steerable kernel spaces. Method is GCNNs. OtherScientificTerm are G - steerability constraint, and Gsteerable kernel spaces. Generic is it. ","This paper studies Group equivariant convolutional networks (GCNNs) for learning convolutions with symmetry priors. The authors show that the equivariance constraint on the kernels of G-steerable kernels of convolutions can be used to define models that are G-stakeable, i.e. a compact group of groups. The paper also shows that there exist constraints on steerable kernels and spherical tensor operators based on quantum mechanics.   The authors also show that steerable kernel spaces can be obtained from generalized reduced matrix elements, ClebschGordan coefficients, and harmonic basis functions in homogeneous spaces via the Wigner-Eckart theorem.  The main contribution of this paper is to show that GCNNs satisfy the G-Steerability constraint, and that it can be seen as a special case of Gsteerability.","This paper studies Group equivariant convolutional networks (GCNNs) for learning convolutions with symmetry priors. The authors show that the equivariance constraint on the kernels of G-steerable kernels of convolutions can be used to define models that are G-stakeable, i.e. a compact group of groups. The paper also shows that there exist constraints on steerable kernels and spherical tensor operators based on quantum mechanics.   The authors also show that steerable kernel spaces can be obtained from generalized reduced matrix elements, ClebschGordan coefficients, and harmonic basis functions in homogeneous spaces via the Wigner-Eckart theorem.  The main contribution of this paper is to show that GCNNs satisfy the G-Steerability constraint, and that it can be seen as a special case of Gsteerability."
585,SP:7b2ea39069277ad0f4f79476a77ef84587a804d9,"it USED-FOR accuracy disparities. average accuracies EVALUATE-FOR selective classification. accuracy EVALUATE-FOR selective classification. selective classification USED-FOR full - coverage accuracy disparities. models EVALUATE-FOR selective classification. full - coverage accuracies EVALUATE-FOR distributionally - robust models. Method is Selective classification. OtherScientificTerm are abstentions, spurious correlations, margin distribution, symmetric margin distributions, and left - log - concavity. Material is vision and NLP datasets. Metric is accuracies. Generic is distribution. ","Selective classification is a popular technique to avoid abstentions due to spurious correlations between classes. However, it has been shown that it can be problematic due to accuracy disparities. This paper studies the issue of average accuracies of selective classification, which is a measure of the accuracy of a classifier on a subset of classes.    Selective classification can be seen as a way to reduce the gap between the average accuracy of the classifier and the average classifier of the entire class. The paper shows that selective classification can reduce the full-covering accuracy disparities between classes in both vision and NLP datasets.  The paper also shows that, under certain conditions, selective classification on models that are distributionally-robust (i.e. have symmetric margin distributions) can achieve better accuracies than models that do not have this distribution. In particular, the authors show that the margin distribution is symmetric if and only if there is a left-log-concavity between the margin and the true margin distribution. The authors also show that such a distribution can be used to obtain better full-coverage accuracies for distributionally -robust models.","Selective classification is a popular technique to avoid abstentions due to spurious correlations between classes. However, it has been shown that it can be problematic due to accuracy disparities. This paper studies the issue of average accuracies of selective classification, which is a measure of the accuracy of a classifier on a subset of classes.    Selective classification can be seen as a way to reduce the gap between the average accuracy of the classifier and the average classifier of the entire class. The paper shows that selective classification can reduce the full-covering accuracy disparities between classes in both vision and NLP datasets.  The paper also shows that, under certain conditions, selective classification on models that are distributionally-robust (i.e. have symmetric margin distributions) can achieve better accuracies than models that do not have this distribution. In particular, the authors show that the margin distribution is symmetric if and only if there is a left-log-concavity between the margin and the true margin distribution. The authors also show that such a distribution can be used to obtain better full-coverage accuracies for distributionally -robust models."
594,SP:f1d57ee27e901daf7e4e2b84139019e945818911,multi - layer network analysis CONJUNCTION temporal document classification. temporal document classification CONJUNCTION multi - layer network analysis. temporal document classification CONJUNCTION video data analysis. video data analysis CONJUNCTION temporal document classification. topic modeling USED-FOR applications. complex multi - modal structure FEATURE-OF applications. complex multi - modal structure FEATURE-OF topic modeling. complex multi - modal structure FEATURE-OF large - scale data. latent hierarchical structure FEATURE-OF multi - modal data. large - scale data USED-FOR topic modeling. video data analysis HYPONYM-OF applications. multi - layer network analysis HYPONYM-OF applications. temporal document classification HYPONYM-OF applications. Neural NCPD USED-FOR hierarchical topic modeling. Neural NCPD HYPONYM-OF training method. multi - modal tensor data USED-FOR hierarchical topic modeling. neural network architecture CONJUNCTION backpropagation. backpropagation CONJUNCTION neural network architecture. backpropagation USED-FOR error propagation. hierarchical NCPD USED-FOR error propagation. neural network architecture USED-FOR Neural NCPD. backpropagation USED-FOR Neural NCPD. ,"This paper studies the problem of topic modeling on large-scale data with complex multi-modal structure in applications such as multi-layer network analysis, temporal document classification, and video data analysis. The authors propose a new training method called Neural NCPD, which is a training method for hierarchical topic modeling with multi- modal tensor data with a latent hierarchical structure.    The authors show that the neural network architecture and backpropagation for error propagation in hierarchical NCPD is similar to the one used in previous work. ","This paper studies the problem of topic modeling on large-scale data with complex multi-modal structure in applications such as multi-layer network analysis, temporal document classification, and video data analysis. The authors propose a new training method called Neural NCPD, which is a training method for hierarchical topic modeling with multi- modal tensor data with a latent hierarchical structure.    The authors show that the neural network architecture and backpropagation for error propagation in hierarchical NCPD is similar to the one used in previous work. "
603,SP:b6ddc3a560aa7155e7e927bf5360bedc36586597,"node classification CONJUNCTION image segmentation. image segmentation CONJUNCTION node classification. graph CONJUNCTION image. image CONJUNCTION graph. image segmentation CONJUNCTION named - entity recognition. named - entity recognition CONJUNCTION image segmentation. classifier USED-FOR tasks. named - entity recognition HYPONYM-OF tasks. node classification HYPONYM-OF tasks. image segmentation HYPONYM-OF tasks. adversarial robustness certificates USED-FOR tasks. locality property USED-FOR collective certificate. single - node certificates PART-OF collective certificate. locality property CONJUNCTION perturbations. perturbations CONJUNCTION locality property. collective certificate USED-FOR node classification. Citeseer dataset EVALUATE-FOR collective certificate. OtherScientificTerm are perturbed inputs, and perturbation. Method are collective robustness certificate, and Graph Neural Networks. ",This paper proposes a new adversarial robustness certificate for graph neural networks (GNNs) based on the idea that adversarial perturbations can be applied to any node in the graph. The authors show that the proposed certificate can be used to certify that a GNN classifier is robust to adversarial attacks. They also show that this certificate is more robust than existing ones.   ,This paper proposes a new adversarial robustness certificate for graph neural networks (GNNs) based on the idea that adversarial perturbations can be applied to any node in the graph. The authors show that the proposed certificate can be used to certify that a GNN classifier is robust to adversarial attacks. They also show that this certificate is more robust than existing ones.   
612,SP:cc93dd2f68e415e2457166e78627865dc1b44697,"generative models USED-FOR complex real - world data. Generative Adversarial Networks ( GANs ) USED-FOR complex real - world data. Generative Adversarial Networks ( GANs ) HYPONYM-OF generative models. generative and discriminative neural networks USED-FOR Learning high - dimensional probability distributions. non - convergence problem CONJUNCTION mode collapse. mode collapse CONJUNCTION non - convergence problem. mode collapse CONJUNCTION gradient explosion or vanishing. gradient explosion or vanishing CONJUNCTION mode collapse. non - convergence problem FEATURE-OF GANs. Least Squares GAN ( LSGANs ) CONJUNCTION Wasserstein GANs ( WGAN ). Wasserstein GANs ( WGAN ) CONJUNCTION Least Squares GAN ( LSGANs ). Wasserstein GANs ( WGAN ) HYPONYM-OF GANs. Least Squares GAN ( LSGANs ) HYPONYM-OF GANs. LSGANs USED-FOR mode collapse. quantile regression USED-FOR 1 - Wasserstein distance. modification of loss functions USED-FOR GANs. approach USED-FOR modification of loss functions. approach USED-FOR GANs. quantile regression USED-FOR Quantile Regression GAN ( QRGAN ). discriminator CONJUNCTION gradients. gradients CONJUNCTION discriminator. QRGAN USED-FOR mode collapse problem. robustness EVALUATE-FOR QRGAN. QRGAN COMPARE GANs. GANs COMPARE QRGAN. generation performance assessment EVALUATE-FOR GANs. evaluation EVALUATE-FOR Frechet Inception Distance ( FID ). Frechet Inception Distance ( FID ) USED-FOR generation performance assessment. Frechet Inception Distance ( FID ) EVALUATE-FOR GANs. Frechet Inception Distance ( FID ) EVALUATE-FOR QRGAN. generation performance assessment EVALUATE-FOR QRGAN. evaluation EVALUATE-FOR QRGAN. Method are modification methodology of loss functions, WGANs, and Wasserstein distance approximation. OtherScientificTerm are local minima, inefficient computation, and real and generated data distribution. ","Generative Adversarial Networks (GANs) are a class of generative models that can be applied to complex real-world data. Learning high-dimensional probability distributions is an important problem for both generative and discriminative neural networks. However, existing GANs (Least Squares GAN (LSGANs), WGANs, WGAN) suffer from the non-convergence problem, mode collapse, gradient explosion or vanishing. This paper proposes a modification methodology of loss functions to alleviate these issues. Specifically, the authors propose to use quantile regression to approximate the 1-Wasserstein distance between the discriminator and the gradients of the generated and target samples. The authors show that LSGANs avoid mode collapse and LSGAN can avoid local minima, but that the mode collapse problem of LSGAN is not alleviated by the proposed modification methodology. They also show that the proposed approach to modify loss functions of GAN's can be used to improve the robustness of QRGAN (Quantile Regression GAN, QRGAN). QRGAN is a modification of the original QRGAN, which is a GAN with a Wasserstein-based loss function. QRGAN can be seen as an efficient way to mitigate the nonconvexity of the Wassersteins, which allows QRGAN to be more robust to mode collapse without inefficient computation. The paper also shows that QRGAN improves the generation performance assessment using the Frechet Inception Distance (FID) of the real and generated data distribution.    The main contribution of this paper is the modification of loss function of WGAN and QRGAN. The modification methodology is based on WGAN, and the authors provide a theoretical analysis that shows that the WGAN is more robust than QRGAN in terms of mode collapse. ","Generative Adversarial Networks (GANs) are a class of generative models that can be applied to complex real-world data. Learning high-dimensional probability distributions is an important problem for both generative and discriminative neural networks. However, existing GANs (Least Squares GAN (LSGANs), WGANs, WGAN) suffer from the non-convergence problem, mode collapse, gradient explosion or vanishing. This paper proposes a modification methodology of loss functions to alleviate these issues. Specifically, the authors propose to use quantile regression to approximate the 1-Wasserstein distance between the discriminator and the gradients of the generated and target samples. The authors show that LSGANs avoid mode collapse and LSGAN can avoid local minima, but that the mode collapse problem of LSGAN is not alleviated by the proposed modification methodology. They also show that the proposed approach to modify loss functions of GAN's can be used to improve the robustness of QRGAN (Quantile Regression GAN, QRGAN). QRGAN is a modification of the original QRGAN, which is a GAN with a Wasserstein-based loss function. QRGAN can be seen as an efficient way to mitigate the nonconvexity of the Wassersteins, which allows QRGAN to be more robust to mode collapse without inefficient computation. The paper also shows that QRGAN improves the generation performance assessment using the Frechet Inception Distance (FID) of the real and generated data distribution.    The main contribution of this paper is the modification of loss function of WGAN and QRGAN. The modification methodology is based on WGAN, and the authors provide a theoretical analysis that shows that the WGAN is more robust than QRGAN in terms of mode collapse. "
621,SP:4ddb47ee77c374ae6c3e419412d92ca77260692e,relevance metrics USED-FOR similarity - based explanation. cosine similarity FEATURE-OF gradients. Method is machine learning models. Generic is metrics. ,This paper studies the relevance metrics for similarity-based explanation for machine learning models. The authors show that the cosine similarity between gradients of two samples of the same class is a relevant metric. They also show that these metrics can be used as a proxy for other relevant metrics.  ,This paper studies the relevance metrics for similarity-based explanation for machine learning models. The authors show that the cosine similarity between gradients of two samples of the same class is a relevant metric. They also show that these metrics can be used as a proxy for other relevant metrics.  
630,SP:6c2cbf2bc0f6dabe974e80ec1e82d2d12189906e,"generalization power EVALUATE-FOR Graph Neural Networks ( GNNs ). algorithmic alignment USED-FOR graph isomorphism test. LRGA module PART-OF GNNs. LRGA USED-FOR it. sample complexity EVALUATE-FOR kernel ’s feature map. 2 - FWL update step USED-FOR RGNN. LRGA USED-FOR RGNN. randomly initialized two - layer MLP USED-FOR RGNN. randomly initialized two - layer MLP USED-FOR kernel ’s feature map. polynomial kernels USED-FOR RGNN. polynomial kernels USED-FOR 2 - FWL update step. LRGA USED-FOR GNN layers. LRGA USED-FOR GNN architectures. Method are dot - product attention, and expressive GNNs. OtherScientificTerm is generalization properties. Generic is kernel. Material is GNN benchmarks. ","Graph Neural Networks (GNNs) have been shown to have good generalization power. However, their generalization properties are not well-studied. In this paper, the authors propose a new graph isomorphism test based on algorithmic alignment. Specifically, they introduce an LRGA module in GNNs, and show that it can be used to improve the sample complexity of the kernel’s feature map. The authors also show that the 2-FWL update step of an RGNN with polynomial kernels with LRGA is equivalent to a randomly initialized two-layer MLP, and that the LRGA can be applied to any GNN layers.   The authors show that LRGA improves the generalization performance of existing GNN architectures. In particular, they show that dot-product attention improves the sample efficiency of a kernel, which improves the performance of GNN benchmarks. They also demonstrate that the expressive performance of LRGA does not depend on the number of layers, but rather on the size of the network. ","Graph Neural Networks (GNNs) have been shown to have good generalization power. However, their generalization properties are not well-studied. In this paper, the authors propose a new graph isomorphism test based on algorithmic alignment. Specifically, they introduce an LRGA module in GNNs, and show that it can be used to improve the sample complexity of the kernel’s feature map. The authors also show that the 2-FWL update step of an RGNN with polynomial kernels with LRGA is equivalent to a randomly initialized two-layer MLP, and that the LRGA can be applied to any GNN layers.   The authors show that LRGA improves the generalization performance of existing GNN architectures. In particular, they show that dot-product attention improves the sample efficiency of a kernel, which improves the performance of GNN benchmarks. They also demonstrate that the expressive performance of LRGA does not depend on the number of layers, but rather on the size of the network. "
639,SP:b4abdd28504b4c1de239eabd4e0e27d370efee71,"objectness measures USED-FOR calibration. objectness measures USED-FOR Convolutional Neural Networks ( CNNs ). calibration EVALUATE-FOR Convolutional Neural Networks ( CNNs ). loss functions USED-FOR classification CNNs. CNNs USED-FOR classifiers. transformation USED-FOR CNN. random crops USED-FOR approaches. Context dependence FEATURE-OF safety - critical applications. objectness CONJUNCTION label smoothing. label smoothing CONJUNCTION objectness. approach USED-FOR classification. label smoothing USED-FOR training. objectness USED-FOR approach. label smoothing USED-FOR approach. relative object size USED-FOR smoothing factor. approach USED-FOR confidences. adaptive label smoothing USED-FOR CNNs. approach COMPARE baselines. baselines COMPARE approach. MS COCO COMPARE hard label approach. hard label approach COMPARE MS COCO. transfer learning USED-FOR MS COCO. transfer learning COMPARE hard label approach. hard label approach COMPARE transfer learning. Generic are they, and methods. Material are ImageNet-1 K, ImageNet, and context only images. Method is class activation maps. Task is classification and transfer learning tasks. ","This paper proposes to use objectness measures to improve the calibration performance of Convolutional Neural Networks (CNNs) on a variety of objectness-based classification tasks. The authors propose two methods to do so. The first method, called adaptive label smoothing, is based on the observation that the class activation maps of classifiers trained on CNNs trained on ImageNet-1K are highly sensitive to changes in the objectness of the training images. The second approach, called MS COCO, uses objectness to adjust the loss functions of classification CNNs. Both approaches are based on random crops, but they differ in the way that they are applied. Context dependence in safety-critical applications is a key factor in the two approaches. The proposed approach is applied to both classification and transfer learning tasks. In classification, the authors use the relative object size as a smoothing factor, and in transfer learning, they use the classifier to adaptively adjust the smoothing of the loss function. They show that the proposed approach outperforms the baselines on the classification task and outperforms a hard label approach on transfer learning. They also show that their approach can improve the confidences of CNNs on classification and learning on the transfer learning task.    The paper is well-written and well-motivated. However, there is a lack of comparison between the proposed methods. In particular, the contribution of this paper is limited to the context only images. ","This paper proposes to use objectness measures to improve the calibration performance of Convolutional Neural Networks (CNNs) on a variety of objectness-based classification tasks. The authors propose two methods to do so. The first method, called adaptive label smoothing, is based on the observation that the class activation maps of classifiers trained on CNNs trained on ImageNet-1K are highly sensitive to changes in the objectness of the training images. The second approach, called MS COCO, uses objectness to adjust the loss functions of classification CNNs. Both approaches are based on random crops, but they differ in the way that they are applied. Context dependence in safety-critical applications is a key factor in the two approaches. The proposed approach is applied to both classification and transfer learning tasks. In classification, the authors use the relative object size as a smoothing factor, and in transfer learning, they use the classifier to adaptively adjust the smoothing of the loss function. They show that the proposed approach outperforms the baselines on the classification task and outperforms a hard label approach on transfer learning. They also show that their approach can improve the confidences of CNNs on classification and learning on the transfer learning task.    The paper is well-written and well-motivated. However, there is a lack of comparison between the proposed methods. In particular, the contribution of this paper is limited to the context only images. "
648,SP:5254658923e594294b69d124a8d004166852822a,"Neural networks USED-FOR inverse problems. convex dual network USED-FOR interpreting training and prediction. convex solvers USED-FOR convex dual network. neural networks USED-FOR path sparsity. weight decay regularization FEATURE-OF neural networks. piecewise linear filtering USED-FOR prediction. MNIST and fastMRI datasets EVALUATE-FOR dual network optimization problem. Task is medical imaging. Method are convex duality framework, and convex optimization. ","This paper studies the duality of neural networks for inverse problems in medical imaging. In particular, the authors consider the convex duality framework and show that neural networks with weight decay regularization can be seen as convex solvers for solving the dual network optimization problem. The authors also show that the neural networks trained with path sparsity can be used to solve the inverse problems. Finally, the paper shows that the dual dual network can be viewed as a convex optimization of a conveX dual network for interpreting training and prediction. The main contribution of the paper is the application of piecewise linear filtering to the problem of prediction. Experiments are conducted on MNIST and fastMRI datasets to validate the theoretical findings.","This paper studies the duality of neural networks for inverse problems in medical imaging. In particular, the authors consider the convex duality framework and show that neural networks with weight decay regularization can be seen as convex solvers for solving the dual network optimization problem. The authors also show that the neural networks trained with path sparsity can be used to solve the inverse problems. Finally, the paper shows that the dual dual network can be viewed as a convex optimization of a conveX dual network for interpreting training and prediction. The main contribution of the paper is the application of piecewise linear filtering to the problem of prediction. Experiments are conducted on MNIST and fastMRI datasets to validate the theoretical findings."
657,SP:085cad6bc143c8713580bddfaa71f06496dac314,processing stages PART-OF text - to - speech synthesis pipelines. models USED-FOR raw speech audio outputs. character or phoneme input sequences USED-FOR models. generator USED-FOR inference. generator USED-FOR training. training CONJUNCTION inference. inference CONJUNCTION training. token length prediction USED-FOR differentiable alignment scheme. differentiable alignment scheme USED-FOR generator. adversarial feedback CONJUNCTION prediction losses. prediction losses CONJUNCTION adversarial feedback. total duration CONJUNCTION mel - spectrogram. mel - spectrogram CONJUNCTION total duration. prediction losses USED-FOR It. adversarial feedback USED-FOR It. soft dynamic time warping USED-FOR spectrogram - based prediction loss. soft dynamic time warping USED-FOR model. model COMPARE models. models COMPARE model. mean opinion score EVALUATE-FOR model. multi - stage training USED-FOR models. OtherScientificTerm is normalised text or phonemes. ,This paper proposes a novel approach to improve the performance of text-to-speech (TTS) models. The authors propose to use a differentiable alignment scheme to align the output of the generator during training and inference. They also propose a soft dynamic time warping to improve spectrogram-based prediction loss. Experiments show that the proposed approach outperforms the state-of-the-art in terms of mean opinion score. ,This paper proposes a novel approach to improve the performance of text-to-speech (TTS) models. The authors propose to use a differentiable alignment scheme to align the output of the generator during training and inference. They also propose a soft dynamic time warping to improve spectrogram-based prediction loss. Experiments show that the proposed approach outperforms the state-of-the-art in terms of mean opinion score. 
666,SP:01148cea55db606aa78d27e900818684a8bce9ab,"attributed graphs FEATURE-OF real - world graphs. non - topological features FEATURE-OF nodes. attributes PART-OF attributed graph. lower - dimensional space FEATURE-OF discrete distributions. Wasserstein metric USED-FOR lower - dimensional space. Wasserstein metric USED-FOR discrete distributions. Wasserstein graph diffusion USED-FOR distribution representations of nodes. topology structure CONJUNCTION attributes. attributes CONJUNCTION topology structure. point representations USED-FOR downstream tasks. it USED-FOR node classification. algorithms USED-FOR node classification. algorithms USED-FOR matrix completion. node classification CONJUNCTION matrix completion. matrix completion CONJUNCTION node classification. it USED-FOR matrix completion. algorithms EVALUATE-FOR representation method. missing attributes USED-FOR node classification. it USED-FOR algorithms. Method are node representation learning approaches, and non - parametric framework. OtherScientificTerm are incomplete information, decomposition of the attribute matrix, node features, Wasserstein space, and local neighborhoods. Metric is distortion. ","This paper proposes a non-parametric representation learning method for node attributes in real-world graphs. The proposed method is based on the Wasserstein distance between nodes in a graph. The authors show that the distance between two nodes in the graph can be approximated by the WASSERstein distance, which is a lower-dimensional space that can be used to represent discrete distributions. The paper also shows that the proposed method can be applied to node classification and matrix completion tasks. ","This paper proposes a non-parametric representation learning method for node attributes in real-world graphs. The proposed method is based on the Wasserstein distance between nodes in a graph. The authors show that the distance between two nodes in the graph can be approximated by the WASSERstein distance, which is a lower-dimensional space that can be used to represent discrete distributions. The paper also shows that the proposed method can be applied to node classification and matrix completion tasks. "
675,SP:aeeb5909f7123ef631f569b469af9715205c881f,"Adversarially Motivated Intrinsic GOals USED-FOR goal - conditioned “ student ” policy. AMIGO HYPONYM-OF agent. meta - learning USED-FOR agent. goal - generating teacher PART-OF agent. intrinsic motivation CONJUNCTION RL methods. RL methods CONJUNCTION intrinsic motivation. Task are reinforcement learning ( RL ), and procedurally - generated tasks. OtherScientificTerm are sparse extrinsic rewards, environment reward, and constructively adversarial ” objective. Generic is method. ","This paper proposes a new method for reinforcement learning (RL) in the presence of sparse extrinsic rewards. The authors propose an Adversarially Motivated Intrinsic GOals to learn a goal-conditioned “student” policy. The agent, called AMIGO, is trained using meta-learning, where the agent is trained with a “goal-generating teacher” that is trained to maximize the likelihood of the agent achieving the goal. The teacher is trained in a similar way to the way that the agent was trained in prior work, but with an additional “constructively adversarial” objective that encourages the agent to achieve goals that are more likely to be generated by the agent than the environment reward. Experiments are conducted on procedurally-generated tasks, and show that the proposed method outperforms prior work in terms of intrinsic motivation and RL methods.","This paper proposes a new method for reinforcement learning (RL) in the presence of sparse extrinsic rewards. The authors propose an Adversarially Motivated Intrinsic GOals to learn a goal-conditioned “student” policy. The agent, called AMIGO, is trained using meta-learning, where the agent is trained with a “goal-generating teacher” that is trained to maximize the likelihood of the agent achieving the goal. The teacher is trained in a similar way to the way that the agent was trained in prior work, but with an additional “constructively adversarial” objective that encourages the agent to achieve goals that are more likely to be generated by the agent than the environment reward. Experiments are conducted on procedurally-generated tasks, and show that the proposed method outperforms prior work in terms of intrinsic motivation and RL methods."
684,SP:3d05bc7dca97681cb582298e318b9b973841eed3,"user distortion CONJUNCTION user privacy constraint. user privacy constraint CONJUNCTION user distortion. dataset of files USED-FOR information retrieval. distortion FEATURE-OF retrieval process. private information retrieval USED-FOR model. download rate CONJUNCTION distortion. distortion CONJUNCTION download rate. distortion CONJUNCTION user privacy leakage. user privacy leakage CONJUNCTION distortion. mutual information FEATURE-OF information - theoretical formulation. download rate EVALUATE-FOR schemes. generative adversarial models USED-FOR data - driven framework. constrained minimax game USED-FOR scheme. download rate CONJUNCTION distortion. distortion CONJUNCTION download rate. synthetic Gaussian dataset CONJUNCTION MNIST and CIFAR-10 datasets. MNIST and CIFAR-10 datasets CONJUNCTION synthetic Gaussian dataset. MNIST and CIFAR-10 datasets EVALUATE-FOR scheme. synthetic Gaussian dataset EVALUATE-FOR scheme. MNIST dataset EVALUATE-FOR data - driven approach. data - driven approach COMPARE achievable scheme. achievable scheme COMPARE data - driven approach. MNIST dataset EVALUATE-FOR achievable scheme. source coding USED-FOR achievable scheme. OtherScientificTerm are privacy level, perfect privacy requirement, and distortion constraint. Metric are rate - distortion - leakage tradeoff, and rate - distortion tradeoff curve. Material is CIFAR-10. ","This paper considers the problem of information retrieval from a dataset of files with a certain privacy level, where the goal is to minimize the rate-distortion-leakage tradeoff between the user distortion and the user privacy constraint. The paper proposes a data-driven framework based on generative adversarial models, where a model is trained with private information retrieval and the retrieval process is constrained to be free of distortion. The authors show that the perfect privacy requirement is satisfied if the mutual information of the information-theoretical formulation of mutual information between the data and the distortion constraint is equal to the true mutual information. They also show that there exist schemes that achieve a trade-off between a good download rate, distortion, and user privacy leakage. The proposed scheme is based on a constrained minimax game, and is evaluated on a synthetic Gaussian dataset, the MNIST and CIFAR-10 datasets. They show that their scheme achieves better trade-offs in terms of both the number of samples required to achieve a good tradeoff, and also shows that the rate -distortion tradeoff curve converges to the optimal tradeoff. Finally, they show that an achievable scheme based on source coding is also shown to achieve better tradeoffs between the desired trade-of-download rate and distortion.    The authors also demonstrate that the proposed data -driven approach outperforms the achievable scheme on the synthetic gaussian dataset and MNIST dataset, and outperforms an achievable approach based on the data from the source coding. They do not evaluate their scheme on Cifar-10, but show that they are able to match the performance of the achievable approach.","This paper considers the problem of information retrieval from a dataset of files with a certain privacy level, where the goal is to minimize the rate-distortion-leakage tradeoff between the user distortion and the user privacy constraint. The paper proposes a data-driven framework based on generative adversarial models, where a model is trained with private information retrieval and the retrieval process is constrained to be free of distortion. The authors show that the perfect privacy requirement is satisfied if the mutual information of the information-theoretical formulation of mutual information between the data and the distortion constraint is equal to the true mutual information. They also show that there exist schemes that achieve a trade-off between a good download rate, distortion, and user privacy leakage. The proposed scheme is based on a constrained minimax game, and is evaluated on a synthetic Gaussian dataset, the MNIST and CIFAR-10 datasets. They show that their scheme achieves better trade-offs in terms of both the number of samples required to achieve a good tradeoff, and also shows that the rate -distortion tradeoff curve converges to the optimal tradeoff. Finally, they show that an achievable scheme based on source coding is also shown to achieve better tradeoffs between the desired trade-of-download rate and distortion.    The authors also demonstrate that the proposed data -driven approach outperforms the achievable scheme on the synthetic gaussian dataset and MNIST dataset, and outperforms an achievable approach based on the data from the source coding. They do not evaluate their scheme on Cifar-10, but show that they are able to match the performance of the achievable approach."
693,SP:3f9e2db00fc3dcd7a40588adcb638503ec10dc09,"Graph Neural Networks ( GNNs ) USED-FOR graph - related applications. they USED-FOR large scale settings. fidelity FEATURE-OF model. fidelity EVALUATE-FOR sampling - based methods. decoupled greedy learning method USED-FOR GNNs ( DGL - GNN ). greedy auxiliary objectives USED-FOR module. method USED-FOR time or memory limited applications. efficiency EVALUATE-FOR method. sampling - based acceleration COMPARE model. model COMPARE sampling - based acceleration. efficiency CONJUNCTION accuracy. accuracy CONJUNCTION efficiency. decoupled approach COMPARE methods. methods COMPARE decoupled approach. sampling PART-OF GNN training. sampling HYPONYM-OF it. OtherScientificTerm are node embeddings, and GNN layers. Method are GNN, lazy - update scheme, and DGL - GNN model. Generic are modules, and approach. Task is parallel GNN training. ","Graph Neural Networks (GNNs) have been widely used in graph-related applications, but they are not well suited for large scale settings. This paper proposes a decoupled greedy learning method for GNNs (DGL-GNN) that decouples the node embeddings of the GNN into two modules. The first module uses greedy auxiliary objectives, and the second module uses a lazy-update scheme. The authors show that the proposed method can be used for time or memory limited applications, and that the fidelity of the model is improved over sampling-based methods in terms of fidelity. The paper also shows that the sampling in GNN training (i.e. the sampling of a node embedding) can be decomposed into two parts: (1) the sampling that is performed at each layer of GNN layers, and (2) the number of layers that are updated at each iteration.   The authors also show that their method is able to achieve better efficiency and accuracy compared to previous methods. The main contribution of the paper is the decoupling of the two modules and the use of a lazy update scheme. They also show how the proposed approach can be applied to parallel GNN learning, and show that it can be combined with sampling in order to improve the efficiency of the DGL- GNN model. ","Graph Neural Networks (GNNs) have been widely used in graph-related applications, but they are not well suited for large scale settings. This paper proposes a decoupled greedy learning method for GNNs (DGL-GNN) that decouples the node embeddings of the GNN into two modules. The first module uses greedy auxiliary objectives, and the second module uses a lazy-update scheme. The authors show that the proposed method can be used for time or memory limited applications, and that the fidelity of the model is improved over sampling-based methods in terms of fidelity. The paper also shows that the sampling in GNN training (i.e. the sampling of a node embedding) can be decomposed into two parts: (1) the sampling that is performed at each layer of GNN layers, and (2) the number of layers that are updated at each iteration.   The authors also show that their method is able to achieve better efficiency and accuracy compared to previous methods. The main contribution of the paper is the decoupling of the two modules and the use of a lazy update scheme. They also show how the proposed approach can be applied to parallel GNN learning, and show that it can be combined with sampling in order to improve the efficiency of the DGL- GNN model. "
702,SP:5ecb1b288f7fc02aead4493f81640867bc349290,Neural link predictors USED-FOR missing edges. missing edges PART-OF large scale Knowledge Graphs. logical conjunctions ( ∧ ) CONJUNCTION disjunctions. disjunctions CONJUNCTION logical conjunctions ( ∧ ). disjunctions CONJUNCTION existential quantifiers. existential quantifiers CONJUNCTION disjunctions. framework USED-FOR complex queries. incomplete Knowledge Graphs USED-FOR complex queries. solutions USED-FOR optimisation problem. gradient - based and combinatorial search HYPONYM-OF optimisation problem. gradient - based and combinatorial search HYPONYM-OF solutions. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. state - of - the - art methods COMPARE neural models. neural models COMPARE state - of - the - art methods. approach COMPARE neural models. neural models COMPARE approach. Hits@3 EVALUATE-FOR knowledge graphs. factual information FEATURE-OF knowledge graphs. intermediate solutions USED-FOR complex query atoms. intermediate solutions USED-FOR model. Generic is models. OtherScientificTerm is end - to - end differentiable objective. Method is neural link predictor. ,"Neural link predictors are used to predict missing edges in large scale Knowledge Graphs. The paper proposes a framework to solve complex queries in incomplete Knowledge Graph with missing edges. The authors consider logical conjunctions (√ll), disjunctions, and existential quantifiers. The proposed models are trained end-to-end, and the authors propose two solutions to the optimisation problem: gradient-based and combinatorial search. The approach is compared to state-of-the-art methods and neural models, and is shown to outperform the neural models on Hits@3. The model is trained with intermediate solutions to complex query atoms, which is an end- to-end differentiable objective. The neural link predictor is trained to predict the existence of missing edges, and then the model is fine-tuned on the intermediate solutions. ","Neural link predictors are used to predict missing edges in large scale Knowledge Graphs. The paper proposes a framework to solve complex queries in incomplete Knowledge Graph with missing edges. The authors consider logical conjunctions (√ll), disjunctions, and existential quantifiers. The proposed models are trained end-to-end, and the authors propose two solutions to the optimisation problem: gradient-based and combinatorial search. The approach is compared to state-of-the-art methods and neural models, and is shown to outperform the neural models on Hits@3. The model is trained with intermediate solutions to complex query atoms, which is an end- to-end differentiable objective. The neural link predictor is trained to predict the existence of missing edges, and then the model is fine-tuned on the intermediate solutions. "
711,SP:f04a522fd04c503754fdb8c52da68646d31271a4,"procedure USED-FOR local robustness. procedure USED-FOR feed - forward neural networks. local robustness FEATURE-OF feed - forward neural networks. piecewise - linear activation functions FEATURE-OF feed - forward neural networks. decision boundaries USED-FOR assessing robustness. highly - parallel GPU implementation USED-FOR ` 2 norm. approach COMPARE approximate verification approaches. approximate verification approaches COMPARE approach. approximate verification approaches COMPARE verifiers. verifiers COMPARE approximate verification approaches. approach COMPARE verifiers. verifiers COMPARE approach. Task is Local robustness. Generic are model, networks, network, and algorithm. OtherScientificTerm are ` p - ball consistently, adversarial inputs, convex polyhedral regions, and geometric projections. Metric is robustness. ","This paper proposes a new procedure for verifying local robustness of feed-forward neural networks with piecewise-linear activation functions. Local robustness is defined as the ability of a model to be robust to adversarial perturbations to a `p-ball consistently (i.e., the number of times an input is perturbed in a certain region of the input space). The paper proposes to use decision boundaries as a metric for assessing robustness. The paper shows that the `2 norm of the decision boundary can be computed in a highly-parallel GPU implementation, and that the robustness can be verified in the presence of adversarial inputs that lie in convex polyhedral regions. The proposed approach is compared to other approximate verification approaches and verifiers, and is shown to be more robust than existing verifiers. The authors also show that their algorithm is computationally tractable. ","This paper proposes a new procedure for verifying local robustness of feed-forward neural networks with piecewise-linear activation functions. Local robustness is defined as the ability of a model to be robust to adversarial perturbations to a `p-ball consistently (i.e., the number of times an input is perturbed in a certain region of the input space). The paper proposes to use decision boundaries as a metric for assessing robustness. The paper shows that the `2 norm of the decision boundary can be computed in a highly-parallel GPU implementation, and that the robustness can be verified in the presence of adversarial inputs that lie in convex polyhedral regions. The proposed approach is compared to other approximate verification approaches and verifiers, and is shown to be more robust than existing verifiers. The authors also show that their algorithm is computationally tractable. "
720,SP:5297651ff873f97c07b9c47ed3eff52251661844,"approach USED-FOR embedding of objects. affordance space FEATURE-OF embedding of objects. embedding COMPARE approaches. approaches COMPARE embedding. dimensions USED-FOR mental representation of objects. human judgements of object similarity USED-FOR mental representation of objects. Generic are knowledge, and they. OtherScientificTerm are object “ affordance ”, and human judgments of affordance. Material is text corpora. ","This paper proposes an approach to learn the embedding of objects in the affordance space, i.e., the knowledge of an object’s “affordance”. This embedding is different from existing approaches in that they do not rely on human judgements of object affordance, but rather on text corpora. The authors show that this embedding can be learned in a similar way as existing approaches, and that it can be used to learn a mental representation of objects across different dimensions. They also show that their embedding performs well on a number of datasets, and can be combined with existing methods to learn representations that are more interpretable to humans. ","This paper proposes an approach to learn the embedding of objects in the affordance space, i.e., the knowledge of an object’s “affordance”. This embedding is different from existing approaches in that they do not rely on human judgements of object affordance, but rather on text corpora. The authors show that this embedding can be learned in a similar way as existing approaches, and that it can be used to learn a mental representation of objects across different dimensions. They also show that their embedding performs well on a number of datasets, and can be combined with existing methods to learn representations that are more interpretable to humans. "
729,SP:72b4f3b40c6c6fa2eb53e95ed9a10a4077ffa049,"Individuality USED-FOR human society. efficiency CONJUNCTION productivity. productivity CONJUNCTION efficiency. It USED-FOR division of labor. efficiency EVALUATE-FOR It. productivity EVALUATE-FOR It. it USED-FOR multi - agent cooperation. method USED-FOR emergence of individuality ( EOI ). emergence of individuality ( EOI ) PART-OF multi - agent reinforcement learning ( MARL ). probabilistic classifier USED-FOR probability distribution. EOI USED-FOR probabilistic classifier. regularizers USED-FOR classifier. intrinsic reward USED-FOR emergence of individuality. regularizers USED-FOR emergence of individuality. MARL algorithms USED-FOR EOI. EOI COMPARE methods. methods COMPARE EOI. multi - agent cooperative scenarios EVALUATE-FOR methods. multi - agent cooperative scenarios EVALUATE-FOR EOI. OtherScientificTerm are individuality, and intrinsic reward signals. ","This paper proposes a method for the emergence of individuality (EOI) in multi-agent reinforcement learning (MARL). Individuality is important for human society. It improves efficiency and productivity. It can be used for the division of labor, and it can also be used in the context of multi-agents cooperation. The authors propose a probabilistic classifier based on EOI to predict the probability distribution of the agent’s individualities. They also propose two regularizers to improve the performance of the classifier. Finally, they propose to use intrinsic reward as an intrinsic reward to encourage emergence of individualism in MARL algorithms. Experiments show that the proposed methods outperform existing methods in several multi- agent cooperative scenarios.   ","This paper proposes a method for the emergence of individuality (EOI) in multi-agent reinforcement learning (MARL). Individuality is important for human society. It improves efficiency and productivity. It can be used for the division of labor, and it can also be used in the context of multi-agents cooperation. The authors propose a probabilistic classifier based on EOI to predict the probability distribution of the agent’s individualities. They also propose two regularizers to improve the performance of the classifier. Finally, they propose to use intrinsic reward as an intrinsic reward to encourage emergence of individualism in MARL algorithms. Experiments show that the proposed methods outperform existing methods in several multi- agent cooperative scenarios.   "
738,SP:112509d6d3573a9d495d182fdfae6ec0327cddf5,"certified robustness EVALUATE-FOR Randomized smoothing. base classifier USED-FOR randomized smoothing. Smoothed WEighted ENsembling ( SWEEN ) scheme USED-FOR randomized smoothed classifiers. SWEEN USED-FOR optimal certified robustness. adaptive prediction algorithm USED-FOR SWEEN models. adaptive prediction algorithm USED-FOR prediction and certification cost. SWEEN models COMPARE candidate models. candidate models COMPARE SWEEN models. training time EVALUATE-FOR SWEEN models. small models USED-FOR SWEEN models. OtherScientificTerm are l2 - norm adversarial attacks, and ensembling generality. Method is SWEEN model. ","Randomized smoothing has been shown to improve certified robustness against l2-norm adversarial attacks. However, randomized smoothing requires the base classifier to be trained on the entire training set. The authors propose the Smoothed WEighted ENsembling (SWEEN) scheme to train randomized smoothed classifiers. SWEEN is able to achieve optimal certified robustess in the presence of ensembling generality. The main contribution of the paper is the adaptive prediction algorithm to reduce the prediction and certification cost of the SWEen models. Experiments show that the SweEN models with small models can achieve competitive performance in terms of training time and certification performance.    The paper is well-written and well-motivated, and the authors have done a good job of explaining the benefits of the proposed adaptive prediction method. ","Randomized smoothing has been shown to improve certified robustness against l2-norm adversarial attacks. However, randomized smoothing requires the base classifier to be trained on the entire training set. The authors propose the Smoothed WEighted ENsembling (SWEEN) scheme to train randomized smoothed classifiers. SWEEN is able to achieve optimal certified robustess in the presence of ensembling generality. The main contribution of the paper is the adaptive prediction algorithm to reduce the prediction and certification cost of the SWEen models. Experiments show that the SweEN models with small models can achieve competitive performance in terms of training time and certification performance.    The paper is well-written and well-motivated, and the authors have done a good job of explaining the benefits of the proposed adaptive prediction method. "
747,SP:ea892e3d199ed6121279b20061a87f43afae8796,hierarchical structures USED-FOR learning process. hierarchical structures USED-FOR generalization. Ordered Memory Policy Network ( OMPN ) USED-FOR subtask hierarchy. subtask hierarchy USED-FOR task decomposition. subtask boundaries PART-OF unstructured demonstration. Craft CONJUNCTION Dial. Dial CONJUNCTION Craft. model COMPARE baselines. baselines COMPARE model. Craft EVALUATE-FOR model. Dial EVALUATE-FOR model. task decomposition EVALUATE-FOR model. unsupervised and weakly supervised settings EVALUATE-FOR model. OMPN USED-FOR partially observable environments. task decomposition EVALUATE-FOR OMPN. subtask hierarchy PART-OF model. Task is complex real - world tasks. OtherScientificTerm is inductive bias. ,"This paper proposes to use hierarchical structures in the learning process to improve generalization to complex real-world tasks. Specifically, the authors propose Ordered Memory Policy Network (OMPN) to learn a subtask hierarchy for task decomposition. The subtask boundaries are learned in an unstructured demonstration. The model is evaluated on two tasks (Craft and Dial) and compared with several baselines. The authors show that OMPN performs well in both unsupervised and weakly supervised settings. They also show that the model is able to generalize to partially observable environments where the inductive bias is less strong. Finally, they show that their model can generalize well to a task-agnostic setting, and that the subtask hierarchies learned in the model can be used to improve the performance of the model. ","This paper proposes to use hierarchical structures in the learning process to improve generalization to complex real-world tasks. Specifically, the authors propose Ordered Memory Policy Network (OMPN) to learn a subtask hierarchy for task decomposition. The subtask boundaries are learned in an unstructured demonstration. The model is evaluated on two tasks (Craft and Dial) and compared with several baselines. The authors show that OMPN performs well in both unsupervised and weakly supervised settings. They also show that the model is able to generalize to partially observable environments where the inductive bias is less strong. Finally, they show that their model can generalize well to a task-agnostic setting, and that the subtask hierarchies learned in the model can be used to improve the performance of the model. "
756,SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,semantic factor CONJUNCTION variation factor. variation factor CONJUNCTION semantic factor. deep ones HYPONYM-OF supervised learning methods. methods USED-FOR OOD prediction. causal reasoning USED-FOR Causal Semantic Generative model ( CSG ). learning CONJUNCTION prediction. prediction CONJUNCTION learning. variational Bayes USED-FOR learning. variational Bayes USED-FOR prediction. causal invariance principle USED-FOR methods. CSG USED-FOR semantic factor. semantic - identification USED-FOR adaptation. OOD EVALUATE-FOR baselines. OtherScientificTerm is domain - specific correlation. Metric is OOD generalization error. ,This paper proposes a Causal Semantic Generative model (CSG) for out-of-distribution (OOD) generalization based on causal reasoning. The CSG is a generative model that learns a semantic factor and a variation factor that is invariant to domain-specific correlation. The authors propose two methods for OOD prediction based on the causal invariance principle. The first method is based on variational Bayes for learning and prediction. The second method uses semantic-identification for adaptation. Experiments show that the proposed baselines outperform the baselines on OOD generalization error. ,This paper proposes a Causal Semantic Generative model (CSG) for out-of-distribution (OOD) generalization based on causal reasoning. The CSG is a generative model that learns a semantic factor and a variation factor that is invariant to domain-specific correlation. The authors propose two methods for OOD prediction based on the causal invariance principle. The first method is based on variational Bayes for learning and prediction. The second method uses semantic-identification for adaptation. Experiments show that the proposed baselines outperform the baselines on OOD generalization error. 
765,SP:be3f34a59e5e61dcdbc7cb085f031ba4a5a5b758,"adversarially corrupted rewards FEATURE-OF online learning algorithms. small regret FEATURE-OF algorithms. algorithm USED-FOR corrupted rewards. uncorrupted reward distribution FEATURE-OF regret. robust estimation USED-FOR unsupervised learning problems. stochastic multi - armed bandits CONJUNCTION linear contextual bandits. linear contextual bandits CONJUNCTION stochastic multi - armed bandits. linear contextual bandits CONJUNCTION Markov Decision Processes ( MDPs ). Markov Decision Processes ( MDPs ) CONJUNCTION linear contextual bandits. robust estimation USED-FOR robust online algorithms. robust online algorithms USED-FOR scenarios. stochastic rewards and transitions FEATURE-OF Markov Decision Processes ( MDPs ). near optimal regret FEATURE-OF robust online algorithms. Markov Decision Processes ( MDPs ) HYPONYM-OF scenarios. stochastic multi - armed bandits HYPONYM-OF scenarios. linear contextual bandits HYPONYM-OF scenarios. synthetic and real datasets EVALUATE-FOR algorithms. Method are online algorithm, and online learning. OtherScientificTerm are stochastic reward, and noise rate. ","This paper studies the problem of online learning algorithms with adversarially corrupted rewards. The authors consider the setting where an online algorithm is trained with a stochastic reward, and the goal is to learn an algorithm that is robust to corrupted rewards and has a small regret. In this setting, the authors show that for any algorithm with uncorrupted reward distribution, the regret with respect to the original reward is a function of the noise rate of the algorithm. They also show that robust estimation for unsupervised learning problems can be used to obtain robust online algorithms that have near optimal regret in these scenarios, including the standard scenarios of the so-called ""stochastic multi-armed bandits"", the linear contextual bandits, and Markov Decision Processes (MDPs) with stochedastic rewards and transitions. The algorithms are tested on both synthetic and real datasets, and are shown to be robust to a wide range of noise rates. ","This paper studies the problem of online learning algorithms with adversarially corrupted rewards. The authors consider the setting where an online algorithm is trained with a stochastic reward, and the goal is to learn an algorithm that is robust to corrupted rewards and has a small regret. In this setting, the authors show that for any algorithm with uncorrupted reward distribution, the regret with respect to the original reward is a function of the noise rate of the algorithm. They also show that robust estimation for unsupervised learning problems can be used to obtain robust online algorithms that have near optimal regret in these scenarios, including the standard scenarios of the so-called ""stochastic multi-armed bandits"", the linear contextual bandits, and Markov Decision Processes (MDPs) with stochedastic rewards and transitions. The algorithms are tested on both synthetic and real datasets, and are shown to be robust to a wide range of noise rates. "
774,SP:6d62a80aaebb2988df3953d4d7164e5a2fa1aa6d,"Encoder - decoder architecture USED-FOR neural machine translation ( NMT ). rewriter CONJUNCTION evaluator. evaluator CONJUNCTION rewriter. rewriter PART-OF It. evaluator PART-OF It. evaluator USED-FOR translation quality. rewriter CONJUNCTION evaluator. evaluator CONJUNCTION rewriter. prioritized gradient descent ( PGD ) method USED-FOR rewriter. PGD method USED-FOR Rewriter - Evaluator. framework USED-FOR NMT models. Transformer HYPONYM-OF NMT models. framework COMPARE baselines. baselines COMPARE framework. translation tasks EVALUATE-FOR framework. NMT models COMPARE baselines. baselines COMPARE NMT models. framework USED-FOR NMT models. Chinese - English and English - German HYPONYM-OF translation tasks. Generic is it. OtherScientificTerm is termination policy. Method are RewriterEvaluator, decoding, and encoder - decoder models. Task is rewriting process. ","This paper proposes RewriterEvaluator, an encoder-decoder architecture for neural machine translation (NMT). It consists of a rewriter, an evaluator and a termination policy. The rewriter uses a prioritized gradient descent (PGD) method to guide the rewriter through the rewriting process, and the evaluation policy is based on the termination policy of the Rewriter. The authors show that this framework can improve the performance of existing NMT models (e.g., Transformer) on two translation tasks (Chinese-English and English-German). They also show that the proposed framework outperforms several baselines. ","This paper proposes RewriterEvaluator, an encoder-decoder architecture for neural machine translation (NMT). It consists of a rewriter, an evaluator and a termination policy. The rewriter uses a prioritized gradient descent (PGD) method to guide the rewriter through the rewriting process, and the evaluation policy is based on the termination policy of the Rewriter. The authors show that this framework can improve the performance of existing NMT models (e.g., Transformer) on two translation tasks (Chinese-English and English-German). They also show that the proposed framework outperforms several baselines. "
783,SP:9761fca8848868dfc9cacdab2537f8276ca76e0f,"images CONJUNCTION unsystematic annotation. unsystematic annotation CONJUNCTION images. Ambiguities CONJUNCTION unsystematic annotation. unsystematic annotation CONJUNCTION Ambiguities. Ambiguities CONJUNCTION images. images CONJUNCTION Ambiguities. empirical frequency FEATURE-OF sampled predictions. two - stage, cascaded strategy USED-FOR calibrated adversarial refinement. adversarial network USED-FOR coherent predictions. black - box segmentation framework USED-FOR learning of calibrated stochastic mappings. model USED-FOR learning of calibrated stochastic mappings. model PART-OF black - box segmentation framework. multigrader LIDC dataset CONJUNCTION Cityscapes dataset. Cityscapes dataset CONJUNCTION multigrader LIDC dataset. multigrader LIDC dataset EVALUATE-FOR approach. Cityscapes dataset EVALUATE-FOR approach. core design USED-FOR tasks. framework USED-FOR semantic segmentation. core design USED-FOR calibrated predictive distribution. toy regression dataset EVALUATE-FOR framework. calibrated predictive distribution USED-FOR tasks. OtherScientificTerm are distribution over predictions, empirical distribution, multimodal predictive distribution, categorical likelihood, and calibrated stochastic mappings. Method is probabilistic networks. Generic is these. ","This paper proposes a black-box segmentation framework that incorporates a model for the learning of calibrated stochastic mappings between Ambiguities, images, and unsystematic annotation. The authors propose a two-stage, cascaded strategy for calibrated adversarial refinement, where a distribution over predictions is sampled from a multimodal predictive distribution, and the empirical frequency of the sampled predictions is used as a metric to measure the quality of the calibrated mappings. The proposed approach is evaluated on the multigrader LIDC dataset, the Cityscapes dataset, and on a toy regression dataset. The core design of the proposed core design is to learn a calibrated predictive distribution for tasks where the categorical likelihood is low. The adversarial network is trained to produce coherent predictions, and these are then used to train a model that can be used to refine the calibrated predictions. The framework is applied to the task of semantic segmentation, and is shown to outperform existing methods. ","This paper proposes a black-box segmentation framework that incorporates a model for the learning of calibrated stochastic mappings between Ambiguities, images, and unsystematic annotation. The authors propose a two-stage, cascaded strategy for calibrated adversarial refinement, where a distribution over predictions is sampled from a multimodal predictive distribution, and the empirical frequency of the sampled predictions is used as a metric to measure the quality of the calibrated mappings. The proposed approach is evaluated on the multigrader LIDC dataset, the Cityscapes dataset, and on a toy regression dataset. The core design of the proposed core design is to learn a calibrated predictive distribution for tasks where the categorical likelihood is low. The adversarial network is trained to produce coherent predictions, and these are then used to train a model that can be used to refine the calibrated predictions. The framework is applied to the task of semantic segmentation, and is shown to outperform existing methods. "
792,SP:ce965758f1b795a56c02f45d6a8d06cb8bdf29cb,"distributed compute systems USED-FOR stochastic optimization algorithms. stochastic optimization algorithms USED-FOR large - scale machine learning applications. distributed compute systems USED-FOR large - scale machine learning applications. error feedback ( EF ) USED-FOR compressed communication. Top - K or PowerSGD HYPONYM-OF contractive compressors. EF USED-FOR contractive compressors. alternative USED-FOR contractive compressors. alternative USED-FOR EF. construction USED-FOR contractive compressor. construction USED-FOR induced unbiased compressor. contractive compressor CONJUNCTION induced unbiased compressor. induced unbiased compressor CONJUNCTION contractive compressor. approach COMPARE EF. EF COMPARE approach. reduced memory requirements CONJUNCTION communication complexity guarantees. communication complexity guarantees CONJUNCTION reduced memory requirements. communication complexity guarantees CONJUNCTION assumptions. assumptions CONJUNCTION communication complexity guarantees. partial participation FEATURE-OF federated learning. Generic are systems, and transformation. OtherScientificTerm are communication overhead, stochastic gradients, and unbiased compressors. ","This paper proposes a new algorithm for distributed stochastic optimization (DSO) for large-scale machine learning problems. The proposed algorithm is based on the idea of error feedback (EF), which is used to train an unbiased compressor to compress the gradient of the algorithm. The authors show that EF can be used to improve the communication efficiency of existing algorithms. They also show that the proposed algorithm can be applied to federated learning with partial participation. ","This paper proposes a new algorithm for distributed stochastic optimization (DSO) for large-scale machine learning problems. The proposed algorithm is based on the idea of error feedback (EF), which is used to train an unbiased compressor to compress the gradient of the algorithm. The authors show that EF can be used to improve the communication efficiency of existing algorithms. They also show that the proposed algorithm can be applied to federated learning with partial participation. "
801,SP:4fd702490293e481c79614852ba27dd3ce9215a4,"hyperparameter optimization ( HPO ) USED-FOR HPO. HT - AA baseline algorithms CONJUNCTION benchmarks. benchmarks CONJUNCTION HT - AA baseline algorithms. baseline COMPARE HPO algorithm. HPO algorithm COMPARE baseline. HPO PART-OF ML development. python packages USED-FOR baselines. python packages USED-FOR benchmarks. baselines CONJUNCTION benchmarks. benchmarks CONJUNCTION baselines. python packages USED-FOR HT - AA. Method are machine learning ( ML ) algorithm, ML algorithms, and neural architectures. OtherScientificTerm are hyperparameter settings, hyperparameter search space, and hyperparameter search spaces. Generic are approaches, and research framework. ","This paper studies hyperparameter optimization (HPO) in the context of a machine learning (ML) algorithm. HPO is a well-studied problem in ML development, and the authors propose a new baseline for HPO, which is based on the observation that the performance of existing ML algorithms is highly dependent on the hyper parameter settings. The authors propose two approaches to this problem: (1) finding the optimal hyper parameter search space, and (2) learning neural architectures that are well-suited to this search space.  The authors show that the proposed baseline outperforms the existing HPO algorithm on a number of benchmark datasets. The paper also shows that the baselines can be found in python packages that are compatible with existing baselines and benchmarks. Finally, the authors also provide a theoretical analysis of the proposed research framework.   ","This paper studies hyperparameter optimization (HPO) in the context of a machine learning (ML) algorithm. HPO is a well-studied problem in ML development, and the authors propose a new baseline for HPO, which is based on the observation that the performance of existing ML algorithms is highly dependent on the hyper parameter settings. The authors propose two approaches to this problem: (1) finding the optimal hyper parameter search space, and (2) learning neural architectures that are well-suited to this search space.  The authors show that the proposed baseline outperforms the existing HPO algorithm on a number of benchmark datasets. The paper also shows that the baselines can be found in python packages that are compatible with existing baselines and benchmarks. Finally, the authors also provide a theoretical analysis of the proposed research framework.   "
810,SP:e8f99bae5853de525450fcb8facd23cf973fc161,"audio labels COMPARE categorical probabilities. categorical probabilities COMPARE audio labels. image classifier USED-FOR classification. image classifier USED-FOR audio labels. audio labels COMPARE numerical probabilities. numerical probabilities COMPARE audio labels. numerical probabilities CONJUNCTION text. text CONJUNCTION numerical probabilities. audio labels COMPARE text. text COMPARE audio labels. they USED-FOR error signal. spectrograms CONJUNCTION shuffled spectrograms. shuffled spectrograms CONJUNCTION spectrograms. shuffled spectrograms CONJUNCTION Gaussian mixtures. Gaussian mixtures CONJUNCTION shuffled spectrograms. constant matrices CONJUNCTION spectrograms. spectrograms CONJUNCTION constant matrices. Gaussian mixtures CONJUNCTION uniform random matrices. uniform random matrices CONJUNCTION Gaussian mixtures. dimensionalities FEATURE-OF uniform random matrices. uniform random matrices HYPONYM-OF label representations. constant matrices HYPONYM-OF label representations. Gaussian mixtures HYPONYM-OF label representations. shuffled spectrograms HYPONYM-OF label representations. spectrograms HYPONYM-OF label representations. high dimensional, high entropy labels COMPARE text ( categorical ) labels. text ( categorical ) labels COMPARE high dimensional, high entropy labels. robustness EVALUATE-FOR features. image classification task EVALUATE-FOR high dimensional, high entropy labels. image classification task EVALUATE-FOR text ( categorical ) labels. accuracy EVALUATE-FOR high dimensional, high entropy labels. label representations USED-FOR features. OtherScientificTerm are data labels, and adversarial attacks. Generic is models. Method are high dimensional, high entropy label representations, and label representation. ","This paper studies the robustness of high-dimensional, high-entropy label representations. The authors show that audio labels are more robust to adversarial attacks than numerical probabilities and text labels. They also show that the adversarial examples are more likely to be low-dimensional than text labels, and that they are more sensitive to the error signal.    The authors further show that high-dimensionality of the labels is more important for robustness than low dimensionality for classification performance. ","This paper studies the robustness of high-dimensional, high-entropy label representations. The authors show that audio labels are more robust to adversarial attacks than numerical probabilities and text labels. They also show that the adversarial examples are more likely to be low-dimensional than text labels, and that they are more sensitive to the error signal.    The authors further show that high-dimensionality of the labels is more important for robustness than low dimensionality for classification performance. "
819,SP:4e8d924cba7367af0999b30d79250b4dc40413e1,approaches USED-FOR ensemble neural networks. forward passes USED-FOR prediction. forward passes USED-FOR methods. single model ’s capacity USED-FOR subnetworks. CIFAR100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR100. accuracy CONJUNCTION calibration error. calibration error CONJUNCTION accuracy. negative log - likelihood CONJUNCTION accuracy. accuracy CONJUNCTION negative log - likelihood. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. out - of - distribution variants COMPARE methods. methods COMPARE out - of - distribution variants. ImageNet CONJUNCTION out - of - distribution variants. out - of - distribution variants CONJUNCTION ImageNet. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. Generic is network. OtherScientificTerm is forward pass. Metric is model robustness. ,"This paper proposes two approaches to train ensemble neural networks. Both methods rely on forward passes for prediction, where a single model’s capacity is used to partition the network into subnetworks, and each subnetwork is trained independently. The authors show that the forward pass improves the negative log-likelihood, accuracy, and calibration error. They also show that their out-of-distribution variants outperform existing methods in terms of model robustness. Experiments are conducted on CIFAR10, CIFar100, and ImageNet.","This paper proposes two approaches to train ensemble neural networks. Both methods rely on forward passes for prediction, where a single model’s capacity is used to partition the network into subnetworks, and each subnetwork is trained independently. The authors show that the forward pass improves the negative log-likelihood, accuracy, and calibration error. They also show that their out-of-distribution variants outperform existing methods in terms of model robustness. Experiments are conducted on CIFAR10, CIFar100, and ImageNet."
828,SP:d2f1c23b67c6744101034dc5e1c70765a733b169,"teacher network CONJUNCTION student network. student network CONJUNCTION teacher network. method USED-FOR intermediate knowledge. Sparse Representation Matching ( SRM ) USED-FOR intermediate knowledge. Convolutional Neural Network ( CNN ) USED-FOR intermediate knowledge. sparse representation learning USED-FOR Sparse Representation Matching ( SRM ). sparse representation learning USED-FOR method. pixellevel and image - level labels USED-FOR intermediate feature maps. intermediate feature maps PART-OF student network. sparse representations of the hidden features PART-OF teacher CNN. sparse representations of the hidden features USED-FOR SRM. neural processing block USED-FOR SRM. stochastic gradient descent USED-FOR neural processing block. stochastic gradient descent USED-FOR SRM. SRM COMPARE KD techniques. KD techniques COMPARE SRM. Task is Knowledge Distillation. Method are CNN, and teacher and student networks. ","This paper proposes a novel method for Knowledge Distillation, where the intermediate knowledge of a Convolutional Neural Network (CNN) is shared between a teacher network and a student network. The proposed method, Sparse Representation Matching (SRM), is based on sparse representation learning. The intermediate feature maps of the teacher and student networks are learned using pixellevel and image-level labels. SRM uses sparse representations of the hidden features in the teacher CNN and a neural processing block based on stochastic gradient descent. Experiments show that SRM outperforms other KD techniques. ","This paper proposes a novel method for Knowledge Distillation, where the intermediate knowledge of a Convolutional Neural Network (CNN) is shared between a teacher network and a student network. The proposed method, Sparse Representation Matching (SRM), is based on sparse representation learning. The intermediate feature maps of the teacher and student networks are learned using pixellevel and image-level labels. SRM uses sparse representations of the hidden features in the teacher CNN and a neural processing block based on stochastic gradient descent. Experiments show that SRM outperforms other KD techniques. "
837,SP:e8c0f43bd5debf6544f588cd3442dc3dd62d0eee,Reinforcement learning methods USED-FOR policies. sequential structure PART-OF representation learning process. sequential structure PART-OF reinforcement learning. approach COMPARE approaches. approaches COMPARE approach. theoretically motivated policy similarity metric ( PSM ) USED-FOR behavioral similarity. contrastive representation learning procedure USED-FOR state similarity metric. contrastive representation learning procedure USED-FOR policy similarity embeddings ( PSEs1 ). PSM USED-FOR policy similarity embeddings ( PSEs1 ). LQR CONJUNCTION jumping task. jumping task CONJUNCTION LQR. jumping task CONJUNCTION Distracting DM Control Suite. Distracting DM Control Suite CONJUNCTION jumping task. generalization EVALUATE-FOR benchmarks. spurious correlations FEATURE-OF LQR. benchmarks EVALUATE-FOR PSEs. generalization EVALUATE-FOR PSEs. Distracting DM Control Suite HYPONYM-OF benchmarks. jumping task HYPONYM-OF benchmarks. LQR HYPONYM-OF benchmarks. Generic is structure. OtherScientificTerm is optimal policies. ,"This paper proposes a novel approach to learn policies that have a sequential structure in the representation learning process. In contrast to existing reinforcement learning methods that focus on learning policies with a fixed structure, this paper proposes to learn a theoretically motivated policy similarity metric (PSM) that encourages behavioral similarity between the optimal policies. The authors propose a contrastive representation learning procedure to learn policy similarity embeddings (PSEs1) based on the PSM, which can be used as a state similarity metric. Experiments on three benchmarks (LQR, jumping task, and Distracting DM Control Suite) show that PSEs improve the generalization performance of these benchmarks in the presence of spurious correlations in LQR. ","This paper proposes a novel approach to learn policies that have a sequential structure in the representation learning process. In contrast to existing reinforcement learning methods that focus on learning policies with a fixed structure, this paper proposes to learn a theoretically motivated policy similarity metric (PSM) that encourages behavioral similarity between the optimal policies. The authors propose a contrastive representation learning procedure to learn policy similarity embeddings (PSEs1) based on the PSM, which can be used as a state similarity metric. Experiments on three benchmarks (LQR, jumping task, and Distracting DM Control Suite) show that PSEs improve the generalization performance of these benchmarks in the presence of spurious correlations in LQR. "
846,SP:92f3b4942da9075440dda618f561a85f8fde5a5c,"approach USED-FOR disentanglement. latent representation FEATURE-OF model. approach USED-FOR disentanglement. rotations CONJUNCTION translations. translations CONJUNCTION rotations. topological defects FEATURE-OF transformations. images FEATURE-OF transformations. affine transformations HYPONYM-OF transformations. translations HYPONYM-OF affine transformations. rotations HYPONYM-OF affine transformations. approach USED-FOR disentanglement. distributed equivariant operators USED-FOR approach. approach USED-FOR disentangle affine transformations. distributed operators USED-FOR disentanglement. distributed operators USED-FOR models. Task is Machine Learning. OtherScientificTerm are object shape, encoder, and latent space. Generic is factors. Method is group representation theory. ","This paper proposes a new approach to disentanglement in Machine Learning. The approach is based on distributed equivariant operators, where the model is trained on a latent representation of the input object shape. The key idea is to learn disentangled transformations (i.e., affine transformations) that are invariant to topological defects in the input images, e.g., rotations, translations, etc.). The authors show that this approach can disentangle the disentangling effects of a number of different transformations on the input image, including images with different transformations with different deformations. The authors also show that their approach disentangles the effects of disentangler and disentange the encoder and decoder in the latent space.  The authors further show that the proposed approach can be used to train models with distributed operators that can be applied to a large number of transformations, and that the approach can also be used in order to train a model with disentanged representations. Finally, the authors provide a theoretical analysis of the factors that lead to the group representation theory.   ","This paper proposes a new approach to disentanglement in Machine Learning. The approach is based on distributed equivariant operators, where the model is trained on a latent representation of the input object shape. The key idea is to learn disentangled transformations (i.e., affine transformations) that are invariant to topological defects in the input images, e.g., rotations, translations, etc.). The authors show that this approach can disentangle the disentangling effects of a number of different transformations on the input image, including images with different transformations with different deformations. The authors also show that their approach disentangles the effects of disentangler and disentange the encoder and decoder in the latent space.  The authors further show that the proposed approach can be used to train models with distributed operators that can be applied to a large number of transformations, and that the approach can also be used in order to train a model with disentanged representations. Finally, the authors provide a theoretical analysis of the factors that lead to the group representation theory.   "
855,SP:ef0f58c462bc5dd1c7b78f562c42a4e17f0f252b,statistical framework USED-FOR timedependent interaction of neuronal spiking activities. Hawkes process USED-FOR timedependent interaction of neuronal spiking activities. Hawkes process USED-FOR statistical framework. Hawkes process USED-FOR inhibitory interactions. nonlinear Hawkes process USED-FOR influence pattern. excitatory or inhibitory interactions FEATURE-OF influence pattern. latent marked Poisson processes CONJUNCTION sparsity variables. sparsity variables CONJUNCTION latent marked Poisson processes. Pólya - Gamma variables CONJUNCTION latent marked Poisson processes. latent marked Poisson processes CONJUNCTION Pólya - Gamma variables. auxiliary latent variables USED-FOR functional connection weights. Gaussian form FEATURE-OF functional connection weights. analytical updates USED-FOR iterative algorithm. sparsity variables HYPONYM-OF auxiliary latent variables. latent marked Poisson processes HYPONYM-OF auxiliary latent variables. Pólya - Gamma variables HYPONYM-OF auxiliary latent variables. expectationmaximization ( EM ) algorithm USED-FOR maximum a posteriori ( MAP ) estimate. accuracy EVALUATE-FOR algorithm. synthetic and real data EVALUATE-FOR algorithm. algorithm USED-FOR temporal dynamics of interaction. algorithm USED-FOR interpretable functional connectivity. interpretable functional connectivity FEATURE-OF neural spike trains. real neural recordings EVALUATE-FOR algorithm. ,"This paper proposes a statistical framework based on the Hawkes process to study the timedependent interaction of neuronal spiking activities. The authors propose to use nonlinear Hawkes processes to model the influence pattern induced by excitatory or inhibitory interactions. They propose an iterative algorithm to estimate the functional connection weights using auxiliary latent variables (Pólya-Gamma variables, latent marked Poisson processes, and sparsity variables in Gaussian form). They also propose an expectationmaximization (EM) algorithm to obtain a maximum a posteriori (MAP) estimate. They evaluate their algorithm on synthetic and real data and show that their algorithm is able to capture the temporal dynamics of interaction. They also show that the algorithm can capture the interpretable functional connectivity of neural spike trains. ","This paper proposes a statistical framework based on the Hawkes process to study the timedependent interaction of neuronal spiking activities. The authors propose to use nonlinear Hawkes processes to model the influence pattern induced by excitatory or inhibitory interactions. They propose an iterative algorithm to estimate the functional connection weights using auxiliary latent variables (Pólya-Gamma variables, latent marked Poisson processes, and sparsity variables in Gaussian form). They also propose an expectationmaximization (EM) algorithm to obtain a maximum a posteriori (MAP) estimate. They evaluate their algorithm on synthetic and real data and show that their algorithm is able to capture the temporal dynamics of interaction. They also show that the algorithm can capture the interpretable functional connectivity of neural spike trains. "
864,SP:1156d3deac022829bda930ffcb081947609d972b,"gradient descent ( GD ) algorithm USED-FOR two - layer neural network models. under - parameterized regime FEATURE-OF GD dynamics. quenched ” neurons USED-FOR continued activation and deactivation process. quenching - activation process USED-FOR GD. random featurelike behavior FEATURE-OF it. quenching process USED-FOR implicit regularization ”. mean - field ” scaling FEATURE-OF GD dynamics. OtherScientificTerm are parameter regimes, neural network - like behavior, and inner - layer parameters. Method is random feature model. Generic is dynamics. ","This paper studies the gradient descent (GD) algorithm for two-layer neural network models. In the under-parameterized regime, the authors show that the GD dynamics in the parameter regimes where the number of quenched “neural network-like” neurons is small (i.e., in the case of a random feature model) exhibits a neural network - like behavior. The authors also show that GD can be seen as a quenching-activation process, where the “quenched” neuron’s continued activation and deactivation process is the result of the ‘implicit regularization’ of the inner-layer parameters. The paper also shows that GD dynamics with “mean-field” scaling can be explained as a result of this ‘quenching process’, and that it exhibits a “random featurelike behavior”. ","This paper studies the gradient descent (GD) algorithm for two-layer neural network models. In the under-parameterized regime, the authors show that the GD dynamics in the parameter regimes where the number of quenched “neural network-like” neurons is small (i.e., in the case of a random feature model) exhibits a neural network - like behavior. The authors also show that GD can be seen as a quenching-activation process, where the “quenched” neuron’s continued activation and deactivation process is the result of the ‘implicit regularization’ of the inner-layer parameters. The paper also shows that GD dynamics with “mean-field” scaling can be explained as a result of this ‘quenching process’, and that it exhibits a “random featurelike behavior”. "
873,SP:9e81401a6f30c70d870a12cce0cf600557f92b80,"constrained Markov decision process ( CMDP ) problems USED-FOR reinforcement learning problems. model USED-FOR CMDP problem. reconnaissance MDP ( R - MDP ) CONJUNCTION planning MDP ( P - MDP ). planning MDP ( P - MDP ) CONJUNCTION reconnaissance MDP ( R - MDP ). MDPs PART-OF CMDP. planning MDP ( P - MDP ) HYPONYM-OF MDPs. reconnaissance MDP ( R - MDP ) HYPONYM-OF MDPs. threat function CONJUNCTION Q - function analogue of danger. Q - function analogue of danger CONJUNCTION threat function. threat function USED-FOR R - MDP. reward - seeking policy USED-FOR P - MDP. fixed threat function USED-FOR reward - seeking policy. generative model USED-FOR threat function. reward CONJUNCTION danger - constraint. danger - constraint CONJUNCTION reward. threat function USED-FOR baseline policy. reward FEATURE-OF CMDP problems. danger - constraint FEATURE-OF CMDP problems. approximation method USED-FOR R - MDP. approximation method USED-FOR threat function. method COMPARE approaches. approaches COMPARE method. benchmark dataset CONJUNCTION complex collision - free navigation tasks. complex collision - free navigation tasks CONJUNCTION benchmark dataset. complex collision - free navigation tasks EVALUATE-FOR method. complex collision - free navigation tasks EVALUATE-FOR approaches. benchmark dataset EVALUATE-FOR method. benchmark dataset EVALUATE-FOR approaches. OtherScientificTerm are prescribed safety constraints, and state - action pair. ","This paper studies constrained Markov decision process (CMDP) problems for reinforcement learning problems with prescribed safety constraints. The authors propose a new model for the CMDP problem, which is based on the observation that there are two types of MDPs in a CMDP: reconnaissance MDP (R-MDP) and planning MDP(P- MDP). The authors show that the reward-seeking policy in P-MDEs can be approximated by a fixed threat function, and that the threat function is a combination of a threat function and a Q-function analogue of danger. They also show that a baseline policy can be learned from the threat functions.  The authors also propose a generative model to learn the risk function for the R-DP, and show that an approximation method can be used to approximate this threat function for R-MODE.  Finally, the authors demonstrate that the proposed method outperforms existing approaches on a benchmark dataset and complex collision-free navigation tasks. ","This paper studies constrained Markov decision process (CMDP) problems for reinforcement learning problems with prescribed safety constraints. The authors propose a new model for the CMDP problem, which is based on the observation that there are two types of MDPs in a CMDP: reconnaissance MDP (R-MDP) and planning MDP(P- MDP). The authors show that the reward-seeking policy in P-MDEs can be approximated by a fixed threat function, and that the threat function is a combination of a threat function and a Q-function analogue of danger. They also show that a baseline policy can be learned from the threat functions.  The authors also propose a generative model to learn the risk function for the R-DP, and show that an approximation method can be used to approximate this threat function for R-MODE.  Finally, the authors demonstrate that the proposed method outperforms existing approaches on a benchmark dataset and complex collision-free navigation tasks. "
882,SP:f1d4ac7d5516dd0df742e224c8c09c721d0d0886,neural architectures USED-FOR classification tasks. crossentropy loss COMPARE square loss. square loss COMPARE crossentropy loss. crossentropy loss USED-FOR neural architectures. benchmark datasets USED-FOR NLP. automatic speech recognition ( ASR ) CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION automatic speech recognition ( ASR ). neural architectures CONJUNCTION benchmark datasets. benchmark datasets CONJUNCTION neural architectures. neural architectures USED-FOR NLP. benchmark datasets USED-FOR automatic speech recognition ( ASR ). NLP CONJUNCTION automatic speech recognition ( ASR ). automatic speech recognition ( ASR ) CONJUNCTION NLP. hyper - parameter settings USED-FOR architectures. square loss USED-FOR architectures. square loss USED-FOR NLP. Cross - entropy USED-FOR computer vision tasks. square loss USED-FOR classification. cross - entropy FEATURE-OF deep learning. equal footing FEATURE-OF deep learning. OtherScientificTerm is cross - entropy loss. Task is non - vision tasks. ,"This paper proposes a new crossentropy loss for neural architectures for classification tasks, which is more powerful than the square loss used in previous work. The authors show that crossentropies of neural architectures and benchmark datasets for NLP, NLP with automatic speech recognition (ASR) and computer vision tasks, as well as for non-vision tasks, can be obtained with hyper-parameter settings. Cross-entropy can be used for computer vision and NLP tasks, while square loss is used in NLP for classification. The paper also shows that deep learning on equal footing with respect to the cross-entropy of deep learning can be achieved with square loss. ","This paper proposes a new crossentropy loss for neural architectures for classification tasks, which is more powerful than the square loss used in previous work. The authors show that crossentropies of neural architectures and benchmark datasets for NLP, NLP with automatic speech recognition (ASR) and computer vision tasks, as well as for non-vision tasks, can be obtained with hyper-parameter settings. Cross-entropy can be used for computer vision and NLP tasks, while square loss is used in NLP for classification. The paper also shows that deep learning on equal footing with respect to the cross-entropy of deep learning can be achieved with square loss. "
891,SP:915f1f0fc4850507c28c1d609239b41775863ebe,"self - supervised objectives FEATURE-OF reward maximization. exponential moving average USED-FOR encoder. encoder USED-FOR target representations. prior methods USED-FOR sample - efficient deep RL. future prediction objective COMPARE prior methods. prior methods COMPARE future prediction objective. data augmentation USED-FOR future prediction loss. future prediction CONJUNCTION data augmentation. data augmentation CONJUNCTION future prediction. median human - normalized score EVALUATE-FOR self - supervised objective. Atari EVALUATE-FOR self - supervised objective. future prediction PART-OF self - supervised objective. data augmentation PART-OF self - supervised objective. SPR COMPARE expert human scores. expert human scores COMPARE SPR. limited data regime EVALUATE-FOR SPR. Method are deep reinforcement learning, Self - Predictive Representations ( SPR ), and transition model. OtherScientificTerm are limited interaction, latent state representations, agent ’s representations, and environment interaction. Generic are method, and state - of - the - art. ","This paper proposes a self-supervised representation learning method for reinforcement learning. The proposed method, called Self-Predictive Representations (SPR), is based on the idea of self-predictive representations (SPRs), which is a generalization of the Self-Predictive Representation Learning (SPL) framework. The authors show that the proposed method is sample efficient and can be applied to a wide range of RL tasks, including reinforcement learning, reinforcement learning with limited interaction, and reinforcement learning without interaction.","This paper proposes a self-supervised representation learning method for reinforcement learning. The proposed method, called Self-Predictive Representations (SPR), is based on the idea of self-predictive representations (SPRs), which is a generalization of the Self-Predictive Representation Learning (SPL) framework. The authors show that the proposed method is sample efficient and can be applied to a wide range of RL tasks, including reinforcement learning, reinforcement learning with limited interaction, and reinforcement learning without interaction."
900,SP:983f01c170909c8c67fd3be25f121bd61bdd8307,method USED-FOR generating single - node representations. InstantEmbedding USED-FOR generating single - node representations. InstantEmbedding HYPONYM-OF method. local PageRank computations USED-FOR InstantEmbedding. local PageRank computations USED-FOR method. approach USED-FOR globally consistent representations. DeepWalk CONJUNCTION node2vec. node2vec CONJUNCTION DeepWalk. node2vec CONJUNCTION VERSE. VERSE CONJUNCTION node2vec. VERSE CONJUNCTION FastRP. FastRP CONJUNCTION VERSE. InstantEmbedding COMPARE methods. methods COMPARE InstantEmbedding. InstantEmbedding USED-FOR single node ’s embedding. computation time CONJUNCTION memory. memory CONJUNCTION computation time. FastRP HYPONYM-OF methods. DeepWalk HYPONYM-OF methods. VERSE HYPONYM-OF methods. node2vec HYPONYM-OF methods. computation time EVALUATE-FOR InstantEmbedding. node classification CONJUNCTION link prediction. link prediction CONJUNCTION node classification. unsupervised representation learning USED-FOR tasks. unsupervised representation learning USED-FOR node classification. unsupervised representation learning USED-FOR link prediction. method USED-FOR representations. method COMPARE unsupervised representation learning. unsupervised representation learning COMPARE method. link prediction HYPONYM-OF tasks. node classification HYPONYM-OF tasks. social networks CONJUNCTION chemical molecules. chemical molecules CONJUNCTION social networks. chemical molecules CONJUNCTION knowledge graphs. knowledge graphs CONJUNCTION chemical molecules. approach USED-FOR graphs. node PART-OF graph. d - dimensional embedding vector USED-FOR node. compact representations of graphs USED-FOR approach. d - dimensional embedding vector USED-FOR graph. node classification CONJUNCTION link prediction. link prediction CONJUNCTION node classification. visualization CONJUNCTION node classification. node classification CONJUNCTION visualization. Unsupervised embeddings USED-FOR machine learning tasks. visualization HYPONYM-OF machine learning tasks. link prediction HYPONYM-OF machine learning tasks. node classification HYPONYM-OF machine,"This paper proposes a new method called InstantEmbedding for generating single-node representations. The proposed method is based on local PageRank computations, and the authors claim that the proposed approach is able to learn globally consistent representations that can be used for unsupervised representation learning for a wide range of machine learning tasks, including node classification, link prediction, and link prediction. The authors show that the instantiation of the proposed method, which is a simple extension of existing methods such as DeepWalk, node2vec, DeepWalk and VERSE, can achieve comparable performance to existing methods (DeepWalk, FastRP, and FastRP) in terms of computation time and memory.   The authors also show that their approach can be applied to any graphs, and that the single node’s embedding can be learned using Instant Embedding. In particular, the authors propose to learn a d-dimensional embedding vector for each node in a graph, and then use this d-dimensionality to learn the embedding of a single node in the graph. Unsupervised embeddings are then used for a range of different kinds of tasks, such as visualization, node classification (for node classification), link prediction (for link prediction), and node classification for node classification.  The proposed approach can also be used to learn compact representations of graphs. The paper is well-written and easy to follow. ","This paper proposes a new method called InstantEmbedding for generating single-node representations. The proposed method is based on local PageRank computations, and the authors claim that the proposed approach is able to learn globally consistent representations that can be used for unsupervised representation learning for a wide range of machine learning tasks, including node classification, link prediction, and link prediction. The authors show that the instantiation of the proposed method, which is a simple extension of existing methods such as DeepWalk, node2vec, DeepWalk and VERSE, can achieve comparable performance to existing methods (DeepWalk, FastRP, and FastRP) in terms of computation time and memory.   The authors also show that their approach can be applied to any graphs, and that the single node’s embedding can be learned using Instant Embedding. In particular, the authors propose to learn a d-dimensional embedding vector for each node in a graph, and then use this d-dimensionality to learn the embedding of a single node in the graph. Unsupervised embeddings are then used for a range of different kinds of tasks, such as visualization, node classification (for node classification), link prediction (for link prediction), and node classification for node classification.  The proposed approach can also be used to learn compact representations of graphs. The paper is well-written and easy to follow. "
909,SP:d11037b8fe2b10aee672ba82f69410b40181f0f9,"Graph coarsening USED-FOR graph. deep learning on graphs USED-FOR graph coarsening. Laplace operator CONJUNCTION projection / lift operators. projection / lift operators CONJUNCTION Laplace operator. Laplace operator USED-FOR coarse graph. framework USED-FOR coarsening algorithm. edge weight USED-FOR coarse graph. it USED-FOR coarsening quality. graph neural networks USED-FOR weight assignment map. reduction ratios CONJUNCTION graph sizes. graph sizes CONJUNCTION reduction ratios. graph sizes CONJUNCTION graph types. graph types CONJUNCTION graph sizes. method COMPARE graph coarsening methods. graph coarsening methods COMPARE method. metrics CONJUNCTION reduction ratios. reduction ratios CONJUNCTION metrics. synthetic and real networks EVALUATE-FOR method. metrics EVALUATE-FOR graph coarsening methods. metrics EVALUATE-FOR method. reduction ratios EVALUATE-FOR method. It USED-FOR graphs. OtherScientificTerm are large - scale graphs, and essential properties. Material is large graph data. Method is data - driven methods. ","Graph coarsening is an important problem for large-scale graphs. In this paper, the authors propose to apply deep learning on graphs to improve the performance of graph coarsens. In particular, they propose a new framework for learning a coarsened version of the Laplace operator and the projection/lift operators for a coarse graph with edge weight. This framework can be applied to any existing coarsense algorithm, and it can be used to control the quality of the resulting graph. The authors also propose to use graph neural networks to learn the weight assignment map. The proposed method is evaluated on both synthetic and real networks, and compared with several existing graph-coarsening methods on various metrics, reduction ratios, graph sizes, graph types, and graph sizes. It is shown that the proposed method outperforms existing graphcoarsens methods on all of these metrics and graphs. The paper also shows that the method can be trained on large graph data, and that it can learn the essential properties of the graph, which is a nice contribution to existing data-driven methods.","Graph coarsening is an important problem for large-scale graphs. In this paper, the authors propose to apply deep learning on graphs to improve the performance of graph coarsens. In particular, they propose a new framework for learning a coarsened version of the Laplace operator and the projection/lift operators for a coarse graph with edge weight. This framework can be applied to any existing coarsense algorithm, and it can be used to control the quality of the resulting graph. The authors also propose to use graph neural networks to learn the weight assignment map. The proposed method is evaluated on both synthetic and real networks, and compared with several existing graph-coarsening methods on various metrics, reduction ratios, graph sizes, graph types, and graph sizes. It is shown that the proposed method outperforms existing graphcoarsens methods on all of these metrics and graphs. The paper also shows that the method can be trained on large graph data, and that it can learn the essential properties of the graph, which is a nice contribution to existing data-driven methods."
918,SP:0d680213339f0e2aedb0be4aeed51423706b8bf6,localization CONJUNCTION acoustic scene analysis. acoustic scene analysis CONJUNCTION localization. environmental acoustic effects CONJUNCTION localization. localization CONJUNCTION environmental acoustic effects. Acoustic properties USED-FOR 3D audio content creation. 3D audio content creation CONJUNCTION environmental acoustic effects. environmental acoustic effects CONJUNCTION 3D audio content creation. environmental acoustic effects CONJUNCTION acoustic scene analysis. acoustic scene analysis CONJUNCTION environmental acoustic effects. Acoustic properties USED-FOR environmental acoustic effects. scattering characteristics FEATURE-OF Acoustic properties. numeric solvers USED-FOR acoustic properties. numeric solvers USED-FOR interactive applications. geometric deep learning algorithm USED-FOR characteristics. characteristics USED-FOR 3D objects. interactive rates FEATURE-OF 3D objects. discrete - laplacian and implicit encoders USED-FOR geometric deep learning algorithm. multi - layer network USED-FOR acoustic properties. multi - layer network USED-FOR arbitrary topologies. arbitrary topologies FEATURE-OF acoustic properties. NVIDIA GeForce RTX 2080 Ti GPU USED-FOR multi - layer network. accuracy EVALUATE-FOR learning method. dynamic environments FEATURE-OF generating environmental acoustic effects. Method is point cloud approximation. OtherScientificTerm is high - dimensional latent space. ,"This paper proposes a geometric deep learning algorithm for learning acoustic properties of 3D objects. Acoustic properties such as scattering characteristics, localization, and acoustic scene analysis are important for 3D audio content creation, environmental acoustic effects, and 3D acoustic content creation. The paper proposes to use numeric solvers to learn the acoustic properties for these three interactive applications.   The paper uses discrete-laplacian and implicit encoders to learn these characteristics, and then uses a multi-layer network on a NVIDIA GeForce GeForce RTX 2080 Ti GPU to learn acoustic properties on arbitrary topologies. The authors show that these characteristics can be used to learn 3D object properties at interactive rates, and that the learned characteristics are robust to point cloud approximation.  The authors also show that the learning method can achieve high accuracy in terms of accuracy in generating environmental acoustic effect in dynamic environments, and can be applied to any high-dimensional latent space.","This paper proposes a geometric deep learning algorithm for learning acoustic properties of 3D objects. Acoustic properties such as scattering characteristics, localization, and acoustic scene analysis are important for 3D audio content creation, environmental acoustic effects, and 3D acoustic content creation. The paper proposes to use numeric solvers to learn the acoustic properties for these three interactive applications.   The paper uses discrete-laplacian and implicit encoders to learn these characteristics, and then uses a multi-layer network on a NVIDIA GeForce GeForce RTX 2080 Ti GPU to learn acoustic properties on arbitrary topologies. The authors show that these characteristics can be used to learn 3D object properties at interactive rates, and that the learned characteristics are robust to point cloud approximation.  The authors also show that the learning method can achieve high accuracy in terms of accuracy in generating environmental acoustic effect in dynamic environments, and can be applied to any high-dimensional latent space."
927,SP:afc33a782c43e3d4c5c4fbf047d0b1108bc30bae,"model USED-FOR extreme distributional shifts. extreme distributional shifts FEATURE-OF model ’s sensitivity. model ’s sensitivity EVALUATE-FOR model. robust optimization USED-FOR Risk Extrapolation ( REx ). perturbation set of extrapolated domains ( MMREx ) USED-FOR robust optimization. REx USED-FOR causal mechanisms. causally induced distributional shifts CONJUNCTION covariate shift. covariate shift CONJUNCTION causally induced distributional shifts. REx COMPARE methods. methods COMPARE REx. robustness EVALUATE-FOR REx. covariate shift FEATURE-OF robustness. causally induced distributional shifts FEATURE-OF robustness. Invariant Risk Minimization HYPONYM-OF methods. Task is Distributional shift. Method is machine learning prediction systems. OtherScientificTerm is causal and anti - causal elements. Generic are approach, and variant. ","Distributional shift is an important problem in machine learning prediction systems, where the model’s sensitivity to extreme distributional shifts can be highly sensitive to the causal and anti-causal elements. This paper proposes Risk Extrapolation (REx), a new approach for robust optimization based on the perturbation set of extrapolated domains (MMREx). The authors show that REx is robust to both causal mechanisms and to covariate shifts. The authors also show that the proposed REx outperforms existing methods such as Invariant Risk Minimization in terms of robustness to causally induced distributional shift and covariate shift. ","Distributional shift is an important problem in machine learning prediction systems, where the model’s sensitivity to extreme distributional shifts can be highly sensitive to the causal and anti-causal elements. This paper proposes Risk Extrapolation (REx), a new approach for robust optimization based on the perturbation set of extrapolated domains (MMREx). The authors show that REx is robust to both causal mechanisms and to covariate shifts. The authors also show that the proposed REx outperforms existing methods such as Invariant Risk Minimization in terms of robustness to causally induced distributional shift and covariate shift. "
936,SP:411d5bcf7698d534ad60f581d479ff74849ba4de,neural networks USED-FOR mappings between finite - dimensional Euclidean spaces. this USED-FOR neural operators. neural operators USED-FOR mappings between function spaces. neural operators USED-FOR mapping. neural operators USED-FOR partial differential equations ( PDEs ). they USED-FOR PDEs. Fourier space FEATURE-OF integral kernel. integral kernel USED-FOR neural operator. Burgers ’ equation CONJUNCTION Darcy flow. Darcy flow CONJUNCTION Burgers ’ equation. Darcy flow CONJUNCTION Navier - Stokes equation. Navier - Stokes equation CONJUNCTION Darcy flow. Fourier neural operator HYPONYM-OF ML - based method. Fourier neural operator USED-FOR turbulent flows. ML - based method USED-FOR turbulent flows. zero - shot super - resolution FEATURE-OF Fourier neural operator. zero - shot super - resolution FEATURE-OF turbulent flows. It COMPARE PDE solvers. PDE solvers COMPARE It. it COMPARE learning - based solvers. learning - based solvers COMPARE it. accuracy EVALUATE-FOR learning - based solvers. fixed resolution FEATURE-OF learning - based solvers. fixed resolution FEATURE-OF it. accuracy EVALUATE-FOR it. OtherScientificTerm is functional parametric dependence. Generic is architecture. ,"This paper proposes to use neural networks to learn mappings between finite-dimensional Euclidean spaces. The idea is that neural operators can be used to learn the mapping between function spaces, and that this can be applied to neural operators for solving partial differential equations (PDEs). The neural operator is learned by learning an integral kernel in the Fourier space, and the functional parametric dependence between the input and the output of the neural operators is used to define the mapping. The authors show that they are able to solve PDEs in this way, and demonstrate that they can solve Burgers’ equation, the Darcy flow, and Navier-Stokes equation. They also propose a new ML-based method called Fourier neural operator, which is a zero-shot super-resolution for turbulent flows. It outperforms existing PDE solvers in terms of accuracy, and it outperforms other learning-based solvers with fixed resolution.   ","This paper proposes to use neural networks to learn mappings between finite-dimensional Euclidean spaces. The idea is that neural operators can be used to learn the mapping between function spaces, and that this can be applied to neural operators for solving partial differential equations (PDEs). The neural operator is learned by learning an integral kernel in the Fourier space, and the functional parametric dependence between the input and the output of the neural operators is used to define the mapping. The authors show that they are able to solve PDEs in this way, and demonstrate that they can solve Burgers’ equation, the Darcy flow, and Navier-Stokes equation. They also propose a new ML-based method called Fourier neural operator, which is a zero-shot super-resolution for turbulent flows. It outperforms existing PDE solvers in terms of accuracy, and it outperforms other learning-based solvers with fixed resolution.   "
945,SP:41d268d0eac9b4c84baa156fb641aa6d3060b5a4,"gradient flow USED-FOR linear neural network training. infinitesimal step size FEATURE-OF gradient descent. gradient descent HYPONYM-OF gradient flow. convergence direction FEATURE-OF network parameters. formulation USED-FOR convergence direction. singular vectors PART-OF tensor. network USED-FOR tensor. singular vectors USED-FOR convergence direction. tensor USED-FOR convergence direction. gradient flow USED-FOR separable classification. gradient flow USED-FOR ` 2 / L max - margin problem. transformed ” input space FEATURE-OF ` 2 / L max - margin problem. network USED-FOR transformed ” input space. orthogonally decomposable FEATURE-OF L - layer linear tensor networks. gradient flow USED-FOR global minimum. transformed input space FEATURE-OF weighted ` 1 and ` 2 norms. norm - like function FEATURE-OF global minimum. transformed input space FEATURE-OF norm - like function. weighted ` 1 and ` 2 norms FEATURE-OF norm - like function. gradient flow USED-FOR underdetermined regression. Method are tensor formulation of neural networks, and linear tensor networks. OtherScientificTerm is convergence assumptions. ","This paper studies the convergence of gradient flow in linear neural network training with infinitesimal step size. In particular, the authors focus on the tensor formulation of neural networks and show that gradient flow (i.e., gradient descent) converges to a global minimum under the assumption that the convergence direction of the network parameters are orthogonally decomposable. The authors show that under this formulation, the gradient flow for separable classification is orthogonal. They also show that for L-layer linear tensor networks, the convergence assumptions are also orthogonsistent. Finally, they show that the global minimum of a gradient flow is a norm-like function in the transformed input space of a network with a “transformed” input space. ","This paper studies the convergence of gradient flow in linear neural network training with infinitesimal step size. In particular, the authors focus on the tensor formulation of neural networks and show that gradient flow (i.e., gradient descent) converges to a global minimum under the assumption that the convergence direction of the network parameters are orthogonally decomposable. The authors show that under this formulation, the gradient flow for separable classification is orthogonal. They also show that for L-layer linear tensor networks, the convergence assumptions are also orthogonsistent. Finally, they show that the global minimum of a gradient flow is a norm-like function in the transformed input space of a network with a “transformed” input space. "
954,SP:e27907ef4a4e6e0f5841618fcaa7e7e0db443f91,"prediction error CONJUNCTION computational cost. computational cost CONJUNCTION prediction error. storage cost EVALUATE-FOR model. FLOPs HYPONYM-OF computational cost. They USED-FOR resource - constrained settings. mobile devices HYPONYM-OF resource - constrained settings. slimmable neural networks USED-FOR sub - networks. width - multiplier USED-FOR sub - networks. performance profiles FEATURE-OF sub - networks. prediction accuracy EVALUATE-FOR network. width - multiplier USED-FOR slimmable neural networks. approach USED-FOR slimmable networks. approach USED-FOR widthmultipliers. shared weights CONJUNCTION width - multipliers. width - multipliers CONJUNCTION shared weights. algorithm USED-FOR shared weights. algorithm USED-FOR width - multipliers. width - multipliers USED-FOR sub - networks. algorithm USED-FOR sub - networks. multiobjective optimization lens USED-FOR slimmable networks. FLOPs CONJUNCTION memory footprint. memory footprint CONJUNCTION FLOPs. method COMPARE alternatives. alternatives COMPARE method. network and dataset combinations CONJUNCTION cost objectives. cost objectives CONJUNCTION network and dataset combinations. FLOPs HYPONYM-OF cost objectives. memory footprint HYPONYM-OF cost objectives. FLOPs CONJUNCTION memory footprint. memory footprint CONJUNCTION FLOPs. ImageNet dataset EVALUATE-FOR MobileNetV2. channel counts USED-FOR layers. Method is Slimmable neural networks. OtherScientificTerm are FLOP requirements, and heterogeneous width - multipliers. Task is optimizing slimmable networks. Metric is top-1 accuracy. ","Slimmable neural networks have been a popular topic of interest in recent years. They are particularly useful in resource-constrained settings (e.g. mobile devices) where the storage cost of the model can be prohibitively expensive and the FLOP requirements are high. This paper proposes to use slimmable neural networks with a width-multiplier to learn sub-networks with different performance profiles.    The approach proposed in this paper is to learn the width multipliers for each sub-network based on a multi-objective optimization lens. The authors propose an algorithm to learn shared weights and width-multiplyers for all sub-nets, and then apply the algorithm to optimize the shared weights, width- multipliers, and the number of layers.  The main contribution of the paper is that the authors propose to use heterogeneous width-pl multipliers across sub-nets, which is an interesting idea.  They show that the proposed method outperforms existing alternatives in terms of top-1 accuracy, prediction accuracy, FLOPs, and memory footprint. The experiments on the ImageNet dataset show that MobileNetV2 achieves state-of-the-art performance on MobileNetv2 with lower channel counts. The paper also shows that the method can be applied to a variety of network and dataset combinations, cost objectives (FLOPs or memory footprint) and different cost objectives, and achieves competitive performance. ","Slimmable neural networks have been a popular topic of interest in recent years. They are particularly useful in resource-constrained settings (e.g. mobile devices) where the storage cost of the model can be prohibitively expensive and the FLOP requirements are high. This paper proposes to use slimmable neural networks with a width-multiplier to learn sub-networks with different performance profiles.    The approach proposed in this paper is to learn the width multipliers for each sub-network based on a multi-objective optimization lens. The authors propose an algorithm to learn shared weights and width-multiplyers for all sub-nets, and then apply the algorithm to optimize the shared weights, width- multipliers, and the number of layers.  The main contribution of the paper is that the authors propose to use heterogeneous width-pl multipliers across sub-nets, which is an interesting idea.  They show that the proposed method outperforms existing alternatives in terms of top-1 accuracy, prediction accuracy, FLOPs, and memory footprint. The experiments on the ImageNet dataset show that MobileNetV2 achieves state-of-the-art performance on MobileNetv2 with lower channel counts. The paper also shows that the method can be applied to a variety of network and dataset combinations, cost objectives (FLOPs or memory footprint) and different cost objectives, and achieves competitive performance. "
963,SP:cf59403abb6ca89ccee4f8e77e9a33d99e6a00f5,"Federated SemiSupervised Learning ( FSSL ) HYPONYM-OF federated learning problem. scenarios PART-OF FSSL. method USED-FOR problems. labeled and unlabeled data USED-FOR disjoint learning. inter - client consistency loss USED-FOR FedMatch. federated learning and semi - supervised learning approaches USED-FOR FedMatch. federated learning CONJUNCTION semi - supervised learning. semi - supervised learning CONJUNCTION federated learning. method COMPARE baselines. baselines COMPARE method. method COMPARE local semi - supervised learning. local semi - supervised learning COMPARE method. local semi - supervised learning CONJUNCTION baselines. baselines CONJUNCTION local semi - supervised learning. method COMPARE method. method COMPARE method. federated learning USED-FOR baselines. semi - supervised learning USED-FOR baselines. Method is federated learning approaches. Metric is labeling cost. Task are annotation, and Federated Matching ( FedMatch ). OtherScientificTerm is expert knowledge. Material are private data, and labeled data. Generic is scenario. ","This paper proposes a new federated semi-supervised learning problem called Federated SemiSupervised Learning (FSSL), which is a variant of the federated learning problem known as Federated Matching (FedMatch). In FedMatch, the goal is to minimize the labeling cost for each client, which is motivated by the fact that the expert knowledge of each client is not available to the other clients. The authors propose two scenarios in FSSL: (1) disjoint learning with both labeled and unlabeled data, and (2) federated matching with private data. The proposed method is applicable to both of these problems. In the first scenario, the authors propose FedMatch with an inter-client consistency loss, which penalizes the difference between the label of the private data and the labeled data of the public data.  In the second scenario, FedMatch is based on both the standard Federated Learning and Semi-Supervised learning approaches, where the annotation is provided to each client.  The authors compare the proposed method with baselines based on federated training and local semi supervised learning, and show that their method outperforms the baselines in both cases. ","This paper proposes a new federated semi-supervised learning problem called Federated SemiSupervised Learning (FSSL), which is a variant of the federated learning problem known as Federated Matching (FedMatch). In FedMatch, the goal is to minimize the labeling cost for each client, which is motivated by the fact that the expert knowledge of each client is not available to the other clients. The authors propose two scenarios in FSSL: (1) disjoint learning with both labeled and unlabeled data, and (2) federated matching with private data. The proposed method is applicable to both of these problems. In the first scenario, the authors propose FedMatch with an inter-client consistency loss, which penalizes the difference between the label of the private data and the labeled data of the public data.  In the second scenario, FedMatch is based on both the standard Federated Learning and Semi-Supervised learning approaches, where the annotation is provided to each client.  The authors compare the proposed method with baselines based on federated training and local semi supervised learning, and show that their method outperforms the baselines in both cases. "
972,SP:9457b6d430a2cd864d526d7e90bf3e1ab13d6df4,real - world users FEATURE-OF discrete event sequences. discrete event sequences USED-FOR self - supervised learning. low - dimensional fixed - length vector representations USED-FOR downstream machine learning tasks. low - dimensional fixed - length vector representations USED-FOR Self - supervised learning. contrastive learning USED-FOR audio and computer vision domains. CoLES USED-FOR discrete event sequences domain. self - supervised setting USED-FOR discrete event sequences domain. contrastive learning USED-FOR CoLES. augmentation method USED-FOR discrete event sequences. augmentation method USED-FOR CoLES. CoLES USED-FOR discrete event sequences. CoLES representations COMPARE methods. methods COMPARE CoLES representations. downstream tasks EVALUATE-FOR CoLES representations. downstream tasks EVALUATE-FOR methods. public datasets EVALUATE-FOR CoLES. ,"Self-supervised learning with discrete event sequences for real-world users requires low-dimensional fixed-length vector representations for downstream machine learning tasks. This paper proposes CoLES, which extends contrastive learning from audio and computer vision domains to the discrete event sequence domain under the self-supervision setting. CoLES uses a novel augmentation method to augment the representation of discrete events in order to improve the performance of the discrete events. Experiments on public datasets show that CoLES representations outperform existing methods on several downstream tasks.","Self-supervised learning with discrete event sequences for real-world users requires low-dimensional fixed-length vector representations for downstream machine learning tasks. This paper proposes CoLES, which extends contrastive learning from audio and computer vision domains to the discrete event sequence domain under the self-supervision setting. CoLES uses a novel augmentation method to augment the representation of discrete events in order to improve the performance of the discrete events. Experiments on public datasets show that CoLES representations outperform existing methods on several downstream tasks."
981,SP:385942a5bcee7384bb722a1669b541f2fac0cd36,constituency grammar USED-FOR assembly of one or several corresponded words. dependency grammar CONJUNCTION constituency grammar. constituency grammar CONJUNCTION dependency grammar. constituency grammar HYPONYM-OF natural language grammars. dependency grammar HYPONYM-OF natural language grammars. StructFormer USED-FOR dependency and constituency structure. model USED-FOR dependency and constituency structure. StructFormer HYPONYM-OF model. constituency tree CONJUNCTION dependency graph. dependency graph CONJUNCTION constituency tree. parsing framework USED-FOR constituency tree. parsing framework USED-FOR dependency graph. induced dependency relations PART-OF transformer. dependency - constrained self - attention mechanism USED-FOR transformer. unsupervised dependency parsing CONJUNCTION masked language modeling. masked language modeling CONJUNCTION unsupervised dependency parsing. unsupervised constituency parsing CONJUNCTION unsupervised dependency parsing. unsupervised dependency parsing CONJUNCTION unsupervised constituency parsing. model USED-FOR unsupervised constituency parsing. model USED-FOR unsupervised dependency parsing. model USED-FOR masked language modeling. Method is unsupervised parsing methods. ,"This paper proposes a new model, StructFormer, to learn the dependency and constituency structure of natural language grammars such as dependency grammar and constituency grammar for assembly of one or several corresponded words. The authors propose a new parsing framework to learn a constituency tree and a dependency graph. The proposed model is based on StructFormer which is a model that learns the dependency tree and the dependency graph in a transformer with a dependency-constrained self-attention mechanism. The induced dependency relations are incorporated into the transformer and the authors show that the proposed model can learn unsupervised dependency parsing and masked language modeling. Experiments are conducted on several datasets to demonstrate the effectiveness of the model.    The paper is well-written and well-motivated. However, there is a lack of comparison with other unsupervisory parsing methods.","This paper proposes a new model, StructFormer, to learn the dependency and constituency structure of natural language grammars such as dependency grammar and constituency grammar for assembly of one or several corresponded words. The authors propose a new parsing framework to learn a constituency tree and a dependency graph. The proposed model is based on StructFormer which is a model that learns the dependency tree and the dependency graph in a transformer with a dependency-constrained self-attention mechanism. The induced dependency relations are incorporated into the transformer and the authors show that the proposed model can learn unsupervised dependency parsing and masked language modeling. Experiments are conducted on several datasets to demonstrate the effectiveness of the model.    The paper is well-written and well-motivated. However, there is a lack of comparison with other unsupervisory parsing methods."
990,SP:078966ff62775bba6031e47d374bda95f4a7dde3,images USED-FOR structured representations. nodes of scene graphs CONJUNCTION object bounding boxes. object bounding boxes CONJUNCTION nodes of scene graphs. annotated mapping USED-FOR nodes of scene graphs. annotated mapping USED-FOR methods. scene graph nodes CONJUNCTION visual objects. visual objects CONJUNCTION scene graph nodes. object features CONJUNCTION relational features. relational features CONJUNCTION object features. visual objects CONJUNCTION scene graph nodes. scene graph nodes CONJUNCTION visual objects. Visual Genome ( VG ) CONJUNCTION Visual Relation Detection ( VRD ) datasets. Visual Relation Detection ( VRD ) datasets CONJUNCTION Visual Genome ( VG ). model COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE model. scene graph grounding task EVALUATE-FOR state - of - the - art approaches. scene graph grounding task EVALUATE-FOR model. scene graph parsing task EVALUATE-FOR method. scene graph parsing task EVALUATE-FOR model. model COMPARE method. method COMPARE model. OtherScientificTerm is weak supervision. ,"This paper proposes a method for learning structured representations from images. The proposed methods are based on an annotated mapping between nodes of scene graphs and object bounding boxes. The idea is to combine scene graph nodes, visual objects, and scene graphs with object features and relational features. Experiments are conducted on the Visual Genome (VG) and Visual Relation Detection (VRD) datasets. Results show that the proposed model outperforms state-of-the-art approaches on the scene graph grounding task, and the proposed method also outperforms the existing method on a scene graph parsing task with weak supervision.","This paper proposes a method for learning structured representations from images. The proposed methods are based on an annotated mapping between nodes of scene graphs and object bounding boxes. The idea is to combine scene graph nodes, visual objects, and scene graphs with object features and relational features. Experiments are conducted on the Visual Genome (VG) and Visual Relation Detection (VRD) datasets. Results show that the proposed model outperforms state-of-the-art approaches on the scene graph grounding task, and the proposed method also outperforms the existing method on a scene graph parsing task with weak supervision."
999,SP:4644dbf7466b6234d8abf69995fdfb357efcc119,"Relational regularized autoencoder ( RAE ) USED-FOR distribution of data. framework USED-FOR distribution of data. reconstruction loss CONJUNCTION relational regularization. relational regularization CONJUNCTION reconstruction loss. Relational regularized autoencoder ( RAE ) HYPONYM-OF framework. relational regularization FEATURE-OF latent space. sliced fused GromovWasserstein ( SFG ) USED-FOR distributions. discrepancy CONJUNCTION relational regularization. relational regularization CONJUNCTION discrepancy. relational discrepancy USED-FOR discrepancy. spherical sliced fused Gromov Wasserstein ( SSFG ) HYPONYM-OF relational discrepancy. mixture of von Mises - Fisher distributions USED-FOR vMF distribution. power spherical distribution USED-FOR sampling time. high dimension settings FEATURE-OF sampling time. power spherical distribution USED-FOR vMF distribution. discrepancies USED-FOR variants. discrepancies USED-FOR RAE framework. image generation CONJUNCTION reconstruction. reconstruction CONJUNCTION image generation. learning latent manifold structure CONJUNCTION image generation. image generation CONJUNCTION learning latent manifold structure. autoencoders USED-FOR learning latent manifold structure. autoencoders USED-FOR image generation. autoencoders USED-FOR reconstruction. OtherScientificTerm are inner discrepancy, von Mises - Fisher distribution, and latent manifold structure. Generic are approach, it, and variant. Task is discriminative task. Method is SSFG. ","This paper proposes a new framework called Relational regularized autoencoder (RAE) to learn the distribution of data with the help of reconstruction loss and relational regularization in the latent space. The approach is based on the observation that the inner discrepancy between the von Mises-Fisher distribution and the vMF distribution is a mixture of two distributions, which is a variant of sliced fused GromovWasserstein (SFG). The paper shows that the discrepancy is a product of the relational discrepancy, i.e., the difference between the two distributions. The paper proposes two variants of this relational discrepancy: spherical sliced fused fused gromov Wasserstein(SSFG) and SSFG. The SSFG uses the power spherical distribution to reduce the sampling time in high dimension settings, and the SSFG is a variation of the mixture of mixture of von Mise and Fisher distributions. Both variants of the RAE framework are based on these two discrepancies.   The paper also shows how to use autoencoders for learning latent manifold structure, image generation and reconstruction, and how to train autoenconders for both reconstruction and learning the discriminative task. ","This paper proposes a new framework called Relational regularized autoencoder (RAE) to learn the distribution of data with the help of reconstruction loss and relational regularization in the latent space. The approach is based on the observation that the inner discrepancy between the von Mises-Fisher distribution and the vMF distribution is a mixture of two distributions, which is a variant of sliced fused GromovWasserstein (SFG). The paper shows that the discrepancy is a product of the relational discrepancy, i.e., the difference between the two distributions. The paper proposes two variants of this relational discrepancy: spherical sliced fused fused gromov Wasserstein(SSFG) and SSFG. The SSFG uses the power spherical distribution to reduce the sampling time in high dimension settings, and the SSFG is a variation of the mixture of mixture of von Mise and Fisher distributions. Both variants of the RAE framework are based on these two discrepancies.   The paper also shows how to use autoencoders for learning latent manifold structure, image generation and reconstruction, and how to train autoenconders for both reconstruction and learning the discriminative task. "
1008,SP:5ae2c0af82cac89a65f1cc38c43e2d05ea298901,"natural language processing CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION natural language processing. computational costs CONJUNCTION training time. training time CONJUNCTION computational costs. approach USED-FOR training. training USED-FOR deep networks. approach USED-FOR deep networks. repeated structures PART-OF deep networks. transformer module HYPONYM-OF repeated structures. deep linear networks USED-FOR theoretic analysis. theoretic analysis USED-FOR adaptive untying criterion. deep linear networks USED-FOR adaptive untying criterion. method USED-FOR BERT. training time EVALUATE-FOR BERT. training time EVALUATE-FOR method. OtherScientificTerm are deep learning model sizes, repeated layers, weight sharing, and monitoring gradient statistics. Method is deep network. ","This paper proposes a new approach to reduce the computational costs and training time of training for deep networks. The approach is based on the observation that deep learning model sizes tend to increase as the number of layers increases, and that the repeated structures (e.g., the transformer module) in deep networks tend to have repeated layers. The authors propose an adaptive untying criterion based on a theoretic analysis of deep linear networks. They also propose a new method to reduce BERT's training time by removing the need for weight sharing and monitoring gradient statistics. Experiments on natural language processing and computer vision tasks demonstrate the effectiveness of the proposed method.","This paper proposes a new approach to reduce the computational costs and training time of training for deep networks. The approach is based on the observation that deep learning model sizes tend to increase as the number of layers increases, and that the repeated structures (e.g., the transformer module) in deep networks tend to have repeated layers. The authors propose an adaptive untying criterion based on a theoretic analysis of deep linear networks. They also propose a new method to reduce BERT's training time by removing the need for weight sharing and monitoring gradient statistics. Experiments on natural language processing and computer vision tasks demonstrate the effectiveness of the proposed method."
1017,SP:a51710551142316b67e2fccd969fea1ece35ba39,interaction inside adversarial perturbations USED-FOR adversarial transferability. adversarial transferability CONJUNCTION interaction. interaction CONJUNCTION adversarial transferability. negative correlation FEATURE-OF adversarial transferability. DNNs USED-FOR negative correlation. negative correlation USED-FOR transferability - boosting methods. methods USED-FOR transferability. interactions USED-FOR attacking process. OtherScientificTerm is adversarial perturbations. ,"This paper studies the relationship between adversarial transferability and interaction inside adversarial perturbations. It shows that the negative correlation between DNNs can be used as a metric to measure the transferability of a perturbation, and that transferability-boosting methods rely on negative correlation. The paper also shows that existing methods for improving transferability are not robust to interactions in the attacking process. ","This paper studies the relationship between adversarial transferability and interaction inside adversarial perturbations. It shows that the negative correlation between DNNs can be used as a metric to measure the transferability of a perturbation, and that transferability-boosting methods rely on negative correlation. The paper also shows that existing methods for improving transferability are not robust to interactions in the attacking process. "
1033,SP:f1565319075c1442c2cb52d96443facb492c06c2,"neural network ( hidden ) representations CONJUNCTION task semantics. task semantics CONJUNCTION neural network ( hidden ) representations. deeper layers USED-FOR forgetting. sequential training USED-FOR task representational subspaces. Methods USED-FOR forgetting. Methods USED-FOR deeper layers. maximal forgetting FEATURE-OF task sequences. forgetting CONJUNCTION task semantic similarity. task semantic similarity CONJUNCTION forgetting. intermediate similarity FEATURE-OF task sequences. Task is Catastrophic forgetting. Method are deep learning models, and neural representations. Generic are some, and others. OtherScientificTerm are feature reuse, task representations, and interference. ","Catastrophic forgetting is a well-studied problem in deep learning models. This paper studies the relationship between neural network (hidden) representations and task semantics. The authors show that deeper layers are responsible for forgetting, and that sequential training can be used to learn task representational subspaces. Methods are proposed to improve the forgetting of deeper layers, but some of the contributions are limited to feature reuse (e.g. removing interference), while others are not. The paper also shows that the maximal forgetting of task sequences with intermediate similarity to the original task representations is not a result of interference, but of learning neural representations. ","Catastrophic forgetting is a well-studied problem in deep learning models. This paper studies the relationship between neural network (hidden) representations and task semantics. The authors show that deeper layers are responsible for forgetting, and that sequential training can be used to learn task representational subspaces. Methods are proposed to improve the forgetting of deeper layers, but some of the contributions are limited to feature reuse (e.g. removing interference), while others are not. The paper also shows that the maximal forgetting of task sequences with intermediate similarity to the original task representations is not a result of interference, but of learning neural representations. "
1049,SP:30d7532cdcf420bff3be6b92eea3d93bce59e6bd,"XLNet CONJUNCTION T5. T5 CONJUNCTION XLNet. BERT CONJUNCTION XLNet. XLNet CONJUNCTION BERT. Deep, heavily overparameterized language models USED-FOR natural language processing ( NLP ) tasks. T5 HYPONYM-OF Deep, heavily overparameterized language models. XLNet HYPONYM-OF Deep, heavily overparameterized language models. BERT HYPONYM-OF Deep, heavily overparameterized language models. training time USED-FOR pre - training and fine - tuning. computation resources CONJUNCTION training time. training time CONJUNCTION computation resources. computation resources USED-FOR model complexity. model compression USED-FOR large NLP models. large batch sizes USED-FOR pre - training time. training algorithm USED-FOR pre - training and fine - tuning. Early - Bird Lottery Tickets USED-FOR computer vision tasks. training algorithm USED-FOR large - scale language models. EarlyBERT HYPONYM-OF training algorithm. pre - training and fine - tuning USED-FOR large - scale language models. self - attention CONJUNCTION fully - connected sub - layers. fully - connected sub - layers CONJUNCTION self - attention. fully - connected sub - layers PART-OF transformer. structured winning tickets PART-OF BERT training. tickets USED-FOR BERT training. GLUE and SQuAD downstream tasks EVALUATE-FOR pre - training and fine - tuning. EarlyBERT COMPARE BERT. BERT COMPARE EarlyBERT. training time EVALUATE-FOR EarlyBERT. training time EVALUATE-FOR BERT. Metric is inference time. OtherScientificTerm are training process, and computational resource demands. ","This paper proposes a new training algorithm, called EarlyBERT, for pre-training and fine-tuning large-scale language models such as BERT, XLNet, and T5, which are Deep, heavily overparameterized language models for natural language processing (NLP) tasks. The authors argue that the inference time of BERT and XLNet is too slow due to the large batch sizes used during the training process, and that large NLP models should be pre-trained with model compression to reduce the training time. To achieve this, the authors propose to use Early-Bird Lottery Tickets for computer vision tasks, where tickets are randomly selected from a lottery ticket pool. The tickets are used during BERT training, and the authors show that their training algorithm can be used to speed up the training of large language models in terms of computation resources, training time, and model complexity.  The authors also show that the structured winning tickets can be incorporated into the transformer in order to reduce computational resource demands. They further show that this can be combined with self-attention and fully-connected sub-layers in the transformer, which can further reduce the computational cost.  Experiments are conducted on GLUE and SQuAD downstream tasks, and show that pre-trains with the proposed training algorithm (earlyBERT) outperforms BERT in training time and performance.   ","This paper proposes a new training algorithm, called EarlyBERT, for pre-training and fine-tuning large-scale language models such as BERT, XLNet, and T5, which are Deep, heavily overparameterized language models for natural language processing (NLP) tasks. The authors argue that the inference time of BERT and XLNet is too slow due to the large batch sizes used during the training process, and that large NLP models should be pre-trained with model compression to reduce the training time. To achieve this, the authors propose to use Early-Bird Lottery Tickets for computer vision tasks, where tickets are randomly selected from a lottery ticket pool. The tickets are used during BERT training, and the authors show that their training algorithm can be used to speed up the training of large language models in terms of computation resources, training time, and model complexity.  The authors also show that the structured winning tickets can be incorporated into the transformer in order to reduce computational resource demands. They further show that this can be combined with self-attention and fully-connected sub-layers in the transformer, which can further reduce the computational cost.  Experiments are conducted on GLUE and SQuAD downstream tasks, and show that pre-trains with the proposed training algorithm (earlyBERT) outperforms BERT in training time and performance.   "
1065,SP:c547f23ff6caaf5e9f35d258490b86ae0ac8ed03,"classifier ’s predictions CONJUNCTION supervised labels. supervised labels CONJUNCTION classifier ’s predictions. f -divergence measure USED-FOR classifier ’s predictions. variational form USED-FOR decoupling property. variational difference CONJUNCTION bias term. bias term CONJUNCTION variational difference. decoupling property USED-FOR f -divergence measures. clean distribution FEATURE-OF variational difference. variational difference PART-OF divergence. bias term PART-OF divergence. robustness EVALUATE-FOR f -divergence functions. robustness FEATURE-OF f -divergence functions. f -divergence functions USED-FOR metrics. OtherScientificTerm are label noise, noise, and labels ’ noise rate. Task is learning with noisy labels. Generic is they. Material is UCSC - REAL. Metric is Robust - f - divergence - measures. ","This paper proposes a new f-divergence measure for learning with label noise. The authors show that f-Divergence measures have the decoupling property in the form of a variational form, and that they are robust to label noise in the sense that they decouple the classifier’s predictions and the supervised labels. They also show that this decoupled property holds for any divergence that includes the variational difference between the clean distribution and the noisy distribution.    The paper also shows that the robustness of f-derivergence functions is a function of the number of labels, the noise rate, and the labels’ noise rate. Robust-f-distivergence-measure has been shown to be robust to noisy labels in UCSC-REAL.  The authors also propose two new metrics that are based on f-disturbance functions. ","This paper proposes a new f-divergence measure for learning with label noise. The authors show that f-Divergence measures have the decoupling property in the form of a variational form, and that they are robust to label noise in the sense that they decouple the classifier’s predictions and the supervised labels. They also show that this decoupled property holds for any divergence that includes the variational difference between the clean distribution and the noisy distribution.    The paper also shows that the robustness of f-derivergence functions is a function of the number of labels, the noise rate, and the labels’ noise rate. Robust-f-distivergence-measure has been shown to be robust to noisy labels in UCSC-REAL.  The authors also propose two new metrics that are based on f-disturbance functions. "
1081,SP:841888179dcdac901889c8d62cb5234311fe28f1,"Q - ensemble USED-FOR uncertainty estimates. uncertainty estimates USED-FOR ensemble - based weighted Bellman backups. method USED-FOR learning. continuous and discrete control benchmarks EVALUATE-FOR method. weighted Bellman backups COMPARE Bellman backups. Bellman backups COMPARE weighted Bellman backups. weighted Bellman backups CONJUNCTION UCB Exploration. UCB Exploration CONJUNCTION weighted Bellman backups. ensemble USED-FOR weighted Bellman backups. off - policy RL algorithms USED-FOR continuous and discrete control tasks. lowdimensional and high - dimensional environments FEATURE-OF continuous and discrete control tasks. Bootstrap USED-FOR diversity. Soft Actor - Critic and Rainbow DQN HYPONYM-OF off - policy RL algorithms. Material is challenging domains. Task is Q - learning. OtherScientificTerm are Q - estimates, and noisy rewards. Metric is signal - to - noise aspect. ","This paper proposes an ensemble-based weighted Bellman backups based on uncertainty estimates from Q-ensemble. The method is evaluated on a number of continuous and discrete control benchmarks and shows that the method is able to improve the diversity of learning in challenging domains. The authors also show that the Q-learning is more robust to noisy rewards, and that the uncertainty of Q-estimates is more sensitive to the signal-to-noise aspect.   The paper also shows that, in contrast to the standard Bellman backup, the ensemble of the ensemble is more diverse than the traditional Bellman Backup, and UCB Exploration. Bootstrap is also used to encourage diversity.  The authors show that, when using off-policy RL algorithms (such as Soft Actor-Critic and Rainbow DQN), the proposed method outperforms the state-of-the-art in both lowdimensional and high-dimensional environments, and the performance is comparable to that of the standard weights of the Bellman Backbone. ","This paper proposes an ensemble-based weighted Bellman backups based on uncertainty estimates from Q-ensemble. The method is evaluated on a number of continuous and discrete control benchmarks and shows that the method is able to improve the diversity of learning in challenging domains. The authors also show that the Q-learning is more robust to noisy rewards, and that the uncertainty of Q-estimates is more sensitive to the signal-to-noise aspect.   The paper also shows that, in contrast to the standard Bellman backup, the ensemble of the ensemble is more diverse than the traditional Bellman Backup, and UCB Exploration. Bootstrap is also used to encourage diversity.  The authors show that, when using off-policy RL algorithms (such as Soft Actor-Critic and Rainbow DQN), the proposed method outperforms the state-of-the-art in both lowdimensional and high-dimensional environments, and the performance is comparable to that of the standard weights of the Bellman Backbone. "
1097,SP:afc08f203562b841180811aef943bfb63a1659ea,"meta - learning algorithms USED-FOR fewshot classification problems. few - shot classification framework USED-FOR modeling uncertainty. meta - training USED-FOR model. class - wise similarities USED-FOR distributional mismatch. meta - learning models PART-OF method. training strategy USED-FOR model. training strategy USED-FOR calibrated classification. Task are prediction of uncertainty, and random sampling of tasks. OtherScientificTerm is dataset shift. Metric is accuracy. ","This paper proposes a new few-shot classification framework for modeling uncertainty in the meta-learning algorithms for fewshot classification problems. The prediction of uncertainty is based on class-wise similarities between the training data and the test data to avoid distributional mismatch due to the random sampling of tasks. The proposed method combines meta-training with a few existing methods to improve the performance of a model trained with meta-testing. The method combines several existing meta learning models, and the authors also propose a new training strategy to train the model for calibrated classification. Experiments show that the proposed training strategy improves the accuracy of the model, especially when the dataset shift is small. ","This paper proposes a new few-shot classification framework for modeling uncertainty in the meta-learning algorithms for fewshot classification problems. The prediction of uncertainty is based on class-wise similarities between the training data and the test data to avoid distributional mismatch due to the random sampling of tasks. The proposed method combines meta-training with a few existing methods to improve the performance of a model trained with meta-testing. The method combines several existing meta learning models, and the authors also propose a new training strategy to train the model for calibrated classification. Experiments show that the proposed training strategy improves the accuracy of the model, especially when the dataset shift is small. "
1113,SP:12ae325ea3bce1e60195afac7d85895d2d20c29c,"dominant paradigm USED-FOR video - text representations. generative model USED-FOR method. VATEX CONJUNCTION ActivityNet. ActivityNet CONJUNCTION VATEX. ActivityNet CONJUNCTION MSVD. MSVD CONJUNCTION ActivityNet. MSR - VTT CONJUNCTION VATEX. VATEX CONJUNCTION MSR - VTT. method COMPARE others. others COMPARE method. MSVD EVALUATE-FOR method. ActivityNet EVALUATE-FOR method. VATEX EVALUATE-FOR method. MSR - VTT EVALUATE-FOR others. MSR - VTT EVALUATE-FOR method. Method are noise contrastive learning, and dissimilar representations. Generic is representations. OtherScientificTerm are visually similar videos, and depicted action. ","This paper proposes a new paradigm for learning video-text representations that are dissimilar to the dominant paradigm in the literature. The proposed method is based on noise contrastive learning, where the representations are learned from visually similar videos. The method uses a generative model to learn the dissimilar representations. The authors show that their method outperforms others on MSR-VTT, VATEX, ActivityNet, MSVD, and MSVD. They also show that the learned representations are more dissimilar than the ones learned from the original video. ","This paper proposes a new paradigm for learning video-text representations that are dissimilar to the dominant paradigm in the literature. The proposed method is based on noise contrastive learning, where the representations are learned from visually similar videos. The method uses a generative model to learn the dissimilar representations. The authors show that their method outperforms others on MSR-VTT, VATEX, ActivityNet, MSVD, and MSVD. They also show that the learned representations are more dissimilar than the ones learned from the original video. "
1129,SP:8a71d8fad25a126aff01431cacf348c05de75667,pre - trained language models ( PLMs ) USED-FOR Chinese natural language processing ( NLP ) tasks. single vocabulary USED-FOR masked language model pre - training. Chinese word segmentation ( CWS ) CONJUNCTION subword tokenization. subword tokenization CONJUNCTION Chinese word segmentation ( CWS ). seg tok USED-FOR Chinese BERT. Chinese word segmentation ( CWS ) USED-FOR method. subword tokenization USED-FOR method. multi - vocabulary pretraining ( MVP ) USED-FOR models expressiveness. char based vocabulary COMPARE seg tok. seg tok COMPARE char based vocabulary. MVP USED-FOR PLMs. seg tok USED-FOR Chinese PLMs. it USED-FOR seg tok. char based vocabulary USED-FOR Chinese PLMs. seg tok COMPARE it. it COMPARE seg tok. sequence labeling tasks EVALUATE-FOR it. sentence level tasks EVALUATE-FOR Chinese PLMs. sequence labeling tasks EVALUATE-FOR seg tok. sentence level tasks EVALUATE-FOR seg tok. OtherScientificTerm is Chinese characters. ,"This paper presents a method for pre-trained language models (PLMs) for Chinese natural language processing (NLP) tasks. The proposed method is based on Chinese word segmentation (CWS) and subword tokenization. The idea is to pre-train a masked language model pre-training on a single vocabulary, and then use seg tok for Chinese BERT. The authors also propose multi-vocabulary pretraining (MVP) to improve the models expressiveness. They show that using MVP improves the expressiveness of Chinese PLMs, and that it improves the performance of seg-tok on sentence level tasks. They also show that the char based vocabulary is more expressive than segtok, and Chinese PLM can be trained on Chinese characters.   ","This paper presents a method for pre-trained language models (PLMs) for Chinese natural language processing (NLP) tasks. The proposed method is based on Chinese word segmentation (CWS) and subword tokenization. The idea is to pre-train a masked language model pre-training on a single vocabulary, and then use seg tok for Chinese BERT. The authors also propose multi-vocabulary pretraining (MVP) to improve the models expressiveness. They show that using MVP improves the expressiveness of Chinese PLMs, and that it improves the performance of seg-tok on sentence level tasks. They also show that the char based vocabulary is more expressive than segtok, and Chinese PLM can be trained on Chinese characters.   "
1145,SP:b93ec7bc02b48068073ffe705f71d2643e663d51,"Graph Convolutional Networks ( GCNs ) USED-FOR graph - based learning tasks. graph partition CONJUNCTION distributed training. distributed training CONJUNCTION graph partition. memory CONJUNCTION communications. communications CONJUNCTION memory. boundary nodes PART-OF partitioned subgraph. BDS - GCN USED-FOR distributed GCN training. method USED-FOR distributed GCN training. BDS - GCN HYPONYM-OF method. unbiased boundary sampling strategy USED-FOR BDS - GCN. full - graph accuracy EVALUATE-FOR method. full - graph accuracy EVALUATE-FOR BDS - GCN. unbiased boundary sampling strategy USED-FOR method. accuracy EVALUATE-FOR state - of - the - art methods. throughput EVALUATE-FOR BDS - GCN. BDS - GCN USED-FOR GCN training. Method are GCNs, and GCN architectures. Material is real - world large graphs. OtherScientificTerm is GCN structures. Metric is memory usage. ","Graph Convolutional Networks (GCNs) have been widely used for graph-based learning tasks, but there are many issues with GCNs, especially in real-world large graphs. In this paper, the authors propose a method called BDS-GCN, a method for distributed GCN training that can handle both graph partition and distributed training. In particular, the proposed method is based on an unbiased boundary sampling strategy, where each partitioned subgraph is partitioned into a set of boundary nodes, and the boundary nodes of each subgraph are sampled from the set of GCN structures. The authors show that this method can achieve full-graph accuracy with a fraction of the memory and communication costs of previous state-of-the-art methods. They also show that the performance of the method is comparable to the state of the art in terms of accuracy, throughput, and memory usage. In addition, they show that their method can be applied to GCN architectures that are more robust to changes in the size of the graph. ","Graph Convolutional Networks (GCNs) have been widely used for graph-based learning tasks, but there are many issues with GCNs, especially in real-world large graphs. In this paper, the authors propose a method called BDS-GCN, a method for distributed GCN training that can handle both graph partition and distributed training. In particular, the proposed method is based on an unbiased boundary sampling strategy, where each partitioned subgraph is partitioned into a set of boundary nodes, and the boundary nodes of each subgraph are sampled from the set of GCN structures. The authors show that this method can achieve full-graph accuracy with a fraction of the memory and communication costs of previous state-of-the-art methods. They also show that the performance of the method is comparable to the state of the art in terms of accuracy, throughput, and memory usage. In addition, they show that their method can be applied to GCN architectures that are more robust to changes in the size of the graph. "
1161,SP:2d4ba873d11e969ebd1fc31f9b5ab450c964d154,"Machine Learning ( ML ) USED-FOR large - scale physics - based simulations. models USED-FOR real large - scale and complex problems. quantum chemistry simulations USED-FOR catalyst discovery. model USED-FOR quantum chemistry simulations. catalyst discovery USED-FOR renewable energy applications. model USED-FOR catalyst discovery. ForceNet USED-FOR quantum chemistry simulations. ForceNet HYPONYM-OF graph neural network. graph neural network USED-FOR per - atom forces. surrounding 3D molecular structure USED-FOR per - atom forces. surrounding 3D molecular structure USED-FOR graph neural network. model scaling USED-FOR ForceNet. expressive message passing architecture USED-FOR ForceNet. basis and non - linear activation functions USED-FOR ForceNet. ForceNet COMPARE ML models. ML models COMPARE ForceNet. ForceNet USED-FOR atomic forces. ForceNet USED-FOR large - scale catalyst dataset. OC20 HYPONYM-OF large - scale catalyst dataset. ForceNet USED-FOR quantum chemistry simulations. ForceNet COMPARE ML models. ML models COMPARE ForceNet. success rate EVALUATE-FOR ML models. success rate EVALUATE-FOR ForceNet. ML - based simulations COMPARE physics - based simulations. physics - based simulations COMPARE ML - based simulations. Task is atomic simulations. OtherScientificTerm are 3D space, forces, and out - of - distribution structures. ","This paper proposes a graph neural network (GNN) architecture for large-scale physics-based quantum chemistry simulations. The proposed model, called ForceNet, is based on graph neural networks, which is able to learn the per-atom forces of atoms in 3D space. The authors demonstrate that the proposed model can be applied to a number of complex quantum chemistry problems, and can be used to perform quantum chemistry simulation on a large molecule-by-molecule basis. The paper also shows that the model can also be used for the discovery of new molecules. ","This paper proposes a graph neural network (GNN) architecture for large-scale physics-based quantum chemistry simulations. The proposed model, called ForceNet, is based on graph neural networks, which is able to learn the per-atom forces of atoms in 3D space. The authors demonstrate that the proposed model can be applied to a number of complex quantum chemistry problems, and can be used to perform quantum chemistry simulation on a large molecule-by-molecule basis. The paper also shows that the model can also be used for the discovery of new molecules. "
1177,SP:8bdcf4fe6abf4739d4732b7ea8538513135dcccc,regularisation USED-FOR fine - tuning. approaches USED-FOR regularisation. approaches USED-FOR fine - tuning. regularisation USED-FOR deep neural networks. Rademacher complexity USED-FOR neural network generalisation bound. bound COMPARE bounds. bounds COMPARE bound. bounds USED-FOR convolutional networks. bound USED-FOR fine - tuning. learning USED-FOR generalisation. initialisation FEATURE-OF network. transfer learning USED-FOR initialisation. transfer learning USED-FOR network. fine - tuning algorithm USED-FOR hypothesis class. generalisation EVALUATE-FOR transfer learning. It COMPARE fine - tuning competitors. fine - tuning competitors COMPARE It. It COMPARE penalty - based alternatives. penalty - based alternatives COMPARE It. Generic is algorithm. OtherScientificTerm is radius of the search space. ,"This paper studies the generalization properties of deep neural networks trained with regularisation for fine-tuning. In particular, the authors consider two approaches to regularisation in the context of fine-tuning. The authors derive a neural network generalisation bound based on the Rademacher complexity of the algorithm. This bound matches the existing bounds for convolutional networks, and is a generalization of the previous bounds for the case that the radius of the search space is larger than the number of training samples. They also show that this bound can be extended to the case where the network is trained with transfer learning for initialisation of the network.   The authors also provide a theoretical analysis of the generalisation performance of transfer learning in terms of generalisation. They show that transfer learning improves generalisation when the initialisation is a function of the learning rate, and that this learning is more robust to the size of the training set.  Finally, they propose a new fine tuning algorithm for learning a hypothesis class. It is shown to outperform the state-of-the-art penalty-based competitors, and it is also shown to be more robust than penalty-free alternatives.","This paper studies the generalization properties of deep neural networks trained with regularisation for fine-tuning. In particular, the authors consider two approaches to regularisation in the context of fine-tuning. The authors derive a neural network generalisation bound based on the Rademacher complexity of the algorithm. This bound matches the existing bounds for convolutional networks, and is a generalization of the previous bounds for the case that the radius of the search space is larger than the number of training samples. They also show that this bound can be extended to the case where the network is trained with transfer learning for initialisation of the network.   The authors also provide a theoretical analysis of the generalisation performance of transfer learning in terms of generalisation. They show that transfer learning improves generalisation when the initialisation is a function of the learning rate, and that this learning is more robust to the size of the training set.  Finally, they propose a new fine tuning algorithm for learning a hypothesis class. It is shown to outperform the state-of-the-art penalty-based competitors, and it is also shown to be more robust than penalty-free alternatives."
1193,SP:3a3249e97ef2345ea2264de5ed8287e16687838e,"model evaluation EVALUATE-FOR mask discovery. training configuration USED-FOR mask. mask discovery ( Hfind ) CONJUNCTION mask evaluation ( Heval ). mask evaluation ( Heval ) CONJUNCTION mask discovery ( Hfind ). hyperparameters USED-FOR mask evaluation ( Heval ). hyperparameters USED-FOR mask discovery ( Hfind ). unstructured magnitude pruning USED-FOR vision classification tasks. hyperparameters USED-FOR stages. unstructured magnitude pruning USED-FOR decoupled find - eval phenomenon. hyperparameters USED-FOR masks. Hfind values USED-FOR masks. layerwise pruning ratios FEATURE-OF masks. ratios USED-FOR decoupled find - eval phenomenon. Task is model pruning. Generic are model, and models. Method are lottery ticket framework, and one - shot structured pruning. OtherScientificTerm is decoupling hyperparameters. ","This paper studies the problem of model pruning in the lottery ticket framework, where the goal is to find a mask that maximizes the likelihood of the model to be pruned. The paper proposes a lottery ticket-based method for mask discovery (Hfind) and mask evaluation (Heval) for model evaluation, where a mask is found based on the training configuration and the number of layers in the model. Two different hyperparameters are used for these two stages: mask discovery and Heval. The decoupled find-evaluation phenomenon is attributed to the use of unstructured magnitude pruning for vision classification tasks, which decouples the decoupling of mask discovery from mask evaluation.   The paper shows that masks are found using different layerwise pruning ratios, and that these masks can be found using the same Hfind values. The authors also show that these ratios can be used to decouple masks in a one-shot structured pruning, where models are pruned sequentially.  The authors further propose a lottery-ticket-based lottery ticket for the lottery-prioritization of hyperparameter values, which is based on decouplings hyperparametry from the mask values. ","This paper studies the problem of model pruning in the lottery ticket framework, where the goal is to find a mask that maximizes the likelihood of the model to be pruned. The paper proposes a lottery ticket-based method for mask discovery (Hfind) and mask evaluation (Heval) for model evaluation, where a mask is found based on the training configuration and the number of layers in the model. Two different hyperparameters are used for these two stages: mask discovery and Heval. The decoupled find-evaluation phenomenon is attributed to the use of unstructured magnitude pruning for vision classification tasks, which decouples the decoupling of mask discovery from mask evaluation.   The paper shows that masks are found using different layerwise pruning ratios, and that these masks can be found using the same Hfind values. The authors also show that these ratios can be used to decouple masks in a one-shot structured pruning, where models are pruned sequentially.  The authors further propose a lottery-ticket-based lottery ticket for the lottery-prioritization of hyperparameter values, which is based on decouplings hyperparametry from the mask values. "
1209,SP:2d6f5d72b21675f74ff4cde4d16bfb36abd5795f,"training USED-FOR alignment of per - example gradients. metrics COMPARE m - coherence. m - coherence COMPARE metrics. m - coherence COMPARE O(m ). O(m ) COMPARE m - coherence. memorization CONJUNCTION generalization. generalization CONJUNCTION memorization. ResNet CONJUNCTION EfficientNet models. EfficientNet models CONJUNCTION ResNet. m - coherence USED-FOR evolution of alignment of per - example gradients. label noise FEATURE-OF variants. ImageNet EVALUATE-FOR EfficientNet models. m - coherence COMPARE real labels. real labels COMPARE m - coherence. neural networks USED-FOR generalization. OtherScientificTerm are gradient, and gradient diversity. Method are Coherent Gradients ( CG ) theory, over - parameterized neural networks, and CG. ","This paper studies the evolution of alignment of per-example gradients during training. The authors consider the Coherent Gradients (CG) theory and propose two metrics, m-coherence and O(m). They show that m-cherence is more robust to label noise than other metrics, and that the gradient diversity is more important than the gradient coherence. They also show that over-parameterized neural networks are more likely to memorize than non-overparametrized neural networks. Finally, they empirically show that ResNet and EfficientNet models trained on ImageNet with m-Coherence outperform real labels in terms of memorization and generalization. The main contribution of this paper is that the authors propose two variants of Coherent CG, one with label noise, and one with gradient diversity. ","This paper studies the evolution of alignment of per-example gradients during training. The authors consider the Coherent Gradients (CG) theory and propose two metrics, m-coherence and O(m). They show that m-cherence is more robust to label noise than other metrics, and that the gradient diversity is more important than the gradient coherence. They also show that over-parameterized neural networks are more likely to memorize than non-overparametrized neural networks. Finally, they empirically show that ResNet and EfficientNet models trained on ImageNet with m-Coherence outperform real labels in terms of memorization and generalization. The main contribution of this paper is that the authors propose two variants of Coherent CG, one with label noise, and one with gradient diversity. "
1225,SP:e7c5de9a475d0ba71bc79580e8436024fb2c6f59,"summary statistics USED-FOR implicit generative models. approach USED-FOR approximate Bayesian computation. approximate Bayesian computation CONJUNCTION neural likelihood methods. neural likelihood methods CONJUNCTION approximate Bayesian computation. approach USED-FOR neural likelihood methods. tasks EVALUATE-FOR approach. Task are evaluation of the likelihood function, and constructing sufficient statistics. OtherScientificTerm are likelihood function, sufficient statistics, and density or density ratio. Generic is model. Method are deep neural networks, and infomax learning procedure. ","This paper considers the problem of constructing sufficient statistics for implicit generative models based on summary statistics. The authors propose a new approach to approximate Bayesian computation and neural likelihood methods. The evaluation of the likelihood function is based on the assumption that the true likelihood function can be decomposed into sufficient statistics and sufficient statistics that are not sufficient statistics, i.e. that the density or density ratio of the model is equal to the sum of the density of the true distribution and the sum over all the labels of the training data points.   The authors show that this approach can be applied to a variety of tasks, and that their approach is able to improve the performance of existing neural likelihood algorithms. They also show that the infomax learning procedure can be extended to deep neural networks, which is an interesting direction. ","This paper considers the problem of constructing sufficient statistics for implicit generative models based on summary statistics. The authors propose a new approach to approximate Bayesian computation and neural likelihood methods. The evaluation of the likelihood function is based on the assumption that the true likelihood function can be decomposed into sufficient statistics and sufficient statistics that are not sufficient statistics, i.e. that the density or density ratio of the model is equal to the sum of the density of the true distribution and the sum over all the labels of the training data points.   The authors show that this approach can be applied to a variety of tasks, and that their approach is able to improve the performance of existing neural likelihood algorithms. They also show that the infomax learning procedure can be extended to deep neural networks, which is an interesting direction. "
1241,SP:c5997bf2348e94949684f45fbd418661e85220c1,"set - level supervision USED-FOR data collection. paired images CONJUNCTION domain labels. domain labels CONJUNCTION paired images. model COMPARE set - level supervised model. set - level supervised model COMPARE model. pseudo domains HYPONYM-OF hyperparameters. full labels USED-FOR set - level supervised model. TUNIT USED-FOR semi - supervised scenario. Method is image - to - image translation model. Task is image - to - image translation. OtherScientificTerm are image domains, and estimated domains. ","This paper proposes a new image-to-image translation model, called TUNIT, for the task of image to image translation. The key idea is to use set-level supervision for data collection, where the goal is to ensure that the paired images and the domain labels are consistent across all image domains. The proposed model is shown to outperform a standard set -level supervised model with full labels, and is also shown to be more robust to hyperparameters (e.g., pseudo domains). The authors also show that the proposed model can be used in a semi-supervised scenario where the estimated domains are not fully labeled, and that the model is able to generalize well to unseen domains. Experiments are conducted on a few datasets to validate the effectiveness of the proposed method.  ","This paper proposes a new image-to-image translation model, called TUNIT, for the task of image to image translation. The key idea is to use set-level supervision for data collection, where the goal is to ensure that the paired images and the domain labels are consistent across all image domains. The proposed model is shown to outperform a standard set -level supervised model with full labels, and is also shown to be more robust to hyperparameters (e.g., pseudo domains). The authors also show that the proposed model can be used in a semi-supervised scenario where the estimated domains are not fully labeled, and that the model is able to generalize well to unseen domains. Experiments are conducted on a few datasets to validate the effectiveness of the proposed method.  "
1257,SP:0cd97e64e638cabbeea0fdef3e9c5b33f4000f72,"probability distribution USED-FOR network parameters. it USED-FOR initialization procedures. probability distribution USED-FOR curvature penalty function. asymmetric initialization USED-FOR constant curvature penalty. natural cubic spline interpolation USED-FOR solution function. uniform distribution USED-FOR asymmetric initialization. multivariate regression CONJUNCTION activation functions. activation functions CONJUNCTION multivariate regression. regularization strength FEATURE-OF spatially adaptive smoothing splines. spatially adaptive smoothing splines USED-FOR training trajectories. Method are wide neural networks, and width - n shallow ReLU network. OtherScientificTerm are implicit bias in function space, and weighted second derivative. Task is 1D regression. ","This paper considers wide neural networks with implicit bias in function space. The authors propose a curvature penalty function based on the probability distribution of the network parameters, and show that it can be applied to different initialization procedures. They show that for a width-n shallow ReLU network, the solution function is a natural cubic spline interpolation. They also show that an asymmetric initialization with uniform distribution can be used to compute a constant curvature, and that it is equivalent to a weighted second derivative. They then show that a uniform distribution is sufficient to compute the symmetric initialization for the same solution function. Finally, they show that spatially adaptive smoothing splines with a certain regularization strength are sufficient for training trajectories for multivariate regression and activation functions.  ","This paper considers wide neural networks with implicit bias in function space. The authors propose a curvature penalty function based on the probability distribution of the network parameters, and show that it can be applied to different initialization procedures. They show that for a width-n shallow ReLU network, the solution function is a natural cubic spline interpolation. They also show that an asymmetric initialization with uniform distribution can be used to compute a constant curvature, and that it is equivalent to a weighted second derivative. They then show that a uniform distribution is sufficient to compute the symmetric initialization for the same solution function. Finally, they show that spatially adaptive smoothing splines with a certain regularization strength are sufficient for training trajectories for multivariate regression and activation functions.  "
1273,SP:8b885142facbb3b8db41ec9d83822cee81324694,Weight decay HYPONYM-OF regularization technique. regularization technique USED-FOR deep neural networks. L2 regularization USED-FOR weight decay. L2 regularization USED-FOR deep learning libraries. L2 regularization COMPARE weight decay. weight decay COMPARE L2 regularization. weight decay USED-FOR adaptive gradient methods. L2 regularization USED-FOR adaptive gradient methods. Decoupled Weight Decay ( AdamW ) USED-FOR Adam. Adaptive Momentum Estimation ( Adam ) HYPONYM-OF adaptive gradient methods. L2 regularization CONJUNCTION decoupled weight decay. decoupled weight decay CONJUNCTION L2 regularization. decoupled weight decay PART-OF deep learning libraries. decoupled weight decay HYPONYM-OF weight decay. L2 regularization HYPONYM-OF weight decay. unstable weight decay USED-FOR optimizers. stochastic gradient descent ( SGD ) HYPONYM-OF Momentum. stochastic gradient descent ( SGD ) HYPONYM-OF optimizers. Momentum USED-FOR optimizers. decoupled weight decay USED-FOR adaptive gradient methods. Stable Weight Decay ( SWD ) method USED-FOR unstable weight decay problem. L2 regularization CONJUNCTION decoupled weight decay. decoupled weight decay CONJUNCTION L2 regularization. SWD method COMPARE decoupled weight decay. decoupled weight decay COMPARE SWD method. SWD method COMPARE L2 regularization. L2 regularization COMPARE SWD method. weight decay PART-OF Adam. hyperparameters FEATURE-OF Adam variants. SWD USED-FOR Adam. SWD USED-FOR weight decay. OtherScientificTerm is hyperparameter. ,"Weight decay is a popular regularization technique for training deep neural networks. In this paper, the authors show that L2 regularization for weight decay is equivalent to weight decay for adaptive gradient methods such as Adaptive Momentum Estimation (Adam) and decoupled weight decay (Decoupled Weight Decay (AdamW) in deep learning libraries). They also show that unstable weight decay in these optimizers such as Momentum (e.g. stochastic gradient descent (SGD) can cause the optimizers to overfit to the hyperparameter. The authors then propose a Stable Weight decay (SWD) method to solve the unstable weight decaying problem. They show that the SWD method is more stable than the L2 and the decoupling weight decay used in adaptive gradient algorithms. They also demonstrate that SWD can be used to improve the performance of Adam by incorporating weight decay into the training process. Finally, they show that Adam variants with different hyperparameters can be trained with SWD. ","Weight decay is a popular regularization technique for training deep neural networks. In this paper, the authors show that L2 regularization for weight decay is equivalent to weight decay for adaptive gradient methods such as Adaptive Momentum Estimation (Adam) and decoupled weight decay (Decoupled Weight Decay (AdamW) in deep learning libraries). They also show that unstable weight decay in these optimizers such as Momentum (e.g. stochastic gradient descent (SGD) can cause the optimizers to overfit to the hyperparameter. The authors then propose a Stable Weight decay (SWD) method to solve the unstable weight decaying problem. They show that the SWD method is more stable than the L2 and the decoupling weight decay used in adaptive gradient algorithms. They also demonstrate that SWD can be used to improve the performance of Adam by incorporating weight decay into the training process. Finally, they show that Adam variants with different hyperparameters can be trained with SWD. "
1289,SP:a3206dc71e32ba1830895bf442d3840f3331a532,"Translation Memory ( TM ) USED-FOR neural machine translation ( NMT ). translation quality EVALUATE-FOR neural machine translation ( NMT ). translation quality EVALUATE-FOR Translation Memory ( TM ). encoder USED-FOR TM. TM CONJUNCTION NMT. NMT CONJUNCTION TM. method USED-FOR NMT. method USED-FOR TM. encoder USED-FOR TM information. pre - trained language model ( PLM ) USED-FOR encoder. sentence level retrieval method USED-FOR n - gram retrieval method. methods USED-FOR information flow. TM CONJUNCTION NMT decoder. NMT decoder CONJUNCTION TM. translation quality EVALUATE-FOR methods. OtherScientificTerm are semantic relationship, and similarity score. Method is sentence level retrieval approach. ",This paper proposes a new method to improve the translation quality of neural machine translation (NMT) using Translation Memory (TM) for improving the performance of NMT. The proposed method is based on the observation that the encoder of a pre-trained language model (PLM) that is used to encode the semantic relationship between two sentences is more likely to encode information about the similarity score between the two sentences. The authors propose a sentence level retrieval approach to retrieve the information from this encoder for the purpose of improving the quality of the TM and NMT performance. They also propose an n-gram retrieval method that uses a sentence-level retrieval method. Experiments show that the proposed methods can improve the information flow between the TM encoder and the NMT decoder while maintaining the same translation quality.  ,This paper proposes a new method to improve the translation quality of neural machine translation (NMT) using Translation Memory (TM) for improving the performance of NMT. The proposed method is based on the observation that the encoder of a pre-trained language model (PLM) that is used to encode the semantic relationship between two sentences is more likely to encode information about the similarity score between the two sentences. The authors propose a sentence level retrieval approach to retrieve the information from this encoder for the purpose of improving the quality of the TM and NMT performance. They also propose an n-gram retrieval method that uses a sentence-level retrieval method. Experiments show that the proposed methods can improve the information flow between the TM encoder and the NMT decoder while maintaining the same translation quality.  
1305,SP:72b43991a242872b2ceb1861e8ffbdf26c9f4818,rate reduction CONJUNCTION ( shift ) invariant classification. ( shift ) invariant classification CONJUNCTION rate reduction. rate reduction USED-FOR deep ( convolutional ) networks. ( shift ) invariant classification USED-FOR deep ( convolutional ) networks. iterative gradient ascent scheme USED-FOR rate reduction of learned features. iterative gradient ascent scheme USED-FOR deep network. components PART-OF network. network USED-FOR discriminative deep representation. linear operators PART-OF multi - channel convolutions. spectral domain FEATURE-OF convolutional network. Generic is architectures. Method is back propagation training. Task is classification. ,"This paper studies the connection between rate reduction and (shift) invariant classification in deep (convolutional) networks. The authors propose an iterative gradient ascent scheme for rate reduction of learned features in a deep network. They show that linear operators in multi-channel convolutions can be reduced to linear operators, and that the resulting architectures are invariant to back propagation training. They also show that a convolutional network in the spectral domain can be transformed into a discriminative deep representation by removing components of the network that are not relevant to the classification.","This paper studies the connection between rate reduction and (shift) invariant classification in deep (convolutional) networks. The authors propose an iterative gradient ascent scheme for rate reduction of learned features in a deep network. They show that linear operators in multi-channel convolutions can be reduced to linear operators, and that the resulting architectures are invariant to back propagation training. They also show that a convolutional network in the spectral domain can be transformed into a discriminative deep representation by removing components of the network that are not relevant to the classification."
1321,SP:f8b02cf1b918b0956761829ec6ef9127596071ec,"implicit acceleration of gradient flow USED-FOR over - parameterized two - layer linear models. conservation law USED-FOR implicit acceleration. spectrum USED-FOR acceleration. matrix factorization problem CONJUNCTION Riccati type differential equations. Riccati type differential equations CONJUNCTION matrix factorization problem. small, balanced or spectral initialization FEATURE-OF weights. Method is gradient flow. OtherScientificTerm is Gramian matrices. ","This paper studies the implicit acceleration of gradient flow for over-parameterized two-layer linear models. The authors prove a conservation law for implicit acceleration under the spectrum of the gradient flow. They also prove that the acceleration depends on the spectrum, which is a result of the matrix factorization problem and Riccati type differential equations. They show that the weights with small, balanced or spectral initialization have small, but non-negative, gradient flow, and that the corresponding Gramian matrices are non-degenerate.","This paper studies the implicit acceleration of gradient flow for over-parameterized two-layer linear models. The authors prove a conservation law for implicit acceleration under the spectrum of the gradient flow. They also prove that the acceleration depends on the spectrum, which is a result of the matrix factorization problem and Riccati type differential equations. They show that the weights with small, balanced or spectral initialization have small, but non-negative, gradient flow, and that the corresponding Gramian matrices are non-degenerate."
1337,SP:e5f086c806be88d50e461a782b5b00124f4656fb,"approach USED-FOR opaque model ’s behavior. uniform sampling of user - defined subspaces USED-FOR framework. framework USED-FOR ML model. CLIME USED-FOR ML model. CLIME HYPONYM-OF framework. Method are machine learning techniques, LIME, surrogate interpretable model, LIME framework, and estimation algorithm. Task are explainable AI, OOD sampling problem, OOD sampling, and real - world problems. OtherScientificTerm are LIME ’s explanations, adversarial attacks, perturbation procedure, and logical constraints. Generic is model. Metric are fidelity, and accuracy. ","This paper proposes a new approach to explain the opaque model’s behavior in machine learning techniques. The approach is called explainable AI. The framework is based on the uniform sampling of user-defined subspaces, which is known as LIME. The authors propose a surrogate interpretable model called CLIME, which can be seen as an extension of the LIME framework. CLIME can be used to train an ML model with a standard ML model (e.g., LIME) and then explain the model to the user.    The main contribution of the paper is to propose a new OOD sampling problem that is motivated by the observation that LIME “explains” the opaque behavior of the model. The main idea is to use adversarial attacks to fool the model by perturbing it with a perturbation procedure that has logical constraints. The paper also proposes an estimation algorithm that can be applied to any OOD sample from the model, and the authors show that their approach is more robust to adversarial perturbations.  The authors also show that CLIME is able to be used for real-world problems where the model is not fully explainable, and that the fidelity of the explanation is lower than that of the original model.","This paper proposes a new approach to explain the opaque model’s behavior in machine learning techniques. The approach is called explainable AI. The framework is based on the uniform sampling of user-defined subspaces, which is known as LIME. The authors propose a surrogate interpretable model called CLIME, which can be seen as an extension of the LIME framework. CLIME can be used to train an ML model with a standard ML model (e.g., LIME) and then explain the model to the user.    The main contribution of the paper is to propose a new OOD sampling problem that is motivated by the observation that LIME “explains” the opaque behavior of the model. The main idea is to use adversarial attacks to fool the model by perturbing it with a perturbation procedure that has logical constraints. The paper also proposes an estimation algorithm that can be applied to any OOD sample from the model, and the authors show that their approach is more robust to adversarial perturbations.  The authors also show that CLIME is able to be used for real-world problems where the model is not fully explainable, and that the fidelity of the explanation is lower than that of the original model."
1353,SP:b1d5ef15772e192eb8c8a0e65b3c21ee7c794295,Pre - trained language models USED-FOR natural language understanding ( NLU ). BERT HYPONYM-OF Pre - trained language models. Chinese HYPONYM-OF languages. English HYPONYM-OF languages. multi - word expressions USED-FOR natural lexical units. fine - grained and coarse - grained tokenizations USED-FOR pre - trained language models. AMBERT HYPONYM-OF Multi - grained BERT. AMBERT HYPONYM-OF pre - trained language model. fine - grained and coarse - grained tokenizations USED-FOR pre - trained language model. contextualized representations of the words CONJUNCTION contextualized representations of the phrases. contextualized representations of the phrases CONJUNCTION contextualized representations of the words. encoder CONJUNCTION encoder. encoder CONJUNCTION encoder. English USED-FOR AMBERT. encoder USED-FOR AMBERT. GLUE CONJUNCTION SQuAD. SQuAD CONJUNCTION GLUE. SQuAD CONJUNCTION RACE. RACE CONJUNCTION SQuAD. CLUE CONJUNCTION GLUE. GLUE CONJUNCTION CLUE. Chinese CONJUNCTION English. English CONJUNCTION Chinese. Chinese HYPONYM-OF benchmark datasets. English HYPONYM-OF benchmark datasets. RACE HYPONYM-OF benchmark datasets. SQuAD HYPONYM-OF benchmark datasets. CLUE HYPONYM-OF benchmark datasets. GLUE HYPONYM-OF benchmark datasets. AMBERT COMPARE models. models COMPARE AMBERT. Chinese EVALUATE-FOR AMBERT. AMBERT COMPARE AMBERT. AMBERT COMPARE AMBERT. Method is coarse - grained tokenization. Metric is inference time. ,"Pre-trained language models such as BERT and AMBERT have been shown to be effective for natural language understanding (NLU) in languages such as Chinese, English, and French. Multi-word expressions are used to represent natural lexical units, and the authors show that fine-grained and coarse-rigorous tokenizations can be used to improve the performance of the pre-trained language models.   Multi-granular BERT (AMBERT) is a well-known pre-trained BERT model that uses fine-rigid tokenization. AMBERt is trained on English and French, and is a pre-training language model.  The authors use three benchmark datasets (CLUE, GLUE, SQuAD, and RACE) to evaluate the performance on different languages. They show that the performance improves when the encoder and encoder are fine-tuned, and when the contextualized representations of the words are fine tuned, and that the inference time is reduced. They also show that AMBERTs are able to outperform models trained on Chinese and English. ","Pre-trained language models such as BERT and AMBERT have been shown to be effective for natural language understanding (NLU) in languages such as Chinese, English, and French. Multi-word expressions are used to represent natural lexical units, and the authors show that fine-grained and coarse-rigorous tokenizations can be used to improve the performance of the pre-trained language models.   Multi-granular BERT (AMBERT) is a well-known pre-trained BERT model that uses fine-rigid tokenization. AMBERt is trained on English and French, and is a pre-training language model.  The authors use three benchmark datasets (CLUE, GLUE, SQuAD, and RACE) to evaluate the performance on different languages. They show that the performance improves when the encoder and encoder are fine-tuned, and when the contextualized representations of the words are fine tuned, and that the inference time is reduced. They also show that AMBERTs are able to outperform models trained on Chinese and English. "
1369,SP:fd1cfe80343d3789227d99d836a5674374a234f5,"task USED-FOR natural language utterance. Semantic parsing HYPONYM-OF task. natural language utterance USED-FOR machine - understandable information representation. task USED-FOR machine - understandable information representation. Transformer USED-FOR semantic parsing. PhraseTransformer architecture USED-FOR meaning representation. phrase dependencies USED-FOR PhraseTransformer architecture. phrase dependencies USED-FOR meaning representation. Long Short - Term Memory ( LSTM ) USED-FOR local context of phrases. Self - Attention mechanism USED-FOR local context of phrases. Long Short - Term Memory ( LSTM ) PART-OF Self - Attention mechanism. Self - Attention mechanism PART-OF Transformer. model COMPARE Transformer. Transformer COMPARE model. model USED-FOR detailed meaning. model USED-FOR local context awareness. Neural Network USED-FOR Atis dataset. Geo, MSParS datasets EVALUATE-FOR model. Method is Neural Machine Translation. OtherScientificTerm is long - range word dependencies. ","Semantic parsing is an important task for learning machine-understandable information representation from natural language utterance. In this paper, the authors propose a PhraseTransformer architecture that learns a meaning representation based on phrase dependencies using the phrase dependencies. The Self-Attention mechanism in the Transformer is replaced by a Long Short-Term Memory (LSTM) to capture the local context of phrases, which is used to incorporate long-range word dependencies. Experiments on the Geo, MSParS datasets show that the proposed model outperforms the original Transformer for semantic parsing. The model is also able to capture local context awareness through the use of a Neural Network on the Atis dataset.    The paper is well-written and well-motivated. The idea of Neural Machine Translation is interesting, and the model is able to learn detailed meaning from a single sentence. ","Semantic parsing is an important task for learning machine-understandable information representation from natural language utterance. In this paper, the authors propose a PhraseTransformer architecture that learns a meaning representation based on phrase dependencies using the phrase dependencies. The Self-Attention mechanism in the Transformer is replaced by a Long Short-Term Memory (LSTM) to capture the local context of phrases, which is used to incorporate long-range word dependencies. Experiments on the Geo, MSParS datasets show that the proposed model outperforms the original Transformer for semantic parsing. The model is also able to capture local context awareness through the use of a Neural Network on the Atis dataset.    The paper is well-written and well-motivated. The idea of Neural Machine Translation is interesting, and the model is able to learn detailed meaning from a single sentence. "
1385,SP:2056a65a7500d79465685af883083cd706277c1f,"combinations of multiple perturbations FEATURE-OF DNN robustness. composite adversarial training ( CAT ) HYPONYM-OF training method. robustness EVALUATE-FOR individual perturbations. training method USED-FOR multiple adversarial losses. pixel perturbations CONJUNCTION spatial transformations. spatial transformations CONJUNCTION pixel perturbations. CAT COMPARE adversarial training methods. adversarial training methods COMPARE CAT. benchmark datasets EVALUATE-FOR CAT. spatial transformations HYPONYM-OF adversarial perturbation models. Method are deep neural networks ( DNNs ), and individual perturbation models. OtherScientificTerm is adversarial perturbations. ","This paper proposes a new training method called composite adversarial training (CAT) to improve the robustness of deep neural networks (DNNs) against combinations of multiple adversarial perturbations. The authors claim that individual perturbation models are not robust enough to achieve the same robustness as individual adversarial losses, so they propose a training method that combines multiple perturbational losses to achieve better robustness. They show that the proposed CAT outperforms other state-of-the-art adversarial robustness methods on a number of benchmark datasets. They also show that CAT can be applied to adversarial models that are more robust to different types of adversarial attacks, such as pixel-based and spatial-based.","This paper proposes a new training method called composite adversarial training (CAT) to improve the robustness of deep neural networks (DNNs) against combinations of multiple adversarial perturbations. The authors claim that individual perturbation models are not robust enough to achieve the same robustness as individual adversarial losses, so they propose a training method that combines multiple perturbational losses to achieve better robustness. They show that the proposed CAT outperforms other state-of-the-art adversarial robustness methods on a number of benchmark datasets. They also show that CAT can be applied to adversarial models that are more robust to different types of adversarial attacks, such as pixel-based and spatial-based."
1401,SP:006e5b9ac9a8eb7223843731488bfefbd8eb09bd,"Emergent Symbol Binding Network ( ESBN ) HYPONYM-OF recurrent network. external memory USED-FOR variable - binding. external memory USED-FOR recurrent network. binding mechanism USED-FOR symbol - like representations. ESBN USED-FOR rules. learning process USED-FOR symbol - like representations. architecture COMPARE competitive neural network architectures. competitive neural network architectures COMPARE architecture. Task are human intelligence, and induction of abstract rules. OtherScientificTerm are abstract rules, and symbol - processing machinery. Material are high - dimensional sensory data, and high - dimensional data. Method are Deep neural network algorithms, and symbol - processing mechanisms. ","This paper proposes a new recurrent network called the Emergent Symbol Binding Network (ESBN), which is a recurrent network with an external memory for variable-bounding and a binding mechanism to learn symbol-like representations during the learning process. The authors show that the ESBN is able to learn abstract rules that can be used to guide the induction of abstract rules from high-dimensional sensory data. They also show that ESBN can learn rules that are invariant to changes in the input representation of the input, which is an important property of the symbol-processing machinery. Finally, the authors demonstrate that the proposed architecture outperforms competitive neural network architectures on a variety of tasks, and that the learned rules are more interpretable. Deep neural network algorithms have been shown to be interpretable in the past. However, this paper shows that this is not always the case when the input is high dimensional. The paper also shows that the learning of abstract rule induction is not tractable when high-dimensionality is used.    The paper is well-written and well-motivated, and the authors have done a good job of providing a theoretical analysis of the inductive bias of existing symbol-processing mechanisms.  However, the paper is lacking in novelty.","This paper proposes a new recurrent network called the Emergent Symbol Binding Network (ESBN), which is a recurrent network with an external memory for variable-bounding and a binding mechanism to learn symbol-like representations during the learning process. The authors show that the ESBN is able to learn abstract rules that can be used to guide the induction of abstract rules from high-dimensional sensory data. They also show that ESBN can learn rules that are invariant to changes in the input representation of the input, which is an important property of the symbol-processing machinery. Finally, the authors demonstrate that the proposed architecture outperforms competitive neural network architectures on a variety of tasks, and that the learned rules are more interpretable. Deep neural network algorithms have been shown to be interpretable in the past. However, this paper shows that this is not always the case when the input is high dimensional. The paper also shows that the learning of abstract rule induction is not tractable when high-dimensionality is used.    The paper is well-written and well-motivated, and the authors have done a good job of providing a theoretical analysis of the inductive bias of existing symbol-processing mechanisms.  However, the paper is lacking in novelty."
1417,SP:4171ce45966ac499f51450a19fb233934c0847f0,"nested named entity recognition CONJUNCTION relation classification. relation classification CONJUNCTION nested named entity recognition. framework USED-FOR structured prediction language tasks. relation classification CONJUNCTION semantic role labeling. semantic role labeling CONJUNCTION relation classification. event extraction CONJUNCTION coreference resolution. coreference resolution CONJUNCTION event extraction. semantic role labeling CONJUNCTION event extraction. event extraction CONJUNCTION semantic role labeling. joint entity and relation extraction CONJUNCTION nested named entity recognition. nested named entity recognition CONJUNCTION joint entity and relation extraction. coreference resolution CONJUNCTION dialogue state tracking. dialogue state tracking CONJUNCTION coreference resolution. dialogue state tracking HYPONYM-OF structured prediction language tasks. coreference resolution HYPONYM-OF structured prediction language tasks. event extraction HYPONYM-OF structured prediction language tasks. semantic role labeling HYPONYM-OF structured prediction language tasks. joint entity and relation extraction HYPONYM-OF structured prediction language tasks. relation classification HYPONYM-OF structured prediction language tasks. nested named entity recognition HYPONYM-OF structured prediction language tasks. translation task USED-FOR it. augmented natural languages USED-FOR translation task. task - specific discriminative classifiers USED-FOR problem. FewRel CONJUNCTION TACRED. TACRED CONJUNCTION FewRel. approach COMPARE task - specific models. task - specific models COMPARE approach. relation classification CONJUNCTION semantic role labeling. semantic role labeling CONJUNCTION relation classification. joint entity and relation extraction CONJUNCTION relation classification. relation classification CONJUNCTION joint entity and relation extraction. relation classification CONJUNCTION FewRel. FewRel CONJUNCTION relation classification. relation classification CONJUNCTION TACRED. TACRED CONJUNCTION relation classification. approach COMPARE,., COMPARE approach. relation classification EVALUATE-FOR,. joint entity and relation extraction EVALUATE-FOR,. tasks EVALUATE-FOR task - specific models. tasks EVALUATE-FOR approach. joint entity and relation extraction EVALUATE-FOR approach. model USED-FOR tasks. hyperparameters USED-FOR tasks. architecture USED-FOR tasks. architecture CONJUNCTION","This paper proposes a new framework for structured prediction language tasks such as joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. The authors propose a new translation task based on augmented natural languages and use it as a translation task. They also propose task-specific discriminative classifiers to solve the problem. The proposed approach is evaluated on a variety of tasks and shows superior performance over the task -specific models on most of the tasks. The paper also shows that the proposed architecture can be applied to other tasks with different hyperparameters.   The authors also show that their approach outperforms the state-of-the-art models on the tasks of joint entity (JE), relation classification (RCT), FewRel, and TACRED.","This paper proposes a new framework for structured prediction language tasks such as joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. The authors propose a new translation task based on augmented natural languages and use it as a translation task. They also propose task-specific discriminative classifiers to solve the problem. The proposed approach is evaluated on a variety of tasks and shows superior performance over the task -specific models on most of the tasks. The paper also shows that the proposed architecture can be applied to other tasks with different hyperparameters.   The authors also show that their approach outperforms the state-of-the-art models on the tasks of joint entity (JE), relation classification (RCT), FewRel, and TACRED."
1433,SP:8f1b2fc6829e0bdfcc981020b0dcf3e63a947910,"named entity recognition ( NER ) models USED-FOR unlabeled entity problem. approach USED-FOR misguidance. unlabeled entities USED-FOR NER models. synthetic datasets CONJUNCTION real - world datasets. real - world datasets CONJUNCTION synthetic datasets. model COMPARE prior baselines. prior baselines COMPARE model. real - world datasets EVALUATE-FOR model. synthetic datasets EVALUATE-FOR model. model USED-FOR unlabeled entity problem. well - annotated datasets EVALUATE-FOR model. OtherScientificTerm is negative instances. Method are pretraining language models, and negative sampling. ","This paper studies the unlabeled entity recognition (NER) problem. The authors propose a novel approach to address the issue of misguidance in NER models, which they call “negative instances”. Specifically, the authors propose to use a pre-trained NER model to sample negative instances from the training data, and then use the negative instances to improve the performance of the model. They show that the proposed approach outperforms prior work on both synthetic datasets and two real-world datasets. ","This paper studies the unlabeled entity recognition (NER) problem. The authors propose a novel approach to address the issue of misguidance in NER models, which they call “negative instances”. Specifically, the authors propose to use a pre-trained NER model to sample negative instances from the training data, and then use the negative instances to improve the performance of the model. They show that the proposed approach outperforms prior work on both synthetic datasets and two real-world datasets. "
1449,SP:dd76ece8d92a8a230a8b43033d8cb2368c677a94,"stochastic neighbor embedding ( SNE ) USED-FOR sequential inputs. stochastic neighbor embedding ( SNE ) USED-FOR vector space of fixed, reduced dimensions. Acoustic Neighbor Embeddings HYPONYM-OF acoustic word embedding. Euclidean distance USED-FOR phonetic confusability. acoustic encoder CONJUNCTION text encoder. text encoder CONJUNCTION acoustic encoder. acoustic encoder USED-FOR speech signals. frame - wise subword posterior probabilities USED-FOR acoustic encoder. frame - wise subword posterior probabilities FEATURE-OF speech signals. acoustic model USED-FOR frame - wise subword posterior probabilities. acoustic encoder HYPONYM-OF encoder neural networks. text encoder HYPONYM-OF encoder neural networks. triplet loss criterion COMPARE method. method COMPARE triplet loss criterion. gradients USED-FOR neural network training. method USED-FOR neural network training. gradients EVALUATE-FOR method. text encoder network USED-FOR approximate phonetic matching task. encoder networks USED-FOR word ( name ) recognition task. low - dimensional embeddings USED-FOR it. encoder networks USED-FOR it. recognition accuracy EVALUATE-FOR finite state transducer(FST)-based decoding. test data USED-FOR finite state transducer(FST)-based decoding. Euclidean nearest - neighbor search USED-FOR isolated name recognition task. Material is speech. OtherScientificTerm are embedding space, subword transcriptions, embedding vectors, and embeddings. ","This paper introduces Acoustic Neighbor Embeddings, an acoustic word embedding (i.e., Acoustic Neighbour Embedding) that is based on stochastic neighbor embedding for sequential inputs.    The authors propose to embed the input sequence into a vector space of fixed, reduced dimensions, and use an encoder neural network to map the input to the embedding space.  The acoustic encoder and text encoder are used to encode speech signals into frame-wise subword posterior probabilities, which are then used to train an acoustic model to predict the subword transcriptions of the input.  In order to achieve phonetic confusability, the authors use the Euclidean distance between the embeddings of the subwords and their corresponding embedding vectors. The authors show that the triplet loss criterion is more robust than the proposed method for neural network training in terms of gradients, and that it can be applied to any encoder networks for word (name) recognition task, as long-range embedding can be used.  Experiments are performed on the isolated name recognition task using Euclideans, where the encoder network is trained on the test data of an approximate phonetic matching task, and the text network is used to learn a text-to-speech embedding of a given word.  Experimental results show that this method is able to improve the recognition accuracy of finite state transducer(FST)-based decoding on test data, and it is also able to learn low-dimensional embedding to achieve better performance.  Empirical results are also presented for Euclideal nearest-neighbor search on the problem of isolated name retrieval from a given input, using Euclidesan nearest-nodes. ","This paper introduces Acoustic Neighbor Embeddings, an acoustic word embedding (i.e., Acoustic Neighbour Embedding) that is based on stochastic neighbor embedding for sequential inputs.    The authors propose to embed the input sequence into a vector space of fixed, reduced dimensions, and use an encoder neural network to map the input to the embedding space.  The acoustic encoder and text encoder are used to encode speech signals into frame-wise subword posterior probabilities, which are then used to train an acoustic model to predict the subword transcriptions of the input.  In order to achieve phonetic confusability, the authors use the Euclidean distance between the embeddings of the subwords and their corresponding embedding vectors. The authors show that the triplet loss criterion is more robust than the proposed method for neural network training in terms of gradients, and that it can be applied to any encoder networks for word (name) recognition task, as long-range embedding can be used.  Experiments are performed on the isolated name recognition task using Euclideans, where the encoder network is trained on the test data of an approximate phonetic matching task, and the text network is used to learn a text-to-speech embedding of a given word.  Experimental results show that this method is able to improve the recognition accuracy of finite state transducer(FST)-based decoding on test data, and it is also able to learn low-dimensional embedding to achieve better performance.  Empirical results are also presented for Euclideal nearest-neighbor search on the problem of isolated name retrieval from a given input, using Euclidesan nearest-nodes. "
1465,SP:9142189126b8612ac0acee6fe18a0cfcb70b6545,reinforcement learning algorithm USED-FOR stationary mean - field games. mean - field state USED-FOR Nash equilibrium. mean - field state CONJUNCTION policy. policy CONJUNCTION mean - field state. gradient - descent CONJUNCTION proximal policy optimization. proximal policy optimization CONJUNCTION gradient - descent. mean - field state CONJUNCTION policy. policy CONJUNCTION mean - field state. proximal policy optimization USED-FOR fictitious play algorithm. gradient - descent USED-FOR fictitious play algorithm. gradient - descent USED-FOR policy. proximal policy optimization USED-FOR policy. algorithm USED-FOR single - agent reinforcement learning problem. fictitious play algorithm USED-FOR Nash equilibrium. OtherScientificTerm is optimum. Task is mean - field games. ,"This paper proposes a new reinforcement learning algorithm for stationary mean-field games, where the optimum is a stationary point of the game. The authors show that the mean- field state and the policy converge to a Nash equilibrium when the number of agents is large enough. They then propose a fictitious play algorithm based on gradient-descent and proximal policy optimization to optimize the mean - field state, policy, and policy. The algorithm is then applied to the single-agent reinforcement learning problem, and is shown to converge to the Nash equilibrium.","This paper proposes a new reinforcement learning algorithm for stationary mean-field games, where the optimum is a stationary point of the game. The authors show that the mean- field state and the policy converge to a Nash equilibrium when the number of agents is large enough. They then propose a fictitious play algorithm based on gradient-descent and proximal policy optimization to optimize the mean - field state, policy, and policy. The algorithm is then applied to the single-agent reinforcement learning problem, and is shown to converge to the Nash equilibrium."
1481,SP:c498f8a199da1818fe64ed88b0825c5aad688aec,joint distribution USED-FOR probabilistic inference. normalizing flow model USED-FOR probabilistic inference. normalizing flow model USED-FOR joint distribution. flow models USED-FOR task. framework USED-FOR approximate probabilistic inference. method USED-FOR generative model. flow model USED-FOR distribution. variational inference USED-FOR it. arbitrary differentiable transformations USED-FOR conditioning. likelihood evaluation CONJUNCTION inversion. inversion CONJUNCTION likelihood evaluation. it USED-FOR likelihood evaluation. inversion CONJUNCTION sampling. sampling CONJUNCTION inversion. it USED-FOR sampling. it USED-FOR inversion. inference tasks USED-FOR inverse problems. inference tasks EVALUATE-FOR method. approach COMPARE MCMC baselines. MCMC baselines COMPARE approach. sample quality EVALUATE-FOR MCMC baselines. sample quality EVALUATE-FOR approach. Generic is model. OtherScientificTerm is approximate posterior. ,"This paper proposes a normalizing flow model for probabilistic inference with a joint distribution over the joint distribution generated by a pair of flow models for a given task. The authors propose a framework for approximate probabilistically inference, where the distribution is learned by a flow model, and the model is conditioned on the distribution. The method learns a generative model by conditioning the distribution on a set of data points, and then uses variational inference to learn a conditional distribution over this conditional distribution. This conditioning is based on arbitrary differentiable transformations, and it can be used for likelihood evaluation, inversion, and sampling. The proposed method is evaluated on a number of inference tasks for inverse problems, where it is shown to outperform MCMC baselines in terms of likelihood evaluation and inversion. ","This paper proposes a normalizing flow model for probabilistic inference with a joint distribution over the joint distribution generated by a pair of flow models for a given task. The authors propose a framework for approximate probabilistically inference, where the distribution is learned by a flow model, and the model is conditioned on the distribution. The method learns a generative model by conditioning the distribution on a set of data points, and then uses variational inference to learn a conditional distribution over this conditional distribution. This conditioning is based on arbitrary differentiable transformations, and it can be used for likelihood evaluation, inversion, and sampling. The proposed method is evaluated on a number of inference tasks for inverse problems, where it is shown to outperform MCMC baselines in terms of likelihood evaluation and inversion. "
1497,SP:1d0f27f61c9d32911b8bd15d6b82ef5eec644f0f,Computer vision technology USED-FOR biological and medical data analysis and understanding. Ultra - high Resolution Image Segmentation dataset USED-FOR Cell membrane. iterative annotations CONJUNCTION uncompressed high - resolution raw data. uncompressed high - resolution raw data CONJUNCTION iterative annotations. annotated Electron Microscopy ( EM ) dataset USED-FOR Cell membrane. U - RISC HYPONYM-OF annotated Electron Microscopy ( EM ) dataset. U - RISC HYPONYM-OF Ultra - high Resolution Image Segmentation dataset. segmentation evaluation criteria COMPARE human perception. human perception COMPARE segmentation evaluation criteria. evaluation criterion USED-FOR cell membrane segmentation. Perceptual Hausdorff Distance ( PHD ) HYPONYM-OF evaluation criterion. evaluation criteria CONJUNCTION PHD. PHD CONJUNCTION evaluation criteria. segmentation methods CONJUNCTION iterative manual annotation. iterative manual annotation CONJUNCTION segmentation methods. evaluation criteria USED-FOR iterative manual annotation. ,"This paper presents a new ultra-high resolution image segmentation dataset, U-RISC, for cell membrane segmentation. Cell membrane is segmented using an annotated Electron Microscopy (EM) dataset, which is a variant of the recently proposed U-RCS dataset. The paper also introduces a new evaluation criterion, the Perceptual Hausdorff Distance (PHD), which is used to compare the performance of different segmentation methods and iterative manual annotation with uncompressed high-resolution raw data. Experiments show that the proposed segmentation evaluation criteria outperforms human perception and is competitive with existing evaluation criteria and PHD.","This paper presents a new ultra-high resolution image segmentation dataset, U-RISC, for cell membrane segmentation. Cell membrane is segmented using an annotated Electron Microscopy (EM) dataset, which is a variant of the recently proposed U-RCS dataset. The paper also introduces a new evaluation criterion, the Perceptual Hausdorff Distance (PHD), which is used to compare the performance of different segmentation methods and iterative manual annotation with uncompressed high-resolution raw data. Experiments show that the proposed segmentation evaluation criteria outperforms human perception and is competitive with existing evaluation criteria and PHD."
1513,SP:8ca7aff87c82be69c9542550c814f52c9419ab0a,"Continual Learning ( CL ) USED-FOR catastrophic forgetting. benchmarks USED-FOR CL algorithms. benchmarks USED-FOR forgetting. short streams of tasks USED-FOR benchmarks. short streams of tasks USED-FOR forgetting. modules USED-FOR atomic skills. modules PART-OF modular architecture. learning algorithm USED-FOR learning. exponential search space FEATURE-OF task - driven prior. task - driven prior USED-FOR learning algorithm. modular architecture CONJUNCTION learning algorithm. learning algorithm CONJUNCTION modular architecture. benchmarks EVALUATE-FOR learning algorithm. CL benchmarks EVALUATE-FOR modular architecture. CL benchmarks EVALUATE-FOR learning algorithm. Method is CL system. Generic are task, and Benchmark. ","This paper studies the problem of catastrophic forgetting in the context of Continual Learning (CL) and proposes two benchmarks for CL algorithms. The benchmarks are designed to address the issue of forgetting in short streams of tasks, where a CL system is trained on a small number of tasks. The authors propose a modular architecture that consists of modules for atomic skills, and a learning algorithm that uses a task-driven prior in the exponential search space to guide the learning. The learning algorithm is evaluated on a number of standard CL benchmarks, and is shown to outperform the state-of-the-art on each task. Benchmark results show that the proposed modular architecture and learning algorithm outperforms the state of the art on all benchmarks.","This paper studies the problem of catastrophic forgetting in the context of Continual Learning (CL) and proposes two benchmarks for CL algorithms. The benchmarks are designed to address the issue of forgetting in short streams of tasks, where a CL system is trained on a small number of tasks. The authors propose a modular architecture that consists of modules for atomic skills, and a learning algorithm that uses a task-driven prior in the exponential search space to guide the learning. The learning algorithm is evaluated on a number of standard CL benchmarks, and is shown to outperform the state-of-the-art on each task. Benchmark results show that the proposed modular architecture and learning algorithm outperforms the state of the art on all benchmarks."
1529,SP:cc819c61f408e88f247eb87946187ccec3dad32e,random selection CONJUNCTION clustering and/or augmentation. clustering and/or augmentation CONJUNCTION random selection. unsupervised meta - learning approaches USED-FOR synthetic meta - tasks. techniques USED-FOR synthetic meta - tasks. clustering and/or augmentation HYPONYM-OF techniques. random selection HYPONYM-OF techniques. approach USED-FOR metatasks. generative models USED-FOR approach. generative models USED-FOR metatasks. algorithms USED-FOR synthetic classes. synthetic classes PART-OF meta - task. approach COMPARE unsupervised learning baselines. unsupervised learning baselines COMPARE approach. benchmark datasets EVALUATE-FOR few - shot classification tasks. few - shot classification tasks EVALUATE-FOR unsupervised learning baselines. few - shot classification tasks EVALUATE-FOR approach. benchmark datasets EVALUATE-FOR unsupervised learning baselines. OtherScientificTerm is latent space. ,This paper proposes a meta-learning approach for few-shot learning of synthetic meta-tasks. The proposed approach is based on a generative model that is trained on a set of synthetic tasks. The authors show that the proposed approach outperforms a number of baselines on several benchmark datasets.  ,This paper proposes a meta-learning approach for few-shot learning of synthetic meta-tasks. The proposed approach is based on a generative model that is trained on a set of synthetic tasks. The authors show that the proposed approach outperforms a number of baselines on several benchmark datasets.  
1545,SP:b25771e5c214a352f74ba6196fbd88bca6c43c98,"inverse problems CONJUNCTION compressed sensing. compressed sensing CONJUNCTION inverse problems. it USED-FOR inference. Injectivity USED-FOR generative models. generative priors USED-FOR compressed sensing. injectivity FEATURE-OF fullyconnected and convolutional ReLU layers and networks. weight matrices USED-FOR injectivity. expansivity USED-FOR global injectivity. iid Gaussian matrices USED-FOR global injectivity. worst - case Lipschitz constants USED-FOR stability. arguments USED-FOR deep networks. differential topology USED-FOR arguments. injective ReLU network USED-FOR Lipschitz map. argument USED-FOR injectivity. random projections USED-FOR argument. neural networks USED-FOR nonlinear inverse and inference problems. OtherScientificTerm is well posedness. Method are layerwise analysis, tractable model, and injective network. ","This paper studies the injectivity of generative priors for fully connected and convolutional ReLU layers and networks. Injectivity is an important property for generative models, and it is particularly important for inference, especially for inverse problems and compressed sensing. In this paper, the authors show that injectivity is a property that is related to the expansivity of the weight matrices, and that it depends on the well posedness of the weights. The global injectivity depends on expansivity, i.e., the number of iid Gaussian matrices. The authors also show that the worst-case Lipschitz constants of the stability of the injective ReLU network can be derived from the weights of an injective network.   The authors provide a layerwise analysis of injectivity, and show that a tractable model can be learned that is well-posed. They also provide arguments to explain why injectivity exists in deep networks. The arguments are based on the differential topology, and the authors also provide an argument to explain injectivity in the case of random projections.  The paper concludes with a discussion of nonlinear inverse and inference problems that can be solved by neural networks. ","This paper studies the injectivity of generative priors for fully connected and convolutional ReLU layers and networks. Injectivity is an important property for generative models, and it is particularly important for inference, especially for inverse problems and compressed sensing. In this paper, the authors show that injectivity is a property that is related to the expansivity of the weight matrices, and that it depends on the well posedness of the weights. The global injectivity depends on expansivity, i.e., the number of iid Gaussian matrices. The authors also show that the worst-case Lipschitz constants of the stability of the injective ReLU network can be derived from the weights of an injective network.   The authors provide a layerwise analysis of injectivity, and show that a tractable model can be learned that is well-posed. They also provide arguments to explain why injectivity exists in deep networks. The arguments are based on the differential topology, and the authors also provide an argument to explain injectivity in the case of random projections.  The paper concludes with a discussion of nonlinear inverse and inference problems that can be solved by neural networks. "
1561,SP:a95a153d3fe9bcf535ebf8514f51d00df483f210,"generative model USED-FOR image generation. continuous conditional generative adversarial network ( CcGAN ) HYPONYM-OF generative model. conditional GANs ( cGANs ) USED-FOR categorical conditions. class labels HYPONYM-OF categorical conditions. hidden map CONJUNCTION one - hot encoded label. one - hot encoded label CONJUNCTION hidden map. hidden map PART-OF generator / discriminator. one - hot encoded label HYPONYM-OF label input methods. hidden map HYPONYM-OF label input methods. empirical cGAN losses HYPONYM-OF cGAN losses. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. empirical cGAN losses USED-FOR continuous scenario. regression labels PART-OF discriminator. regression labels PART-OF generator. method USED-FOR regression labels. method USED-FOR generator. method USED-FOR discriminator. empirical cGAN losses USED-FOR CcGAN. hard vicinal discriminator loss ( HVDL ) CONJUNCTION soft vicinal discriminator loss ( SVDL ). soft vicinal discriminator loss ( SVDL ) CONJUNCTION hard vicinal discriminator loss ( HVDL ). empirical generator loss HYPONYM-OF empirical discriminator losses. soft vicinal discriminator loss ( SVDL ) HYPONYM-OF empirical discriminator losses. hard vicinal discriminator loss ( HVDL ) HYPONYM-OF empirical discriminator losses. HVDL CONJUNCTION SVDL. SVDL CONJUNCTION HVDL. error bounds FEATURE-OF discriminator. SVDL USED-FOR discriminator. HVDL USED-FOR discriminator. benchmark dataset USED-FOR generative image modeling. RC-49 USED-FOR generative image modeling. RC-49 HYPONYM-OF benchmark dataset. RC-49 CONJUNCTION UTKFace datasets. UTKFace datasets CONJUNCTION RC-49. Circular 2 - D Gaussians CONJUNCTION RC-49. RC-49 CONJUNCTION Circular 2 - D Gaussians. regression label FEATURE-OF image distribution. CcGAN COMPARE cGAN. cGAN COMPARE CcGAN. OtherScientificTerm are continuous, scal","This paper proposes a continuous conditional generative adversarial network (CcGAN) which is a generative model for image generation. CcGAN is a generalization of conditional GANs (cGANs) to categorical conditions (i.e., class labels) that are continuous, scalar, and continuous, i.e. categorical. The authors propose a method to learn a discriminator that is able to discriminate between continuous and categorical labels. The generator/discriminator consists of a hidden map and a one-hot encoded label, which are two different label input methods: the hidden map is used to train the generator and the label is used for training the discriminator. The empirical cGAN losses (e.g., empirical generator loss, empirical discriminator loss) are used in the continuous scenario, while the empirical losses in the categorical scenario are used for the continuous case. The proposed method can be applied to any continuous, continuous, categorical, or categorical continuous GAN.  The generator and discriminator are trained in an unsupervised way. The discriminator is trained with regression labels from the regression labels in the generator.   The authors show that the proposed method learns the generator by minimizing the difference between the true and the regression label of the image distribution. They also provide error bounds for their discriminator trained with HVDL and SVDL.  Experiments are conducted on a benchmark dataset for generative image modeling, called RC-49, which is an extension of the standard benchmark dataset Circular 2-D Gaussians. The results show that CdGAN outperforms cGAN in terms of accuracy. ","This paper proposes a continuous conditional generative adversarial network (CcGAN) which is a generative model for image generation. CcGAN is a generalization of conditional GANs (cGANs) to categorical conditions (i.e., class labels) that are continuous, scalar, and continuous, i.e. categorical. The authors propose a method to learn a discriminator that is able to discriminate between continuous and categorical labels. The generator/discriminator consists of a hidden map and a one-hot encoded label, which are two different label input methods: the hidden map is used to train the generator and the label is used for training the discriminator. The empirical cGAN losses (e.g., empirical generator loss, empirical discriminator loss) are used in the continuous scenario, while the empirical losses in the categorical scenario are used for the continuous case. The proposed method can be applied to any continuous, continuous, categorical, or categorical continuous GAN.  The generator and discriminator are trained in an unsupervised way. The discriminator is trained with regression labels from the regression labels in the generator.   The authors show that the proposed method learns the generator by minimizing the difference between the true and the regression label of the image distribution. They also provide error bounds for their discriminator trained with HVDL and SVDL.  Experiments are conducted on a benchmark dataset for generative image modeling, called RC-49, which is an extension of the standard benchmark dataset Circular 2-D Gaussians. The results show that CdGAN outperforms cGAN in terms of accuracy. "
1577,SP:10dd09ab315870631d1451d200f2c87a023f8226,"sample complexity EVALUATE-FOR deep learning ( DL ). Semisupervised learning ( SSL ) USED-FOR task. unlabeled instances USED-FOR Semisupervised learning ( SSL ). unlabeled instances USED-FOR task. sample complexity EVALUATE-FOR Active learning ( AL ). SSL CONJUNCTION AL. AL CONJUNCTION SSL. SSL USED-FOR fully - supervised learning ( SL ). AL USED-FOR fully - supervised learning ( SL ). labeled samples USED-FOR fully - supervised learning ( SL ). SSL USED-FOR DL - based AL algorithms. annotation efficiency EVALUATE-FOR AL algorithms. diversity FEATURE-OF AL algorithms. SSL USED-FOR AL algorithms. AL algorithm USED-FOR classification network. AL algorithm USED-FOR convergence rate. convergence rate FEATURE-OF classification network. CRC CONJUNCTION SSL algorithm. SSL algorithm CONJUNCTION CRC. deep neural network COMPARE SL. SL COMPARE deep neural network. labeled samples USED-FOR deep neural network. SSL algorithm USED-FOR deep neural network. CRC USED-FOR deep neural network. AL CONJUNCTION SSL. SSL CONJUNCTION AL. our method USED-FOR ASSL. OtherScientificTerm is human - in - the - loop. Method are pool - based AL, convergence rate control ( CRC ), and AL and SSL ( ASSL ) algorithms. Metric is rate of convergence. Generic is method. ","This paper studies the sample complexity of deep learning (DL) under the setting of Semisupervised learning (SSL) with unlabeled instances for a given task. Active learning (AL) has been shown to have a similar sample complexity to SSL, but the difference between SSL and AL is that SSL is used for fully-supervised learning with labeled samples, while AL is used to train a classification network. The authors show that DL-based AL algorithms trained with SSL improve the annotation efficiency and diversity of the training data, and that the rate of convergence of AL algorithms is faster than SSL. They also show that the convergence rate of the classification network trained with the AL algorithm converges faster than that of the SSL algorithm.    The authors propose a new algorithm, called pool-based SSL (CRC), which is based on the idea of convergence rate control (CCR). The authors also propose a variant of the AL and SSL (ASSL) algorithms. The main contribution of the paper is that the deep neural network trained using the proposed CRC and SSL algorithm is able to converge faster than SL, and the authors also provide a theoretical analysis of the convergence of the algorithm. Finally, the authors demonstrate that their method can be applied to ASSL, and show that their algorithm can be used to improve the performance of AL, SSL, and AL + SSL. ","This paper studies the sample complexity of deep learning (DL) under the setting of Semisupervised learning (SSL) with unlabeled instances for a given task. Active learning (AL) has been shown to have a similar sample complexity to SSL, but the difference between SSL and AL is that SSL is used for fully-supervised learning with labeled samples, while AL is used to train a classification network. The authors show that DL-based AL algorithms trained with SSL improve the annotation efficiency and diversity of the training data, and that the rate of convergence of AL algorithms is faster than SSL. They also show that the convergence rate of the classification network trained with the AL algorithm converges faster than that of the SSL algorithm.    The authors propose a new algorithm, called pool-based SSL (CRC), which is based on the idea of convergence rate control (CCR). The authors also propose a variant of the AL and SSL (ASSL) algorithms. The main contribution of the paper is that the deep neural network trained using the proposed CRC and SSL algorithm is able to converge faster than SL, and the authors also provide a theoretical analysis of the convergence of the algorithm. Finally, the authors demonstrate that their method can be applied to ASSL, and show that their algorithm can be used to improve the performance of AL, SSL, and AL + SSL. "
1593,SP:7f3947c3fa5b09674507d8f3e10d9280376ecb94,"federated learning method USED-FOR distributively training neural network models. devices USED-FOR parallelizing gradient computation. scheme USED-FOR training. devices CONJUNCTION partial participation and unbalanced data. partial participation and unbalanced data CONJUNCTION devices. scheme USED-FOR convex and non - convex settings. Task are Federated Learning problem, and device level computations. OtherScientificTerm are local - device level empirical loss, global empirical loss, and device heterogeneity. Method are inexact minimization, and dynamic regularizer. Material is real and synthetic data. ","This paper proposes a federated learning method for distributively training neural network models across devices for parallelizing gradient computation. The authors consider the Federated Learning problem where each device has multiple clients and each client has its own local-device level empirical loss, and the goal is to minimize the global empirical loss across all devices.   The authors propose a scheme for training that minimizes the inexact minimization of the global loss, which is a dynamic regularizer that is sensitive to device level computations. This scheme is applicable to convex and non-convex settings, and can be applied to both devices with partial participation and unbalanced data. Experiments on both real and synthetic data show that the proposed scheme is able to achieve state-of-the-art performance in both cases. ","This paper proposes a federated learning method for distributively training neural network models across devices for parallelizing gradient computation. The authors consider the Federated Learning problem where each device has multiple clients and each client has its own local-device level empirical loss, and the goal is to minimize the global empirical loss across all devices.   The authors propose a scheme for training that minimizes the inexact minimization of the global loss, which is a dynamic regularizer that is sensitive to device level computations. This scheme is applicable to convex and non-convex settings, and can be applied to both devices with partial participation and unbalanced data. Experiments on both real and synthetic data show that the proposed scheme is able to achieve state-of-the-art performance in both cases. "
1609,SP:a3fbb073b0e2371b20d5d9df6ab829673f90354f,"supervised counterparts USED-FOR computer vision tasks. representations COMPARE supervised counterparts. supervised counterparts COMPARE representations. representations USED-FOR computer vision tasks. self - supervised approaches USED-FOR representations. accuracy EVALUATE-FOR contrastive learning algorithms. contrastive learning USED-FOR similarity ( dissimilarity ). similarity FEATURE-OF intermediate layers. similarity USED-FOR similarity. intermediate losses USED-FOR selection. SimCLR CONJUNCTION SwAV. SwAV CONJUNCTION SimCLR. MOCO CONJUNCTION SimCLR. SimCLR CONJUNCTION MOCO. method USED-FOR MOCO. method USED-FOR SimCLR. method USED-FOR SwAV. ImageNet linear classification CONJUNCTION downstream tasks. downstream tasks CONJUNCTION ImageNet linear classification. Method are self - supervised methods, and back - propagation. Task is optimizing similarity ( dissimilarity ). OtherScientificTerm are intermediate contrastive losses, and gradient descent update. Metric is computational cost. ",This paper proposes a self-supervised contrastive learning approach to learn representations that are more dissimilar to their supervised counterparts for a variety of computer vision tasks. The key idea is to use the contrastive loss in contrastive contrastive training to improve the performance of the learned representations. The authors show that the proposed approach is able to achieve better performance on ImageNet linear classification and image classification tasks compared to supervised methods. ,This paper proposes a self-supervised contrastive learning approach to learn representations that are more dissimilar to their supervised counterparts for a variety of computer vision tasks. The key idea is to use the contrastive loss in contrastive contrastive training to improve the performance of the learned representations. The authors show that the proposed approach is able to achieve better performance on ImageNet linear classification and image classification tasks compared to supervised methods. 
1625,SP:5b5e705ea1ee1b857e17e64d560a39052804949d,actor - critic HYPONYM-OF reinforcement learning algorithms. global convergence CONJUNCTION global optimality. global optimality CONJUNCTION global convergence. global optimality FEATURE-OF actor - critic. global convergence FEATURE-OF actor - critic. bi - level or two - timescale updates USED-FOR actor - critic. policy gradient direction USED-FOR actor. critic USED-FOR policy gradient direction. Bellman evaluation operator USED-FOR critic update. linear or deep neural networks USED-FOR actor. linear or deep neural networks USED-FOR function approximation settings. rate of convergence CONJUNCTION global optimality. global optimality CONJUNCTION rate of convergence. global optimality FEATURE-OF single - timescale actor - critic. rate of convergence FEATURE-OF single - timescale actor - critic. linear function approximation USED-FOR single - timescale actor - critic. actorcritic with deep neural network USED-FOR globally optimal policy. nonlinear function approximation USED-FOR policy optimization. Task is single - timescale setting. Metric is sublinear O(K−1/2 ) rate. OtherScientificTerm is sublinear rate. ,"This paper studies the global convergence and global optimality of actor-critic, one of the most commonly used reinforcement learning algorithms. In the single-timescale setting, the authors consider bi-level or two- timescale updates to the policy gradient direction of the actor and the critic, respectively. They show that under certain function approximation settings, linear or deep neural networks can be used to train an actor that converges to a globally optimal policy in a sublinear O(K−1/2) rate. They also show that the sublinear rate can be improved to O(1/\sqrt{T}^T) if the critic update is based on the Bellman evaluation operator. Finally, they show that in the linear function approximation case, the rate of convergence of single-timely actor - critic is O(O(T)^T, where T is the number of timescales.   ","This paper studies the global convergence and global optimality of actor-critic, one of the most commonly used reinforcement learning algorithms. In the single-timescale setting, the authors consider bi-level or two- timescale updates to the policy gradient direction of the actor and the critic, respectively. They show that under certain function approximation settings, linear or deep neural networks can be used to train an actor that converges to a globally optimal policy in a sublinear O(K−1/2) rate. They also show that the sublinear rate can be improved to O(1/\sqrt{T}^T) if the critic update is based on the Bellman evaluation operator. Finally, they show that in the linear function approximation case, the rate of convergence of single-timely actor - critic is O(O(T)^T, where T is the number of timescales.   "
1641,SP:26705a4dc305cce336f657c5937d1f5b4209548a,events CONJUNCTION messages. messages CONJUNCTION events. messages CONJUNCTION transactions. transactions CONJUNCTION messages. Log files USED-FOR events. Log files USED-FOR messages. Log files USED-FOR transactions. computer systems FEATURE-OF Log files. they USED-FOR structured textual and numerical data. natural languages CONJUNCTION temporal signals. temporal signals CONJUNCTION natural languages. logs USED-FOR sequential forms of data. natural languages HYPONYM-OF sequential forms of data. temporal signals HYPONYM-OF sequential forms of data. log level CONJUNCTION log sequence level. log sequence level CONJUNCTION log level. field level CONJUNCTION log level. log level CONJUNCTION field level. representation USED-FOR level. vector format FEATURE-OF representations. Transformer Networks ( TNs ) USED-FOR numerical and textual information. Transformer Networks ( TNs ) USED-FOR log embeddings. representation USED-FOR log processing applications. Material is Logs. ,"This paper proposes to use Log files in computer systems to represent events, messages, and transactions. Log files are a common resource for structured textual and numerical data, and they can be used to represent both structured and non-structured data. Logs can be seen as sequential forms of data, such as natural languages and temporal signals, and Log files can represent both events and messages. Transformer Networks (TNs) are used to encode both numerical and textual information into log embeddings, and the authors show that they are able to capture both the field level, log level, and log sequence level. The representations are encoded in a vector format, and a representation for each level is learned. The authors also show that this representation can be applied to various log processing applications. ","This paper proposes to use Log files in computer systems to represent events, messages, and transactions. Log files are a common resource for structured textual and numerical data, and they can be used to represent both structured and non-structured data. Logs can be seen as sequential forms of data, such as natural languages and temporal signals, and Log files can represent both events and messages. Transformer Networks (TNs) are used to encode both numerical and textual information into log embeddings, and the authors show that they are able to capture both the field level, log level, and log sequence level. The representations are encoded in a vector format, and a representation for each level is learned. The authors also show that this representation can be applied to various log processing applications. "
1657,SP:165c51a16f17fb8726e968f8b34742b62011d60e,"CNN kernels CONJUNCTION oriented Gabor filters. oriented Gabor filters CONJUNCTION CNN kernels. freely - trained mixture weights USED-FOR wavelet packet decompositions. AlexNet architecture USED-FOR image classification. AlexNet architecture FEATURE-OF wavelet decompositions. wavelet decompositions USED-FOR approach. directional selectivity CONJUNCTION shift invariance. shift invariance CONJUNCTION directional selectivity. feature extraction properties USED-FOR two. shift invariance HYPONYM-OF feature extraction properties. directional selectivity HYPONYM-OF feature extraction properties. separable wavelet packet transform USED-FOR variant. accuracy rate EVALUATE-FOR AlexNet. mathematical theory USED-FOR network. Task is deep convolutional neural networks ( CNNs ). Generic are formalism, and them. Method is convolutional layers. ","This paper proposes a wavelet decomposition-based architecture for deep convolutional neural networks (CNNs). The approach is based on wavelet packet decompositions with freely-trained mixture weights, which is a formalism that allows to decompose CNN kernels and oriented Gabor filters into CNN kernels. The authors propose a variant of the AlexNet architecture for image classification based on this approach. The two main feature extraction properties of the two are the directional selectivity and shift invariance, and the authors show that both of them are satisfied. The paper also proposes a variant based on a separable wavelet transfer, which can be applied to any existing variant. The accuracy rate of the proposed AlexNet is shown to be competitive with the state-of-the-art on several datasets, and it is also shown that the proposed variant of AlexNet can achieve a higher accuracy rate than the state of the art in terms of accuracy rate.    The paper is well-written and well-motivated. The mathematical theory behind the proposed network is clear and the experimental results are convincing.  However, there are a few concerns that need to be addressed.","This paper proposes a wavelet decomposition-based architecture for deep convolutional neural networks (CNNs). The approach is based on wavelet packet decompositions with freely-trained mixture weights, which is a formalism that allows to decompose CNN kernels and oriented Gabor filters into CNN kernels. The authors propose a variant of the AlexNet architecture for image classification based on this approach. The two main feature extraction properties of the two are the directional selectivity and shift invariance, and the authors show that both of them are satisfied. The paper also proposes a variant based on a separable wavelet transfer, which can be applied to any existing variant. The accuracy rate of the proposed AlexNet is shown to be competitive with the state-of-the-art on several datasets, and it is also shown that the proposed variant of AlexNet can achieve a higher accuracy rate than the state of the art in terms of accuracy rate.    The paper is well-written and well-motivated. The mathematical theory behind the proposed network is clear and the experimental results are convincing.  However, there are a few concerns that need to be addressed."
1673,SP:d0a284da462584724ba6a3a48c9e986d391233f6,"dynamic composition FEATURE-OF Coordinating teams. variational objective USED-FOR learning. attention mechanism USED-FOR dynamic team composition. attention mechanism USED-FOR heterogeneous agents. multi - agent particle environment FEATURE-OF resource collection tasks. resource collection tasks EVALUATE-FOR methods. zero - shot generalization USED-FOR team compositions. heterogeneous agents USED-FOR zero - shot generalization. coach USED-FOR dynamic teams. Task are real - world multi - agent teams, and real - world team sports. OtherScientificTerm is optimal team strategy. Method are coach - player framework, adaptive communication method, and adaptive communication strategy. Generic is method. ","This paper studies the problem of training real-world multi-agent teams. Coordinating teams with dynamic composition is an important problem. The authors propose a coach-player framework where the optimal team strategy is learned in an adaptive communication method. The learning is based on a variational objective, where the goal is to maximize the mutual information between the agent and the coach. The proposed method is evaluated on resource collection tasks in a multiagent particle environment, and the authors show that the proposed methods achieve zero-shot generalization to new team compositions with heterogeneous agents using an attention mechanism for dynamic team composition. They also demonstrate that the adaptive communication strategy is more effective than using a single coach for training dynamic teams.  ","This paper studies the problem of training real-world multi-agent teams. Coordinating teams with dynamic composition is an important problem. The authors propose a coach-player framework where the optimal team strategy is learned in an adaptive communication method. The learning is based on a variational objective, where the goal is to maximize the mutual information between the agent and the coach. The proposed method is evaluated on resource collection tasks in a multiagent particle environment, and the authors show that the proposed methods achieve zero-shot generalization to new team compositions with heterogeneous agents using an attention mechanism for dynamic team composition. They also demonstrate that the adaptive communication strategy is more effective than using a single coach for training dynamic teams.  "
1689,SP:4eb662b527d556758aaa1a0b589495fcc337fad0,"machine learning interpretability CONJUNCTION uncertainty estimation. uncertainty estimation CONJUNCTION machine learning interpretability. Influence functions USED-FOR machine learning interpretability. Influence functions USED-FOR uncertainty estimation. gradients CONJUNCTION Hessian. Hessian CONJUNCTION gradients. Hessian FEATURE-OF model. gradients FEATURE-OF model. Hessian USED-FOR post - hoc method. gradients USED-FOR post - hoc method. influence functions USED-FOR linear models. Influence functions PART-OF deep learning. non - convex loss functions FEATURE-OF deep learning. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. Iris CONJUNCTION MNIST. MNIST CONJUNCTION Iris. influence functions PART-OF neural network models. datasets USED-FOR neural network models. Iris HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. CIFAR-10 HYPONYM-OF datasets. MNIST HYPONYM-OF datasets. model parameterization CONJUNCTION regularization techniques. regularization techniques CONJUNCTION model parameterization. regularization techniques USED-FOR influence functions. influence functions EVALUATE-FOR network architecture. accuracy EVALUATE-FOR influence estimates. influence estimates EVALUATE-FOR shallow networks. influence estimation methods USED-FOR non - convex setups. influence functions PART-OF deep learning. OtherScientificTerm are test - time predictions, convexity of the underlying loss function, and model changes. Task is estimating group influences. Metric is accuracy of influence functions. Generic is deeper networks. Method are network architectures, and weight - decay regularization. ","This paper proposes a post-hoc method to estimate influence functions for machine learning interpretability and uncertainty estimation. Influence functions are commonly used in deep learning with non-convex loss functions, but the accuracy of influence functions has not been studied before. This paper considers the problem of estimating group influences, i.e., the influence of a group on test-time predictions. The authors propose to use gradients and the Hessian of the underlying model to estimate the influence functions of a model.   The authors show that influence functions are useful for linear models, but not for deep learning. They also show that for deep networks, influence functions can be used to improve the performance of neural network models on standard datasets such as Iris, MNIST, CIFAR-10, and ImageNet.  The paper also shows that influence estimates of deep learning can be improved by using model parameterization and regularization techniques. The paper shows that the influence estimates are more sensitive to the network architecture and the number of layers, and that deep networks are more robust to changes in network architectures.  Finally, the paper shows how to improve influence estimates for shallow networks by using weight-decay regularization. In addition, the authors show how to use influence estimation methods to improve performance in non-consvex setups, and how to ensure that the convexity of a loss function is preserved. ","This paper proposes a post-hoc method to estimate influence functions for machine learning interpretability and uncertainty estimation. Influence functions are commonly used in deep learning with non-convex loss functions, but the accuracy of influence functions has not been studied before. This paper considers the problem of estimating group influences, i.e., the influence of a group on test-time predictions. The authors propose to use gradients and the Hessian of the underlying model to estimate the influence functions of a model.   The authors show that influence functions are useful for linear models, but not for deep learning. They also show that for deep networks, influence functions can be used to improve the performance of neural network models on standard datasets such as Iris, MNIST, CIFAR-10, and ImageNet.  The paper also shows that influence estimates of deep learning can be improved by using model parameterization and regularization techniques. The paper shows that the influence estimates are more sensitive to the network architecture and the number of layers, and that deep networks are more robust to changes in network architectures.  Finally, the paper shows how to improve influence estimates for shallow networks by using weight-decay regularization. In addition, the authors show how to use influence estimation methods to improve performance in non-consvex setups, and how to ensure that the convexity of a loss function is preserved. "
1705,SP:5fea74a2031d097a99dacf613bedcb054b0c3831,Autoregressive language models USED-FOR downstream tasks. Autoregressive language models USED-FOR next word prediction. large text corpora USED-FOR next word prediction. large text corpora USED-FOR Autoregressive language models. zero - shot usage USED-FOR downstream tasks. next word prediction CONJUNCTION text classification. text classification CONJUNCTION next word prediction. language modeling HYPONYM-OF pretraining task. sentence completion tasks USED-FOR classification tasks of interest. language modeling USED-FOR downstream tasks. language models USED-FOR classification tasks. language models USED-FOR features. features USED-FOR classification tasks. crossentropy ( log - perplexity ) FEATURE-OF language models. objective function USED-FOR classification tasks. ,"Autoregressive language models trained on large text corpora are shown to perform well on next word prediction and text classification tasks with zero-shot usage. The authors show that language models with high crossentropy (log-perturbity) are able to learn features that are useful for classification tasks of interest, and that language modeling, a pretraining task, can be used as an objective function to improve the performance of classification tasks on sentence completion tasks. ","Autoregressive language models trained on large text corpora are shown to perform well on next word prediction and text classification tasks with zero-shot usage. The authors show that language models with high crossentropy (log-perturbity) are able to learn features that are useful for classification tasks of interest, and that language modeling, a pretraining task, can be used as an objective function to improve the performance of classification tasks on sentence completion tasks. "
1721,SP:a67da438e9821010284416170c3699ae7ff96c99,"MIA approaches USED-FOR classification models. image translation HYPONYM-OF conditional image generation models. approach USED-FOR membership attacks. reconstruction error USED-FOR approach. difficulty score CONJUNCTION reconstruction error. reconstruction error CONJUNCTION difficulty score. difficulty score USED-FOR membership error. MIA accuracy EVALUATE-FOR membership error. Task is Membership inference attacks ( MIA ). Method are neural network model, machine learning, and MIA. OtherScientificTerm are overfitting, and Reconstruction error. Metric is reconstruction errors. Material is training set. ","Membership inference attacks (MIA) are attacks where a neural network model is trained to infer the membership of a group of training samples. Membership inference attacks are an important problem in machine learning, and there are several MIA approaches to attack classification models. This paper proposes a new approach for membership attacks based on the reconstruction error of the training set. The authors show that this approach can be applied to a wide range of conditional image generation models (e.g., image translation). Reconstruction error is used as a measure of membership error, and the difficulty score is used to compare membership error to the membership error in terms of MIA accuracy. The paper also shows that reconstruction errors can be used as an indicator of overfitting, which is a common problem in MIA.","Membership inference attacks (MIA) are attacks where a neural network model is trained to infer the membership of a group of training samples. Membership inference attacks are an important problem in machine learning, and there are several MIA approaches to attack classification models. This paper proposes a new approach for membership attacks based on the reconstruction error of the training set. The authors show that this approach can be applied to a wide range of conditional image generation models (e.g., image translation). Reconstruction error is used as a measure of membership error, and the difficulty score is used to compare membership error to the membership error in terms of MIA accuracy. The paper also shows that reconstruction errors can be used as an indicator of overfitting, which is a common problem in MIA."
1737,SP:6fe23ebe09f2a4e42a21598f8e9c79edeca99863,"it USED-FOR distribution learning problem. differentiable architecture search method USED-FOR distribution learning problem. random variables FEATURE-OF continuously relaxed architecture mixing weight. Dirichlet distribution USED-FOR random variables. pathwise derivatives USED-FOR Dirichlet parameters. gradient - based optimizer USED-FOR Dirichlet parameters. formulation USED-FOR stochasticity. generalization ability EVALUATE-FOR formulation. progressive learning scheme USED-FOR searching. searching USED-FOR large - scale tasks. progressive learning scheme USED-FOR large - scale tasks. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. Method are differentiable NAS, and neural architecture search algorithms. Generic are method, and datasets. Metric is test error. Material is NASBench-201. ","This paper proposes a differentiable architecture search method to solve the distribution learning problem in differentiable NAS. Specifically, it proposes a new approach to solving the distribution of random variables in a continuously relaxed architecture mixing weight, where the random variables are drawn from a Dirichlet distribution. The authors propose a gradient-based optimizer to optimize the Dirichlett parameters of the pathwise derivatives of the weights, which is a well-studied problem in neural architecture search algorithms. The formulation is shown to preserve stochasticity and improve the generalization ability of the proposed method. The proposed progressive learning scheme is also shown to improve the performance of searching on large-scale tasks. Experiments are conducted on CIFAR-10 and ImageNet, and the authors show that the proposed algorithm achieves the best test error on NASBench-201 and NAS-201 datasets.","This paper proposes a differentiable architecture search method to solve the distribution learning problem in differentiable NAS. Specifically, it proposes a new approach to solving the distribution of random variables in a continuously relaxed architecture mixing weight, where the random variables are drawn from a Dirichlet distribution. The authors propose a gradient-based optimizer to optimize the Dirichlett parameters of the pathwise derivatives of the weights, which is a well-studied problem in neural architecture search algorithms. The formulation is shown to preserve stochasticity and improve the generalization ability of the proposed method. The proposed progressive learning scheme is also shown to improve the performance of searching on large-scale tasks. Experiments are conducted on CIFAR-10 and ImageNet, and the authors show that the proposed algorithm achieves the best test error on NASBench-201 and NAS-201 datasets."
1753,SP:c590d0ed2487b42480b53fc077546a4a0bc27a78,"signed distance functions CONJUNCTION neural radiance fields. neural radiance fields CONJUNCTION signed distance functions. function approximators USED-FOR low - dimensional - but - complex functions. neural networks USED-FOR low - dimensional - but - complex functions. neural networks USED-FOR function approximators. high dimensional inputs USED-FOR deep networks. pixel coordinates USED-FOR images. sinusoidal nonlinearities CONJUNCTION Fourier features. Fourier features CONJUNCTION sinusoidal nonlinearities. elements USED-FOR positional encodings. Fourier features USED-FOR positional encodings. elements COMPARE ReLU networks. ReLU networks COMPARE elements. positional encodings COMPARE ReLU networks. ReLU networks COMPARE positional encodings. Fourier features HYPONYM-OF elements. sinusoidal nonlinearities HYPONYM-OF elements. function approximators USED-FOR problems. multiplicative filter networks HYPONYM-OF problems. multiplicative filter networks HYPONYM-OF function approximators. Fourier or Gabor basis functions USED-FOR linear function approximator. ReLU networks CONJUNCTION sinusoidal activation networks. sinusoidal activation networks CONJUNCTION ReLU networks. Fourier features CONJUNCTION ReLU networks. ReLU networks CONJUNCTION Fourier features. multiplicative filter networks COMPARE approaches. approaches COMPARE multiplicative filter networks. Fourier features CONJUNCTION sinusoidal activation networks. sinusoidal activation networks CONJUNCTION Fourier features. sinusoidal activation networks USED-FOR approaches. ReLU networks USED-FOR approaches. Fourier features USED-FOR approaches. OtherScientificTerm are differential equations, and compositional depth. Generic are networks, and representation. ","This paper studies the use of neural networks as function approximators for low-dimensional-but-complex functions. The authors consider the problem of training deep networks with high dimensional inputs, where the underlying differential equations are non-convex and non-linear, and the input is a set of images with pixel coordinates.   The authors propose to use signed distance functions and neural radiance fields as input, and show that these two elements can be used to learn positional encodings that are more robust to sinusoidal nonlinearities and Fourier features compared to ReLU networks. They also show that the compositional depth of the learned representations of the two networks is a function of the number of layers and compositional depths of the representation.  The main contribution of the paper is that the authors show that two types of functions can be approximated by neural networks that are trained with neural networks, and that the learned representation is robust to these functions.  In addition, the authors demonstrate that these functions are more computationally efficient than existing functions approximated with functions based on Fourier or Gabor basis functions, and can be applied to a wide range of problems such as multiplicative filter networks, which can be seen as a generalization of existing work.  Finally, they show that their approaches can be combined with existing approaches based on the Fourier feature and sinusoidal nonlinearity, Fourier networks, sinusolic activation networks, etc., and compare their performance to the state-of-the-art approaches using ReLU and ReLu networks.","This paper studies the use of neural networks as function approximators for low-dimensional-but-complex functions. The authors consider the problem of training deep networks with high dimensional inputs, where the underlying differential equations are non-convex and non-linear, and the input is a set of images with pixel coordinates.   The authors propose to use signed distance functions and neural radiance fields as input, and show that these two elements can be used to learn positional encodings that are more robust to sinusoidal nonlinearities and Fourier features compared to ReLU networks. They also show that the compositional depth of the learned representations of the two networks is a function of the number of layers and compositional depths of the representation.  The main contribution of the paper is that the authors show that two types of functions can be approximated by neural networks that are trained with neural networks, and that the learned representation is robust to these functions.  In addition, the authors demonstrate that these functions are more computationally efficient than existing functions approximated with functions based on Fourier or Gabor basis functions, and can be applied to a wide range of problems such as multiplicative filter networks, which can be seen as a generalization of existing work.  Finally, they show that their approaches can be combined with existing approaches based on the Fourier feature and sinusoidal nonlinearity, Fourier networks, sinusolic activation networks, etc., and compare their performance to the state-of-the-art approaches using ReLU and ReLu networks."
1769,SP:f5be855300f63c185a006834302bd4b033b56258,"task - specific models CONJUNCTION meta - model. meta - model CONJUNCTION task - specific models. task - specific models PART-OF Gradient - based meta - learning. gradients USED-FOR meta - model. algorithm USED-FOR task - specific models. algorithm USED-FOR meta - model. meta - gradients USED-FOR meta - model. inner loop USED-FOR algorithm. inner loop USED-FOR task - specific models. teacherstudent scheme USED-FOR gradient - based meta - learning algorithms. student network USED-FOR task - specific models. lightweight computation graph USED-FOR meta - gradients. few - shot learning CONJUNCTION long - tailed classification. long - tailed classification CONJUNCTION few - shot learning. long - tailed classification CONJUNCTION meta - attack. meta - attack CONJUNCTION long - tailed classification. it USED-FOR meta - learning algorithms. it USED-FOR tasks. tasks EVALUATE-FOR meta - learning algorithms. meta - attack HYPONYM-OF tasks. meta - attack HYPONYM-OF meta - learning algorithms. few - shot learning HYPONYM-OF tasks. few - shot learning HYPONYM-OF meta - learning algorithms. long - tailed classification HYPONYM-OF tasks. long - tailed classification HYPONYM-OF meta - learning algorithms. Generic are loop, and approach. OtherScientificTerm are inner - loop optimization steps, high - order derivatives, and big memory footprints. ","Gradient-based meta-learning involves the use of task-specific models and a meta-model that uses gradients from all the task specific models in the loop. In this paper, the authors propose an algorithm that uses the meta-gradients from the inner loop to train a meta - model. The algorithm is based on the teacherstudent scheme, where a student network is used to learn the gradient-based gradient of the inner-loop optimization steps. The authors show that this approach can be applied to a wide range of tasks, including few-shot learning, long-tailed classification, meta-attack, and more. They also show that meta- gradients can be computed on a lightweight computation graph, and that it can be used to improve the performance of meta-learners on a variety of tasks. ","Gradient-based meta-learning involves the use of task-specific models and a meta-model that uses gradients from all the task specific models in the loop. In this paper, the authors propose an algorithm that uses the meta-gradients from the inner loop to train a meta - model. The algorithm is based on the teacherstudent scheme, where a student network is used to learn the gradient-based gradient of the inner-loop optimization steps. The authors show that this approach can be applied to a wide range of tasks, including few-shot learning, long-tailed classification, meta-attack, and more. They also show that meta- gradients can be computed on a lightweight computation graph, and that it can be used to improve the performance of meta-learners on a variety of tasks. "
1785,SP:0361e02d56b7d121cb5ede1cb582284cc18fc599,"Offline Reinforcement Learning ( RL ) USED-FOR policies. off - policy RL algorithms USED-FOR Offline RL. Behavior regularization USED-FOR off - policy algorithms. analytical upper bound USED-FOR behavior regularizor. analytical upper bound USED-FOR KL divergence. state - dependent Lagrange multipliers USED-FOR regularization term. state - dependent Lagrange multipliers USED-FOR distributing KL divergence penalty. Lagrange multipliers USED-FOR freedom of deviation. gradient penalty term USED-FOR gradient of the Q value. gradient penalty term USED-FOR policy evaluation objective. out - of - distribution actions FEATURE-OF gradient of the Q value. gradient penalty term USED-FOR catastrophic performance degradation. out - ofdistribution actions FEATURE-OF Q values. offline RL benchmarks EVALUATE-FOR BRAC+. offline RL benchmarks EVALUATE-FOR model - free and model - based approaches. BRAC+ COMPARE model - free and model - based approaches. model - free and model - based approaches COMPARE BRAC+. Method are Reinforcement Learning agent, and behavior regularized offline reinforcement learning. OtherScientificTerm are outof - distribution ( less explored ) actions, sample based estimations, sampled batch, low probability ( less explored ) states, and rare out - of - distribution actions. ","Offline Reinforcement Learning (RL) is an important problem in RL, where policies are learned offline, and the goal is to learn policies that are robust to out-of-distribution (less explored) actions. Offline RL is a challenging problem, and many off-policy RL algorithms have been proposed to address this problem. Behavior regularization has been proposed as a way to improve the performance of existing offline RL algorithms. In this paper, the authors propose behavior regularized offline reinforcement learning (BRAC+) that penalizes the behavior of an offline Reinforcement learning agent if it takes out of distribution (out of distribution) actions during training. The authors propose a novel analytical upper bound for the KL divergence between the policy and the behavior regularizor, and propose a new regularization term based on state-dependent Lagrange multipliers for the distribution of the distribution over the sampled batch.    The authors show that the distributing KL divergence penalty can be computed using the standard distribution over all sampled samples, and that the distribution can be estimated using the analytical upper bounds.  They also propose a regularization for the policy evaluation objective based on the gradient penalty term for the gradient of the Q value in the case of out- of distribution actions, which is based on sample based estimations.  The main contribution of the paper is that the authors show how to compute the freedom of deviation between the Q values in the out ofdistribution actions and in the in-distributions using Lagrange multiplier.  BRAC+ is evaluated on several offline RL benchmarks, and compared with model-free and model-based approaches, and is shown to outperform BRAC+. The authors also show that BRAC can be used to mitigate the catastrophic performance degradation due to the catastrophic behavior regularization, which can be alleviated by adding a gradient penalty to the policy to prevent the Q-value from being negatively affected by rare out-out-distributed actions.","Offline Reinforcement Learning (RL) is an important problem in RL, where policies are learned offline, and the goal is to learn policies that are robust to out-of-distribution (less explored) actions. Offline RL is a challenging problem, and many off-policy RL algorithms have been proposed to address this problem. Behavior regularization has been proposed as a way to improve the performance of existing offline RL algorithms. In this paper, the authors propose behavior regularized offline reinforcement learning (BRAC+) that penalizes the behavior of an offline Reinforcement learning agent if it takes out of distribution (out of distribution) actions during training. The authors propose a novel analytical upper bound for the KL divergence between the policy and the behavior regularizor, and propose a new regularization term based on state-dependent Lagrange multipliers for the distribution of the distribution over the sampled batch.    The authors show that the distributing KL divergence penalty can be computed using the standard distribution over all sampled samples, and that the distribution can be estimated using the analytical upper bounds.  They also propose a regularization for the policy evaluation objective based on the gradient penalty term for the gradient of the Q value in the case of out- of distribution actions, which is based on sample based estimations.  The main contribution of the paper is that the authors show how to compute the freedom of deviation between the Q values in the out ofdistribution actions and in the in-distributions using Lagrange multiplier.  BRAC+ is evaluated on several offline RL benchmarks, and compared with model-free and model-based approaches, and is shown to outperform BRAC+. The authors also show that BRAC can be used to mitigate the catastrophic performance degradation due to the catastrophic behavior regularization, which can be alleviated by adding a gradient penalty to the policy to prevent the Q-value from being negatively affected by rare out-out-distributed actions."
1801,SP:b2cfb380aa2a21f72f508b453cf5949257a5b4ec,"Smaller networks USED-FOR edge - devices. one - shot learning paradigm USED-FOR networks. regularization behavior FEATURE-OF adjoint training paradigm. Imagenet USED-FOR resnet-50. CIFAR-100 EVALUATE-FOR architecture. datasets EVALUATE-FOR network. network COMPARE network. network COMPARE network. datasets EVALUATE-FOR network. top-1 accuracy EVALUATE-FOR network. top-1 accuracy EVALUATE-FOR network. Task is compressing deep neural networks. Method are deep neural networks, Adjoined networks, CNN - based neural architecture, and adjoint networks. Generic is architectures. Metric are inference time, and accuracy. ","This paper studies the problem of compressing deep neural networks. Adjoined networks are the most commonly used architecture for compressing neural networks, but they are expensive to train. Smaller networks are more suitable for edge-devices, and the one-shot learning paradigm is used to compress the networks. The authors show that the regularization behavior of the adjoint training paradigm can be explained by a simple CNN-based neural architecture. They also show that a resnet-50 trained on Imagenet can be reduced to the same architecture on CIFAR-100. The paper also shows that a network trained on the same datasets can achieve similar top-1 accuracy as a previous network, but with a much smaller number of parameters.    The authors also provide a theoretical analysis of the performance of different architectures and the inference time. The main finding is that adjoint networks can be much more efficient in terms of inference time and accuracy. ","This paper studies the problem of compressing deep neural networks. Adjoined networks are the most commonly used architecture for compressing neural networks, but they are expensive to train. Smaller networks are more suitable for edge-devices, and the one-shot learning paradigm is used to compress the networks. The authors show that the regularization behavior of the adjoint training paradigm can be explained by a simple CNN-based neural architecture. They also show that a resnet-50 trained on Imagenet can be reduced to the same architecture on CIFAR-100. The paper also shows that a network trained on the same datasets can achieve similar top-1 accuracy as a previous network, but with a much smaller number of parameters.    The authors also provide a theoretical analysis of the performance of different architectures and the inference time. The main finding is that adjoint networks can be much more efficient in terms of inference time and accuracy. "
1817,SP:dba40073f79143e5355d194aa16db9eee0267a5d,"exploration USED-FOR reinforcement learning ( RL ). exploration methods COMPARE counterparts. counterparts COMPARE exploration methods. -greedy HYPONYM-OF counterparts. -greedy USED-FOR exploration algorithm. duration distributions USED-FOR exploration. ecological models of animal foraging behaviour USED-FOR distributions. Generic is problem. Metric are complexity, and generality. OtherScientificTerm are dithering, temporal persistence, local optima, and random duration. Method is greedy exploration. ","This paper studies the problem of exploration in reinforcement learning (RL) and proposes a new exploration algorithm called -greedy, which is a generalization of existing exploration methods (e.g. -greeder) that are more general than their counterparts. The main idea of the paper is to use duration distributions for exploration, which are based on ecological models of animal foraging behaviour, to reduce the complexity and improve the generality of the problem. The authors show that dithering (i.e. temporal persistence) and greedy exploration are not sufficient to achieve local optima, and propose to use random duration.   ","This paper studies the problem of exploration in reinforcement learning (RL) and proposes a new exploration algorithm called -greedy, which is a generalization of existing exploration methods (e.g. -greeder) that are more general than their counterparts. The main idea of the paper is to use duration distributions for exploration, which are based on ecological models of animal foraging behaviour, to reduce the complexity and improve the generality of the problem. The authors show that dithering (i.e. temporal persistence) and greedy exploration are not sufficient to achieve local optima, and propose to use random duration.   "
1833,SP:5efb581a368ace3bd085d48801a899559d6a43ef,"Matrix factorization USED-FOR implicit regularization of gradient descent. infinitesimal initialization USED-FOR Gradient Flow. gradient flow COMPARE heuristic rank minimization algorithm. heuristic rank minimization algorithm COMPARE gradient flow. infinitesimal initialization USED-FOR gradient flow. Greedy Low - Rank Learning HYPONYM-OF heuristic rank minimization algorithm. gradient flow USED-FOR depth-2 matrix factorization. OtherScientificTerm are nuclear norm, implicit regularization, and initialization magnitude. Method are norm minimization, rank minimization view, and rank minimization. Generic is convergence. ","This paper studies the implicit regularization of gradient descent with Matrix factorization with respect to the nuclear norm. In particular, the authors show that gradient flow with infinitesimal initialization is equivalent to a heuristic rank minimization algorithm called Greedy Low-Rank Learning, which is an extension of the standard norm minimization. The authors also show that the gradient flow converges to a depth-2 matrix factorization, and that this gradient flow can be viewed as a variant of a gradient flow that is more efficient than the standard gradient flow in the sense that it does not rely on the nuclear regularization.    The main contribution of this paper is that the authors provide a new view of the rank maximization view from the perspective of implicit regularized gradient descent. They show that this view is a generalization of the previous view that norm minimisation is a special case of the generalization view, and they show that under certain assumptions on the initialization magnitude, the convergence of gradient flow is guaranteed. ","This paper studies the implicit regularization of gradient descent with Matrix factorization with respect to the nuclear norm. In particular, the authors show that gradient flow with infinitesimal initialization is equivalent to a heuristic rank minimization algorithm called Greedy Low-Rank Learning, which is an extension of the standard norm minimization. The authors also show that the gradient flow converges to a depth-2 matrix factorization, and that this gradient flow can be viewed as a variant of a gradient flow that is more efficient than the standard gradient flow in the sense that it does not rely on the nuclear regularization.    The main contribution of this paper is that the authors provide a new view of the rank maximization view from the perspective of implicit regularized gradient descent. They show that this view is a generalization of the previous view that norm minimisation is a special case of the generalization view, and they show that under certain assumptions on the initialization magnitude, the convergence of gradient flow is guaranteed. "
1849,SP:7f997cf7a63a7330fc12fd525516080c91a3cb9b,"Classifiers PART-OF machine learning. two - stage framework USED-FOR robustness. data augmentations USED-FOR subgroup features. data augmentations USED-FOR classifier. CycleGAN USED-FOR intra - class, inter - subgroup augmentations. theoretically - motivated subgroup consistency regularizer CONJUNCTION robust objective. robust objective CONJUNCTION theoretically - motivated subgroup consistency regularizer. CycleGAN USED-FOR CAMEL. CAMEL USED-FOR model patching. robust error EVALUATE-FOR baseline. CAMEL COMPARE baseline. baseline COMPARE CAMEL. robust error EVALUATE-FOR CAMEL. benchmark datasets EVALUATE-FOR CAMEL. CAMEL USED-FOR model. Generic is models. Task is skin cancer classification. OtherScientificTerm are spurious bandage, subgroup differences, class information, semantic transformations, and spurious features. Method is Model patching. Material is real - world skin cancer dataset. ","This paper proposes a two-stage framework to improve robustness of classifiers in machine learning. The first step is to patch the classifier using data augmentations to subgroup features. The second stage is to train the models to be robust to a spurious bandage.    The paper studies the problem of skin cancer classification, where the subgroup differences between two groups are known. The authors propose a theoretically-motivated subgroup consistency regularizer and a robust objective. CycleGAN is used to learn intra-class, inter-subgroup augmentations. Model patching is performed on top of CAMEL. CAMEL is shown to improve the robust error of the model for model patching on several benchmark datasets. The paper also presents a real-world skin cancer dataset and shows that CAMEL improves robust error over a baseline that does not patch the subgroups. The main contribution of the paper is that the authors show that the model is robust to spurious bandages and that the class information is preserved in the patching process.  The authors also show that semantic transformations can be used to remove spurious features. ","This paper proposes a two-stage framework to improve robustness of classifiers in machine learning. The first step is to patch the classifier using data augmentations to subgroup features. The second stage is to train the models to be robust to a spurious bandage.    The paper studies the problem of skin cancer classification, where the subgroup differences between two groups are known. The authors propose a theoretically-motivated subgroup consistency regularizer and a robust objective. CycleGAN is used to learn intra-class, inter-subgroup augmentations. Model patching is performed on top of CAMEL. CAMEL is shown to improve the robust error of the model for model patching on several benchmark datasets. The paper also presents a real-world skin cancer dataset and shows that CAMEL improves robust error over a baseline that does not patch the subgroups. The main contribution of the paper is that the authors show that the model is robust to spurious bandages and that the class information is preserved in the patching process.  The authors also show that semantic transformations can be used to remove spurious features. "
1865,SP:de6cea1e35a0555175e17546a93422e9a96a511e,transparent inner structures CONJUNCTION model expressivity. model expressivity CONJUNCTION transparent inner structures. model interpretability FEATURE-OF transparent inner structures. decision trees HYPONYM-OF Rule - based models. large data sets EVALUATE-FOR rule - based models. Ensemble methods CONJUNCTION fuzzy / soft rules. fuzzy / soft rules CONJUNCTION Ensemble methods. interpretable nonfuzzy rules USED-FOR data representation. classifier USED-FOR interpretable nonfuzzy rules. Rulebased Representation Learner ( RRL ) HYPONYM-OF classifier. it USED-FOR continuous space. training method USED-FOR discrete model. Gradient Grafting HYPONYM-OF training method. gradient descent USED-FOR training method. gradient descent USED-FOR discrete model. logical activation functions USED-FOR RRL. scalability EVALUATE-FOR RRL. it USED-FOR continuous features. logical activation functions USED-FOR it. RRL COMPARE approaches. approaches COMPARE RRL. small and 4 large data sets EVALUATE-FOR RRL. RRL COMPARE decision trees. decision trees COMPARE RRL. complexity EVALUATE-FOR decision trees. complexity EVALUATE-FOR RRL. OtherScientificTerm is discrete parameters and structures. Method is non - differentiable RRL. ,"Rule-based models (e.g., decision trees, ensembles, etc.) have been shown to perform well on large data sets. However, there is a trade-off between transparent inner structures and model expressivity in terms of model interpretability. Ensemble methods and fuzzy/soft rules can be seen as a tradeoff between interpretable nonfuzzy rules for the data representation. In this paper, the authors propose a new classifier called Rulebased Representation Learner (RRL), which is a non-differentiable RRL. RRL learns a discrete model by gradient descent, and it can be applied to any continuous space. In particular, RRL uses logical activation functions to learn the discrete model, and a new training method called Gradient Grafting is proposed. The authors show that RRL can achieve better scalability than decision trees in the sense that it is able to learn continuous features, and RRL outperforms existing approaches on both small and 4 large datasets. ","Rule-based models (e.g., decision trees, ensembles, etc.) have been shown to perform well on large data sets. However, there is a trade-off between transparent inner structures and model expressivity in terms of model interpretability. Ensemble methods and fuzzy/soft rules can be seen as a tradeoff between interpretable nonfuzzy rules for the data representation. In this paper, the authors propose a new classifier called Rulebased Representation Learner (RRL), which is a non-differentiable RRL. RRL learns a discrete model by gradient descent, and it can be applied to any continuous space. In particular, RRL uses logical activation functions to learn the discrete model, and a new training method called Gradient Grafting is proposed. The authors show that RRL can achieve better scalability than decision trees in the sense that it is able to learn continuous features, and RRL outperforms existing approaches on both small and 4 large datasets. "
1881,SP:e36388a9452e557dd51bf0170bf2f9da22271a49,"molecular property prediction HYPONYM-OF biochemical applications. models USED-FOR biochemical applications. molecular scaffolds CONJUNCTION protein families. protein families CONJUNCTION molecular scaffolds. natural environments USED-FOR tasks. complex descriptors USED-FOR natural environments. protein families HYPONYM-OF complex descriptors. molecular scaffolds HYPONYM-OF complex descriptors. regret minimization ( RGM ) algorithm USED-FOR structured environments. representation USED-FOR predictor. hindsight access FEATURE-OF held - out environments. representation USED-FOR RGM. invariant risk minimization ( IRM ) USED-FOR RGM. specialized domain perturbations USED-FOR structured extension. RGM COMPARE baselines. baselines COMPARE RGM. molecular property prediction CONJUNCTION protein homology and stability prediction. protein homology and stability prediction CONJUNCTION molecular property prediction. applications EVALUATE-FOR RGM. molecular property prediction EVALUATE-FOR method. applications EVALUATE-FOR method. protein homology and stability prediction HYPONYM-OF applications. molecular property prediction HYPONYM-OF applications. OtherScientificTerm are environments, simultaneous optimality condition, and complex environments. Metric is predictive regret. ","This paper proposes a novel regret minimization (RGM) algorithm for learning in structured environments, where the goal is to learn models that can be applied to a wide range of biochemical applications such as molecular property prediction and protein homology prediction. The authors propose to use natural environments with complex descriptors (molecular scaffolds and protein families) to learn the tasks, and then apply RGM to learn a representation of these environments that maximizes the predictive regret of the learned predictor.  The authors show that under a simultaneous optimality condition, the proposed RGM can achieve a regret of $O(1/\sqrt{T})$ under the held-out environments with hindsight access. They also show that RGM is invariant to invariant risk minimization and can be extended to more complex environments with specialized domain perturbations. The proposed method is evaluated on a variety of applications such that the proposed method outperforms the baselines in terms of RGM regret, and achieves state-of-the-art performance on two applications (Molecular property prediction, and proteinhomology and stability prediction).","This paper proposes a novel regret minimization (RGM) algorithm for learning in structured environments, where the goal is to learn models that can be applied to a wide range of biochemical applications such as molecular property prediction and protein homology prediction. The authors propose to use natural environments with complex descriptors (molecular scaffolds and protein families) to learn the tasks, and then apply RGM to learn a representation of these environments that maximizes the predictive regret of the learned predictor.  The authors show that under a simultaneous optimality condition, the proposed RGM can achieve a regret of $O(1/\sqrt{T})$ under the held-out environments with hindsight access. They also show that RGM is invariant to invariant risk minimization and can be extended to more complex environments with specialized domain perturbations. The proposed method is evaluated on a variety of applications such that the proposed method outperforms the baselines in terms of RGM regret, and achieves state-of-the-art performance on two applications (Molecular property prediction, and proteinhomology and stability prediction)."
1897,SP:cad3ed2fba57faf17a3e8899dc5a744d5358aa68,BERT USED-FOR NLP tasks. BERT USED-FOR text - vision BERT models. cross - modal attentions USED-FOR text - vision BERT models. text - vision BERT models USED-FOR language - vision tasks. text - image retrieval HYPONYM-OF language - vision tasks. cross - modal attentions USED-FOR textvision BERT models. cross - modal attentions USED-FOR textvision retrieval. textvision retrieval USED-FOR large - scale search. computation cost EVALUATE-FOR cross - modal attentions. cross - probe BERT HYPONYM-OF architecture. text and vision probes USED-FOR cross - modal attentions. text and vision probes USED-FOR It. It USED-FOR crossmodal attention. Generic is method. ,"This paper proposes to use cross-modal attentions to train text-vision BERT models based on BERT for NLP tasks. It uses both text and vision probes to learn crossmodal attention. The authors show that cross-imagenet attention can reduce the computation cost for language-vision tasks (e.g., text-image retrieval, text-video retrieval, and text-text classification). They also show that the cross-mutual attentions can be used for textvision retrieval for large-scale search. The proposed architecture is called cross-probe BERT, and the method is simple and effective.","This paper proposes to use cross-modal attentions to train text-vision BERT models based on BERT for NLP tasks. It uses both text and vision probes to learn crossmodal attention. The authors show that cross-imagenet attention can reduce the computation cost for language-vision tasks (e.g., text-image retrieval, text-video retrieval, and text-text classification). They also show that the cross-mutual attentions can be used for textvision retrieval for large-scale search. The proposed architecture is called cross-probe BERT, and the method is simple and effective."
1913,SP:51fd82de525fcb738fdeaeeae20fbb2cdf975f0c,Actor USED-FOR Actor - Critic algorithms. FORK HYPONYM-OF Actor. forward - looking Actor HYPONYM-OF Actor. FORK PART-OF model - free ActorCritic algorithm. continuous state and action spaces FEATURE-OF Box2D and MuJoCo environments. FORK USED-FOR BipedalWalkerHardcore. GPU USED-FOR FORK. Generic is algorithms. ,"This paper proposes a new Actor-Critic algorithms based on the Actor called FORK, which is a forward-looking Actor. The proposed model-free ActorCritic algorithm, called BipedalWalkerHardcore, incorporates FORK into the core of its model. Experiments on the Box2D and MuJoCo environments with continuous state and action spaces show that the proposed algorithms outperform the baselines. The authors also show that for the case where the GPU is not available, the proposed FORK can be trained on the GPU and can be used to improve the performance of the model. ","This paper proposes a new Actor-Critic algorithms based on the Actor called FORK, which is a forward-looking Actor. The proposed model-free ActorCritic algorithm, called BipedalWalkerHardcore, incorporates FORK into the core of its model. Experiments on the Box2D and MuJoCo environments with continuous state and action spaces show that the proposed algorithms outperform the baselines. The authors also show that for the case where the GPU is not available, the proposed FORK can be trained on the GPU and can be used to improve the performance of the model. "
1929,SP:6e730239e6e8b43c4988dd61dca30f15dc039ef7,"Federated learning USED-FOR global model. local models PART-OF global model. Bayesian inference perspective USED-FOR aggregation algorithm. FEDBE HYPONYM-OF aggregation algorithm. Bayesian model Ensemble USED-FOR them. Gaussian or Dirichlet distribution USED-FOR local models. Gaussian or Dirichlet distribution USED-FOR model distribution. FEDBE USED-FOR regularizing users ’ model training. Material is non - i.i.d. data. Method are global models, neural networks, aggregation method, and federated learning algorithm. Generic is it. ","This paper studies the problem of federated learning for non-i.i.d. data, where the goal is to learn a global model from a set of local models. The authors propose a new aggregation algorithm based on the Bayesian inference perspective, called FEDBE, which aggregates the local models of the global model and aggregates them via a Bayesian model Ensemble. They show that the model distribution is approximated by a Gaussian or Dirichlet distribution, and that the aggregation method can be used to regularize the training of neural networks. In particular, they show that by regularizing users’ model training with FED BE, they are able to improve the performance of the federated learners. They also show that FEDbe can be applied to regularizing the model training of the local learners.   The authors also provide a theoretical analysis of the proposed aggregation method and show that it is a generalization of the standard federated training algorithm. ","This paper studies the problem of federated learning for non-i.i.d. data, where the goal is to learn a global model from a set of local models. The authors propose a new aggregation algorithm based on the Bayesian inference perspective, called FEDBE, which aggregates the local models of the global model and aggregates them via a Bayesian model Ensemble. They show that the model distribution is approximated by a Gaussian or Dirichlet distribution, and that the aggregation method can be used to regularize the training of neural networks. In particular, they show that by regularizing users’ model training with FED BE, they are able to improve the performance of the federated learners. They also show that FEDbe can be applied to regularizing the model training of the local learners.   The authors also provide a theoretical analysis of the proposed aggregation method and show that it is a generalization of the standard federated training algorithm. "
1945,SP:3ac5f437fc349a33810d0645664d1c448528af74,,"This paper studies the problem of how to improve the generalization performance of deep neural networks. The authors propose a new method to improve generalization ability of deep learning models. The paper proposes to add a regularization term to the loss function to encourage the model to learn to generalize better. Experiments are conducted on CIFAR-10, Cifar-100, and ImageNet.","This paper studies the problem of how to improve the generalization performance of deep neural networks. The authors propose a new method to improve generalization ability of deep learning models. The paper proposes to add a regularization term to the loss function to encourage the model to learn to generalize better. Experiments are conducted on CIFAR-10, Cifar-100, and ImageNet."
1961,SP:efa2343ead47263a0d09e1c17f9aa044605b9650,settling time FEATURE-OF deep neural networks. priori upper bound FEATURE-OF deep neural networks. Lyapunov based analysis USED-FOR loss function. Lyapunov based analysis USED-FOR priori upper bound. settling time FEATURE-OF priori upper bound. control theory framework USED-FOR deep learning. deterministic control theoretic setting FEATURE-OF priori guarantees of finite - time convergence. tracking problem USED-FOR learning. control problem USED-FOR supervised learning framework. analytical formula USED-FOR finite - time upper bound. analytical formula USED-FOR settling time. settling time FEATURE-OF finite - time upper bound. input perturbations FEATURE-OF loss function. Method is priori finite time convergence analysis. Generic is network. OtherScientificTerm is control inputs. ,"This paper studies the settling time of deep neural networks with a priori upper bound on the number of training epochs. The authors derive a Lyapunov based analysis for the loss function under input perturbations, and show that the prior a finite time convergence analysis converges to a stationary point. They also provide a theoretical analysis of the priori guarantees of finite-time convergence in the deterministic control theoretic setting.    The authors extend the control theory framework for deep learning to the setting of deep learning in which the tracking problem for learning is solved as a control problem. In this setting, a supervised learning framework is introduced, and the authors provide an analytical formula for computing the a prior-time upper bound in terms of its settling time. They show that under certain assumptions, the network will converge to the stationary point in a finite number of epochs if the control inputs are sufficiently diverse. ","This paper studies the settling time of deep neural networks with a priori upper bound on the number of training epochs. The authors derive a Lyapunov based analysis for the loss function under input perturbations, and show that the prior a finite time convergence analysis converges to a stationary point. They also provide a theoretical analysis of the priori guarantees of finite-time convergence in the deterministic control theoretic setting.    The authors extend the control theory framework for deep learning to the setting of deep learning in which the tracking problem for learning is solved as a control problem. In this setting, a supervised learning framework is introduced, and the authors provide an analytical formula for computing the a prior-time upper bound in terms of its settling time. They show that under certain assumptions, the network will converge to the stationary point in a finite number of epochs if the control inputs are sufficiently diverse. "
1977,SP:7a0ded4b3b2d08d43765ff7b722da9b9863aabd6,Disentanglement of representations USED-FOR representations. incompressible - flow networks ( GIN ) USED-FOR latent variables. incompressible - flow networks ( GIN ) USED-FOR compact and disentangled representation. GIN USED-FOR informative latent variables selection. method USED-FOR informative latent variables selection. GIN USED-FOR method. mutual information USED-FOR informative latent variables. latent variables CONJUNCTION auxiliary variable. auxiliary variable CONJUNCTION latent variables. mutual information USED-FOR auxiliary variable. synthetic data EVALUATE-FOR method. outlier detection CONJUNCTION adversarial attack defence. adversarial attack defence CONJUNCTION outlier detection. classification CONJUNCTION outlier detection. outlier detection CONJUNCTION classification. downstream tasks EVALUATE-FOR method. synthetic and real data EVALUATE-FOR adversarial attack defence. classification EVALUATE-FOR method. adversarial attack defence HYPONYM-OF downstream tasks. classification HYPONYM-OF downstream tasks. outlier detection HYPONYM-OF downstream tasks. Task is machine learning. Method is nonlinear independent component analysis theory. ,"Disentanglement of representations is an important problem in machine learning. This paper proposes a novel method based on nonlinear independent component analysis theory to learn representations that are disentangled. The authors propose to use incompressible-flow networks (GIN) to learn the latent variables and the auxiliary variable, and use GIN to learn a compact and disentangling representation. The proposed method uses GIN for informative latent variables selection based on mutual information between the latent variable and auxiliary variable. Experiments on synthetic data show the effectiveness of the proposed method on several downstream tasks including classification, outlier detection, adversarial attack defence on synthetic and real data.","Disentanglement of representations is an important problem in machine learning. This paper proposes a novel method based on nonlinear independent component analysis theory to learn representations that are disentangled. The authors propose to use incompressible-flow networks (GIN) to learn the latent variables and the auxiliary variable, and use GIN to learn a compact and disentangling representation. The proposed method uses GIN for informative latent variables selection based on mutual information between the latent variable and auxiliary variable. Experiments on synthetic data show the effectiveness of the proposed method on several downstream tasks including classification, outlier detection, adversarial attack defence on synthetic and real data."
1993,SP:0d9ba12bbf47b13a46c2225f9dc06878418daaea,"Pooling PART-OF convolutional neural networks. pooling operations USED-FOR feature maps. feature maps HYPONYM-OF lossy process. LiftDownPool CONJUNCTION LiftUpPool. LiftUpPool CONJUNCTION LiftDownPool. LiftPool USED-FOR bidirectional pooling layers. Lifting Scheme USED-FOR signal processing. LiftUpPool HYPONYM-OF bidirectional pooling layers. LiftDownPool HYPONYM-OF bidirectional pooling layers. LiftDownPool HYPONYM-OF LiftPool. LiftUpPool HYPONYM-OF LiftPool. LiftDownPool USED-FOR feature map. LiftDownPool USED-FOR downsized sub - bands. pooling function PART-OF LiftDownPool. image classification and semantic segmentation EVALUATE-FOR methods. backbones USED-FOR image classification and semantic segmentation. backbones USED-FOR methods. input corruptions CONJUNCTION perturbations. perturbations CONJUNCTION input corruptions. input corruptions FEATURE-OF robustness. robustness EVALUATE-FOR LiftDownPool. OtherScientificTerm are receptive fields, input variations, and downscaled feature map. Generic is they. Task are downsampling, and image - to - image translation challenges. Method is up - pooling layer LiftUpPool. ","Pooling in convolutional neural networks is a lossy process, where the pooling operations on feature maps (i.e., the feature maps that are shared across different receptive fields) can be seen as a downsampling. This paper proposes two variants of LiftPool, namely LiftDownPool and LiftUpPool, which are bidirectional pooling layers. LiftPool uses the Lifting Scheme for the signal processing, which is a well-known technique to reduce the size of the receptive fields. The authors show that LiftPool can be used to downsize two types of Bidirectional Pooling layers (LiftUpPool and LiftingDownPool) and that they are both robust to input variations. They also show that the downscaled sub-bands can also be downsized using LiftDownpool. Finally, the authors propose a new up-pooling layer LiftUppool, which downsamples the feature map of each sub-band to a single pooling function. The proposed methods are evaluated on image classification and semantic segmentation using two different backbones, and show improved robustness to input corruptions and perturbations. The paper also studies image-to-image translation challenges and shows that the proposed methods can be applied to a variety of datasets. ","Pooling in convolutional neural networks is a lossy process, where the pooling operations on feature maps (i.e., the feature maps that are shared across different receptive fields) can be seen as a downsampling. This paper proposes two variants of LiftPool, namely LiftDownPool and LiftUpPool, which are bidirectional pooling layers. LiftPool uses the Lifting Scheme for the signal processing, which is a well-known technique to reduce the size of the receptive fields. The authors show that LiftPool can be used to downsize two types of Bidirectional Pooling layers (LiftUpPool and LiftingDownPool) and that they are both robust to input variations. They also show that the downscaled sub-bands can also be downsized using LiftDownpool. Finally, the authors propose a new up-pooling layer LiftUppool, which downsamples the feature map of each sub-band to a single pooling function. The proposed methods are evaluated on image classification and semantic segmentation using two different backbones, and show improved robustness to input corruptions and perturbations. The paper also studies image-to-image translation challenges and shows that the proposed methods can be applied to a variety of datasets. "
2009,SP:147239edceb17bade6ea5d3dca44e3a59998aa47,"stable noise - shaping quantization scheme USED-FOR embedding method. fast linear transformation USED-FOR ` 1 norm. fast linear transformation USED-FOR Euclidean distances. ` 1 norm USED-FOR Euclidean distances. well - spread data EVALUATE-FOR method. time complexity EVALUATE-FOR method. space complexity EVALUATE-FOR method. continuous valued Johnson - Lindenstrauss embedding CONJUNCTION quantization error. quantization error CONJUNCTION continuous valued Johnson - Lindenstrauss embedding. polynomial decay FEATURE-OF quantization error. accuracy EVALUATE-FOR binary codes. natural images EVALUATE-FOR method. Material is high - dimensional dataset. OtherScientificTerm are binary sequences, T, sparse Gaussian random matrix, Hamming distance, Walsh - Hadamard matrix, and embedding dimension. Method is binary embedding methods. Task is embedding. Generic are approach, and it. ","This paper proposes a new embedding method based on a stable noise-shaping quantization scheme. The authors consider a high-dimensional dataset where the input is a sequence of binary sequences, and the goal is to learn an embedding of each sequence.    The authors propose to use a fast linear transformation on the `1 norm of the Euclidean distances between the binary codes of a pair of sequences T and T, where T is a sparse Gaussian random matrix, and T is the Hamming distance between the two sequences.  The paper shows that this fast transformation can be used to compute the ` 1 norm of a sequence, and that the resulting embedding can be applied to any continuous valued Johnson-Lindenstrauss embedding and quantization error with polynomial decay. The paper also shows that the proposed method has a time complexity of O(1/\sqrt{T}) and space complexity O(T/T) for well-spread data, which is much faster than existing binary embedding methods.  In addition, the authors show that the embedding dimension of the proposed approach is polynomially differentiable, which means that it does not depend on the number of sequences, but rather on the input sequence. They also show that their method is able to achieve high accuracy on natural images, and they show that it can also be used on synthetic data. ","This paper proposes a new embedding method based on a stable noise-shaping quantization scheme. The authors consider a high-dimensional dataset where the input is a sequence of binary sequences, and the goal is to learn an embedding of each sequence.    The authors propose to use a fast linear transformation on the `1 norm of the Euclidean distances between the binary codes of a pair of sequences T and T, where T is a sparse Gaussian random matrix, and T is the Hamming distance between the two sequences.  The paper shows that this fast transformation can be used to compute the ` 1 norm of a sequence, and that the resulting embedding can be applied to any continuous valued Johnson-Lindenstrauss embedding and quantization error with polynomial decay. The paper also shows that the proposed method has a time complexity of O(1/\sqrt{T}) and space complexity O(T/T) for well-spread data, which is much faster than existing binary embedding methods.  In addition, the authors show that the embedding dimension of the proposed approach is polynomially differentiable, which means that it does not depend on the number of sequences, but rather on the input sequence. They also show that their method is able to achieve high accuracy on natural images, and they show that it can also be used on synthetic data. "
2025,SP:f65e229bca3904095743e7a501b1083cc60f1e22,"robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. process USED-FOR ANNs. synaptic plasticity rules USED-FOR Gradient Descent ( GD ). rule parameters USED-FOR GD. GD USED-FOR rules. plasticity rules USED-FOR recurrent neural nets ( RNNs ). GD USED-FOR plasticity rules. rules USED-FOR MNIST / Fashion MNIST. synthetic data USED-FOR rules. process USED-FOR plasticity rules. adversarial perturbations FEATURE-OF tolerance. tolerance FEATURE-OF classifiers. plasticity rules USED-FOR classifiers. perceptron algorithm CONJUNCTION multiplicative weights method. multiplicative weights method CONJUNCTION perceptron algorithm. GD USED-FOR plasticity rule. GD USED-FOR perceptron algorithm. GD USED-FOR multiplicative weights method. GD USED-FOR learning rules. evolutionary time FEATURE-OF it. Task are learning tasks, and genetic setting. Method are artificial neural nets ( ANNs ), backpropagation, and classification network. Generic is data. OtherScientificTerm is numerical parameter. ","This paper studies the plasticity rules of artificial neural nets (ANNs) in the context of learning tasks where the goal is to improve robustness and generalization. The authors propose a genetic setting where the training process of ANNs is based on a process called ""Synaptic Plasticity Rules"" (SPRs). In this setting, the authors use Gradient Descent (GD) to learn a set of plasticity rule parameters that are used to guide the learning of backpropagation of a classification network. They show that GD can learn rules that are robust to adversarial perturbations and generalize well to new data.    The authors also show that, in the genetic setting, GD can be used to learn rules on MNIST/Fashion MNIST using synthetic data, and that GD is able to learn such rules in a similar way to the way that learning rules are learned in a biological setting.  The paper also shows that, under certain assumptions on the rule parameters of GD, it is possible to train a classifier with a certain level of tolerance to the classifiers trained with different levels of tolerance.  In addition, the paper shows that GD also learns a perceptron algorithm based on GD as well as a multiplicative weights method.  Finally, it shows that the learning rules learned by GD are adaptive and that it can be learned in an evolutionary time. ","This paper studies the plasticity rules of artificial neural nets (ANNs) in the context of learning tasks where the goal is to improve robustness and generalization. The authors propose a genetic setting where the training process of ANNs is based on a process called ""Synaptic Plasticity Rules"" (SPRs). In this setting, the authors use Gradient Descent (GD) to learn a set of plasticity rule parameters that are used to guide the learning of backpropagation of a classification network. They show that GD can learn rules that are robust to adversarial perturbations and generalize well to new data.    The authors also show that, in the genetic setting, GD can be used to learn rules on MNIST/Fashion MNIST using synthetic data, and that GD is able to learn such rules in a similar way to the way that learning rules are learned in a biological setting.  The paper also shows that, under certain assumptions on the rule parameters of GD, it is possible to train a classifier with a certain level of tolerance to the classifiers trained with different levels of tolerance.  In addition, the paper shows that GD also learns a perceptron algorithm based on GD as well as a multiplicative weights method.  Finally, it shows that the learning rules learned by GD are adaptive and that it can be learned in an evolutionary time. "
2041,SP:f435530146fa975cb27cd375a857df9bcbd87682,"visual question generation ( VQG ) USED-FOR human - like questions. image CONJUNCTION side information. side information CONJUNCTION image. image USED-FOR visual question generation ( VQG ). side information USED-FOR human - like questions. image USED-FOR human - like questions. visual objects PART-OF image. side information CONJUNCTION image. image CONJUNCTION side information. image USED-FOR generating referential and meaningful questions. learning paradigm USED-FOR visual questions. answer - awareness CONJUNCTION region - reference. region - reference CONJUNCTION answer - awareness. region - reference USED-FOR learning paradigm. answer - awareness FEATURE-OF visual questions. Double Hints textual answers CONJUNCTION visual regions of interests. visual regions of interests CONJUNCTION Double Hints textual answers. Double Hints textual answers USED-FOR visual questions. methodology USED-FOR visual hints. dynamic graph USED-FOR them. VQA2.0 CONJUNCTION COCO - QA datasets. COCO - QA datasets CONJUNCTION VQA2.0. model COMPARE baselines. baselines COMPARE model. COCO - QA datasets EVALUATE-FOR model. VQA2.0 EVALUATE-FOR model. COCO - QA datasets EVALUATE-FOR baselines. setting EVALUATE-FOR model. Task are VQG, and one - to - many mapping issue. OtherScientificTerm are human annotations, implicit topology end - to - end, and double hints. Method is graph - to - sequence model. ","This paper proposes a new method for visual question generation (VQG) based on a graph-to-sequence model. VQG is an important problem in the context of human-like questions, where the goal is to generate questions that are referential and meaningful to humans. The authors propose a new learning paradigm to generate visual questions with answer-awareness and region-reference, where visual objects in an image are augmented with human annotations. The key idea is to use side information from the side information of the question and the image, as well as the image itself, to guide the learning of a graph to sequence model. Double Hints textual answers and visual regions of interests are used to guide visual questions, and a methodology is also proposed to extract visual hints from the visual hints and use them as part of a dynamic graph. Experiments on VQA2.0 and COCO-QA datasets show that the proposed model outperforms several baselines in this setting, and is able to capture the implicit topology of the visual questions. The paper also shows that the model can also be used to solve a one- to-many mapping issue, and that double hints can be used as additional information.  ","This paper proposes a new method for visual question generation (VQG) based on a graph-to-sequence model. VQG is an important problem in the context of human-like questions, where the goal is to generate questions that are referential and meaningful to humans. The authors propose a new learning paradigm to generate visual questions with answer-awareness and region-reference, where visual objects in an image are augmented with human annotations. The key idea is to use side information from the side information of the question and the image, as well as the image itself, to guide the learning of a graph to sequence model. Double Hints textual answers and visual regions of interests are used to guide visual questions, and a methodology is also proposed to extract visual hints from the visual hints and use them as part of a dynamic graph. Experiments on VQA2.0 and COCO-QA datasets show that the proposed model outperforms several baselines in this setting, and is able to capture the implicit topology of the visual questions. The paper also shows that the model can also be used to solve a one- to-many mapping issue, and that double hints can be used as additional information.  "
2057,SP:53a26ce11647866d3f6ba8b84ca9f13106197a8d,sample size CONJUNCTION model size. model size CONJUNCTION sample size. linear regression CONJUNCTION neural networks. neural networks CONJUNCTION linear regression. linear regression HYPONYM-OF learning algorithms. neural networks HYPONYM-OF learning algorithms. optimal regularization USED-FOR double - descent phenomenon. sample size CONJUNCTION model size. model size CONJUNCTION sample size. isotropic data distribution FEATURE-OF linear regression models. optimally - tuned ` 2 regularization USED-FOR linear regression models. optimally - tuned ` 2 regularization USED-FOR double descent. test risk scalings EVALUATE-FOR algorithms. tuned regularization USED-FOR algorithms. tuned regularization USED-FOR test risk scalings. Task is generalization. ,"This paper studies the double-descent phenomenon of learning algorithms such as linear regression and neural networks, where optimal regularization is used to mitigate the double descent phenomenon. The authors consider linear regression models with isotropic data distribution, where the sample size, model size, and the number of training epochs, as well as the size of the training set, affect the generalization performance. They show that the ‘optimally-tuned `2 regularization can prevent double descent in the case of linear regression, and that the same phenomenon holds for neural networks. They also show that algorithms with tuned regularization improve the test risk scalings.","This paper studies the double-descent phenomenon of learning algorithms such as linear regression and neural networks, where optimal regularization is used to mitigate the double descent phenomenon. The authors consider linear regression models with isotropic data distribution, where the sample size, model size, and the number of training epochs, as well as the size of the training set, affect the generalization performance. They show that the ‘optimally-tuned `2 regularization can prevent double descent in the case of linear regression, and that the same phenomenon holds for neural networks. They also show that algorithms with tuned regularization improve the test risk scalings."
2073,SP:c193ccc74b987beaf8d53a29a8529a0af5e87742,spatial regularities FEATURE-OF images. spatial regularities USED-FOR generative modeling. neural network USED-FOR building image generators ( decoders ). it USED-FOR variational autoencoders ( VAEs ). sequential gating - based mechanism USED-FOR contextual information. feature maps PART-OF deep neural net. sequential gating - based mechanism USED-FOR feature maps. spatial dependency layers USED-FOR density estimation. decoder USED-FOR density estimation. decoder USED-FOR hierarchical VAE. baseline convolutional architectures USED-FOR density estimation. spatial dependency layers USED-FOR hierarchical VAE. spatial dependency layers USED-FOR decoder. SDN USED-FOR large images. SDN decoder USED-FOR learning disentangled representations. neural architectures USED-FOR task. SDN decoder USED-FOR vanilla VAE setting. spatial dependency COMPARE convolutional layers. convolutional layers COMPARE spatial dependency. Method is spatial dependency networks ( SDNs ). OtherScientificTerm is 2 - D space. Generic is models. Material is VAE settings. ,"This paper introduces spatial dependency networks (SDNs), a new type of generative modeling based on spatial regularities in images. The idea is to use a neural network for building image generators (decoders) and then apply it to variational autoencoders (VAEs). The feature maps in a deep neural net are modelled by a sequential gating-based mechanism to capture contextual information. The spatial dependency layers of the decoder are used for density estimation in a hierarchical VAE, while the baseline convolutional architectures are used to model density estimation. The authors show that the spatial dependency of the SDN decoder can be used for learning disentangled representations, and that the performance of SDN for large images is comparable to that of a standard VAE decoder in the vanilla VAE setting. They also show that neural architectures for this task can be trained with different spatial dependencies.    The main contribution of this paper is the introduction of the concept of spatial dependency in 2-D space, and the development of a new class of models. The paper also shows that spatial dependency is more powerful than the conventional convolutions in VAE settings. ","This paper introduces spatial dependency networks (SDNs), a new type of generative modeling based on spatial regularities in images. The idea is to use a neural network for building image generators (decoders) and then apply it to variational autoencoders (VAEs). The feature maps in a deep neural net are modelled by a sequential gating-based mechanism to capture contextual information. The spatial dependency layers of the decoder are used for density estimation in a hierarchical VAE, while the baseline convolutional architectures are used to model density estimation. The authors show that the spatial dependency of the SDN decoder can be used for learning disentangled representations, and that the performance of SDN for large images is comparable to that of a standard VAE decoder in the vanilla VAE setting. They also show that neural architectures for this task can be trained with different spatial dependencies.    The main contribution of this paper is the introduction of the concept of spatial dependency in 2-D space, and the development of a new class of models. The paper also shows that spatial dependency is more powerful than the conventional convolutions in VAE settings. "
2089,SP:db91512a90e75675af03c2f197751c8526d6f5e9,"prior approach USED-FOR offline RL. backup operator USED-FOR algorithm. EMaQ USED-FOR sub - optimality bounds. complexity EVALUATE-FOR offline RL problems. proposal distribution USED-FOR EMaQ. offline RL setting EVALUATE-FOR EMaQ. D4RL benchmarks EVALUATE-FOR EMaQ. EMaQ COMPARE Soft Actor Critic ( SAC ). Soft Actor Critic ( SAC ) COMPARE EMaQ. online RL setting EVALUATE-FOR EMaQ. generative model design USED-FOR estimating behavior policies. complexity EVALUATE-FOR offline RL problems. Method are Off - policy reinforcement learning ( RL ), off - policy RL methods, and BCQ. Generic is methods. OtherScientificTerm are policies, dataset of interactions, heuristic design choice, behavior policy, distribution support, behavior policies, and function approximator. ","This paper proposes EMaQ, a novel approach to offline RL that extends a prior approach, BCQ. Off-policy reinforcement learning (RL) is an important problem in which the goal is to learn policies from a dataset of interactions, but offline RL methods are expensive to train. The authors propose a heuristic design choice, where the algorithm uses a backup operator to ensure that the behavior policy does not change too much in the absence of a distribution support. The algorithm is based on BCQ, which is a prior work that uses a function approximator to estimate the optimal policy from the proposal distribution. The paper shows that EMAQ can achieve sub-optimality bounds on the complexity of offline RL problems in the offline RL setting, and outperforms BCQ on the D4RL benchmarks. EMaq also outperforms Soft Actor Critic (SAC) in the online RL setting. The main contribution of the paper is the generative model design for estimating behavior policies. ","This paper proposes EMaQ, a novel approach to offline RL that extends a prior approach, BCQ. Off-policy reinforcement learning (RL) is an important problem in which the goal is to learn policies from a dataset of interactions, but offline RL methods are expensive to train. The authors propose a heuristic design choice, where the algorithm uses a backup operator to ensure that the behavior policy does not change too much in the absence of a distribution support. The algorithm is based on BCQ, which is a prior work that uses a function approximator to estimate the optimal policy from the proposal distribution. The paper shows that EMAQ can achieve sub-optimality bounds on the complexity of offline RL problems in the offline RL setting, and outperforms BCQ on the D4RL benchmarks. EMaq also outperforms Soft Actor Critic (SAC) in the online RL setting. The main contribution of the paper is the generative model design for estimating behavior policies. "
2105,SP:e2b80adeaa9208e0667a64a3f24661f77b48e487,"fair machine learning model USED-FOR demographic disparity. data preprocessing CONJUNCTION model training. model training CONJUNCTION data preprocessing. Existing techniques USED-FOR model fairness. outer optimizer USED-FOR inner problem. inner optimizer USED-FOR training algorithm. minibatch sizes USED-FOR model fairness. equal opportunity CONJUNCTION equalized odds. equalized odds CONJUNCTION equal opportunity. equalized odds CONJUNCTION demographic parity. demographic parity CONJUNCTION equalized odds. optimization USED-FOR batch selection algorithm. FairBatch HYPONYM-OF batch selection algorithm. fairness measures PART-OF batch selection algorithm. equal opportunity HYPONYM-OF fairness measures. demographic parity HYPONYM-OF fairness measures. equalized odds HYPONYM-OF fairness measures. data preprocessing CONJUNCTION model training. model training CONJUNCTION data preprocessing. PyTorch code USED-FOR batch selection. batch selection PART-OF model training. FairBatch USED-FOR fairness. fine - tuning USED-FOR FairBatch. It CONJUNCTION batch selection techniques. batch selection techniques CONJUNCTION It. faster convergence HYPONYM-OF batch selection techniques. Method are machine learning systems, and bilevel optimization. Generic are functionality, and it. Material is synthetic and benchmark real data. ","This paper studies the problem of fairness in batch selection in machine learning systems. Existing techniques for model fairness in data preprocessing, model training, and fine-tuning are limited due to the demographic disparity between training and test data. The authors propose a new fairness machine learning model, called FairBatch, which aims to address this issue. The basic idea is to use bilevel optimization to optimize a training algorithm, where the outer optimizer optimizes the inner problem, and the inner optimizer is used to optimize the training algorithm. The main contribution of the paper is to propose a batch selection algorithm based on optimization that incorporates fairness measures (e.g., equal opportunity, equalized odds, and demographic parity). The authors show that fair batch selection is possible in both synthetic and benchmark real data. They also show that minibatch sizes can be used to improve model fairness.  The authors also provide a PyTorch code for batch selection, which is used in the paper. They show that FairB batch can be applied to a variety of datasets, and that it can be combined with existing batch selection techniques such as faster convergence. The paper also shows that batch selection can be incorporated into model training and that the fairBatch can improve fairness by fine-tuning. ","This paper studies the problem of fairness in batch selection in machine learning systems. Existing techniques for model fairness in data preprocessing, model training, and fine-tuning are limited due to the demographic disparity between training and test data. The authors propose a new fairness machine learning model, called FairBatch, which aims to address this issue. The basic idea is to use bilevel optimization to optimize a training algorithm, where the outer optimizer optimizes the inner problem, and the inner optimizer is used to optimize the training algorithm. The main contribution of the paper is to propose a batch selection algorithm based on optimization that incorporates fairness measures (e.g., equal opportunity, equalized odds, and demographic parity). The authors show that fair batch selection is possible in both synthetic and benchmark real data. They also show that minibatch sizes can be used to improve model fairness.  The authors also provide a PyTorch code for batch selection, which is used in the paper. They show that FairB batch can be applied to a variety of datasets, and that it can be combined with existing batch selection techniques such as faster convergence. The paper also shows that batch selection can be incorporated into model training and that the fairBatch can improve fairness by fine-tuning. "
2121,SP:72f26b850bb2258223c0fc71598e35ad07d690e6,"robustness guarantees CONJUNCTION generalization bounds. generalization bounds CONJUNCTION robustness guarantees. Lipschitz constants FEATURE-OF deep networks. generalization bounds CONJUNCTION smoothness of decision boundaries. smoothness of decision boundaries CONJUNCTION generalization bounds. bounds USED-FOR models. deep equilibrium ( DEQ ) model HYPONYM-OF models. monotone DEQs HYPONYM-OF DEQs. Lipschitz constants FEATURE-OF monotone DEQs. input - output mapping CONJUNCTION weight - output mapping. weight - output mapping CONJUNCTION input - output mapping. simple - yet - tight bounds USED-FOR input - output mapping. simple - yet - tight bounds USED-FOR weight - output mapping. networks USED-FOR weight - output mapping. bounds USED-FOR monotone DEQ models. multiscale convolutional structure USED-FOR monotone DEQ models. bounds USED-FOR PAC - Bayes generalization bounds. Method are infinitely - deep network, and DNNs. OtherScientificTerm are monotonicity parameter, Lipschitz constant, and exponential depth - dependence of comparable DNN bounds. Generic is they. ","This paper studies the robustness guarantees and generalization bounds of deep networks with Lipschitz constants. The authors consider two models, the deep equilibrium (DEQ) model and monotone DEQs, where the monotonicity parameter is fixed. They provide bounds for both of these models, and show that the bounds are tight for both models. In particular, they show that for monotonic DEQ models with multiscale convolutional structure, the bounds for the input-output mapping and the weight-outward mapping of the networks are tight. They also provide simple-yet-tight bounds for a simple form of input-outlet mapping and a simple yet tight one for a specific type of weight- output mapping. Finally, the authors show that these bounds can be used to derive PAC-Bayes generalization bound for infinitely-deep network.    The main contribution of this paper is that the authors provide bounds that are tight (in terms of the Lipsichitz constant) for monotonically monotonical DNNs, and that are robust to the exponential depth-dependent of comparable DNN bounds. ","This paper studies the robustness guarantees and generalization bounds of deep networks with Lipschitz constants. The authors consider two models, the deep equilibrium (DEQ) model and monotone DEQs, where the monotonicity parameter is fixed. They provide bounds for both of these models, and show that the bounds are tight for both models. In particular, they show that for monotonic DEQ models with multiscale convolutional structure, the bounds for the input-output mapping and the weight-outward mapping of the networks are tight. They also provide simple-yet-tight bounds for a simple form of input-outlet mapping and a simple yet tight one for a specific type of weight- output mapping. Finally, the authors show that these bounds can be used to derive PAC-Bayes generalization bound for infinitely-deep network.    The main contribution of this paper is that the authors provide bounds that are tight (in terms of the Lipsichitz constant) for monotonically monotonical DNNs, and that are robust to the exponential depth-dependent of comparable DNN bounds. "
2137,SP:bcfd4d7fd4590e3bc248a0a5422ce4b67db74a74,imitation learning CONJUNCTION goal - conditioned reinforcement learning. goal - conditioned reinforcement learning CONJUNCTION imitation learning. goal - conditioned reinforcement learning HYPONYM-OF settings. imitation learning HYPONYM-OF settings. probabilistic long - term dynamics CONJUNCTION desired value function. desired value function CONJUNCTION probabilistic long - term dynamics. density estimation USED-FOR approach. it USED-FOR hindsight bias. hindsight bias FEATURE-OF stochastic domains. it USED-FOR sparse rewards. expert data USED-FOR approach. Generic is solutions. Method is goalconditioned reinforcement learning. ,"This paper considers two settings: imitation learning and goal-conditioned reinforcement learning, where the goal is to learn a probabilistic long-term dynamics and a desired value function. The authors propose a novel approach based on density estimation, and show that it alleviates the hindsight bias in stochastic domains, and that it is more robust to sparse rewards. They also show that the proposed approach can be applied to expert data.    The paper is well-written, well-motivated, and easy to follow. However, there are a few issues that need to be addressed, and the proposed solutions are not very novel.  I have a few questions about the details of the paper, and I am happy to increase my score if the authors provide more details. ","This paper considers two settings: imitation learning and goal-conditioned reinforcement learning, where the goal is to learn a probabilistic long-term dynamics and a desired value function. The authors propose a novel approach based on density estimation, and show that it alleviates the hindsight bias in stochastic domains, and that it is more robust to sparse rewards. They also show that the proposed approach can be applied to expert data.    The paper is well-written, well-motivated, and easy to follow. However, there are a few issues that need to be addressed, and the proposed solutions are not very novel.  I have a few questions about the details of the paper, and I am happy to increase my score if the authors provide more details. "
2153,SP:d57550b2f323b356d7e609acc35ee33039f376b4,"probabilistic inference framework USED-FOR simultaneously learning multiple related tasks. variational multi - task learning VMTL HYPONYM-OF probabilistic inference framework. variational Bayesian inference problem USED-FOR multi - task learning. priors USED-FOR task relatedness. mixture of variational posteriors USED-FOR prior. representations CONJUNCTION classifiers. classifiers CONJUNCTION representations. VMTL USED-FOR multi - task learning. limited training data USED-FOR VMTL. limited training data USED-FOR multi - task learning. benchmark datasets EVALUATE-FOR it. Method is Multi - task learning. OtherScientificTerm are Gumbel - softmax priors, mixing weights, and shared inductive bias. Generic is tasks. ","Multi-task learning is an important problem in machine learning. This paper proposes a new probabilistic inference framework for simultaneously learning multiple related tasks, called variational multi-tasks learning VMTL. Multi-task learning is a variational Bayesian inference problem, and the authors propose to use Gumbel-softmax priors to model the task relatedness. The prior is a mixture of variational posteriors, where the priors are learned by mixing weights. The authors show that the learned priors can be shared across tasks, and that the shared inductive bias can be used to learn representations and classifiers that are more robust to changes in the tasks. The paper also shows that the performance of V MTL with limited training data can be improved when the number of tasks is limited, and it is shown to perform well on several benchmark datasets.","Multi-task learning is an important problem in machine learning. This paper proposes a new probabilistic inference framework for simultaneously learning multiple related tasks, called variational multi-tasks learning VMTL. Multi-task learning is a variational Bayesian inference problem, and the authors propose to use Gumbel-softmax priors to model the task relatedness. The prior is a mixture of variational posteriors, where the priors are learned by mixing weights. The authors show that the learned priors can be shared across tasks, and that the shared inductive bias can be used to learn representations and classifiers that are more robust to changes in the tasks. The paper also shows that the performance of V MTL with limited training data can be improved when the number of tasks is limited, and it is shown to perform well on several benchmark datasets."
2169,SP:3ccdf8322f16c8a7bef82e32fad4c03969a510d1,"Transformers USED-FOR long sequence lengths. fast Transformers USED-FOR problem. model quality EVALUATE-FOR vanilla Transformer models. systematic and unified benchmark EVALUATE-FOR model quality. Long - Range Arena EVALUATE-FOR model quality. long - context scenarios FEATURE-OF model quality. Long - Range Arena HYPONYM-OF systematic and unified benchmark. text CONJUNCTION natural, synthetic images. natural, synthetic images CONJUNCTION text. natural, synthetic images CONJUNCTION mathematical expressions. mathematical expressions CONJUNCTION natural, synthetic images. similarity USED-FOR mathematical expressions. Linear Transformers CONJUNCTION Sinkhorn Transformers. Sinkhorn Transformers CONJUNCTION Linear Transformers. Synthesizers CONJUNCTION Sparse Transformers. Sparse Transformers CONJUNCTION Synthesizers. Linformers CONJUNCTION Linear Transformers. Linear Transformers CONJUNCTION Linformers. Performers CONJUNCTION Synthesizers. Synthesizers CONJUNCTION Performers. Sparse Transformers CONJUNCTION Longformers. Longformers CONJUNCTION Sparse Transformers. Sinkhorn Transformers CONJUNCTION Performers. Performers CONJUNCTION Sinkhorn Transformers. Reformers CONJUNCTION Linformers. Linformers CONJUNCTION Reformers. Performers CONJUNCTION Sparse Transformers. Sparse Transformers CONJUNCTION Performers. Synthesizers CONJUNCTION Longformers. Longformers CONJUNCTION Synthesizers. benchmark suite EVALUATE-FOR long - range Transformer models. Longformers HYPONYM-OF long - range Transformer models. Sparse Transformers HYPONYM-OF long - range Transformer models. Sinkhorn Transformers HYPONYM-OF long - range Transformer models. Reformers HYPONYM-OF long - range Transformer models. Synthesizers HYPONYM-OF long - range Transformer models. Linformers HYPONYM-OF long - range Transformer models. Performers HYPONYM-OF long - range Transformer models. Linear Transformers HYPONYM-OF long - range Transformer models. Long - Range Arena USED-FOR Transformer models. Metric are quadratic self - attention complexity, and","This paper presents a systematic and unified benchmark for evaluating the model quality of vanilla Transformer models on Long-Range Arena, which is designed to evaluate model quality in long-context scenarios. The authors show that Transformers have quadratic self-attention complexity in the long-range domain, and that fast Transformers can be used to tackle this problem. They also show that long sequence lengths can be efficiently handled by fast Transformers. The paper also shows that models trained on the benchmark suite are able to generalize well to long sequences of text, natural, synthetic images, and mathematical expressions that share a similarity to the input sequence.   The authors also provide a comprehensive evaluation of various long range Transformer architectures (Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, Longformers) on the Long-range Arena, and show that all of them perform well on the standard benchmark suite. ","This paper presents a systematic and unified benchmark for evaluating the model quality of vanilla Transformer models on Long-Range Arena, which is designed to evaluate model quality in long-context scenarios. The authors show that Transformers have quadratic self-attention complexity in the long-range domain, and that fast Transformers can be used to tackle this problem. They also show that long sequence lengths can be efficiently handled by fast Transformers. The paper also shows that models trained on the benchmark suite are able to generalize well to long sequences of text, natural, synthetic images, and mathematical expressions that share a similarity to the input sequence.   The authors also provide a comprehensive evaluation of various long range Transformer architectures (Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, Longformers) on the Long-range Arena, and show that all of them perform well on the standard benchmark suite. "
2185,SP:e12e410c3335b76133ceda4c865b244fbbab8580,"Structure CONJUNCTION Context. Context CONJUNCTION Structure. Structure USED-FOR machine learning models. Context USED-FOR machine learning models. Structure of source code USED-FOR model. source code CONJUNCTION features. features CONJUNCTION source code. language - agnostic features USED-FOR model. features HYPONYM-OF language - agnostic features. AST USED-FOR features. source code HYPONYM-OF language - agnostic features. programming languages EVALUATE-FOR monolingual code summarization. Structure CONJUNCTION Context. Context CONJUNCTION Structure. Structure USED-FOR representation learning on code. Context USED-FOR representation learning on code. OtherScientificTerm are Source code ( Context ), and computer program. Method is multilingual code summarization model. Material are non - parallel data, and low - resource languages. ",This paper proposes a multilingual code summarization model that can be applied to non-parallel data. Structure of source code (Structure) and Context are used to train machine learning models. Source code (Context) is the representation of a computer program. The authors show that language-agnostic features such as source code and features from AST can be used to improve the performance of the model. The paper also shows that monolingual code summation can be performed in a variety of programming languages. Structure and Context help representation learning on code.  ,This paper proposes a multilingual code summarization model that can be applied to non-parallel data. Structure of source code (Structure) and Context are used to train machine learning models. Source code (Context) is the representation of a computer program. The authors show that language-agnostic features such as source code and features from AST can be used to improve the performance of the model. The paper also shows that monolingual code summation can be performed in a variety of programming languages. Structure and Context help representation learning on code.  
2201,SP:f46e98d48f90071831f1c0069bf74a7993be6db8,"sights USED-FOR sound source. recurrent aggregations of the audio observations USED-FOR models. reinforcement learning approach USED-FOR audio - visual navigation. elements USED-FOR reinforcement learning approach. waypoints PART-OF elements. audio and visual data USED-FOR geometry of an unmapped space. real - world 3D scenes CONJUNCTION Replica. Replica CONJUNCTION real - world 3D scenes. real - world 3D scenes EVALUATE-FOR approach. sights CONJUNCTION sounds. sounds CONJUNCTION sights. sounds CONJUNCTION space. space CONJUNCTION sounds. OtherScientificTerm are agent motion, acoustic memory, and audio_visual_waypoints. Method is navigation policy. Generic is model. ","This paper proposes a reinforcement learning approach for audio-visual navigation based on two elements: (1) waypoints that are recurrent aggregations of the audio observations and (2) an acoustic memory that is used to guide the agent motion. The sound source is sampled from a set of sights and sounds, and the navigation policy is conditioned on the acoustic memory. The key idea is to learn the geometry of an unmapped space from both audio and visual data. The proposed approach is evaluated on two real-world 3D scenes and Replica. The results show that the proposed model is able to learn a good trade-off between sounds, sights, and space.  ","This paper proposes a reinforcement learning approach for audio-visual navigation based on two elements: (1) waypoints that are recurrent aggregations of the audio observations and (2) an acoustic memory that is used to guide the agent motion. The sound source is sampled from a set of sights and sounds, and the navigation policy is conditioned on the acoustic memory. The key idea is to learn the geometry of an unmapped space from both audio and visual data. The proposed approach is evaluated on two real-world 3D scenes and Replica. The results show that the proposed model is able to learn a good trade-off between sounds, sights, and space.  "
2217,SP:23bfe317dcef00a91ea92389b3f39d9b93972454,"optimization methods COMPARE weight initializations. weight initializations COMPARE optimization methods. learning abilities FEATURE-OF neural networks. small CNN USED-FOR update rules. architecture USED-FOR task. task USED-FOR networks. architecture USED-FOR networks. initialization parameters USED-FOR gradient descent. single sign change HYPONYM-OF small perturbations. OtherScientificTerm are lottery tickets, and lottery ticket hypothesis. Method are small convolutional networks, and minimal networks. ","This paper studies the learning abilities of neural networks with different architectures and optimization methods compared to weight initializations. The authors first show that a small CNN can learn the update rules of a small convolutional networks. Then, the authors show that networks trained with the same architecture on the same task can learn networks that are asymptotically similar to lottery tickets. They also show that the lottery ticket hypothesis holds for a number of small perturbations (e.g., single sign change). Finally, they show that minimal networks can be trained with a different initialization parameters for gradient descent.   ","This paper studies the learning abilities of neural networks with different architectures and optimization methods compared to weight initializations. The authors first show that a small CNN can learn the update rules of a small convolutional networks. Then, the authors show that networks trained with the same architecture on the same task can learn networks that are asymptotically similar to lottery tickets. They also show that the lottery ticket hypothesis holds for a number of small perturbations (e.g., single sign change). Finally, they show that minimal networks can be trained with a different initialization parameters for gradient descent.   "
2233,SP:1b5ba618d3e28d48f9205c0780f8288a08fa5392,"Semi - supervised learning ( SSL ) USED-FOR unlabeled data. consistency regularization USED-FOR SSL approaches. RankingMatch HYPONYM-OF method. computational efficiency EVALUATE-FOR objective function. BatchMean Triplet loss HYPONYM-OF objective function. accuracy EVALUATE-FOR SVHN. accuracy EVALUATE-FOR SVHN. accuracy CONJUNCTION accuracy. accuracy CONJUNCTION accuracy. accuracy EVALUATE-FOR RankingMatch. accuracy EVALUATE-FOR RankingMatch. SSL benchmarks EVALUATE-FOR RankingMatch. BatchMean Triplet loss COMPARE Triplet loss. Triplet loss COMPARE BatchMean Triplet loss. ablation study EVALUATE-FOR BatchMean Triplet loss. ablation study EVALUATE-FOR Triplet loss. Material are labeled data, CIFAR-10, and CIFAR-100. Generic is model. OtherScientificTerm is labeled data amounts. ","Semi-supervised learning (SSL) for unlabeled data is a popular technique for improving the performance of a model when the number of labeled data is limited. However, existing SSL approaches rely on a consistency regularization, which can be problematic when the labeled data amounts are limited. This paper proposes a new method called RankingMatch, a method that tries to improve the computational efficiency of the objective function, namely the BatchMean Triplet loss. The authors show that the accuracy of SVHN can be improved by using the proposed objective function. They also show that RankingMatch can improve the accuracy on CIFAR-10, CifAR-100, and Cifar-100. Finally, they show that rankingMatch outperforms the state-of-the-art on the SSL benchmarks on both accuracy and accuracy with respect to the original objective function (which is based on the consistency of the model).   The authors also provide an ablation study that shows that the proposed batchMeans triplet loss performs better than the original Triplet losses. ","Semi-supervised learning (SSL) for unlabeled data is a popular technique for improving the performance of a model when the number of labeled data is limited. However, existing SSL approaches rely on a consistency regularization, which can be problematic when the labeled data amounts are limited. This paper proposes a new method called RankingMatch, a method that tries to improve the computational efficiency of the objective function, namely the BatchMean Triplet loss. The authors show that the accuracy of SVHN can be improved by using the proposed objective function. They also show that RankingMatch can improve the accuracy on CIFAR-10, CifAR-100, and Cifar-100. Finally, they show that rankingMatch outperforms the state-of-the-art on the SSL benchmarks on both accuracy and accuracy with respect to the original objective function (which is based on the consistency of the model).   The authors also provide an ablation study that shows that the proposed batchMeans triplet loss performs better than the original Triplet losses. "
2249,SP:f3abccf4a2566ffbc821aba209fab15058639ad4,"formulation USED-FOR sequential learning setting. meta - training CONJUNCTION adaptation. adaptation CONJUNCTION meta - training. sample complexity CONJUNCTION regret. regret CONJUNCTION sample complexity. sample complexity EVALUATE-FOR empirical risk minimization methods. regret EVALUATE-FOR empirical risk minimization methods. meta - learning USED-FOR online setting. meta - learning COMPARE empirical risk minimization methods. empirical risk minimization methods COMPARE meta - learning. regret EVALUATE-FOR meta - learning. sample complexity EVALUATE-FOR meta - learning. bi - level optimizations FEATURE-OF meta - learning algorithms. meta - training data USED-FOR meta - learning algorithms. meta - training data USED-FOR bi - level optimizations. meta - learning algorithms USED-FOR variable - shot settings. many - shot learning CONJUNCTION zero - shot learning. zero - shot learning CONJUNCTION many - shot learning. variable - shot settings PART-OF sequential learning. meta - learning algorithms USED-FOR sequential learning. zero - shot learning HYPONYM-OF variable - shot settings. meta - learning COMPARE supervised methods. supervised methods COMPARE meta - learning. cumulative performance EVALUATE-FOR supervised methods. sequential learning problems EVALUATE-FOR meta - learning. cumulative performance EVALUATE-FOR meta - learning. meta - learning USED-FOR learning systems. Method are Few - shot meta - learning methods, and metalearning. Generic is problem. ","This paper studies the problem of few-shot meta-learning, where the goal is to learn a learning algorithm that performs well on a small number of tasks in a sequential learning setting. The authors propose a new formulation of the sequential learning problem, which they call Few-Shot Meta-Learning (FSL). They show that the sample complexity of meta-training and adaptation in the online setting is the same as that of empirical risk minimization methods in the offline setting. They also show that meta-learners with bi-level optimization can achieve better sample complexity and regret in the meta-learner setting.    The authors also provide a theoretical analysis of the problem and provide a few experiments to support their claims.  They first show that there exists a connection between meta-train and meta-adaptation, and show that if we use meta-trained data to optimize the meta -learning algorithms, we can achieve similar sample complexity or better regret as empirical risk maximization methods. Then, they show that bi-levels of optimization can be applied to the meta learning algorithms, and that the performance is similar to that of metalearning. Finally, they provide some experimental results on variable-shot settings in sequential learning, including many-shot learning, zero-shot classification, and many-task learning. They find that meta learning outperforms supervised methods in terms of cumulative performance on all sequential learning problems, and they also provide some experiments on learning systems where meta learning is used to train learning systems. ","This paper studies the problem of few-shot meta-learning, where the goal is to learn a learning algorithm that performs well on a small number of tasks in a sequential learning setting. The authors propose a new formulation of the sequential learning problem, which they call Few-Shot Meta-Learning (FSL). They show that the sample complexity of meta-training and adaptation in the online setting is the same as that of empirical risk minimization methods in the offline setting. They also show that meta-learners with bi-level optimization can achieve better sample complexity and regret in the meta-learner setting.    The authors also provide a theoretical analysis of the problem and provide a few experiments to support their claims.  They first show that there exists a connection between meta-train and meta-adaptation, and show that if we use meta-trained data to optimize the meta -learning algorithms, we can achieve similar sample complexity or better regret as empirical risk maximization methods. Then, they show that bi-levels of optimization can be applied to the meta learning algorithms, and that the performance is similar to that of metalearning. Finally, they provide some experimental results on variable-shot settings in sequential learning, including many-shot learning, zero-shot classification, and many-task learning. They find that meta learning outperforms supervised methods in terms of cumulative performance on all sequential learning problems, and they also provide some experiments on learning systems where meta learning is used to train learning systems. "
2265,SP:95cb420d92ec42e12a4bbb0e66224f1c498a7161,"contextual representations USED-FOR NLP tasks. pretrained Transformer models USED-FOR contextual representations. representations USED-FOR sentence - level syntax. self - supervision USED-FOR Transformers networks. probes EVALUATE-FOR Transformer representations. random permutations of n - grams HYPONYM-OF perturbations. syntactic distance FEATURE-OF attention mechanism. local phrase structure FEATURE-OF sensitivity. Generic are they, network, probe, and representation. Task is computational and cognitive neuroscience. OtherScientificTerm are representational invariance, word position, syntactic phrase, global phrase structure, hierarchical phrase structure, and attention weights. Method are Transformer architecture, and Transformers. ","This paper investigates the representation invariance of Transformer-based language models to syntactic perturbations. The authors show that the representations learned by Transformer models are invariant to the syntactic distance between the input sentence and its syntactic context. They also show that this invariance is not only observed in the context of sentence-level syntax, but also in the global phrase-level structure of the sentence. The paper also shows that the invariance to syntactical distance is also observed for the local phrase structure.  ","This paper investigates the representation invariance of Transformer-based language models to syntactic perturbations. The authors show that the representations learned by Transformer models are invariant to the syntactic distance between the input sentence and its syntactic context. They also show that this invariance is not only observed in the context of sentence-level syntax, but also in the global phrase-level structure of the sentence. The paper also shows that the invariance to syntactical distance is also observed for the local phrase structure.  "
2281,SP:cb27b27a6fefc192ad1c2bd083d13eb9e51a5c44,"high - fidelity images USED-FOR Generative Adversarial Networks ( GAN ). large - scale GPU - clusters USED-FOR Generative Adversarial Networks ( GAN ). few - shot image synthesis task USED-FOR GAN. minimum computing cost FEATURE-OF few - shot image synthesis task. 1024 × 1024 resolution EVALUATE-FOR light - weight GAN structure. skip - layer channel - wise excitation module CONJUNCTION self - supervised discriminator. self - supervised discriminator CONJUNCTION skip - layer channel - wise excitation module. feature - encoder USED-FOR self - supervised discriminator. model COMPARE StyleGAN2. StyleGAN2 COMPARE model. datasets EVALUATE-FOR model. datasets EVALUATE-FOR StyleGAN2. image domains FEATURE-OF datasets. OtherScientificTerm are RTX-2080 GPU, and computing budget. ","This paper proposes a new way to train Generative Adversarial Networks (GAN) on large-scale GPU-clusters with high-fidelity images. Specifically, the authors propose a few-shot image synthesis task with minimum computing cost to train a GAN on a single GPU. The light-weight GAN structure is evaluated on a 1024 × 1024 resolution on the Nvidia RTX-2080 GPU, and the authors show that the proposed skip-layer channel-wise excitation module and self-supervised discriminator can be trained with a single feature-encoder. The proposed model outperforms the previous state-of-the-art GAN, StyleGAN2, on two datasets with different image domains and different computing budget. ","This paper proposes a new way to train Generative Adversarial Networks (GAN) on large-scale GPU-clusters with high-fidelity images. Specifically, the authors propose a few-shot image synthesis task with minimum computing cost to train a GAN on a single GPU. The light-weight GAN structure is evaluated on a 1024 × 1024 resolution on the Nvidia RTX-2080 GPU, and the authors show that the proposed skip-layer channel-wise excitation module and self-supervised discriminator can be trained with a single feature-encoder. The proposed model outperforms the previous state-of-the-art GAN, StyleGAN2, on two datasets with different image domains and different computing budget. "
2297,SP:c0dbeb5d94b2388595cf7ad9675c55df0bac7f8e,"neural network bounding USED-FOR neural network verification systems. specialised dual solvers USED-FOR neural network bounds. linear program HYPONYM-OF relaxation. linear relaxation USED-FOR piecewise linear activations. dual algorithm USED-FOR relaxation. tightness CONJUNCTION linear separation oracle. linear separation oracle CONJUNCTION tightness. method USED-FOR relaxation. dual space FEATURE-OF relaxation. tightness HYPONYM-OF relaxation. linear separation oracle HYPONYM-OF relaxation. dual approaches USED-FOR weaker relaxations. massive parallelism CONJUNCTION GPU implementation. GPU implementation CONJUNCTION massive parallelism. dual approaches USED-FOR it. bounds COMPARE off - the - shelf solvers. off - the - shelf solvers COMPARE bounds. speed - accuracy trade - offs EVALUATE-FOR dual solvers. running time EVALUATE-FOR off - the - shelf solvers. Generic is they. Method is customised solver. OtherScientificTerm are dual variables, computational budget, and formal verification speed - ups. ","This paper studies the problem of neural network bounding for neural network verification systems. The authors propose specialised dual solvers for learning neural network bounds that are computationally efficient. They show that they can be obtained by learning a relaxation of a linear program, i.e., a linear relaxation for piecewise linear activations. The relaxation is defined in the dual space, and the authors propose a dual algorithm for learning this relaxation. They also propose a method for learning such a relaxation in the case that the dual variables are non-linear.  The authors show that the proposed method can be applied to any relaxation in dual space (e.g., tightness, linear separation oracle). They also show that it can be combined with existing dual approaches to learn weaker relaxations.   The main contribution of the paper is that the authors introduce a customised solver that can be used in combination with off-the-shelf solvers to achieve faster running time. This is achieved through the use of massive parallelism and a GPU implementation. The paper also shows that the bounds obtained by the proposed bounds outperform the off-sheeter solvers in terms of speed-accuracy trade-offs, and that the computational budget can be significantly reduced.  Finally, the paper shows that formal verification speed-ups can be achieved when the number of clients is small and the dual approaches are used to train the weaker relaxation. ","This paper studies the problem of neural network bounding for neural network verification systems. The authors propose specialised dual solvers for learning neural network bounds that are computationally efficient. They show that they can be obtained by learning a relaxation of a linear program, i.e., a linear relaxation for piecewise linear activations. The relaxation is defined in the dual space, and the authors propose a dual algorithm for learning this relaxation. They also propose a method for learning such a relaxation in the case that the dual variables are non-linear.  The authors show that the proposed method can be applied to any relaxation in dual space (e.g., tightness, linear separation oracle). They also show that it can be combined with existing dual approaches to learn weaker relaxations.   The main contribution of the paper is that the authors introduce a customised solver that can be used in combination with off-the-shelf solvers to achieve faster running time. This is achieved through the use of massive parallelism and a GPU implementation. The paper also shows that the bounds obtained by the proposed bounds outperform the off-sheeter solvers in terms of speed-accuracy trade-offs, and that the computational budget can be significantly reduced.  Finally, the paper shows that formal verification speed-ups can be achieved when the number of clients is small and the dual approaches are used to train the weaker relaxation. "
2313,SP:56e3837417dbcce0d65338dc3aac4e1a20eb0df8,"masked token prediction CONJUNCTION masked span infilling. masked span infilling CONJUNCTION masked token prediction. masked span infilling USED-FOR T5 - style PTLMs. masked token prediction USED-FOR BERT - style PTLMs. everyday concepts FEATURE-OF relational commonsense knowledge. BERT - style PTLMs HYPONYM-OF pre - training objectives. masked token prediction HYPONYM-OF pre - training objectives. masked span infilling HYPONYM-OF pre - training objectives. intermediate self - supervised learning tasks USED-FOR PTLMs. them USED-FOR intermediate self - supervised learning tasks. generative and contrastive objectives USED-FOR common sense. task - specific fine - tuning USED-FOR PTLMs. concept - centric commonsense knowledge USED-FOR PTLMs. pre - training framework USED-FOR generative and contrastive objectives. concept - aware language model ( CALM)1 HYPONYM-OF method. CALM COMPARE PTLMs. PTLMs COMPARE CALM. CALM COMPARE baseline methods. baseline methods COMPARE CALM. CALM USED-FOR PTLM. baseline methods COMPARE PTLMs. PTLMs COMPARE baseline methods. Method are Pre - trained language models ( PTLM ), and text - to - text transformer. Generic is they. OtherScientificTerm are commonsense knowledge, and external knowledge graphs. Task is NLU and NLG tasks. ","Pre-trained language models (PTLM) have been shown to perform well on many tasks, but they are not well suited for tasks that require relational commonsense knowledge (e.g. everyday concepts). This paper proposes two pre-training objectives (masked token prediction and masked span infilling) for BERT-style PTLMs, which are designed to improve the performance of existing PTLM. The authors propose a method called concept-aware language model (CALM)1, which is a text-to-text transformer that learns a concept-centric language model. The key idea is that the generative and contrastive objectives are useful for common sense, and the authors propose to use them for intermediate self-supervised learning tasks to improve performance of PTLNs.  The authors also propose a new task-specific fine-tuning to further improve the generalization performance of the PTLm. The main contribution of the paper is the introduction of a new pretraining framework that uses the pre-trained framework to learn generative (and contrastive) objectives to improve common sense performance. The paper shows that the proposed method, called CALM, outperforms existing baseline methods on NLU and NLG tasks, and is able to learn concepts that are common across different tasks. ","Pre-trained language models (PTLM) have been shown to perform well on many tasks, but they are not well suited for tasks that require relational commonsense knowledge (e.g. everyday concepts). This paper proposes two pre-training objectives (masked token prediction and masked span infilling) for BERT-style PTLMs, which are designed to improve the performance of existing PTLM. The authors propose a method called concept-aware language model (CALM)1, which is a text-to-text transformer that learns a concept-centric language model. The key idea is that the generative and contrastive objectives are useful for common sense, and the authors propose to use them for intermediate self-supervised learning tasks to improve performance of PTLNs.  The authors also propose a new task-specific fine-tuning to further improve the generalization performance of the PTLm. The main contribution of the paper is the introduction of a new pretraining framework that uses the pre-trained framework to learn generative (and contrastive) objectives to improve common sense performance. The paper shows that the proposed method, called CALM, outperforms existing baseline methods on NLU and NLG tasks, and is able to learn concepts that are common across different tasks. "
2329,SP:7ec69bdee021af506293c87a3b75bce1c40a03d7,"frameworks USED-FOR 2D segments. object interactions HYPONYM-OF physics. multi - scale pixel cues CONJUNCTION physical motion cues. physical motion cues CONJUNCTION multi - scale pixel cues. synthetic and real scenes EVALUATE-FOR model. object properties USED-FOR physical events. Task is unsupervised physical object discovery. OtherScientificTerm are 3D geometry, developmental psychology, and observable and partially occluded objects. Material is video. ","This paper tackles the problem of unsupervised physical object discovery, where the goal is to discover objects in a scene that share the same 3D geometry as the scene. The authors propose two new frameworks for learning 2D segments of a scene from video, which are then used to learn 2D segmentation and object interactions (e.g. object interactions between objects). The proposed model is evaluated on both synthetic and real scenes, and the results show that the model is able to learn both multi-scale pixel cues and physical motion cues. It is also shown that the learned object properties are able to capture physical events that are observable and partially occluded in the scene, which is an interesting finding in the context of developmental psychology.","This paper tackles the problem of unsupervised physical object discovery, where the goal is to discover objects in a scene that share the same 3D geometry as the scene. The authors propose two new frameworks for learning 2D segments of a scene from video, which are then used to learn 2D segmentation and object interactions (e.g. object interactions between objects). The proposed model is evaluated on both synthetic and real scenes, and the results show that the model is able to learn both multi-scale pixel cues and physical motion cues. It is also shown that the learned object properties are able to capture physical events that are observable and partially occluded in the scene, which is an interesting finding in the context of developmental psychology."
2345,SP:66997bc19a3ba6548fcf21f114e748bea95cad1c,"Deep neural networks ( DNNs ) USED-FOR adversarial attacks. convolutional neural networks PART-OF Deep neural networks ( DNNs ). attack algorithms USED-FOR Adversarial samples. training method USED-FOR DNN robustness. adversarial noises FEATURE-OF DNN robustness. Increasing Margin Adversarial ( IMA ) Training HYPONYM-OF training method. IMA method USED-FOR margins. decision boundaries USED-FOR DNN model. IMA method USED-FOR training. robustness EVALUATE-FOR IMA method. clean data EVALUATE-FOR accuracy. noisy data EVALUATE-FOR method. accuracy EVALUATE-FOR method. clean data EVALUATE-FOR method. classification accuracy EVALUATE-FOR method. approach USED-FOR robust DNN applications. COVID-19 diagnosis HYPONYM-OF robust DNN applications. CT images USED-FOR COVID-19 diagnosis. Task is life - critical applications. OtherScientificTerm is white noises. Material are COVID-19 CT image dataset, and 100 - PGD white - box adversarial attacks. ","This paper proposes a new training method for improving DNN robustness to white-box adversarial attacks. The authors propose a new method called Increasing Margin Adversarial (IMA) Training, which is based on the observation that adversarial samples generated by existing attack algorithms tend to have large margins in the decision boundaries of convolutional neural networks.  The authors show that the IMA method increases the margins of a DNN model trained on clean data and then fine-tunes it on white noises. The method is shown to improve the accuracy of the method on both clean data as well as classification accuracy on noisy data. The proposed approach is also applied to robust DNN applications such as COVID-19 diagnosis on CT images.    The paper is well-written and well-motivated, and the authors provide a thorough analysis of their approach. The paper also provides a detailed analysis of the robustness of the proposed training method. The experiments are conducted on the COVID19 CT image dataset and show that their method is able to achieve state-of-the-art robustness on the clean data. They also demonstrate that their approach can be applied to a variety of life-critical applications where white noises are present, and demonstrate that the method is effective in the presence of 100-PGD white-Box adversarial attack. ","This paper proposes a new training method for improving DNN robustness to white-box adversarial attacks. The authors propose a new method called Increasing Margin Adversarial (IMA) Training, which is based on the observation that adversarial samples generated by existing attack algorithms tend to have large margins in the decision boundaries of convolutional neural networks.  The authors show that the IMA method increases the margins of a DNN model trained on clean data and then fine-tunes it on white noises. The method is shown to improve the accuracy of the method on both clean data as well as classification accuracy on noisy data. The proposed approach is also applied to robust DNN applications such as COVID-19 diagnosis on CT images.    The paper is well-written and well-motivated, and the authors provide a thorough analysis of their approach. The paper also provides a detailed analysis of the robustness of the proposed training method. The experiments are conducted on the COVID19 CT image dataset and show that their method is able to achieve state-of-the-art robustness on the clean data. They also demonstrate that their approach can be applied to a variety of life-critical applications where white noises are present, and demonstrate that the method is effective in the presence of 100-PGD white-Box adversarial attack. "
2361,SP:276ffd59fbf49e3ee02756da8920218102214917,"Knowledge distillation USED-FOR compact models. teacher model USED-FOR compact student network. poor local optima FEATURE-OF optimization. ProKT HYPONYM-OF model - agnostic method. supervision signals USED-FOR teacher model. student ’s parameter space FEATURE-OF supervision signals. local intermediate targets FEATURE-OF training objective. approximate mirror descent technique USED-FOR local intermediate targets. approximate mirror descent technique USED-FOR projection. quirks USED-FOR optimization. quirks USED-FOR method. ProKT COMPARE knowledge distillation methods. knowledge distillation methods COMPARE ProKT. image and text datasets EVALUATE-FOR ProKT. Method is Deep neural networks. OtherScientificTerm are computation capacity, and local optima. ","Knowledge distillation is a popular technique for training compact models. Deep neural networks are typically trained with a large number of local optima, which limits the computation capacity. In this paper, the authors propose ProKT, a model-agnostic method where a teacher model is used to train a compact student network. The main idea is to use supervision signals from the teacher model in the student’s parameter space to guide the optimization to poor local optimas. To achieve this, the proposed ProKT uses an approximate mirror descent technique to learn local intermediate targets for the training objective. The authors show that ProKT outperforms existing knowledge distillation methods on both image and text datasets. They also show that the proposed method does not rely on any particular set of quirks in the optimization, and that the projection is based on a simple approximation of the local intermediate target. ","Knowledge distillation is a popular technique for training compact models. Deep neural networks are typically trained with a large number of local optima, which limits the computation capacity. In this paper, the authors propose ProKT, a model-agnostic method where a teacher model is used to train a compact student network. The main idea is to use supervision signals from the teacher model in the student’s parameter space to guide the optimization to poor local optimas. To achieve this, the proposed ProKT uses an approximate mirror descent technique to learn local intermediate targets for the training objective. The authors show that ProKT outperforms existing knowledge distillation methods on both image and text datasets. They also show that the proposed method does not rely on any particular set of quirks in the optimization, and that the projection is based on a simple approximation of the local intermediate target. "
2377,SP:906dc21d6988953fcf57d63bbdd12973e5818d16,"channel pruning method USED-FOR compression. compression FEATURE-OF Convolutional Neural Networks ( CNNs ). hyper - structure network USED-FOR architecture of the main network. hypernet COMPARE hyperstructure network. hyperstructure network COMPARE hypernet. regular backpropagation USED-FOR hyperstructure network. regularization term USED-FOR computational resource. regularization term USED-FOR compact network. computational resource FEATURE-OF compact network. FLOPs USED-FOR computational resource. FLOPs USED-FOR regularization. FLOPs USED-FOR it. layer - wise scaling factors USED-FOR gradients. hyper - gradient descent USED-FOR they. ImageNet EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. CIFAR-10 EVALUATE-FOR method. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. Method is channel pruning methods. OtherScientificTerm are layers, and gates. ","This paper proposes a new channel pruning method for compression of Convolutional Neural Networks (CNNs). The authors propose to use a hyper-structure network as a proxy for the architecture of the main network, where the hypernet is trained with regular backpropagation. The authors also propose a regularization term to reduce the computational resource of the compact network using fewer FLOPs. The proposed method is evaluated on CIFAR-10 and ImageNet and compared with state-of-the-art methods. The main contribution of the paper is that the proposed method can be applied to any channel-pruning methods. In particular, it does not require any additional layer-wise scaling factors for the gradients, and it only requires a small number of FLOPS to compress a single layer. The paper also shows that the hyper-gradient descent can be used to prune the layers, and that they can be pruned in a single step.  ","This paper proposes a new channel pruning method for compression of Convolutional Neural Networks (CNNs). The authors propose to use a hyper-structure network as a proxy for the architecture of the main network, where the hypernet is trained with regular backpropagation. The authors also propose a regularization term to reduce the computational resource of the compact network using fewer FLOPs. The proposed method is evaluated on CIFAR-10 and ImageNet and compared with state-of-the-art methods. The main contribution of the paper is that the proposed method can be applied to any channel-pruning methods. In particular, it does not require any additional layer-wise scaling factors for the gradients, and it only requires a small number of FLOPS to compress a single layer. The paper also shows that the hyper-gradient descent can be used to prune the layers, and that they can be pruned in a single step.  "
2393,SP:890fd9454596c051b0e9535baf73b1dd1fae67ca,"exploration mechanism USED-FOR theorem prover. imitation CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION imitation. It USED-FOR prover. imitation USED-FOR prover. reinforcement learning USED-FOR prover. Task are automated higher - order logic theorem proving, and exploration of premises. OtherScientificTerm are human proofs, deep reinforcement learning scenario, and DeepHOL Zero. Method is exploration approach. ","This paper tackles the problem of automated higher-order logic theorem proving. The authors propose a novel exploration mechanism to train a theorem prover. It uses imitation and reinforcement learning to guide the prover through the process of learning. The exploration mechanism is based on the observation that human proofs tend to be more likely to be proved in a deep reinforcement learning scenario, and the authors propose an exploration approach called DeepHOL Zero, which encourages the exploration of premises.  ","This paper tackles the problem of automated higher-order logic theorem proving. The authors propose a novel exploration mechanism to train a theorem prover. It uses imitation and reinforcement learning to guide the prover through the process of learning. The exploration mechanism is based on the observation that human proofs tend to be more likely to be proved in a deep reinforcement learning scenario, and the authors propose an exploration approach called DeepHOL Zero, which encourages the exploration of premises.  "
2409,SP:88209417a8ad07e6103084e41709be900303ce5f,"models USED-FOR machine learning tasks. data augmentation USED-FOR models. augmentation methods USED-FOR data modality. image processing functions CONJUNCTION word - replacing rules. word - replacing rules CONJUNCTION image processing functions. word - replacing rules USED-FOR text data. image processing functions USED-FOR image data. MODALS USED-FOR automated data augmentation. universal data transformation operations USED-FOR transform. MODALS USED-FOR universal data transformation operations. latent space FEATURE-OF universal data transformation operations. Method are Data augmentation, and automated data augmentation approach. Material is artificial data. OtherScientificTerm are modality, and modalities. ","Data augmentation is a popular topic in machine learning, and models trained with data augmentation have been shown to improve the performance of models on many machine learning tasks. However, existing augmentation methods can only be applied to a single data modality, which can be problematic in the presence of artificial data. This paper proposes MODALS, an automated data augmentation approach that augments any modality with a set of universal data transformation operations in the latent space. The idea is to use image processing functions and word-replacing rules to transform image data into text data. MODALS is able to perform universal data transformations across modalities and modalities. ","Data augmentation is a popular topic in machine learning, and models trained with data augmentation have been shown to improve the performance of models on many machine learning tasks. However, existing augmentation methods can only be applied to a single data modality, which can be problematic in the presence of artificial data. This paper proposes MODALS, an automated data augmentation approach that augments any modality with a set of universal data transformation operations in the latent space. The idea is to use image processing functions and word-replacing rules to transform image data into text data. MODALS is able to perform universal data transformations across modalities and modalities. "
2425,SP:6d84670d321b0d584b097c630574bd748e85c9a2,"nonlinear and nontrivial dynamical limit FEATURE-OF learning dynamics. mean field limit HYPONYM-OF nonlinear and nontrivial dynamical limit. neural networks USED-FOR mean field regime. mean field limit USED-FOR large - width neural networks. analysis USED-FOR two - layer networks. mean field regime FEATURE-OF optimization efficiency. global convergence result FEATURE-OF unregularized feedforward three - layer networks. mean field regime FEATURE-OF unregularized feedforward three - layer networks. rigorous framework USED-FOR mean field limit. mean field limit FEATURE-OF three - layer networks. stochastic gradient descent training USED-FOR three - layer networks. stochastic gradient descent training USED-FOR mean field limit. probability space FEATURE-OF neural networks. neural networks PART-OF neuronal embedding. probability space PART-OF neuronal embedding. mean field limit USED-FOR global convergence guarantee. regularity CONJUNCTION convergence mode assumptions. convergence mode assumptions CONJUNCTION regularity. convergence mode assumptions FEATURE-OF global convergence guarantee. regularity USED-FOR global convergence guarantee. universal approximation property FEATURE-OF neural networks. algebraic topology argument USED-FOR universal approximation property. algebraic topology argument USED-FOR neural networks. OtherScientificTerm are global convergence guarantees, multilayer ones, and convexity. ","This paper studies the nonlinear and nontrivial dynamical limit of learning dynamics, i.e., the mean field limit, for large-width neural networks. The authors provide global convergence guarantees for two-layer and three-layer neural networks, and extend this analysis to the case of two-layered networks. They provide a global convergence result for unregularized feedforward 3-layer networks in a mean field regime, and show that this analysis holds for multilayer ones as well. They also provide a rigorous framework to prove the global convergence limit for mean-field limit for stochastic gradient descent training for three-layering networks. Finally, they show that for neural networks with neural networks in the probability space of the neuronal embedding (i.e. neural networks that share the same probability space as the weights of the neural networks), optimization efficiency converges to the mean of the mean in a given time step. The global convergence guarantee is based on the global approximation property of neural networks under certain regularity and convergence mode assumptions, and is shown to be universal for any convexity. The universal approximation property is obtained through an algebraic topology argument that shows that neural networks are invariant to the regularity of the weights and the convergence mode.","This paper studies the nonlinear and nontrivial dynamical limit of learning dynamics, i.e., the mean field limit, for large-width neural networks. The authors provide global convergence guarantees for two-layer and three-layer neural networks, and extend this analysis to the case of two-layered networks. They provide a global convergence result for unregularized feedforward 3-layer networks in a mean field regime, and show that this analysis holds for multilayer ones as well. They also provide a rigorous framework to prove the global convergence limit for mean-field limit for stochastic gradient descent training for three-layering networks. Finally, they show that for neural networks with neural networks in the probability space of the neuronal embedding (i.e. neural networks that share the same probability space as the weights of the neural networks), optimization efficiency converges to the mean of the mean in a given time step. The global convergence guarantee is based on the global approximation property of neural networks under certain regularity and convergence mode assumptions, and is shown to be universal for any convexity. The universal approximation property is obtained through an algebraic topology argument that shows that neural networks are invariant to the regularity of the weights and the convergence mode."
2441,SP:b90f893f927db9c439595fd119a565cf43c971f4,"interpretable parameterizations USED-FOR real - world decision - making. interpretable parameterizations USED-FOR introspecting and auditing policies. counterfactual reasoning USED-FOR batch inverse reinforcement learning. counterfactual reasoning USED-FOR costbenefit tradeoffs. reward functions USED-FOR expert behavior. counterfactuals USED-FOR policy evaluation. batch setting FEATURE-OF policy evaluation. real and simulated medical environments EVALUATE-FOR batch, counterfactual inverse reinforcement learning approach. OtherScientificTerm are unknown reward function, reward function, expert ’s actions, and expert policies. Task are learning explanations of expert decisions, and active experimentation. Material is healthcare. ","This paper presents a method for learning counterfactual explanations of expert behavior in the presence of an unknown reward function. The authors propose to use interpretable parameterizations for real-world decision-making, which can be used for introspecting and auditing policies. In particular, the authors propose a method that uses counterfactually reasoning to learn costbenefit tradeoffs between the expert’s actions and the reward function, and then uses the counterfactuality of the expert behavior to guide the policy evaluation in the batch setting. They show that counterfactulational reasoning can be applied to the case where the reward functions for expert behavior are not known. They also show that learning explanations can be useful for policy evaluation when the expert policies are not available.  The authors evaluate the effectiveness of their method on a number of real and simulated medical environments, and show that their method outperforms the state-of-the-art in terms of policy evaluation performance and active experimentation in healthcare. ","This paper presents a method for learning counterfactual explanations of expert behavior in the presence of an unknown reward function. The authors propose to use interpretable parameterizations for real-world decision-making, which can be used for introspecting and auditing policies. In particular, the authors propose a method that uses counterfactually reasoning to learn costbenefit tradeoffs between the expert’s actions and the reward function, and then uses the counterfactuality of the expert behavior to guide the policy evaluation in the batch setting. They show that counterfactulational reasoning can be applied to the case where the reward functions for expert behavior are not known. They also show that learning explanations can be useful for policy evaluation when the expert policies are not available.  The authors evaluate the effectiveness of their method on a number of real and simulated medical environments, and show that their method outperforms the state-of-the-art in terms of policy evaluation performance and active experimentation in healthcare. "
2457,SP:c92916780418bfa7f0796fd9766b6d28b9eea5ef,"generalisation CONJUNCTION data efficiency. data efficiency CONJUNCTION generalisation. data efficiency CONJUNCTION robustness. robustness CONJUNCTION data efficiency. Multitask Reinforcement Learning USED-FOR models. generalisation EVALUATE-FOR models. they USED-FOR graphs. physical morphology USED-FOR graph. edges USED-FOR nodes. morphological information PART-OF graph. AMORPHEUS HYPONYM-OF transformer - based approach. graph structure USED-FOR GNNs. it COMPARE GNN - based methods. GNN - based methods COMPARE it. morphological information USED-FOR message - passing scheme. GNN - based methods USED-FOR message - passing scheme. AMORPHEUS COMPARE it. it COMPARE AMORPHEUS. AMORPHEUS USED-FOR morphological information. morphological information USED-FOR GNN - based methods. OtherScientificTerm are state and action space dimensions, and limb features. Method is Graph Neural Networks ( GNN ). Generic are They, and methods. Task are graph - based continuous control, and message passing. ","This paper proposes a new graph neural network architecture, called AMORPHEUS, for graph-based continuous control tasks. The proposed architecture is based on graph neural networks (GNNs), which are a generalization of Graph Neural Networks (GANs). The authors show that GNNs are able to learn a graph structure that allows them to generalize across different state and action space dimensions. They also show that they can be used to learn graphs that are more robust to changes in physical morphology (e.g. limb features) and generalize better than existing models based on Multitask Reinforcement Learning (MRL) in terms of generalisation, data efficiency, and robustness. The authors also propose a transformer-based approach, AMOR PHEUS (AMOR-EUS), to incorporate morphological information into the graph. The idea is to use edges in the nodes to encode information about the physical morphology of the graph, which is then used to guide the message passing. The paper shows that it outperforms existing GNN-based methods that do not use morphology information in the message-passing scheme, and that AMOR-US is more robust than existing methods. ","This paper proposes a new graph neural network architecture, called AMORPHEUS, for graph-based continuous control tasks. The proposed architecture is based on graph neural networks (GNNs), which are a generalization of Graph Neural Networks (GANs). The authors show that GNNs are able to learn a graph structure that allows them to generalize across different state and action space dimensions. They also show that they can be used to learn graphs that are more robust to changes in physical morphology (e.g. limb features) and generalize better than existing models based on Multitask Reinforcement Learning (MRL) in terms of generalisation, data efficiency, and robustness. The authors also propose a transformer-based approach, AMOR PHEUS (AMOR-EUS), to incorporate morphological information into the graph. The idea is to use edges in the nodes to encode information about the physical morphology of the graph, which is then used to guide the message passing. The paper shows that it outperforms existing GNN-based methods that do not use morphology information in the message-passing scheme, and that AMOR-US is more robust than existing methods. "
2473,SP:2cf58f5cac20dccdc2034ef60e8e46b7988ebd7d,"modulated convolutions USED-FOR alternative. forward - pass USED-FOR inference. forward - pass USED-FOR MoVie. MoVie USED-FOR counting. module USED-FOR number ’ related questions. number ’ related questions PART-OF generic VQA models. module PART-OF generic VQA models. COCO HYPONYM-OF common object counting. module USED-FOR VQA challenge. counting - specific VQA tasks EVALUATE-FOR MoVie. common object counting HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR prior - art. COCO HYPONYM-OF benchmarks. modulated convolutions USED-FOR reasoning tasks. counting HYPONYM-OF reasoning tasks. MoVie HYPONYM-OF modulated convolutions. Task is visual counting. Material is natural image. Method are explicit, symbolic models, residual bottleneck, and Modulated conVolutional bottlenecks. ","This paper addresses the problem of visual counting, where the goal is to count objects in a natural image. The authors propose an alternative based on modulated convolutions, called MoVie, which uses a forward-pass to speed up inference. This module is applied to generic VQA models and can be applied to number’related questions in a number of existing generic and explicit, symbolic models.   The authors show that the proposed module can be used to solve the VQa challenge in a single forward pass, without the need for a residual bottleneck. They also demonstrate that the modulated conVolutional bottlenecks can be alleviated. MoVIE is evaluated on three benchmarks (common object counting, COCO, and counting) and shows superior performance compared to prior-art on all three benchmarks.  The paper also shows that the module can also be used for the counting-specific VQE tasks.  Overall, the paper is well-written and well-motivated, and the authors have done a good job of providing a comprehensive analysis of modulated CNNs.  I have a few questions:    1. Is it possible to use the proposed Modulated ConVolution (MoVie) module for reasoning tasks (e.g. counting)?   2. Is there a way to remove the residual bottleneck?   3. Can the module be used as an additional module to solve a specific type of question? ","This paper addresses the problem of visual counting, where the goal is to count objects in a natural image. The authors propose an alternative based on modulated convolutions, called MoVie, which uses a forward-pass to speed up inference. This module is applied to generic VQA models and can be applied to number’related questions in a number of existing generic and explicit, symbolic models.   The authors show that the proposed module can be used to solve the VQa challenge in a single forward pass, without the need for a residual bottleneck. They also demonstrate that the modulated conVolutional bottlenecks can be alleviated. MoVIE is evaluated on three benchmarks (common object counting, COCO, and counting) and shows superior performance compared to prior-art on all three benchmarks.  The paper also shows that the module can also be used for the counting-specific VQE tasks.  Overall, the paper is well-written and well-motivated, and the authors have done a good job of providing a comprehensive analysis of modulated CNNs.  I have a few questions:    1. Is it possible to use the proposed Modulated ConVolution (MoVie) module for reasoning tasks (e.g. counting)?   2. Is there a way to remove the residual bottleneck?   3. Can the module be used as an additional module to solve a specific type of question? "
2489,SP:c64e77507e562f236cb69361b22fb1a7951ffb22,poisoning attack USED-FOR model. online convex optimization USED-FOR poisoning attack. model - targeted poisoning attacks COMPARE attack. attack COMPARE model - targeted poisoning attacks. provable convergence FEATURE-OF target classifier. provable convergence FEATURE-OF attack. attack HYPONYM-OF model - targeted poisoning attack. it COMPARE attacks. attacks COMPARE it. attack success rate EVALUATE-FOR it. attack success rate EVALUATE-FOR attacks. attack USED-FOR online attack. Method is classifier. ,"This paper proposes a new poisoning attack on a model. The poisoning attack is based on online convex optimization. Unlike previous model-targeted poisoning attacks, this attack has provable convergence to the target classifier. The authors show that this attack, called the attack, can be seen as an extension of the existing attack on the classifier, and that it has a better attack success rate compared to previous attacks. They also show that the proposed attack can be used as an online attack. ","This paper proposes a new poisoning attack on a model. The poisoning attack is based on online convex optimization. Unlike previous model-targeted poisoning attacks, this attack has provable convergence to the target classifier. The authors show that this attack, called the attack, can be seen as an extension of the existing attack on the classifier, and that it has a better attack success rate compared to previous attacks. They also show that the proposed attack can be used as an online attack. "
2505,SP:a526023ec4cb839b83c574d31f59a9a67bc7af00,"BiPointNet HYPONYM-OF model binarization approach. model binarization approach USED-FOR deep learning on point clouds. resource constraint USED-FOR real - time point cloud applications. edge devices USED-FOR real - time point cloud applications. binarized models USED-FOR point clouds. scale distortion USED-FOR optimization. scale distortion HYPONYM-OF challenges. aggregation - induced feature homogenization HYPONYM-OF challenges. Entropy - Maximizing Aggregation ( EMA ) USED-FOR distribution. Layer - wise Scale Recovery ( LSR ) USED-FOR feature representation capacity. Entropy - Maximizing Aggregation ( EMA ) USED-FOR BiPointNet. Layer - wise Scale Recovery ( LSR ) PART-OF BiPointNet. BiPointNet COMPARE full precision counterpart. full precision counterpart COMPARE BiPointNet. binarization methods COMPARE full precision counterpart. full precision counterpart COMPARE binarization methods. BiPointNet COMPARE binarization methods. binarization methods COMPARE BiPointNet. tasks EVALUATE-FOR techniques. speedup CONJUNCTION storage saving. storage saving CONJUNCTION speedup. storage saving EVALUATE-FOR BiPointNet. real - world resource - constrained devices EVALUATE-FOR BiPointNet. speedup EVALUATE-FOR BiPointNet. Metric are information entropy, and maximum information entropy. OtherScientificTerm is scale - sensitive structures. ","This paper proposes a new model binarization approach, BiPointNet, for deep learning on point clouds under resource constraint on edge devices. The authors propose to use binarized models for point clouds to address two challenges: (1) scale distortion in optimization, and (2) aggregation-induced feature homogenization. To achieve this, the authors use Entropy-Maximizing Aggregation (EMA) to learn a distribution that maximizes the maximum information entropy. Layer-wise Scale Recovery (LSR) is also incorporated to improve the feature representation capacity. Experiments are conducted on three different tasks, and the authors show that the proposed techniques achieve speedup and storage saving on real-world resource-constrained devices. In addition, the paper shows that the performance of biPointNet is comparable to other state-of-the-art methods and outperforms its full precision counterpart. ","This paper proposes a new model binarization approach, BiPointNet, for deep learning on point clouds under resource constraint on edge devices. The authors propose to use binarized models for point clouds to address two challenges: (1) scale distortion in optimization, and (2) aggregation-induced feature homogenization. To achieve this, the authors use Entropy-Maximizing Aggregation (EMA) to learn a distribution that maximizes the maximum information entropy. Layer-wise Scale Recovery (LSR) is also incorporated to improve the feature representation capacity. Experiments are conducted on three different tasks, and the authors show that the proposed techniques achieve speedup and storage saving on real-world resource-constrained devices. In addition, the paper shows that the performance of biPointNet is comparable to other state-of-the-art methods and outperforms its full precision counterpart. "
2521,SP:825b4d1db0c537a607655bb5b4bf221ec672c8af,"natural language processing tasks EVALUATE-FOR Transformer - based models. self - attention architecture USED-FOR transformer. transformer USED-FOR context - aware representations. trainable memory USED-FOR Transformer model. Memory - augmented neural networks ( MANNs ) USED-FOR representations. general - purpose memory USED-FOR representations. neural architectures USED-FOR representations. neural architectures USED-FOR Memory - augmented neural networks ( MANNs ). general - purpose memory USED-FOR neural architectures. MANNs USED-FOR algorithms. MANNs USED-FOR tasks. question answering CONJUNCTION language modeling. language modeling CONJUNCTION question answering. backpropagation USED-FOR tasks. RNNs CONJUNCTION LSTMs. LSTMs CONJUNCTION RNNs. complexity EVALUATE-FOR RNNs. language modeling HYPONYM-OF tasks. backpropagation USED-FOR MANNs. question answering HYPONYM-OF tasks. complexity EVALUATE-FOR LSTMs. Copy HYPONYM-OF algorithms. memory tokens USED-FOR non - local representations. memory bottleneck USED-FOR global information. dedicated layer USED-FOR memory update. machine translation and language modelling tasks EVALUATE-FOR model. GLUE benchmark EVALUATE-FOR tasks. memory tokens USED-FOR masked language model. it USED-FOR global context. it USED-FOR model. model USED-FOR global context. Method are element - wise representations, Transformer baseline, and memory augmented Transformers. OtherScientificTerm is memory. ","This paper proposes a new architecture for Transformer-based models for natural language processing tasks. The key idea is to use the self-attention architecture in a transformer to learn context-aware representations. The Transformer model is trained with a trainable memory, and the authors propose Memory-augmented neural networks (MANNs), which use neural architectures with a general-purpose memory to learn representations that can be used across different neural architectures. The authors show that MANNs outperform existing algorithms using backpropagation on three tasks: question answering, language modeling, and machine translation. They also show that the complexity of RNNs and LSTMs trained with MANNs is comparable to that of the Transformer baseline.    The paper also shows that the memory of a masked language model can be augmented with memory tokens that encode non-local representations, and that it is able to capture global context. The memory bottleneck is used to store global information, and a dedicated layer is used for each memory update. The model is tested on machine translation and language modelling tasks, and on the GLUE benchmark, where it is shown that the model can capture the global context of the input language. The paper shows that memory augmented Transformers are more robust to changes in the amount of information in the memory, as well as to the number of memory tokens. ","This paper proposes a new architecture for Transformer-based models for natural language processing tasks. The key idea is to use the self-attention architecture in a transformer to learn context-aware representations. The Transformer model is trained with a trainable memory, and the authors propose Memory-augmented neural networks (MANNs), which use neural architectures with a general-purpose memory to learn representations that can be used across different neural architectures. The authors show that MANNs outperform existing algorithms using backpropagation on three tasks: question answering, language modeling, and machine translation. They also show that the complexity of RNNs and LSTMs trained with MANNs is comparable to that of the Transformer baseline.    The paper also shows that the memory of a masked language model can be augmented with memory tokens that encode non-local representations, and that it is able to capture global context. The memory bottleneck is used to store global information, and a dedicated layer is used for each memory update. The model is tested on machine translation and language modelling tasks, and on the GLUE benchmark, where it is shown that the model can capture the global context of the input language. The paper shows that memory augmented Transformers are more robust to changes in the amount of information in the memory, as well as to the number of memory tokens. "
2537,SP:f0fa1b7684bc605f6edd4813c44be20988fe8b4c,Prototypical Contrastive Learning ( PCL ) HYPONYM-OF unsupervised representation learning method. contrastive learning CONJUNCTION clustering. clustering CONJUNCTION contrastive learning. it USED-FOR semantic structures. PCL USED-FOR instance discrimination. PCL USED-FOR low - level features. low - level features USED-FOR instance discrimination. PCL USED-FOR semantic structures. embedding space FEATURE-OF semantic structures. clustering USED-FOR semantic structures. prototypes USED-FOR maximum - likelihood estimation. maximum - likelihood estimation USED-FOR network parameters. prototypes USED-FOR latent variables. Expectation - Maximization framework USED-FOR maximum - likelihood estimation. E - step CONJUNCTION M - step. M - step CONJUNCTION E - step. distribution of prototypes USED-FOR E - step. clustering USED-FOR distribution of prototypes. contrastive learning USED-FOR network. InfoNCE loss USED-FOR contrastive learning. ProtoNCE loss HYPONYM-OF InfoNCE loss. PCL COMPARE instance - wise contrastive learning methods. instance - wise contrastive learning methods COMPARE PCL. low - resource transfer learning EVALUATE-FOR PCL. low - resource transfer learning EVALUATE-FOR instance - wise contrastive learning methods. Task is maximum - likelihood estimation of the network parameters. ,"This paper proposes Prototype Contrastive Learning (PCL), an unsupervised representation learning method. PCL uses contrastive learning and clustering to learn semantic structures in the embedding space, and then uses these semantic structures to improve instance discrimination using low-level features. The maximum-likelihood estimation of the network parameters is performed using prototypes for each of the latent variables, using the Expectation-Maximization framework. The E-step is based on the distribution of prototypes, and the M-step uses the contrastive loss from the InfoNCE loss. Experiments show that PCL outperforms other instance-wise contrastive methods in low-resource transfer learning. ","This paper proposes Prototype Contrastive Learning (PCL), an unsupervised representation learning method. PCL uses contrastive learning and clustering to learn semantic structures in the embedding space, and then uses these semantic structures to improve instance discrimination using low-level features. The maximum-likelihood estimation of the network parameters is performed using prototypes for each of the latent variables, using the Expectation-Maximization framework. The E-step is based on the distribution of prototypes, and the M-step uses the contrastive loss from the InfoNCE loss. Experiments show that PCL outperforms other instance-wise contrastive methods in low-resource transfer learning. "
2553,SP:5342a5e1d87fd17b1a2efed967dbbfeafa440ee7,clean images USED-FOR deep neural networks. invisible perturbations FEATURE-OF clean images. block USED-FOR robust features. block USED-FOR adversarial attacks. Orthogonal Multi - Path ( OMP ) block PART-OF neural network. forward learning CONJUNCTION backward correction. backward correction CONJUNCTION forward learning. OMP block USED-FOR neural networks. forward learning USED-FOR OMP block. backward correction USED-FOR OMP block. neural networks USED-FOR features. OMP block USED-FOR features. robustness EVALUATE-FOR neural networks. variety CONJUNCTION accuracy. accuracy CONJUNCTION variety. accuracy EVALUATE-FOR vanilla neural networks. l∞ bound FEATURE-OF white - box PGD attack. accuracy EVALUATE-FOR VGG16. CIFAR10 EVALUATE-FOR vanilla neural networks. OMP block USED-FOR VGG16. OMP block USED-FOR neural networks. accuracy EVALUATE-FOR neural networks. neural networks USED-FOR black - box attacks. white - box and black - box attacks COMPARE adversarial defenders. adversarial defenders COMPARE white - box and black - box attacks. Generic is paths. OtherScientificTerm is orthogonality constraint. ,"This paper proposes a new orthogonal multi-path (OMP) block in a neural network, which is a block of paths that is orthogonality-invariant. The authors show that this block can be used to learn robust features that are invariant to invisible perturbations to clean images. They also show that the OMP block can improve the robustness of neural networks to adversarial attacks.    The authors propose to use the Orthogonal Multi-Path (OPM) block as a regularizer in the forward learning and backward correction of the neural network. They show that by using this block, the neural networks are able to learn features that can be robust to white-box and black-box PGD attacks. The paper also shows that the orthogons of the paths can be learned in a similar way as in orthogonic constraint.  The paper shows that by applying this block to vanilla neural networks, they can increase the variety and accuracy of VGG16 on CIFAR10, and show that neural networks trained with this OMP blocks can achieve better accuracy and robustness to white box and black box attacks. In addition, they show that for white box PGD attack, there is an l∞ bound on the number of times that the neural net can be attacked.  Finally, the paper shows how the proposed neural networks can be trained with the proposed block to improve the accuracy of white box attacks, and demonstrate that their neural networks have better robustness against white- box attacks than adversarial defenders. ","This paper proposes a new orthogonal multi-path (OMP) block in a neural network, which is a block of paths that is orthogonality-invariant. The authors show that this block can be used to learn robust features that are invariant to invisible perturbations to clean images. They also show that the OMP block can improve the robustness of neural networks to adversarial attacks.    The authors propose to use the Orthogonal Multi-Path (OPM) block as a regularizer in the forward learning and backward correction of the neural network. They show that by using this block, the neural networks are able to learn features that can be robust to white-box and black-box PGD attacks. The paper also shows that the orthogons of the paths can be learned in a similar way as in orthogonic constraint.  The paper shows that by applying this block to vanilla neural networks, they can increase the variety and accuracy of VGG16 on CIFAR10, and show that neural networks trained with this OMP blocks can achieve better accuracy and robustness to white box and black box attacks. In addition, they show that for white box PGD attack, there is an l∞ bound on the number of times that the neural net can be attacked.  Finally, the paper shows how the proposed neural networks can be trained with the proposed block to improve the accuracy of white box attacks, and demonstrate that their neural networks have better robustness against white- box attacks than adversarial defenders. "
2569,SP:776df66274ed12449fde8dcef873a593980f397c,Attention mechanism USED-FOR graph neural networks. graph attention model USED-FOR noisy graphs. self - supervised task USED-FOR edges. attention forms USED-FOR edges. attention forms USED-FOR self - supervised task. expressive attention USED-FOR mislinked neighbors. SuperGAT USED-FOR expressive attention. edges USED-FOR SuperGAT. homophily CONJUNCTION average degree. average degree CONJUNCTION homophily. attention forms CONJUNCTION self - supervision. self - supervision CONJUNCTION attention forms. graph characteristics USED-FOR attention forms. self - supervision HYPONYM-OF graph characteristics. average degree HYPONYM-OF self - supervision. homophily HYPONYM-OF self - supervision. average degree HYPONYM-OF graph characteristics. homophily HYPONYM-OF graph characteristics. recipe USED-FOR attention design. graph characteristics USED-FOR attention design. models COMPARE baselines. baselines COMPARE models. real - world datasets EVALUATE-FOR recipe. recipe USED-FOR models. Method is graph attention. Material is graphs. ,"This paper proposes a new graph attention mechanism for graph neural networks. The authors propose a graph attention model for noisy graphs. The key idea is to use a self-supervised task to learn attention forms for edges in the graph, and then apply expressive attention to the mislinked neighbors. SuperGAT is used to learn an expressive attention for the edges. The proposed graph attention is based on graph characteristics such as homophily, average degree, and other graph characteristics. The paper also proposes a recipe for improving the attention design based on these graph characteristics, and shows that the proposed models outperform the baselines on several real-world datasets. ","This paper proposes a new graph attention mechanism for graph neural networks. The authors propose a graph attention model for noisy graphs. The key idea is to use a self-supervised task to learn attention forms for edges in the graph, and then apply expressive attention to the mislinked neighbors. SuperGAT is used to learn an expressive attention for the edges. The proposed graph attention is based on graph characteristics such as homophily, average degree, and other graph characteristics. The paper also proposes a recipe for improving the attention design based on these graph characteristics, and shows that the proposed models outperform the baselines on several real-world datasets. "
2585,SP:80a05296d6b1e4c6e9e2df01938c73029ff8487d,Dialogue system USED-FOR medical automatic diagnosis ( DSMAD ). Dialogue system USED-FOR agent. reinforcement learning methods USED-FOR it. Markov decisionmaking process USED-FOR DSMAD. medical rationality FEATURE-OF inquiring process. diagnostic accuracy EVALUATE-FOR DSMAD agents. agent USED-FOR diagnosis. agent USED-FOR diagnosing processes. agent USED-FOR medical application. inquiry module CONJUNCTION introspective module. introspective module CONJUNCTION inquiry module. inquiry module USED-FOR symptom - inquiries. cooperative modules PART-OF DSMAD agent. introspective module PART-OF DSMAD agent. introspective module HYPONYM-OF cooperative modules. inquiry module HYPONYM-OF cooperative modules. inquiry module CONJUNCTION introspective module. introspective module CONJUNCTION inquiry module. evaluation metrics EVALUATE-FOR DSMAD methods. reliability EVALUATE-FOR DSMAD methods. evaluation metrics EVALUATE-FOR reliability. INS - DS COMPARE methods. methods COMPARE INS - DS. robustness EVALUATE-FOR methods. reliability EVALUATE-FOR methods. reliability CONJUNCTION robustness. robustness CONJUNCTION reliability. robustness EVALUATE-FOR INS - DS. reliability EVALUATE-FOR INS - DS. OtherScientificTerm is inquiring symptoms. Generic is interventions. ,"This paper proposes a dialogue system for medical automatic diagnosis (DSMAD) based on the Dialogue system. The agent is trained using reinforcement learning methods. DSMAD is a Markov decisionmaking process where the agent is given a set of symptoms and is asked to decide whether to investigate the symptoms or not. The authors show that DSMAD agents can achieve better diagnostic accuracy and robustness to interventions. They also show that the agent can be used for any medical application.   The DSMAD agent is composed of two cooperative modules: an inquiry module for symptom-inquiries, and an introspective module that considers the medical rationality of the inquiring process. In the inquiry module, the agent decides whether or not to investigate whether a patient should be treated or not, and the agent uses this decision to guide the diagnosis of the patient.  The agent can also be used to guide other diagnosing processes. The paper also shows that the evaluation metrics of DSMAD methods are more robust to interventions than existing evaluation metrics, and that INS-DS outperforms other methods in terms of reliability, robustness, and accuracy. ","This paper proposes a dialogue system for medical automatic diagnosis (DSMAD) based on the Dialogue system. The agent is trained using reinforcement learning methods. DSMAD is a Markov decisionmaking process where the agent is given a set of symptoms and is asked to decide whether to investigate the symptoms or not. The authors show that DSMAD agents can achieve better diagnostic accuracy and robustness to interventions. They also show that the agent can be used for any medical application.   The DSMAD agent is composed of two cooperative modules: an inquiry module for symptom-inquiries, and an introspective module that considers the medical rationality of the inquiring process. In the inquiry module, the agent decides whether or not to investigate whether a patient should be treated or not, and the agent uses this decision to guide the diagnosis of the patient.  The agent can also be used to guide other diagnosing processes. The paper also shows that the evaluation metrics of DSMAD methods are more robust to interventions than existing evaluation metrics, and that INS-DS outperforms other methods in terms of reliability, robustness, and accuracy. "
2601,SP:10ae09d90d465125433a9b4f15b1405ab017920d,"adaptive batch - wise regularization USED-FOR natural world distribution. Batch Confusion Norm ( BCN ) USED-FOR natural world distribution. fine - grained and long - tailed properties PART-OF natural world distribution. Batch Confusion Norm ( BCN ) USED-FOR adaptive batch - wise regularization. inter - class similarity CONJUNCTION intra - class variations. intra - class variations CONJUNCTION inter - class similarity. task USED-FOR FGVC classifier. attention mechanism USED-FOR discriminative parts. long - tailed distribution of visual classification USED-FOR class imbalance problem. class - balancing strategies CONJUNCTION classifier normalization. classifier normalization CONJUNCTION class - balancing strategies. classifier normalization CONJUNCTION negative gradient of tailed categories. negative gradient of tailed categories CONJUNCTION classifier normalization. adaptive confusion concept USED-FOR problems. BCN term USED-FOR overfitting. network learning USED-FOR cross - entropy loss. class predictions USED-FOR BCN loss. confusion energy - based framework USED-FOR long - tailed scenario. BCN USED-FOR distribution of confusion strength. BCN USED-FOR confusion energy - based framework. extra attention mechanism USED-FOR FGVC model. BCN technique USED-FOR FGVC model. iNaturalist2018 HYPONYM-OF natural world distribution dataset. iNaturalist2018 EVALUATE-FOR approach. natural world distribution dataset EVALUATE-FOR approach. Generic is approaches. OtherScientificTerm are image features of fine details, and tailed and head categories. Material is FGVC datasets. ","This paper proposes an adaptive batch-wise regularization based on Batch Confusion Norm (BCN) to learn a natural world distribution with fine-grained and long-tailed properties. The authors argue that existing approaches do not account for the fact that the FGVC classifier trained on the same task may not be well-suited for the task at hand. They argue that the long-tail distribution of visual classification is responsible for the class imbalance problem, and that the attention mechanism is not well suited for the discriminative parts of the image features of fine details.   The authors propose to use BCN to learn the distribution of confusion strength between the tailed and head categories, which is based on inter-class similarity and intra-class variations. They also propose class-balancing strategies, classifier normalization, and negative gradient of tailed categories. The BCN term is designed to prevent overfitting to the long tailed category, and the cross-entropy loss is optimized using network learning.  The paper also proposes a confusion energy-based framework to tackle the problem of long-tailed scenario, where BCN is used as a regularizer for the distribution.  Experiments on FGVC datasets show that the BCN technique improves the performance of FGVC model trained with an extra attention mechanism. The approach is tested on iNaturalist2018, a popular natural world dataset, and is shown to be effective on a variety of tasks.  In addition, the authors also propose an adaptive confusion concept, which can be applied to other problems, and show that BCN and BCN can be combined to improve performance. ","This paper proposes an adaptive batch-wise regularization based on Batch Confusion Norm (BCN) to learn a natural world distribution with fine-grained and long-tailed properties. The authors argue that existing approaches do not account for the fact that the FGVC classifier trained on the same task may not be well-suited for the task at hand. They argue that the long-tail distribution of visual classification is responsible for the class imbalance problem, and that the attention mechanism is not well suited for the discriminative parts of the image features of fine details.   The authors propose to use BCN to learn the distribution of confusion strength between the tailed and head categories, which is based on inter-class similarity and intra-class variations. They also propose class-balancing strategies, classifier normalization, and negative gradient of tailed categories. The BCN term is designed to prevent overfitting to the long tailed category, and the cross-entropy loss is optimized using network learning.  The paper also proposes a confusion energy-based framework to tackle the problem of long-tailed scenario, where BCN is used as a regularizer for the distribution.  Experiments on FGVC datasets show that the BCN technique improves the performance of FGVC model trained with an extra attention mechanism. The approach is tested on iNaturalist2018, a popular natural world dataset, and is shown to be effective on a variety of tasks.  In addition, the authors also propose an adaptive confusion concept, which can be applied to other problems, and show that BCN and BCN can be combined to improve performance. "
2617,SP:90f1e0fe1e9678d1e9a4dcb519d4e8fd61098ce0,"Bayesian inference USED-FOR ill - posed nature. ill - posed nature FEATURE-OF inverse reinforcement learning problem. Bayesian inference USED-FOR inverse reinforcement learning problem. reward USED-FOR Bayesian inference. methods COMPARE small tabular setting. small tabular setting COMPARE methods. variational approach USED-FOR latent reward. reward FEATURE-OF approximate posterior distribution. variational approach USED-FOR policy. method USED-FOR Bayesian reward inference. real medical data CONJUNCTION control simulations. control simulations CONJUNCTION real medical data. real medical data USED-FOR method. methods USED-FOR Bayesian reward inference. Method are inner - loop MDP solver, non - Bayesian methods, Approximate Variational Reward Imitation Learning ( AVRIL ), and offline imitation learning algorithms. Task is healthcare. ","This paper considers the problem of Bayesian inference for the inverse reinforcement learning problem of ill-posed nature, where the reward of an inner-loop MDP solver is unknown. The authors propose a new method called Approximate Variational Reward Imitation Learning (AVIL), which is a generalization of existing methods in the small tabular setting. The key idea is to use a variational approach to learn the latent reward of the policy, which is then used to approximate the approximate posterior distribution of the reward. The proposed method is shown to outperform existing methods for Bayesian reward inference on real medical data and control simulations. The paper also shows that non-Bayesian methods can be used to improve the performance of the proposed method.    The paper is well-written and well-motivated. The idea of the paper is interesting and the motivation of the work is clear. However, there are some issues in the paper, which I will discuss below.  1. The main contribution of this paper is that the paper proposes a novel method for learning the posterior of a policy in the absence of offline imitation learning algorithms. This is an important problem in healthcare.  2. It is not clear to me that this is a novel idea. ","This paper considers the problem of Bayesian inference for the inverse reinforcement learning problem of ill-posed nature, where the reward of an inner-loop MDP solver is unknown. The authors propose a new method called Approximate Variational Reward Imitation Learning (AVIL), which is a generalization of existing methods in the small tabular setting. The key idea is to use a variational approach to learn the latent reward of the policy, which is then used to approximate the approximate posterior distribution of the reward. The proposed method is shown to outperform existing methods for Bayesian reward inference on real medical data and control simulations. The paper also shows that non-Bayesian methods can be used to improve the performance of the proposed method.    The paper is well-written and well-motivated. The idea of the paper is interesting and the motivation of the work is clear. However, there are some issues in the paper, which I will discuss below.  1. The main contribution of this paper is that the paper proposes a novel method for learning the posterior of a policy in the absence of offline imitation learning algorithms. This is an important problem in healthcare.  2. It is not clear to me that this is a novel idea. "
2633,SP:ccd251d95c0a2d8dc5ad2a148ec29955e105e71e,Search USED-FOR policies. singleand multiagent environments FEATURE-OF policies. prior search approaches USED-FOR partially observable environments. computational cost EVALUATE-FOR hidden information. search procedure USED-FOR partially observable environments. Learned Belief Search ( LBS ) HYPONYM-OF search procedure. supervised task USED-FOR approximate auto - regressive counterfactual belief. supervised task USED-FOR LBS. approximate auto - regressive counterfactual belief USED-FOR LBS. public - private model architecture USED-FOR policies. LBS USED-FOR policies. LBS USED-FOR multi - agent settings. rollouts FEATURE-OF policies. public - private model architecture USED-FOR LBS. Hanabi EVALUATE-FOR LBS. exact search USED-FOR LBS. compute requirements EVALUATE-FOR LBS. OtherScientificTerm is exact belief distribution. Generic is it. Method is search methods. ,"This paper proposes a new search procedure called Learned Belief Search (LBS) to learn policies in both single and multiagent environments. Prior search approaches are limited to partially observable environments due to the computational cost of computing the hidden information. LBS uses a supervised task to learn an approximate auto-regressive counterfactual belief, and then uses a public-private model architecture to train policies with rollouts. The paper shows that LBS outperforms prior work in Hanabi and Hanabi-like environments, and LBS can be applied to multi-agent settings as well. The main contribution of the paper is that the exact belief distribution is learned in LBS, and that it can be used to improve the performance of existing search methods. The empirical results show that Lbs outperforms the state-of-the-art on Hanabi in terms of exact search and compute requirements.","This paper proposes a new search procedure called Learned Belief Search (LBS) to learn policies in both single and multiagent environments. Prior search approaches are limited to partially observable environments due to the computational cost of computing the hidden information. LBS uses a supervised task to learn an approximate auto-regressive counterfactual belief, and then uses a public-private model architecture to train policies with rollouts. The paper shows that LBS outperforms prior work in Hanabi and Hanabi-like environments, and LBS can be applied to multi-agent settings as well. The main contribution of the paper is that the exact belief distribution is learned in LBS, and that it can be used to improve the performance of existing search methods. The empirical results show that Lbs outperforms the state-of-the-art on Hanabi in terms of exact search and compute requirements."
2649,SP:db408e6bfe69a9b3984f3b27ca92b802aa37af42,"MCTS CONJUNCTION random shooting. random shooting CONJUNCTION MCTS. It USED-FOR bias - variance trade - off. Task is Planning in large state spaces. Method are Shoot Tree Search ( STS ), TD(n ), and STS. Generic is algorithm. OtherScientificTerm is tree search context. ",Planning in large state spaces is an important problem. This paper proposes a new algorithm called Shoot Tree Search (STS). It aims to address the bias-variance trade-off between MCTS and random shooting. The algorithm is based on the observation that TD(n) is biased in the tree search context. The authors show that STS can be seen as a special case of TD(N) and show that it can be viewed as a generalization of the previous work (MCTS).,Planning in large state spaces is an important problem. This paper proposes a new algorithm called Shoot Tree Search (STS). It aims to address the bias-variance trade-off between MCTS and random shooting. The algorithm is based on the observation that TD(n) is biased in the tree search context. The authors show that STS can be seen as a special case of TD(N) and show that it can be viewed as a generalization of the previous work (MCTS).
2665,SP:5efc271ccc555fd9aa542548838170bd4c98e957,"transformer networks USED-FOR inductive bias. inductive bias PART-OF neural architectures. transformer networks USED-FOR tasks. tasks USED-FOR inductive bias. datasets USED-FOR inductive bias. induction CONJUNCTION abduction. abduction CONJUNCTION induction. deduction CONJUNCTION induction. induction CONJUNCTION deduction. model USED-FOR synthetic tasks. tasks USED-FOR reasoning biases. Inductive bias USED-FOR Mathematical rEasoning. LIME HYPONYM-OF pre - training methodology. Models COMPARE vanilla transformers. vanilla transformers COMPARE Models. LIME USED-FOR Models. large mathematical reasoning benchmarks EVALUATE-FOR vanilla transformers. large mathematical reasoning benchmarks EVALUATE-FOR Models. computation cost EVALUATE-FOR pre - training approaches. computation cost FEATURE-OF downstream task. pre - training approaches USED-FOR LIME. downstream task USED-FOR LIME. computation cost USED-FOR LIME. Method is architecture engineering. OtherScientificTerm are reasoning primitives, and mathematical knowledge. Generic is they. ","This paper investigates the inductive bias of transformer networks for a variety of tasks that are commonly used in architecture engineering. The authors propose two datasets that are designed to measure inductive biases in transformer networks on a wide range of tasks, including deduction, induction, abduction, and reasoning primitives. They show that a model trained on these synthetic tasks is able to learn reasoning biases on all of these tasks.   Mathematical rEasoning is one of the most commonly used applications of inductively biased models. Inductive bias is a common problem in this field, but it is not well-studied. This paper proposes a new pre-training methodology called LIME, which is based on LIME. Models trained with LIME outperform vanilla transformers on large mathematical reasoning benchmarks. LIME is also able to reduce the computation cost of the downstream task in comparison to other recent state-of-the-art pretraining approaches. The main contribution of this paper is that the authors show that LIME can be used to reduce computation cost in order to improve the performance of a pre-trained model, and that the reasoning biases learned by LIME are more robust to changes in the number of tasks or in the amount of mathematical knowledge used. ","This paper investigates the inductive bias of transformer networks for a variety of tasks that are commonly used in architecture engineering. The authors propose two datasets that are designed to measure inductive biases in transformer networks on a wide range of tasks, including deduction, induction, abduction, and reasoning primitives. They show that a model trained on these synthetic tasks is able to learn reasoning biases on all of these tasks.   Mathematical rEasoning is one of the most commonly used applications of inductively biased models. Inductive bias is a common problem in this field, but it is not well-studied. This paper proposes a new pre-training methodology called LIME, which is based on LIME. Models trained with LIME outperform vanilla transformers on large mathematical reasoning benchmarks. LIME is also able to reduce the computation cost of the downstream task in comparison to other recent state-of-the-art pretraining approaches. The main contribution of this paper is that the authors show that LIME can be used to reduce computation cost in order to improve the performance of a pre-trained model, and that the reasoning biases learned by LIME are more robust to changes in the number of tasks or in the amount of mathematical knowledge used. "
2681,SP:bb8e0b554d3b3314fa343c902d9e60f1a141ea30,"gradient descent USED-FOR weight normalized smooth homogeneous neural nets. inductive bias FEATURE-OF weight normalized smooth homogeneous neural nets. inductive bias FEATURE-OF gradient descent. gradient flow USED-FOR networks. gradient flow path COMPARE gradient flow. gradient flow COMPARE gradient flow path. EWN USED-FOR gradient flow path. adaptive learning rate USED-FOR gradient flow. adaptive learning rate USED-FOR gradient descent. weight normalization ( SWN ) CONJUNCTION unnormalized architectures. unnormalized architectures CONJUNCTION weight normalization ( SWN ). inductive bias CONJUNCTION unnormalized architectures. unnormalized architectures CONJUNCTION inductive bias. inductive bias FEATURE-OF weight normalization ( SWN ). synthetic data sets EVALUATE-FOR unnormalized architectures. simple data sets CONJUNCTION architectures. architectures CONJUNCTION simple data sets. architectures EVALUATE-FOR sparse EWN solutions. SGD USED-FOR sparse EWN solutions. OtherScientificTerm are exponential or cross - entropy loss, radial direction, and asymptotic relative sparsity. Method is exponential weight normalization ( EWN ). Metric is asymptotic convergence rate. Task is learning prunable neural networks. ","This paper studies the inductive bias of gradient descent for weight normalized smooth homogeneous neural nets trained with exponential or cross-entropy loss. The authors propose exponential weight normalization (EWN) and show that gradient descent with adaptive learning rate converges to a gradient flow path that has a radial direction. They show that the gradient flow of the networks trained with EWN converges in the same direction as that of gradient flow in the original gradient flow. They also provide an asymptotic convergence rate for learning prunable neural networks. Finally, the authors show that under certain assumptions, the gradients of EWN converge in the radial direction, which is a result of the exponential or Cross-Entropy loss, and they show that this is the case even if the gradient descent is trained with an adaptive learning rates.    The authors also show that for simple data sets and unnormalized architectures trained on synthetic data sets, the gradient flows of EWN and SGD converge in a similar direction to that of standard gradient flow, and the authors also provide a theoretical analysis that shows that gradient flow is more likely to converge along the gradient direction of the EWN in the case of weight normalisation (SWN) as well as unnormalised architectures. They further show that SGD is able to converge to sparse EWN solutions in the presence of a certain amount of relative sparsity in the training data.  Finally, they also provide some experimental results on learning pruned neural networks, showing that simple datasets and architectures trained with SGD are able to learn sparse solutions.","This paper studies the inductive bias of gradient descent for weight normalized smooth homogeneous neural nets trained with exponential or cross-entropy loss. The authors propose exponential weight normalization (EWN) and show that gradient descent with adaptive learning rate converges to a gradient flow path that has a radial direction. They show that the gradient flow of the networks trained with EWN converges in the same direction as that of gradient flow in the original gradient flow. They also provide an asymptotic convergence rate for learning prunable neural networks. Finally, the authors show that under certain assumptions, the gradients of EWN converge in the radial direction, which is a result of the exponential or Cross-Entropy loss, and they show that this is the case even if the gradient descent is trained with an adaptive learning rates.    The authors also show that for simple data sets and unnormalized architectures trained on synthetic data sets, the gradient flows of EWN and SGD converge in a similar direction to that of standard gradient flow, and the authors also provide a theoretical analysis that shows that gradient flow is more likely to converge along the gradient direction of the EWN in the case of weight normalisation (SWN) as well as unnormalised architectures. They further show that SGD is able to converge to sparse EWN solutions in the presence of a certain amount of relative sparsity in the training data.  Finally, they also provide some experimental results on learning pruned neural networks, showing that simple datasets and architectures trained with SGD are able to learn sparse solutions."
2697,SP:c71f9d2a602516865a0b103028186e83b52e5f00,Generative Adversarial Networks ( GANs ) HYPONYM-OF generative models. mode collapse FEATURE-OF generator. Catastrophic Forgetting PART-OF continual learning. classification accuracy EVALUATE-FOR discriminator. training procedure USED-FOR discriminators. training scheme USED-FOR mode collapse. metrics EVALUATE-FOR GAN evaluation. GAN frameworks USED-FOR mode collapse. training scheme USED-FOR GAN frameworks. metrics EVALUATE-FOR training scheme. Generic is they. Task is mode collapse problem. Method is data generation procedure. ,"Generative Adversarial Networks (GANs) are one of the most popular generative models, but they suffer from the mode collapse problem. Catastrophic Forgetting is an important problem in continual learning, and this paper proposes a training procedure to train discriminators that are more robust to mode collapse in the generator. The authors show that their training scheme is able to prevent mode collapse and improve the classification accuracy of the discriminator. They also show that the training scheme improves the metrics for GAN evaluation. Finally, they show that GAN frameworks trained with their proposed training scheme are able to avoid mode collapse.  ","Generative Adversarial Networks (GANs) are one of the most popular generative models, but they suffer from the mode collapse problem. Catastrophic Forgetting is an important problem in continual learning, and this paper proposes a training procedure to train discriminators that are more robust to mode collapse in the generator. The authors show that their training scheme is able to prevent mode collapse and improve the classification accuracy of the discriminator. They also show that the training scheme improves the metrics for GAN evaluation. Finally, they show that GAN frameworks trained with their proposed training scheme are able to avoid mode collapse.  "
2713,SP:52c48198c95826e042f9e5a512ef3265daaff882,"proxy score USED-FOR head importance. proxy score USED-FOR attention heads. AUBER HYPONYM-OF regularization method. attention heads PART-OF BERT. reinforcement learning USED-FOR regularization method. heuristics CONJUNCTION rule - based policies. rule - based policies CONJUNCTION heuristics. pruning policy USED-FOR attention heads. AUBER USED-FOR pruning policy. rule - based policies USED-FOR AUBER. heuristics USED-FOR AUBER. AUBER COMPARE pruning methods. pruning methods COMPARE AUBER. accuracy EVALUATE-FOR pruning methods. accuracy EVALUATE-FOR AUBER. Generic are it, and they. Method is heuristic - based methods. OtherScientificTerm is regularization. ","This paper proposes AUBER, a regularization method based on reinforcement learning that prunes attention heads in BERT by using a proxy score to measure head importance. The authors argue that it is more efficient than existing heuristic-based methods as it does not require any additional regularization. They also show that AUBer can be combined with existing heuristics and rule-based policies to produce a pruning policy that can prune attention heads efficiently. Experiments are conducted on several datasets and show that the pruning performance is comparable to existing pruning methods, but they are much more efficient. ","This paper proposes AUBER, a regularization method based on reinforcement learning that prunes attention heads in BERT by using a proxy score to measure head importance. The authors argue that it is more efficient than existing heuristic-based methods as it does not require any additional regularization. They also show that AUBer can be combined with existing heuristics and rule-based policies to produce a pruning policy that can prune attention heads efficiently. Experiments are conducted on several datasets and show that the pruning performance is comparable to existing pruning methods, but they are much more efficient. "
2729,SP:abcbbad146f1b0d5d579c215952c95e5499a378a,"physics parameters CONJUNCTION morphology. morphology CONJUNCTION physics parameters. representation CONJUNCTION physics parameters. physics parameters CONJUNCTION representation. unpaired and randomly collected data USED-FOR correspondences. dynamics cycles USED-FOR dynamic robot behavior. cycle - consistency constraint USED-FOR dynamics cycles. simulation CONJUNCTION real robot. real robot CONJUNCTION simulation. real robot HYPONYM-OF problem domains. simulation HYPONYM-OF problem domains. dynamic state - action trajectories FEATURE-OF simulated arm. framework USED-FOR uncalibrated monocular video. framework USED-FOR dynamic state - action trajectories. real robot arm FEATURE-OF uncalibrated monocular video. Task are robotics problems, imitation learning, sim - to - real, and transfer learning. OtherScientificTerm are physics simulators, and robotics environments. Generic are correspondence, and policy. Method is fine - tuning. Material is paired data. ","This paper proposes a new method for imitation learning in the context of robotics problems. In this setting, physics simulators are used, and the goal is to learn a correspondence between a representation, physics parameters, and morphology. The correspondences are learned using both unpaired and randomly collected data. The authors propose to use dynamics cycles to model dynamic robot behavior, and use a cycle-consistency constraint to ensure that the dynamics cycles are consistent.   The authors show that the correspondence between simulation and real robot can be learned in two problem domains: (1) simulation and (2) real robot. They show that imitation learning can be transferred from sim-to-real in both cases, and that fine-tuning can be done in both settings. They also show that their framework can be applied to uncalibrated monocular video from a real robot arm with dynamic state-action trajectories from a simulated arm, and transfer learning is possible in both robotics environments.  The main contribution of the paper is that the authors propose a framework to learn the dynamics of a robot from paired data, and then fine-tune the policy on the paired data. ","This paper proposes a new method for imitation learning in the context of robotics problems. In this setting, physics simulators are used, and the goal is to learn a correspondence between a representation, physics parameters, and morphology. The correspondences are learned using both unpaired and randomly collected data. The authors propose to use dynamics cycles to model dynamic robot behavior, and use a cycle-consistency constraint to ensure that the dynamics cycles are consistent.   The authors show that the correspondence between simulation and real robot can be learned in two problem domains: (1) simulation and (2) real robot. They show that imitation learning can be transferred from sim-to-real in both cases, and that fine-tuning can be done in both settings. They also show that their framework can be applied to uncalibrated monocular video from a real robot arm with dynamic state-action trajectories from a simulated arm, and transfer learning is possible in both robotics environments.  The main contribution of the paper is that the authors propose a framework to learn the dynamics of a robot from paired data, and then fine-tune the policy on the paired data. "
2745,SP:006434d56992836ab9420d7d4215bc70664de304,"Explainability USED-FOR model development. Explainability USED-FOR AI. solutions USED-FOR Shapley explainability. data manifold USED-FOR solutions. data manifold USED-FOR Shapley explainability. Shapley value - function USED-FOR other. generative modelling USED-FOR solution. unintelligible explanations FEATURE-OF higher - dimensional data. OtherScientificTerm are operational nuance, model ’s features, off - manifold ” Shapley values, implicit model dependence, and sensitive attributes. Method are Shapley framework, and on - manifold explainability. Task is data imputations. ","Explainability is an important topic in model development. Explainability in AI is important because of its ability to capture operational nuance and explain the model’s features. This paper proposes two solutions to improve Shapley explainability on the data manifold. The first is based on the Shapley framework, where the “off-manifold” Shapley values are defined to capture the implicit model dependence between the model and the underlying data. The other is a Shapley value-function, which captures the relationship between the data imputations and the model's features. The authors propose a solution based on generative modelling, and show that their solution is more robust to unintelligible explanations on higher-dimensional data. They also show that the on-manit manifold explainability is more sensitive to sensitive attributes.   ","Explainability is an important topic in model development. Explainability in AI is important because of its ability to capture operational nuance and explain the model’s features. This paper proposes two solutions to improve Shapley explainability on the data manifold. The first is based on the Shapley framework, where the “off-manifold” Shapley values are defined to capture the implicit model dependence between the model and the underlying data. The other is a Shapley value-function, which captures the relationship between the data imputations and the model's features. The authors propose a solution based on generative modelling, and show that their solution is more robust to unintelligible explanations on higher-dimensional data. They also show that the on-manit manifold explainability is more sensitive to sensitive attributes.   "
2761,SP:7cda6bccf08887c7cef66d0ac3ccefdea8f5d7c8,Existing methods USED-FOR opponent modelling. local observations USED-FOR Existing methods. chosen actions CONJUNCTION received rewards. received rewards CONJUNCTION chosen actions. observed world state CONJUNCTION chosen actions. chosen actions CONJUNCTION observed world state. variational autoencoders USED-FOR local actions. local observations USED-FOR embeddings. observed world state HYPONYM-OF local observations. chosen actions HYPONYM-OF local observations. variational autoencoders USED-FOR modelling technique. embeddings USED-FOR modelling agent ’s decision policy. deep reinforcement learning USED-FOR embeddings. deep reinforcement learning USED-FOR modelling agent ’s decision policy. method COMPARE baseline method. baseline method COMPARE method. method COMPARE baseline. baseline COMPARE method. baseline COMPARE baseline method. baseline method COMPARE baseline. opponent ’s information USED-FOR baseline. embeddings USED-FOR baseline method. Generic is policy. OtherScientificTerm is opponent observations. ,"Existing methods for opponent modelling are based on local observations from the opponent. This paper proposes a new modelling technique based on variational autoencoders that uses local observations (observed world state, chosen actions, and received rewards) to learn embeddings for embedding the agent’s decision policy using deep reinforcement learning. Experiments show that the proposed method outperforms a baseline method that does not use opponent”s information and learns from the embedding of the policy. The paper also shows that the learned embedding is robust to opponent observations.","Existing methods for opponent modelling are based on local observations from the opponent. This paper proposes a new modelling technique based on variational autoencoders that uses local observations (observed world state, chosen actions, and received rewards) to learn embeddings for embedding the agent’s decision policy using deep reinforcement learning. Experiments show that the proposed method outperforms a baseline method that does not use opponent”s information and learns from the embedding of the policy. The paper also shows that the learned embedding is robust to opponent observations."
2777,SP:c239bc531bcf7293032748af29a1b786e9d893dd,"Contrastive learning USED-FOR unsupervised visual representation learning. consistency regularization term PART-OF contrastive learning framework. consistency regularization USED-FOR semi - supervised learning. Consistent Contrast ( CO2 ) USED-FOR contrastive learning framework. unlabeled data USED-FOR consistency regularization. unlabeled data USED-FOR semi - supervised learning. consistency regularization term PART-OF Consistent Contrast ( CO2 ). CO2 USED-FOR Momentum Contrast ( MoCo ). top-1 accuracy EVALUATE-FOR Momentum Contrast ( MoCo ). ImageNet linear protocol EVALUATE-FOR CO2. top-1 accuracy EVALUATE-FOR CO2. image classification CONJUNCTION object detection. object detection CONJUNCTION image classification. object detection CONJUNCTION semantic segmentation. semantic segmentation CONJUNCTION object detection. It USED-FOR image classification. It USED-FOR semantic segmentation. It USED-FOR object detection. PASCAL VOC USED-FOR semantic segmentation. PASCAL VOC USED-FOR object detection. PASCAL VOC USED-FOR image classification. visual representations USED-FOR tasks. CO2 USED-FOR visual representations. CO2 USED-FOR tasks. OtherScientificTerm are human annotation, heterogeneous similarity, semantic class, consistency term, and labeled semi - supervised settings. Task is instance discrimination task. Generic is task. Method is label assignment strategy. Metric is top-5 accuracy. ","This paper proposes Consistent Contrastive learning for unsupervised visual representation learning. Consistent contrast (CO2) is a contrastive learning framework that incorporates a consistency regularization term to semi-supervised learning with unlabeled data. The paper shows that CO2 improves the top-1 accuracy of Momentum Contrast (MoCo) on the ImageNet linear protocol. CO2 can be applied to image classification, object detection, and semantic segmentation using PASCAL VOC, and is shown to improve top-5 accuracy on the instance discrimination task. The authors also show that the consistency term can be used to improve the performance of the semantic classifier.   The paper also shows that the visual representations learned by CO2 are transferable to other tasks, and that the label assignment strategy is more effective than the one used in MoCo. ","This paper proposes Consistent Contrastive learning for unsupervised visual representation learning. Consistent contrast (CO2) is a contrastive learning framework that incorporates a consistency regularization term to semi-supervised learning with unlabeled data. The paper shows that CO2 improves the top-1 accuracy of Momentum Contrast (MoCo) on the ImageNet linear protocol. CO2 can be applied to image classification, object detection, and semantic segmentation using PASCAL VOC, and is shown to improve top-5 accuracy on the instance discrimination task. The authors also show that the consistency term can be used to improve the performance of the semantic classifier.   The paper also shows that the visual representations learned by CO2 are transferable to other tasks, and that the label assignment strategy is more effective than the one used in MoCo. "
2793,SP:d18bab21790713e2facb053c47298fc9079ab783,"Optimistic Gradient Descent Ascent ( OGDA ) CONJUNCTION Optimistic Multiplicative Weights Update ( OMWU ). Optimistic Multiplicative Weights Update ( OMWU ) CONJUNCTION Optimistic Gradient Descent Ascent ( OGDA ). Optimistic Multiplicative Weights Update ( OMWU ) USED-FOR saddle - point optimization. Optimistic Gradient Descent Ascent ( OGDA ) USED-FOR saddle - point optimization. uniqueness of the optimal solution HYPONYM-OF assumptions. probability simplex FEATURE-OF bilinear games. OGDA CONJUNCTION OMWU. OMWU CONJUNCTION OGDA. OMWU USED-FOR constrained setting. last - iterate convergence FEATURE-OF OGDA. bilinear games FEATURE-OF OMWU. universal constant FEATURE-OF learning rate. simplex FEATURE-OF bilinear games. learning rate USED-FOR linear last - iterate convergence. constant learning rate FEATURE-OF last - iterate convergence rates. last - iterate convergence rates FEATURE-OF OGDA. condition FEATURE-OF bilinear games. polytope FEATURE-OF bilinear games. condition USED-FOR strongly - convex - stronglyconcave functions. Metric is convergence rates. OtherScientificTerm are exponentially small learning rate, equilibrium, smoothness of the objective function, and unique equilibrium assumption. Method is projected OGDA algorithm. ","This paper studies the convergence rates of saddle-point optimization with Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-points. The authors make two assumptions: (1) uniqueness of the optimal solution and (2) the existence of an exponentially small learning rate. They show that under these assumptions, OGDA and OMWU converge linearly to an optimal solution with a constant learning rate, and that OGDA converges with last-iterate convergence in a constrained setting. They also show that in bilinear games with probability simplex, OMWu converges linearly in a convex-strongly-convex setting. Finally, the authors show that for strongly convex and strongly-concave functions, the learning rate of OGDA is a universal constant. ","This paper studies the convergence rates of saddle-point optimization with Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-points. The authors make two assumptions: (1) uniqueness of the optimal solution and (2) the existence of an exponentially small learning rate. They show that under these assumptions, OGDA and OMWU converge linearly to an optimal solution with a constant learning rate, and that OGDA converges with last-iterate convergence in a constrained setting. They also show that in bilinear games with probability simplex, OMWu converges linearly in a convex-strongly-convex setting. Finally, the authors show that for strongly convex and strongly-concave functions, the learning rate of OGDA is a universal constant. "
2809,SP:bbc7f77308b298c332a39747f693bc396f00a89f,"federated setup USED-FOR User Verification ( UV ) models. framework USED-FOR private and secure training of UV models. Federated User Verification ( FedUV ) USED-FOR private and secure training of UV models. secret user - defined linear combination USED-FOR instance embeddings. FedUV COMPARE approaches. approaches COMPARE FedUV. voice, face, and handwriting data USED-FOR user verification. Method are loss functions, and UV models. OtherScientificTerm are user embeddings, linear combinations, error - correcting code, embedding vectors, and embeddings. Generic is model. ","This paper proposes a federated setup to train User Verification (UV) models in a secure manner. The proposed framework, Federated UserVerification (FedUV), is designed to provide private and secure training of UV models. The key idea of FedUV is to train a set of loss functions for each user, which are then shared across all the users. Each user embeddings are obtained by learning linear combinations of the embedding vectors of all the instances of the same user. Each instance embedding is obtained by a secret user-defined linear combination of all instances of a given user. The model is then trained in an end-to-end fashion. The paper shows that FedUV outperforms existing approaches on user verification on voice, face, and handwriting data.","This paper proposes a federated setup to train User Verification (UV) models in a secure manner. The proposed framework, Federated UserVerification (FedUV), is designed to provide private and secure training of UV models. The key idea of FedUV is to train a set of loss functions for each user, which are then shared across all the users. Each user embeddings are obtained by learning linear combinations of the embedding vectors of all the instances of the same user. Each instance embedding is obtained by a secret user-defined linear combination of all instances of a given user. The model is then trained in an end-to-end fashion. The paper shows that FedUV outperforms existing approaches on user verification on voice, face, and handwriting data."
2825,SP:40fa47cc0928e2925ef5ce6d808073f368ca2cd4,"geometry FEATURE-OF class manifolds ( CMs ). geometry FEATURE-OF model. technique USED-FOR boundaries. technique USED-FOR CMs. generalization CONJUNCTION robustness. robustness CONJUNCTION generalization. geometry of CMs CONJUNCTION generalization. generalization CONJUNCTION geometry of CMs. label randomization CONJUNCTION training set size. training set size CONJUNCTION label randomization. ensemble size CONJUNCTION label randomization. label randomization CONJUNCTION ensemble size. stage of training CONJUNCTION class. class CONJUNCTION stage of training. training set size CONJUNCTION model robustness. model robustness CONJUNCTION training set size. architecture CONJUNCTION random initialization. random initialization CONJUNCTION architecture. random initialization CONJUNCTION stage of training. stage of training CONJUNCTION random initialization. class CONJUNCTION ensemble size. ensemble size CONJUNCTION class. data corruption FEATURE-OF model robustness. dataset USED-FOR CM dimension. CMs USED-FOR ensembling. Method are Deep neural network classifiers, and real neural networks. OtherScientificTerm are margin, and random affine subspaces. Generic are method, and models. ","Deep neural network classifiers have been shown to be invariant to margin and geometry of class manifolds (CMs). However, the geometry of a model depends on the architecture of the model and the number of layers. This paper proposes a technique to learn boundaries between CMs. The technique is based on the observation that real neural networks tend to form random affine subspaces. The paper shows that this technique can be applied to CMs to improve generalization, robustness, and ensembling. The method is tested on a variety of settings, including architecture, random initialization, stage of training, class size, ensemble size, label randomization, training set size, and model robustness to data corruption. The results show that the CM dimension of a CM is invariant when the dataset is large enough to cover the entire CM dimension.   ","Deep neural network classifiers have been shown to be invariant to margin and geometry of class manifolds (CMs). However, the geometry of a model depends on the architecture of the model and the number of layers. This paper proposes a technique to learn boundaries between CMs. The technique is based on the observation that real neural networks tend to form random affine subspaces. The paper shows that this technique can be applied to CMs to improve generalization, robustness, and ensembling. The method is tested on a variety of settings, including architecture, random initialization, stage of training, class size, ensemble size, label randomization, training set size, and model robustness to data corruption. The results show that the CM dimension of a CM is invariant when the dataset is large enough to cover the entire CM dimension.   "
2841,SP:09bce202ac7a750c3700a8ef3cd92cfe8ed00c39,"exploration CONJUNCTION exploitation. exploitation CONJUNCTION exploration. RL methods USED-FOR problem. action noise USED-FOR policies. entropy FEATURE-OF policy. entropy temperature FEATURE-OF policy. entropy temperature USED-FOR entropy. Soft Actor - Critic ( SAC ) HYPONYM-OF policies. Curiosity - Aware entropy Temperature USED-FOR SAC ( CAT - SAC ). curiosity mechanism USED-FOR instance - level entropy temperature. curiosity mechanism USED-FOR Curiosity - Aware entropy Temperature. state prediction error USED-FOR curiosity. state prediction error USED-FOR CAT - SAC. entropy USED-FOR CAT - SAC. MuJoCo benchmark EVALUATE-FOR CAT - SAC. sample efficiency EVALUATE-FOR CAT - SAC. Task is reinforcement learning ( RL ). Generic is temperature. OtherScientificTerm are environment states, prediction error, and unfamiliar states. ","This paper considers the problem of reinforcement learning (RL) in which the goal is to learn a policy that maximizes the entropy of the environment states. This problem is well-motivated by the observation that RL methods often fail to learn policies that are robust to action noise due to the lack of exploration and exploitation. To address this problem, the authors propose two policies, namely Soft Actor-Critic (SAC) and Curiosity-Aware entropy temperature (CAT-SAC).    The authors propose to use the entropy temperature of the policy as a measure of the difficulty of a policy to learn. The authors also propose a curiosity mechanism to measure the instance-level entropy temperature, which is used to train two policies.   They show that the proposed SAC (SAT-CAT) improves the sample efficiency by using the Curiosity- Aware entropy Temperature, which uses the state prediction error as a proxy for the entropy. They show empirically that the prediction error is lower when the entropy is higher, and higher when the temperature is lower. They also show that CAT-SAT improves sample efficiency on the MuJoCo benchmark, where they show that they can learn to explore in unfamiliar states. ","This paper considers the problem of reinforcement learning (RL) in which the goal is to learn a policy that maximizes the entropy of the environment states. This problem is well-motivated by the observation that RL methods often fail to learn policies that are robust to action noise due to the lack of exploration and exploitation. To address this problem, the authors propose two policies, namely Soft Actor-Critic (SAC) and Curiosity-Aware entropy temperature (CAT-SAC).    The authors propose to use the entropy temperature of the policy as a measure of the difficulty of a policy to learn. The authors also propose a curiosity mechanism to measure the instance-level entropy temperature, which is used to train two policies.   They show that the proposed SAC (SAT-CAT) improves the sample efficiency by using the Curiosity- Aware entropy Temperature, which uses the state prediction error as a proxy for the entropy. They show empirically that the prediction error is lower when the entropy is higher, and higher when the temperature is lower. They also show that CAT-SAT improves sample efficiency on the MuJoCo benchmark, where they show that they can learn to explore in unfamiliar states. "
2857,SP:dce5eb20581a21c5de0a9fc07a8a79a1fbb28c71,"Reinforcement learning algorithms USED-FOR policies. meta - reinforcement learning methods USED-FOR agents. model identification CONJUNCTION experience relabeling ( MIER ). experience relabeling ( MIER ) CONJUNCTION model identification. experience relabeling ( MIER ) HYPONYM-OF meta - reinforcement learning algorithm. meta - reinforcement learning algorithm USED-FOR out - of - distribution tasks. policies CONJUNCTION value functions. value functions CONJUNCTION policies. dynamics models COMPARE value functions. value functions COMPARE dynamics models. dynamics models COMPARE policies. policies COMPARE dynamics models. off - policy data USED-FOR dynamics models. synthetic experience USED-FOR task. dynamics models USED-FOR policies. Generic are approaches, and method. Method are on - policy meta - training, and meta - reinforcement learning. ","This paper proposes a meta-reinforcement learning algorithm for out-of-distribution (OOD) tasks, where the goal is to learn policies that generalize well to unseen tasks. The authors propose two approaches: (1) model identification and (2) experience relabeling (MIER). The authors show that the proposed approach is able to generalize better than the baselines in both cases. ","This paper proposes a meta-reinforcement learning algorithm for out-of-distribution (OOD) tasks, where the goal is to learn policies that generalize well to unseen tasks. The authors propose two approaches: (1) model identification and (2) experience relabeling (MIER). The authors show that the proposed approach is able to generalize better than the baselines in both cases. "
2873,SP:34d78aa11f9d50baf75a9646a6f9128318c3389a,"meta - learning techniques USED-FOR few - shot learning ( FSL ) problem. label noise USED-FOR FSL. label noise FEATURE-OF meta - learner. gradient noise problem USED-FOR meta - overfitting problem. Eigen - Reptile ( ER ) USED-FOR gradient noise. Eigen - Reptile ( ER ) USED-FOR meta - parameters. historical taskspecific parameters USED-FOR gradient noise. historical taskspecific parameters USED-FOR Eigen - Reptile ( ER ). Introspective Self - paced Learning ( ISPL ) USED-FOR prior models. Eigen - Reptile CONJUNCTION ISPL. ISPL CONJUNCTION Eigen - Reptile. tasks EVALUATE-FOR methods. methods COMPARE state - of - the - art methods. state - of - the - art methods COMPARE methods. tasks EVALUATE-FOR state - of - the - art methods. noisy labels USED-FOR state - of - the - art methods. OtherScientificTerm are meta - overfit, sampling noise, and gradient step. Task is overfitting. ","This paper studies the meta-learning techniques for the few-shot learning (FSL) problem with label noise in the context of FSL. The authors show that meta-overfit happens when the meta learner is sensitive to label noise, and that the gradient noise problem is the main cause of the meta -overfitting problem. To address this problem, the authors propose Eigen-Reptile (ER) that uses historical taskspecific parameters to mitigate gradient noise in meta-parameters. They also propose prior models based on Introspective Self-Paced Learning (ISPL) to mitigate the issue of sampling noise. Experiments are conducted on several tasks to show the effectiveness of the proposed methods. The proposed methods are compared with state-of-the-art methods with noisy labels on a number of tasks, and show that Eigen - Reptile and ISPL are able to avoid overfitting to noisy labels, and the gradient step is more efficient.","This paper studies the meta-learning techniques for the few-shot learning (FSL) problem with label noise in the context of FSL. The authors show that meta-overfit happens when the meta learner is sensitive to label noise, and that the gradient noise problem is the main cause of the meta -overfitting problem. To address this problem, the authors propose Eigen-Reptile (ER) that uses historical taskspecific parameters to mitigate gradient noise in meta-parameters. They also propose prior models based on Introspective Self-Paced Learning (ISPL) to mitigate the issue of sampling noise. Experiments are conducted on several tasks to show the effectiveness of the proposed methods. The proposed methods are compared with state-of-the-art methods with noisy labels on a number of tasks, and show that Eigen - Reptile and ISPL are able to avoid overfitting to noisy labels, and the gradient step is more efficient."
2889,SP:a571bff9ffe4edafd7bc064c4d10609e6b981ce3,"Adversarial training USED-FOR models. feature statistics COMPARE image pixels. image pixels COMPARE feature statistics. distributional shifts USED-FOR models. adversarially crafted distributions USED-FOR images. Stylized - ImageNet CONJUNCTION ImageNetInstagram. ImageNetInstagram CONJUNCTION Stylized - ImageNet. AdvBN USED-FOR semantic segmentation. generalization EVALUATE-FOR semantic segmentation. generalization EVALUATE-FOR AdvBN. goldfinch CONJUNCTION sulphur butterfly. sulphur butterfly CONJUNCTION goldfinch. goldfinch CONJUNCTION bulbul. bulbul CONJUNCTION goldfinch. hummingbird CONJUNCTION goldfinch. goldfinch CONJUNCTION hummingbird. house finch CONJUNCTION goldfinch. goldfinch CONJUNCTION house finch. bulbul CONJUNCTION house finch. house finch CONJUNCTION bulbul. brambling CONJUNCTION guillotine. guillotine CONJUNCTION brambling. bolete CONJUNCTION fox squirrel. fox squirrel CONJUNCTION bolete. fox squirrel CONJUNCTION hen - of - the - woods. hen - of - the - woods CONJUNCTION fox squirrel. gong CONJUNCTION bolete. bolete CONJUNCTION gong. Ibizan hound CONJUNCTION flamingo. flamingo CONJUNCTION Ibizan hound. ResNet-50 model USED-FOR classification scores. OtherScientificTerm are small adversarial perturbations, and mean and variance of deep image features. Method are adversarial training, Adversarial Batch Normalization ( AdvBN ), ResNet-50, ImageNet variants, and Adversarial Batch Normalization module. Generic is method. ","This paper proposes Adversarial batch normalization (Adversarial Batch Normalization, AdvBN) to improve the generalization performance of models trained with adversarial training under distributional shifts. The authors argue that the reason for this is that models trained on adversarially crafted distributions are more robust to small adversarial perturbations, and feature statistics are more similar to image pixels. They show that AdvBN improves generalization for semantic segmentation on Stylized-ImageNet, ImageNetInstagram, and ImageNet variants. They also show that the mean and variance of deep image features can be used as a metric to measure the robustness of a ResNet-50 model to classification scores.   The authors conduct experiments on a variety of datasets, including gong, bolete, fox squirrel, and hen-of-the-woodens. They use Ibizan hound, a hummingbird, a goldfinch, a sulphur butterfly, a house finch, goldfinches, a bulbul, and a brambling and a guillotine as examples, and show that their method outperforms other methods on all of them.  They also demonstrate that the ResNet50 model can be trained to produce classification scores that are robust to adversarial attacks.  The paper also shows that the Adversary Batch normalization module is effective at reducing the variance of feature statistics. ","This paper proposes Adversarial batch normalization (Adversarial Batch Normalization, AdvBN) to improve the generalization performance of models trained with adversarial training under distributional shifts. The authors argue that the reason for this is that models trained on adversarially crafted distributions are more robust to small adversarial perturbations, and feature statistics are more similar to image pixels. They show that AdvBN improves generalization for semantic segmentation on Stylized-ImageNet, ImageNetInstagram, and ImageNet variants. They also show that the mean and variance of deep image features can be used as a metric to measure the robustness of a ResNet-50 model to classification scores.   The authors conduct experiments on a variety of datasets, including gong, bolete, fox squirrel, and hen-of-the-woodens. They use Ibizan hound, a hummingbird, a goldfinch, a sulphur butterfly, a house finch, goldfinches, a bulbul, and a brambling and a guillotine as examples, and show that their method outperforms other methods on all of them.  They also demonstrate that the ResNet50 model can be trained to produce classification scores that are robust to adversarial attacks.  The paper also shows that the Adversary Batch normalization module is effective at reducing the variance of feature statistics. "
2905,SP:6a9c46bd3cf854299f360bff136e1d79d3edb2e4,"proxy metric USED-FOR outliers. outliers PART-OF data distribution. Variance of Gradients ( VoG ) HYPONYM-OF proxy metric. Task are machine learning, and human - in - the - loop auditing. Method are model, models, and VoG. OtherScientificTerm is VoG scores. ","This paper proposes a new proxy metric called Variance of Gradients (VoG) to detect outliers in the data distribution, which is an important problem in machine learning. The paper also proposes a human-in-the-loop auditing method to identify outliers that can be used to improve the performance of the model. The authors show that models trained with VoG are more robust to outliers than models trained without VoG. They also show that VoG scores are more sensitive to the number of outliers.","This paper proposes a new proxy metric called Variance of Gradients (VoG) to detect outliers in the data distribution, which is an important problem in machine learning. The paper also proposes a human-in-the-loop auditing method to identify outliers that can be used to improve the performance of the model. The authors show that models trained with VoG are more robust to outliers than models trained without VoG. They also show that VoG scores are more sensitive to the number of outliers."
2930,SP:074bfacc75837bb19049be8a2890e10de073dd8e,"real - world data FEATURE-OF simulated samples. images HYPONYM-OF simulated samples. generation quality EVALUATE-FOR model. technique USED-FOR generated samples. non - linear Fokker - Plank equation USED-FOR gradient flow. wasteful sample rejection USED-FOR methods. DRS HYPONYM-OF methods. refinement approach USED-FOR GANs. VAEs CONJUNCTION Normalizing Flows. Normalizing Flows CONJUNCTION VAEs. GANs CONJUNCTION deep generative models. deep generative models CONJUNCTION GANs. vector - valued critics CONJUNCTION deep generative models. deep generative models CONJUNCTION vector - valued critics. vector - valued critics USED-FOR GANs. Normalizing Flows HYPONYM-OF deep generative models. VAEs HYPONYM-OF deep generative models. DGf low COMPARE Discriminator Driven Latent Sampling ( DDLS ) methods. Discriminator Driven Latent Sampling ( DDLS ) methods COMPARE DGf low. Discriminator Optimal Transport ( DOT ) CONJUNCTION Discriminator Driven Latent Sampling ( DDLS ) methods. Discriminator Driven Latent Sampling ( DDLS ) methods CONJUNCTION Discriminator Optimal Transport ( DOT ). quality of generated samples EVALUATE-FOR generative models. synthetic, image, and text datasets EVALUATE-FOR DGf low. DGf low USED-FOR generative models. DGf low COMPARE Discriminator Optimal Transport ( DOT ). Discriminator Optimal Transport ( DOT ) COMPARE DGf low. quality of generated samples EVALUATE-FOR DGf low. Method are Deep generative modeling, Discriminator Gradient f low ( DGf low ), McKean - Vlasov process, and GAN variants. OtherScientificTerm are entropy - regularized f -divergences, and real and the generated data distributions. ","Deep generative modeling has been a hot topic in recent years, and this paper proposes a new technique called Discriminator Gradient f low (DGf low) to improve the generation quality of a model. The technique is based on the non-linear Fokker-Plank equation of the gradient flow, which is used to define the entropy-regularized f-divergences between the real and the generated data distributions. The authors propose a refinement approach to GANs and deep generative models such as VAEs, Normalizing Flows, and vector-valued critics. The main contribution of the paper is to propose a technique to improve generated samples from simulated samples from real-world data (e.g. images). The authors show that the proposed method, Discriminators Gradient F low (GDF low) is a generalization of the McKean-Vlasov process, and can be applied to a wide range of GAN variants.  The authors also show that existing methods such as DRS, DDLS, and wasteful sample rejection can be improved by using DGf low. Experiments are conducted on synthetic, image, and text datasets, and the authors demonstrate that the quality of generated samples generated by DGF low is comparable to or better than the quality produced by previous generative methods, e.g., the discriminator Optimal Transport (DOT) and the discretization of GGANs, and discretized discretizations of vector-valued critics, and Discrimator-discriminator (DDR).","Deep generative modeling has been a hot topic in recent years, and this paper proposes a new technique called Discriminator Gradient f low (DGf low) to improve the generation quality of a model. The technique is based on the non-linear Fokker-Plank equation of the gradient flow, which is used to define the entropy-regularized f-divergences between the real and the generated data distributions. The authors propose a refinement approach to GANs and deep generative models such as VAEs, Normalizing Flows, and vector-valued critics. The main contribution of the paper is to propose a technique to improve generated samples from simulated samples from real-world data (e.g. images). The authors show that the proposed method, Discriminators Gradient F low (GDF low) is a generalization of the McKean-Vlasov process, and can be applied to a wide range of GAN variants.  The authors also show that existing methods such as DRS, DDLS, and wasteful sample rejection can be improved by using DGf low. Experiments are conducted on synthetic, image, and text datasets, and the authors demonstrate that the quality of generated samples generated by DGF low is comparable to or better than the quality produced by previous generative methods, e.g., the discriminator Optimal Transport (DOT) and the discretization of GGANs, and discretized discretizations of vector-valued critics, and Discrimator-discriminator (DDR)."
2955,SP:74ecbc5a6d464bfa49337da9e0dd6a0fe714d4bb,encoder - only Transformer CONJUNCTION encoder - decoder Transformer. encoder - decoder Transformer CONJUNCTION encoder - only Transformer. encoder - decoder Transformer USED-FOR generation tasks. encoder - only Transformer USED-FOR understanding tasks. tasks CONJUNCTION frameworks. frameworks CONJUNCTION tasks. encoder - decoder Transformer USED-FOR understanding tasks. model architectures CONJUNCTION pre - training tasks. pre - training tasks CONJUNCTION model architectures. sub - modules USED-FOR understanding and generation tasks. inference USED-FOR understanding and generation tasks. sub - modules PART-OF Transformer block. sub - modules PART-OF VECO. innersequence and cross - sequence masked language modeling USED-FOR sub - modules. sequence labeling CONJUNCTION question answering. question answering CONJUNCTION sequence labeling. question answering CONJUNCTION sentence retrieval. sentence retrieval CONJUNCTION question answering. text classification CONJUNCTION sequence labeling. sequence labeling CONJUNCTION text classification. XTREME benchmark EVALUATE-FOR cross - lingual understanding tasks. cross - lingual understanding tasks EVALUATE-FOR VECO. text classification HYPONYM-OF XTREME benchmark. text classification HYPONYM-OF cross - lingual understanding tasks. sentence retrieval HYPONYM-OF cross - lingual understanding tasks. sequence labeling HYPONYM-OF cross - lingual understanding tasks. XTREME benchmark EVALUATE-FOR VECO. question answering HYPONYM-OF cross - lingual understanding tasks. VECO COMPARE Transformer variants. Transformer variants COMPARE VECO. VECO COMPARE cross - lingual models. cross - lingual models COMPARE VECO. cross - lingual models CONJUNCTION Transformer variants. Transformer variants CONJUNCTION cross - lingual models. generation tasks EVALUATE-FOR Transformer variants. VECO USED-FOR generation tasks. cross - lingual models USED-FOR generation tasks. Method is multilingual representations. Task is downstream cross - lingual tasks. ,"This paper proposes VECO, a multilingual Transformer-based model for cross-lingual understanding and generation tasks. The authors propose to combine the advantages of both encoder-only Transformer and encoder -decoder Transformer for understanding tasks and for generation tasks in order to achieve better performance on a variety of tasks and frameworks. The paper proposes two sub-modules in the Transformer block, which are based on innersequence and cross-sequence masked language modeling, to learn multilingual representations. The proposed model architectures and pre-training tasks are evaluated on the XTREME benchmark for a number of cross-linguistic understanding tasks (text classification, sequence labeling, question answering, sentence retrieval, etc.). The authors show that the proposed sub-modules are able to perform well on understanding and generating tasks without any inference, and can be used for other downstream cross-latent tasks as well.  The authors also show that VECE outperforms the state-of-the-art on the X-Lingual X-Text-X-Text (X-TXT) benchmark on several tasks. They also demonstrate that the sub- modules are also able to generalize well to other tasks. Finally, the authors compare the performance of the proposed model with the state of the art on a few other generation tasks, and show the performance gain on the downstream X-Txt-X task.   The main contribution of the paper is the introduction of VECo, which is an extension of Transformer to the task of multilingual understanding. The experiments are conducted on a subset of X-XT-X tasks, where the proposed by the authors. The results show that, in terms of performance, the proposed VECCOE outperform the other cross-lifelong models and other Transformer variants on the generation tasks on X-X, and that the performance gains are more pronounced on downstream X tasks. ","This paper proposes VECO, a multilingual Transformer-based model for cross-lingual understanding and generation tasks. The authors propose to combine the advantages of both encoder-only Transformer and encoder -decoder Transformer for understanding tasks and for generation tasks in order to achieve better performance on a variety of tasks and frameworks. The paper proposes two sub-modules in the Transformer block, which are based on innersequence and cross-sequence masked language modeling, to learn multilingual representations. The proposed model architectures and pre-training tasks are evaluated on the XTREME benchmark for a number of cross-linguistic understanding tasks (text classification, sequence labeling, question answering, sentence retrieval, etc.). The authors show that the proposed sub-modules are able to perform well on understanding and generating tasks without any inference, and can be used for other downstream cross-latent tasks as well.  The authors also show that VECE outperforms the state-of-the-art on the X-Lingual X-Text-X-Text (X-TXT) benchmark on several tasks. They also demonstrate that the sub- modules are also able to generalize well to other tasks. Finally, the authors compare the performance of the proposed model with the state of the art on a few other generation tasks, and show the performance gain on the downstream X-Txt-X task.   The main contribution of the paper is the introduction of VECo, which is an extension of Transformer to the task of multilingual understanding. The experiments are conducted on a subset of X-XT-X tasks, where the proposed by the authors. The results show that, in terms of performance, the proposed VECCOE outperform the other cross-lifelong models and other Transformer variants on the generation tasks on X-X, and that the performance gains are more pronounced on downstream X tasks. "
2980,SP:3d177ad50727d1a2619b68ab8a897b79d8652beb,"intrinsic motivation USED-FOR Reinforcement Learning ( RL ). K - means USED-FOR auditory event clusters. neural network USED-FOR auditory events. prediction errors USED-FOR intrinsic rewards. Atari games USED-FOR module. model USED-FOR audio - visual exploration. model USED-FOR active learning. Habitat simulator CONJUNCTION active learning. active learning CONJUNCTION Habitat simulator. Habitat simulator USED-FOR model. Habitat simulator USED-FOR audio - visual exploration. ThreeDWorld ( TDW ) simulator USED-FOR active learning. audio signals USED-FOR intrinsic rewards. vision - based models USED-FOR intrinsic rewards. vision - based models USED-FOR RL explorations. audio signals USED-FOR RL explorations. audio signals COMPARE vision - based models. vision - based models COMPARE audio signals. Task are causal understanding of the physical world, and RL exploration. Method is auditory event prediction. Material is acoustic data. ","This paper proposes a new intrinsic motivation for Reinforcement Learning (RL) based on a causal understanding of the physical world. The authors propose to use auditory event prediction as an intrinsic reward for RL exploration. They use K-means to predict auditory event clusters and use a neural network to predict the auditory events. They show that the prediction errors are correlated with the intrinsic rewards. They also propose a module that uses Atari games to train the model for audio-visual exploration using Habitat simulator and active learning on the ThreeDWorld (TDW) simulator. They demonstrate that the audio signals are more informative for intrinsic rewards than vision-based models for RL explorations, and that the model is able to learn from audio signals more efficiently compared to the state of the art.","This paper proposes a new intrinsic motivation for Reinforcement Learning (RL) based on a causal understanding of the physical world. The authors propose to use auditory event prediction as an intrinsic reward for RL exploration. They use K-means to predict auditory event clusters and use a neural network to predict the auditory events. They show that the prediction errors are correlated with the intrinsic rewards. They also propose a module that uses Atari games to train the model for audio-visual exploration using Habitat simulator and active learning on the ThreeDWorld (TDW) simulator. They demonstrate that the audio signals are more informative for intrinsic rewards than vision-based models for RL explorations, and that the model is able to learn from audio signals more efficiently compared to the state of the art."
3005,SP:014f6118ebe55ece6be23c3a10f12e4591e444b1,singleand multimodal data USED-FOR category discovery. end - to - end framework USED-FOR representation. unlabelled data USED-FOR clusters. it USED-FOR labelled and unlabelled data. noise - contrastive estimation USED-FOR self - supervised representation learning. category discrimination CONJUNCTION cross - modal discrimination. cross - modal discrimination CONJUNCTION category discrimination. instance discrimination USED-FOR contrastive learning approaches. cross - modal discrimination USED-FOR instance discrimination. category discrimination USED-FOR instance discrimination. cross - modal discrimination USED-FOR multi - modal data. category discrimination CONJUNCTION labelled data. labelled data CONJUNCTION category discrimination. pairwise pseudo labels USED-FOR unlabelled data. pairwise pseudo labels USED-FOR cluster assignments. CIFAR100 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR100. CIFAR10 CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION CIFAR10. Kinetics-400 CONJUNCTION VGG - Sound. VGG - Sound CONJUNCTION Kinetics-400. image benchmarks CONJUNCTION CIFAR100. CIFAR100 CONJUNCTION image benchmarks. CIFAR10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR10. CIFAR10 HYPONYM-OF image benchmarks. ImageNet HYPONYM-OF image benchmarks. image benchmarks EVALUATE-FOR framework. OtherScientificTerm is shared representation space. ,"This paper proposes an end-to-end framework for learning a representation for singleand multimodal data for the task of category discovery. The key idea is to learn a shared representation space, and then use it for both labelled and unlabelled data for self-supervised representation learning based on noise-contrastive estimation. The idea is that instance discrimination based on category discrimination and cross-modal discrimination can be used to improve contrastive learning approaches. The proposed framework is evaluated on several image benchmarks (CIFAR10, CIFAR100, and ImageNet), including Kinetics-400 and VGG-Sound, and the results show that the proposed framework achieves state-of-the-art performance.    The main contribution of the paper is to use pairwise pseudo labels for cluster assignments based on the cluster assignments of the unlabeled data, and to use category discrimination for the labelled data, as well as the cross modal discrimination for multi-modual data. ","This paper proposes an end-to-end framework for learning a representation for singleand multimodal data for the task of category discovery. The key idea is to learn a shared representation space, and then use it for both labelled and unlabelled data for self-supervised representation learning based on noise-contrastive estimation. The idea is that instance discrimination based on category discrimination and cross-modal discrimination can be used to improve contrastive learning approaches. The proposed framework is evaluated on several image benchmarks (CIFAR10, CIFAR100, and ImageNet), including Kinetics-400 and VGG-Sound, and the results show that the proposed framework achieves state-of-the-art performance.    The main contribution of the paper is to use pairwise pseudo labels for cluster assignments based on the cluster assignments of the unlabeled data, and to use category discrimination for the labelled data, as well as the cross modal discrimination for multi-modual data. "
3030,SP:4df640f502e88ddba2d7e183625231d70b083e82,"image - level tags CONJUNCTION object bounding boxes. object bounding boxes CONJUNCTION image - level tags. image - level tags HYPONYM-OF partial annotations. object bounding boxes HYPONYM-OF partial annotations. broad region coverage FEATURE-OF sparse annotations. Class activation maps USED-FOR coarse labels. conditional random fields USED-FOR sparse labels. Class activation maps USED-FOR segmentation model. Existing methods USED-FOR weak supervision. Class activation maps CONJUNCTION conditional random fields. conditional random fields CONJUNCTION Class activation maps. semi - supervised metric learning problem USED-FOR weakly supervised segmentation. semantic annotation CONJUNCTION co - occurrence. co - occurrence CONJUNCTION semantic annotation. low - level image similarity CONJUNCTION semantic annotation. semantic annotation CONJUNCTION low - level image similarity. co - occurrence CONJUNCTION feature affinity. feature affinity CONJUNCTION co - occurrence. contrastive relationships USED-FOR low - level image similarity. contrastive relationships USED-FOR semantic annotation. partial annotations USED-FOR pixel - wise feature. data - driven grouping CONJUNCTION discriminative feature learning. discriminative feature learning CONJUNCTION data - driven grouping. Pascal VOC EVALUATE-FOR universal weakly supervised segmenter. Task are Weakly supervised segmentation, and pixel localization. Generic are task, and code. OtherScientificTerm are coarse annotations, feature space, and priors. ","Weakly supervised segmentation is an important problem in image classification. Existing methods for weak supervision rely on the assumption that coarse annotations are available for each pixel in the image. However, this task can be seen as a semi-supervised metric learning problem, where the sparse annotations have a broad region coverage. The paper proposes to use partial annotations (e.g., image-level tags and object bounding boxes) for the task. Class activation maps for coarse labels and conditional random fields for the sparse labels are used to train a segmentation model. The authors show that the proposed method is able to achieve state-of-the-art performance on Pascal VOC.    The paper also proposes a new way of pixel localization based on partial annotations. The idea is to use contrastive relationships between low-level image similarity, semantic annotation, co-occurrence, and feature affinity. This is done by learning a pixel-wise feature from the partial annotations, and then using data-driven grouping and discriminative feature learning. The code is well-written and easy to follow, and the experiments show that this universal weakly supervised classifier can be trained on any dataset (i.e., any feature space).  ","Weakly supervised segmentation is an important problem in image classification. Existing methods for weak supervision rely on the assumption that coarse annotations are available for each pixel in the image. However, this task can be seen as a semi-supervised metric learning problem, where the sparse annotations have a broad region coverage. The paper proposes to use partial annotations (e.g., image-level tags and object bounding boxes) for the task. Class activation maps for coarse labels and conditional random fields for the sparse labels are used to train a segmentation model. The authors show that the proposed method is able to achieve state-of-the-art performance on Pascal VOC.    The paper also proposes a new way of pixel localization based on partial annotations. The idea is to use contrastive relationships between low-level image similarity, semantic annotation, co-occurrence, and feature affinity. This is done by learning a pixel-wise feature from the partial annotations, and then using data-driven grouping and discriminative feature learning. The code is well-written and easy to follow, and the experiments show that this universal weakly supervised classifier can be trained on any dataset (i.e., any feature space).  "
3055,SP:f7d6099adb40a0ce2f8a3563dbd5207cf1fdea0f,"slow convergence FEATURE-OF pretext task. small scale models COMPARE supervised counterpart. supervised counterpart COMPARE small scale models. distillation strategy USED-FOR unsupervised learning. ResNet-18 CONJUNCTION ResNet-34. ResNet-34 CONJUNCTION ResNet-18. top-1 accuracies EVALUATE-FOR linear evaluation. BINGO COMPARE baselines. baselines COMPARE BINGO. ImageNet EVALUATE-FOR linear evaluation. small scale models EVALUATE-FOR BINGO. top-1 accuracies EVALUATE-FOR baselines. top-1 accuracies EVALUATE-FOR BINGO. Method are self - supervised learning, contrastive learning based methods, and distillation. Task are optimization, and Bag of InstaNces aGgregatiOn. Generic is method. OtherScientificTerm is bag of instances. ","This paper studies the problem of self-supervised learning in the context of slow convergence of the pretext task. The authors propose a distillation strategy for unsupervised training that aims to improve the performance of small scale models compared to their supervised counterpart. The proposed method, called BINGO, is based on contrastive learning based methods, where the optimization is performed on a Bag of InstaNces aGgregatiOn, which is a bag of instances that are selected to be used for distillation. Experiments on top-1 accuracies on linear evaluation on ImageNet with ResNet-18 and ResNet+34 are conducted, showing that the proposed method outperforms the baselines in terms of the number of samples and the top 1% of top accuracy. ","This paper studies the problem of self-supervised learning in the context of slow convergence of the pretext task. The authors propose a distillation strategy for unsupervised training that aims to improve the performance of small scale models compared to their supervised counterpart. The proposed method, called BINGO, is based on contrastive learning based methods, where the optimization is performed on a Bag of InstaNces aGgregatiOn, which is a bag of instances that are selected to be used for distillation. Experiments on top-1 accuracies on linear evaluation on ImageNet with ResNet-18 and ResNet+34 are conducted, showing that the proposed method outperforms the baselines in terms of the number of samples and the top 1% of top accuracy. "
3071,SP:328866aad6544c81ded8980934df31dc4472435f,"Simulation - based inference ( SBI ) HYPONYM-OF statistical inference. stochastic models USED-FOR statistical inference. SBI algorithms COMPARE generative adversarial networks ( GANs ). generative adversarial networks ( GANs ) COMPARE SBI algorithms. adversarial approach USED-FOR SBI. GATSBI HYPONYM-OF adversarial approach. SBI CONJUNCTION GANs. GANs CONJUNCTION SBI. GATSBI USED-FOR variational objective. variational objective USED-FOR implicit posterior distributions. adversarial setting FEATURE-OF variational objective. high - dimensional posterior spaces USED-FOR Inference. GATSBI USED-FOR Inference. implicit priors USED-FOR Inference. SBI benchmark problems CONJUNCTION high - dimensional simulators. high - dimensional simulators CONJUNCTION SBI benchmark problems. SBI benchmark problems EVALUATE-FOR GATSBI. high - dimensional simulators EVALUATE-FOR GATSBI. GATSBI USED-FOR well - calibrated posterior estimates. model USED-FOR wave propagation. surface of a shallow water body FEATURE-OF wave propagation. high dimensions FEATURE-OF well - calibrated posterior estimates. it USED-FOR high - dimensional posterior. it COMPARE SBI approach. SBI approach COMPARE it. model of camera optics USED-FOR it. implicit prior USED-FOR it. implicit prior USED-FOR high - dimensional posterior. GATSBI USED-FOR sequential posterior estimation. GANs USED-FOR Bayesian inference. GATSBI USED-FOR Bayesian inference. high - dimensional simulation - based models USED-FOR Bayesian inference. GANs USED-FOR GATSBI. OtherScientificTerm are likelihoods, and explicit likelihoods. ","Simulation-based inference (SBI) is a special case of statistical inference in stochastic models where the likelihoods of the posterior distribution of the data are assumed to be high-dimensional. Previous SBI algorithms have been compared with generative adversarial networks (GANs) and GANs, and GATSBI is an adversarial approach to SBI. Inference with implicit priors has been shown to be more efficient than using explicit likelihoods. In this paper, the authors show that the variational objective for implicit posterior distributions in the adversarial setting is the same as that of SBI and GGANs.    Inference in high-dimensionality is achieved by using the implicit posterior of a model of a wave propagation on the surface of a shallow water body.  The authors show empirically that the GATSBI is able to obtain well-calibrated posterior estimates in high dimensions, and that GATs are able to achieve similar performance as SBI on standard SBI benchmark problems as well as high-dimensions simulators.  In addition, they also show that Gatsby can be used for sequential posterior estimation in the presence of a high-divergence between the model and the true posterior.  Finally, they show that using high- dimensional simulation-based models for Bayesian inference with GAN is also possible, and show that it can be applied to the case where the model of camera optics is used as an implicit prior, and it can approximate the true one. ","Simulation-based inference (SBI) is a special case of statistical inference in stochastic models where the likelihoods of the posterior distribution of the data are assumed to be high-dimensional. Previous SBI algorithms have been compared with generative adversarial networks (GANs) and GANs, and GATSBI is an adversarial approach to SBI. Inference with implicit priors has been shown to be more efficient than using explicit likelihoods. In this paper, the authors show that the variational objective for implicit posterior distributions in the adversarial setting is the same as that of SBI and GGANs.    Inference in high-dimensionality is achieved by using the implicit posterior of a model of a wave propagation on the surface of a shallow water body.  The authors show empirically that the GATSBI is able to obtain well-calibrated posterior estimates in high dimensions, and that GATs are able to achieve similar performance as SBI on standard SBI benchmark problems as well as high-dimensions simulators.  In addition, they also show that Gatsby can be used for sequential posterior estimation in the presence of a high-divergence between the model and the true posterior.  Finally, they show that using high- dimensional simulation-based models for Bayesian inference with GAN is also possible, and show that it can be applied to the case where the model of camera optics is used as an implicit prior, and it can approximate the true one. "
3087,SP:2915e82097eae4eb8546dc500f32b3ec37e3766f,"latent variable USED-FOR prognostic score. prognostic score USED-FOR biostatistics. prognostic score USED-FOR TEs. model USED-FOR individualized treatment effects. latent variable USED-FOR prognostic score. method COMPARE methods. methods COMPARE method. ( semi-)synthetic datasets EVALUATE-FOR method. ( semi-)synthetic datasets EVALUATE-FOR methods. Task is causal inference. OtherScientificTerm are limited overlap, features, TE error bounds, and individualized features. Method are generative prognostic model, and variational autoencoder ( VAE ). ","This paper considers the problem of causal inference in the setting where there is limited overlap between treatment effects and individualized treatment effects. The authors propose a generative prognostic model, where a prognostic score is computed from a latent variable for each treatment, and a variational autoencoder (VAE) is used to learn the features of the treatment and the individualized features. They show that the prognostic scores of TEs can be used as a surrogate for the true TEs. They also provide TE error bounds for the generative model. Finally, the authors show that their model can also be used to estimate individualised treatment effects, and that their method outperforms existing methods on (semi-)synthetic datasets. ","This paper considers the problem of causal inference in the setting where there is limited overlap between treatment effects and individualized treatment effects. The authors propose a generative prognostic model, where a prognostic score is computed from a latent variable for each treatment, and a variational autoencoder (VAE) is used to learn the features of the treatment and the individualized features. They show that the prognostic scores of TEs can be used as a surrogate for the true TEs. They also provide TE error bounds for the generative model. Finally, the authors show that their model can also be used to estimate individualised treatment effects, and that their method outperforms existing methods on (semi-)synthetic datasets. "
3103,SP:ca358c9f36aac6e58ed1b3949c349d210c49a48e,"benchmark tasks PART-OF RL. RL algorithms USED-FOR episodic simulated environments. RL algorithms USED-FOR them. real - world platforms USED-FOR them. robots HYPONYM-OF real - world platforms. framework PART-OF simulated benchmark EARL1. simulated tasks PART-OF simulated benchmark EARL1. algorithms USED-FOR reinforcement learning. approaches USED-FOR episodic RL. approaches COMPARE approaches. approaches COMPARE approaches. autonomy FEATURE-OF algorithms. Method are Reinforcement learning ( RL ), and real - world embodied learning. OtherScientificTerm are human supervision, extrinsic intervention, and interventions. ","This paper proposes a new benchmark task for RL, called Embedding Reinforcement learning (EARL1), which is a collection of tasks that require human supervision. The authors present a set of RL algorithms for learning episodic simulated environments and evaluate them on a number of real-world platforms (including robots). The framework is built on top of the simulated benchmark EARL1, which consists of several simulated tasks where the goal is to learn without extrinsic intervention, and to learn with interventions. The paper shows that the proposed algorithms for reinforcement learning outperform existing approaches for episodic RL in terms of autonomy, and outperform other approaches that do not require human intervention. This is an interesting contribution to the field, and it is a good contribution to a growing body of work in the field that aims to address the problem of real world embodied learning.","This paper proposes a new benchmark task for RL, called Embedding Reinforcement learning (EARL1), which is a collection of tasks that require human supervision. The authors present a set of RL algorithms for learning episodic simulated environments and evaluate them on a number of real-world platforms (including robots). The framework is built on top of the simulated benchmark EARL1, which consists of several simulated tasks where the goal is to learn without extrinsic intervention, and to learn with interventions. The paper shows that the proposed algorithms for reinforcement learning outperform existing approaches for episodic RL in terms of autonomy, and outperform other approaches that do not require human intervention. This is an interesting contribution to the field, and it is a good contribution to a growing body of work in the field that aims to address the problem of real world embodied learning."
3119,SP:abe51d4a9817c08f0abde5da0bb8e6ca4e02e7cf,"Question Answering ( QA ) HYPONYM-OF AI and NLP fields. human - level reasoning capability FEATURE-OF QA systems. modules USED-FOR reasoning. QA systems USED-FOR human reasoning process. modules USED-FOR QA systems. Graph Neural Networks ( GNNs ) USED-FOR modules. knowledge graphs ( KGs ) USED-FOR reasoning. pre - trained language models ( LMs ) USED-FOR QA systems. reasoning functionality FEATURE-OF GNN - based modules. GNN - based modules USED-FOR reasoning process. they USED-FOR QA. GNN modules USED-FOR QA. reasoning capability FEATURE-OF GNN modules. CommonsenseQA CONJUNCTION OpenBookQA. OpenBookQA CONJUNCTION CommonsenseQA. graph neural counter COMPARE GNN modules. GNN modules COMPARE graph neural counter. OpenBookQA HYPONYM-OF QA benchmark datasets. knowledge - aware reasoning USED-FOR QA benchmark datasets. OpenBookQA EVALUATE-FOR GNN modules. CommonsenseQA EVALUATE-FOR GNN modules. knowledge - aware GNN modules USED-FOR reasoning. counting HYPONYM-OF reasoning. reasoning modules USED-FOR knowledge - powered QA. Method are LMs, and GNN. ","This paper investigates the use of Graph Neural Networks (GNNs) for the task of QA, a popular topic in AI and NLP fields such as Question Answering (QA) and QA systems with human-level reasoning capability. The authors propose two modules to improve the reasoning capability of existing state-of-the-art QA system by using knowledge graphs (KGs). The modules are built on top of existing GNNs and are called ""knowledge-aware GNN"" (KG-GNN).  The authors show that GNN-based modules have reasoning functionality and that they can be used to improve QA performance. They also show that pre-trained language models (LMs) are able to learn to reason about the reasoning capabilities of the GNN.   The main contribution of the paper is that the authors propose GNN modules for QA that have reasoning capability that is comparable to that of LMs, and they show that the reasoning process of a GNN can be understood as a graph neural counter.  The paper also shows that the graph-based reasoning modules can be applied to existing QA benchmark datasets such as CommonsenseQA and OpenBookQA, and show that they improve the performance of GNN for reasoning in QA.  Finally, the paper shows that knowledge-aware graph neural modules can also be used in the context of knowledge-powered QA and demonstrate that they perform better than the existing state of the art in reasoning (e.g. counting).","This paper investigates the use of Graph Neural Networks (GNNs) for the task of QA, a popular topic in AI and NLP fields such as Question Answering (QA) and QA systems with human-level reasoning capability. The authors propose two modules to improve the reasoning capability of existing state-of-the-art QA system by using knowledge graphs (KGs). The modules are built on top of existing GNNs and are called ""knowledge-aware GNN"" (KG-GNN).  The authors show that GNN-based modules have reasoning functionality and that they can be used to improve QA performance. They also show that pre-trained language models (LMs) are able to learn to reason about the reasoning capabilities of the GNN.   The main contribution of the paper is that the authors propose GNN modules for QA that have reasoning capability that is comparable to that of LMs, and they show that the reasoning process of a GNN can be understood as a graph neural counter.  The paper also shows that the graph-based reasoning modules can be applied to existing QA benchmark datasets such as CommonsenseQA and OpenBookQA, and show that they improve the performance of GNN for reasoning in QA.  Finally, the paper shows that knowledge-aware graph neural modules can also be used in the context of knowledge-powered QA and demonstrate that they perform better than the existing state of the art in reasoning (e.g. counting)."
3135,SP:3ea5a38e7fcd9111dcd299ad039b634e2781685f,pruning CONJUNCTION quantization. quantization CONJUNCTION pruning. pruning HYPONYM-OF Deep Neural Networks ( DNN ) compression. quantization HYPONYM-OF Deep Neural Networks ( DNN ) compression. low - cost devices USED-FOR them. performance CONJUNCTION space consumption. space consumption CONJUNCTION performance. three - stage framework USED-FOR DNN inference. Succinct Compression HYPONYM-OF three - stage framework. Succinct Compression USED-FOR DNN inference. near - optimal compression FEATURE-OF DNN inference. Succinct Data Structures USED-FOR fast queries. compressed representation USED-FOR fast queries. Succinct Data Structures USED-FOR method. method USED-FOR DNN models. formulations USED-FOR DNN models. Element - wise or Block - wise manner FEATURE-OF formulations. method USED-FOR transformed DNN models. Succinct Data Structures USED-FOR method. Succinct Data Structures USED-FOR transformed DNN models. execution pipelines USED-FOR model formulations. method USED-FOR DNN inference. execution pipelines USED-FOR method. method COMPARE Huffman Coding. Huffman Coding COMPARE method. AlexNet / VGG-16 inference EVALUATE-FOR Huffman Coding. near - optimal compression FEATURE-OF method. AlexNet / VGG-16 inference EVALUATE-FOR method. Pruning CONJUNCTION Quantization. Quantization CONJUNCTION Pruning. method COMPARE Quantization. Quantization COMPARE method. method CONJUNCTION Pruning. Pruning CONJUNCTION method. Generic is techniques. OtherScientificTerm is inference runtime. ,"This paper proposes Succinct Compression, a three-stage framework for Deep Neural Networks (DNN) compression (pruning, quantization, and quantization) that aims to compress them on low-cost devices and reduce the performance and space consumption. The proposed method is based on the use of the recently proposed Succint Data Structures (SDS) to perform fast queries in the compressed representation. The authors propose a method to compress DNN models based on Suffinct Data Structured (Succinct) formulations for DNNs in an Element-wise or Block-wise manner. The method uses execution pipelines to train the model formulations and then applies the method to transform the transformed DNN into a single DNN model. Experiments on AlexNet/VGG-16 inference show that the proposed method achieves near-optimal compression compared to Quantization, Pruning, and Quantization. The paper also shows that the method outperforms Huffman Coding in terms of inference runtime.  ","This paper proposes Succinct Compression, a three-stage framework for Deep Neural Networks (DNN) compression (pruning, quantization, and quantization) that aims to compress them on low-cost devices and reduce the performance and space consumption. The proposed method is based on the use of the recently proposed Succint Data Structures (SDS) to perform fast queries in the compressed representation. The authors propose a method to compress DNN models based on Suffinct Data Structured (Succinct) formulations for DNNs in an Element-wise or Block-wise manner. The method uses execution pipelines to train the model formulations and then applies the method to transform the transformed DNN into a single DNN model. Experiments on AlexNet/VGG-16 inference show that the proposed method achieves near-optimal compression compared to Quantization, Pruning, and Quantization. The paper also shows that the method outperforms Huffman Coding in terms of inference runtime.  "
3151,SP:94c395afc794a9cc163e362078769ff83f3d20d0,"training method USED-FOR tiny neural networks. Network Augmentation ( NetAug ) HYPONYM-OF training method. data augmentation CONJUNCTION dropout. dropout CONJUNCTION data augmentation. noise USED-FOR over - fitting. regularization techniques USED-FOR large neural networks. dropout HYPONYM-OF regularization techniques. data augmentation HYPONYM-OF regularization techniques. techniques USED-FOR tiny neural networks. tiny models COMPARE large models. large models COMPARE tiny models. under - fitting CONJUNCTION over - fitting. over - fitting CONJUNCTION under - fitting. NetAug USED-FOR network ( reverse dropout ). It USED-FOR tiny model. tiny model PART-OF larger models. tiny model USED-FOR inference. image classification CONJUNCTION object detection. object detection CONJUNCTION image classification. NetAug USED-FOR image classification. NetAug USED-FOR object detection. ImageNet CONJUNCTION Cars. Cars CONJUNCTION ImageNet. NetAug USED-FOR tiny models. ImageNet EVALUATE-FOR NetAug. Pascal VOC EVALUATE-FOR NetAug. computational cost EVALUATE-FOR NetAug. Generic are model, and it. OtherScientificTerm are limited capacity, network, supervision, and inference overhead. Method is independent model. ","This paper proposes a new training method called Network Augmentation (NetAug) to train tiny neural networks with limited capacity. Previous regularization techniques, such as data augmentation and dropout, are commonly used to train large neural networks, but the authors argue that these techniques are not effective for training tiny networks due to the over-fitting caused by noise in the training data. They argue that the reason is that the tiny models trained with these techniques tend to underfit and over-fit compared to large models. To address this issue, the authors propose to augment the network (reverse dropout) trained with NetAug.    The idea is that a tiny model is trained with the same number of layers as a large model, but with a different number of weights. It is assumed that the network is trained in a self-supervised manner, where each layer of the network receives no supervision. The authors show that NetAug can be used to augment any network (up to a constant factor) and that it can be applied to any tiny model in larger models. They also show that the inference of the tiny model can be performed in parallel with the inference overhead of the larger model.  The authors evaluate the performance of NetAug on image classification and object detection on ImageNet, Cars, and Pascal VOC. NetAug is shown to outperform existing methods in terms of performance and computational cost for training these tiny models, and to be effective for under-fitting, over-fitting, and inference overhead.","This paper proposes a new training method called Network Augmentation (NetAug) to train tiny neural networks with limited capacity. Previous regularization techniques, such as data augmentation and dropout, are commonly used to train large neural networks, but the authors argue that these techniques are not effective for training tiny networks due to the over-fitting caused by noise in the training data. They argue that the reason is that the tiny models trained with these techniques tend to underfit and over-fit compared to large models. To address this issue, the authors propose to augment the network (reverse dropout) trained with NetAug.    The idea is that a tiny model is trained with the same number of layers as a large model, but with a different number of weights. It is assumed that the network is trained in a self-supervised manner, where each layer of the network receives no supervision. The authors show that NetAug can be used to augment any network (up to a constant factor) and that it can be applied to any tiny model in larger models. They also show that the inference of the tiny model can be performed in parallel with the inference overhead of the larger model.  The authors evaluate the performance of NetAug on image classification and object detection on ImageNet, Cars, and Pascal VOC. NetAug is shown to outperform existing methods in terms of performance and computational cost for training these tiny models, and to be effective for under-fitting, over-fitting, and inference overhead."
3167,SP:9c24549b980e415616f818acbf4cf680ef8edb52,"Point cloud sequence HYPONYM-OF data representation. flexible shape and motion information FEATURE-OF data representation. model USED-FOR temporally coherent feature spaces. real - world environments FEATURE-OF point correspondence information. generator USED-FOR temporally coherent output. point cloud sequence USED-FOR temporal coherence. learnable masking module USED-FOR upsampling ratio. point distribution USED-FOR learnable masking module. point distribution USED-FOR upsampling ratio. fluid dynamical system CONJUNCTION human action scanned data. human action scanned data CONJUNCTION fluid dynamical system. particles CONJUNCTION human action scanned data. human action scanned data CONJUNCTION particles. particles PART-OF fluid dynamical system. domains FEATURE-OF point cloud sequences. particles HYPONYM-OF point cloud sequences. fluid dynamical system HYPONYM-OF domains. human action scanned data HYPONYM-OF domains. particles HYPONYM-OF domains. quantitative and qualitative evaluation EVALUATE-FOR method. quantitative and qualitative evaluation EVALUATE-FOR upsampling task. method USED-FOR temporal coherence. quantitative and qualitative evaluation EVALUATE-FOR learning temporal coherence. upsampling task CONJUNCTION learning temporal coherence. learning temporal coherence CONJUNCTION upsampling task. irregular point cloud sequences USED-FOR temporal coherence. upsampling task EVALUATE-FOR method. OtherScientificTerm are scene flow information, and point correspondence annotation. Material is dynamic point cloud sequences. ","This paper proposes a new data representation called Point cloud sequence, which can capture flexible shape and motion information. The authors propose a model to learn temporally coherent feature spaces, where scene flow information is encoded in the form of dynamic point cloud sequences. The point correspondence information in real-world environments can be represented as a sequence of point correspondence annotations. The generator is trained to produce a temporalally coherent output for each point cloud sequence. A learnable masking module is used to learn the upsampling ratio based on the point distribution. The proposed method is evaluated on three different domains: particles in a fluid dynamical system, human action scanned data, and a series of environments with dynamic point correspondence annotation. The quantitative and qualitative evaluation shows that the proposed method achieves state-of-the-art performance on both the downsampling task and learning temporal coherence on irregular point cloud sequential sequences.","This paper proposes a new data representation called Point cloud sequence, which can capture flexible shape and motion information. The authors propose a model to learn temporally coherent feature spaces, where scene flow information is encoded in the form of dynamic point cloud sequences. The point correspondence information in real-world environments can be represented as a sequence of point correspondence annotations. The generator is trained to produce a temporalally coherent output for each point cloud sequence. A learnable masking module is used to learn the upsampling ratio based on the point distribution. The proposed method is evaluated on three different domains: particles in a fluid dynamical system, human action scanned data, and a series of environments with dynamic point correspondence annotation. The quantitative and qualitative evaluation shows that the proposed method achieves state-of-the-art performance on both the downsampling task and learning temporal coherence on irregular point cloud sequential sequences."
3183,SP:67efe60ad37807505369b7852bc0abed29ffdda8,"robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. pre - training USED-FOR detection transformers. it USED-FOR object detection. task adapter USED-FOR it. textual prompts USED-FOR NLP. query positional embeddings USED-FOR model. visual prompts USED-FOR model. visual prompts USED-FOR query positional embeddings. self - attention USED-FOR task adapter. COCO dataset EVALUATE-FOR PT - DETR. it COMPARE detection transformers. detection transformers COMPARE it. robustness CONJUNCTION generalization. generalization CONJUNCTION robustness. generalization EVALUATE-FOR detection transformers. small - size datasets EVALUATE-FOR detection transformers. generalization FEATURE-OF small - size datasets. generalization EVALUATE-FOR it. robustness EVALUATE-FOR it. small - size datasets EVALUATE-FOR it. Method are Large - scale pre - training, 12 - layer transformer, FP - DETR, and encoder - only transformer. OtherScientificTerm are separated training paradigm, and common corruptions. Task is upstream and downstream tasks. Generic is method. ","This paper studies the problem of robustness and generalization in pre-training for detection transformers. The authors propose a new 12-layer transformer, called FP-DETR, and show that it can be used for object detection, and it can also be used as a task adapter for NLP with textual prompts. Specifically, query positional embeddings from visual prompts are used to train the model with the help of visual prompts, and the task adapter is trained with self-attention. The proposed method is evaluated on the COCO dataset and shows that it outperforms the state-of-the-art robustness, generalization, and robustness on small-size datasets. The paper also shows that the separation of upstream and downstream tasks in the separated training paradigm is beneficial, and that the encoder-only transformer is more robust to common corruptions.","This paper studies the problem of robustness and generalization in pre-training for detection transformers. The authors propose a new 12-layer transformer, called FP-DETR, and show that it can be used for object detection, and it can also be used as a task adapter for NLP with textual prompts. Specifically, query positional embeddings from visual prompts are used to train the model with the help of visual prompts, and the task adapter is trained with self-attention. The proposed method is evaluated on the COCO dataset and shows that it outperforms the state-of-the-art robustness, generalization, and robustness on small-size datasets. The paper also shows that the separation of upstream and downstream tasks in the separated training paradigm is beneficial, and that the encoder-only transformer is more robust to common corruptions."
3199,SP:a1f9897496303984fc7ad469222106b14b4a6233,"Federated Averaging ( FedAvg HYPONYM-OF federated learning algorithm. FedPAGE HYPONYM-OF federated learning algorithm. communication complexity EVALUATE-FOR FedPAGE. optimal PAGE method USED-FOR federated learning algorithm. optimal PAGE method USED-FOR FedPAGE. local methods USED-FOR federated convex and nonconvex optimization. FedPAGE COMPARE local methods. local methods COMPARE FedPAGE. FedPAGE USED-FOR federated convex and nonconvex optimization. communication rounds USED-FOR FedPAGE. communication rounds FEATURE-OF FedPAGE. nonconvex setting FEATURE-OF FedPAGE. number of communication rounds EVALUATE-FOR FedPAGE. FedPAGE CONJUNCTION SCAFFOLD. SCAFFOLD CONJUNCTION FedPAGE. FedPAGE USED-FOR federated convex and nonconvex optimization. communication complexity EVALUATE-FOR FedPAGE. Method are Local - SGD ), local SGD steps, and federated learning. Task is convex setting. OtherScientificTerm are communication round, and communication cost. ","This paper proposes a new federated learning algorithm, FedPAGE, which is a variant of Federated Averaging (FedAvg) that is based on the optimal PAGE method. In the convex setting, the authors show that the communication cost of the optimal algorithm (called Local-SGD) is $O(\sqrt{T}/T}$, where $T$ is the number of local SGD steps, $T=T$ the communication round, and $T(T) = T$. In the nonconvex setting (where $T = T$, $T+T$), the authors prove that the optimal Page method is optimal for the federated convex learning algorithm. The authors also show that in the case of convex case, the communication complexity of FedPAAGE is O(1/T) times smaller than that of FedAvg.  The authors then show that FedPAEG can achieve the same communication complexity as FedAvg with fewer communication rounds than FedAvg in a convex federated setting and O(O(T^2) times less communication rounds in a nonconvergent setting.  Finally, they show that under certain assumptions on the communication costs of the clients (i.e., that the clients do not have to share the same number of communication rounds as the clients), FedPAERGE can achieve a communication cost as low as $O(1 / T)$, which is the optimal for convex cases.   The main contribution of the paper is that the authors provide a theoretical analysis of the communication efficiency of the FedPAEg and FedPAG. They show that, under some assumptions, the optimal FL algorithm (FedPAGE) can achieve communication complexity O(T/T). They also provide an empirical analysis that shows that FedGE and FedPGD are both competitive with local methods for federated concave convex and nonconformity, and that FedPGA and FedSGD are competitive with the optimal optimal FL algorithms. Finally, in the non-conformist setting, they provide an experimental analysis that compares FedGE to FedAvg, FedAvg and SCAFFOLD and show that their algorithm is more efficient. ","This paper proposes a new federated learning algorithm, FedPAGE, which is a variant of Federated Averaging (FedAvg) that is based on the optimal PAGE method. In the convex setting, the authors show that the communication cost of the optimal algorithm (called Local-SGD) is $O(\sqrt{T}/T}$, where $T$ is the number of local SGD steps, $T=T$ the communication round, and $T(T) = T$. In the nonconvex setting (where $T = T$, $T+T$), the authors prove that the optimal Page method is optimal for the federated convex learning algorithm. The authors also show that in the case of convex case, the communication complexity of FedPAAGE is O(1/T) times smaller than that of FedAvg.  The authors then show that FedPAEG can achieve the same communication complexity as FedAvg with fewer communication rounds than FedAvg in a convex federated setting and O(O(T^2) times less communication rounds in a nonconvergent setting.  Finally, they show that under certain assumptions on the communication costs of the clients (i.e., that the clients do not have to share the same number of communication rounds as the clients), FedPAERGE can achieve a communication cost as low as $O(1 / T)$, which is the optimal for convex cases.   The main contribution of the paper is that the authors provide a theoretical analysis of the communication efficiency of the FedPAEg and FedPAG. They show that, under some assumptions, the optimal FL algorithm (FedPAGE) can achieve communication complexity O(T/T). They also provide an empirical analysis that shows that FedGE and FedPGD are both competitive with local methods for federated concave convex and nonconformity, and that FedPGA and FedSGD are competitive with the optimal optimal FL algorithms. Finally, in the non-conformist setting, they provide an experimental analysis that compares FedGE to FedAvg, FedAvg and SCAFFOLD and show that their algorithm is more efficient. "
3215,SP:81e74765abc6524edd8fdf9a3ba107d7bddaa04b,"mathematical operations USED-FOR Artificial neural networks ( ANNs ). networks USED-FOR adversarial input perturbations. decision boundary geometry FEATURE-OF ANN classifiers. adversarial perturbations USED-FOR decision boundary geometry. adversarial subspace COMPARE random subspace. random subspace COMPARE adversarial subspace. adversarial attacks PART-OF training procedure. redistribution of proximal class labels CONJUNCTION boundary curvature. boundary curvature CONJUNCTION redistribution of proximal class labels. boundary distance CONJUNCTION redistribution of proximal class labels. redistribution of proximal class labels CONJUNCTION boundary distance. Generic are network, and analysis. OtherScientificTerm are adversarial subspaces, minimal perturbation, decision boundary, boundary, and dimensionality of the subspace. Task is test - time adversarial attacks. Method is adversarial training. ","This paper studies the decision boundary geometry of ANN classifiers in the presence of adversarial input perturbations.   Artificial neural networks (ANNs) are trained using mathematical operations, and it is known that these networks are robust to adversarial inputs.  This paper shows that adversarial subspaces can be found in the training process, and that the adversarial perturbation can be used to perturb a decision boundary of the network.  The analysis is based on the observation that the minimal perturbance is the one that is close to the boundary of a subspace.  It is shown that the boundary is a function of the dimensionality of the subspace and the number of samples.  In addition, the authors show that if the dimension of this subspace is large enough, then the boundary can be reached.  They also show that the test-time adversarial attacks can be incorporated into the training procedure.  Finally, they show that under certain conditions, adversarial training is more robust to test time attacks. ","This paper studies the decision boundary geometry of ANN classifiers in the presence of adversarial input perturbations.   Artificial neural networks (ANNs) are trained using mathematical operations, and it is known that these networks are robust to adversarial inputs.  This paper shows that adversarial subspaces can be found in the training process, and that the adversarial perturbation can be used to perturb a decision boundary of the network.  The analysis is based on the observation that the minimal perturbance is the one that is close to the boundary of a subspace.  It is shown that the boundary is a function of the dimensionality of the subspace and the number of samples.  In addition, the authors show that if the dimension of this subspace is large enough, then the boundary can be reached.  They also show that the test-time adversarial attacks can be incorporated into the training procedure.  Finally, they show that under certain conditions, adversarial training is more robust to test time attacks. "
3231,SP:af5c25ecf38c5c3f3387720bdc80c2c54c5699fe,"hashtags USED-FOR auxiliary information. similar representations CONJUNCTION dissimilar representations. dissimilar representations CONJUNCTION similar representations. self - supervised representations COMPARE auxiliary - information - infused representations. auxiliary - information - infused representations COMPARE self - supervised representations. auxiliary - information - infused representations COMPARE supervised representations. supervised representations COMPARE auxiliary - information - infused representations. direct downstream labels USED-FOR supervision signals. self - supervised representations COMPARE supervised representations. supervised representations COMPARE self - supervised representations. direct downstream labels USED-FOR supervised representations. approach COMPARE baseline representation learning methods. baseline representation learning methods COMPARE approach. approach COMPARE approach. approach COMPARE approach. approach COMPARE baseline representation learning methods. baseline representation learning methods COMPARE approach. auxiliary data information USED-FOR approach. auxiliary data information USED-FOR baseline representation learning methods. approach USED-FOR unsupervised constructed clusters. approach USED-FOR unsupervised representation learning approach. auxiliary information HYPONYM-OF unsupervised constructed clusters. OtherScientificTerm are data clustering information, and cluster. Material is Instagram image. Method is weakly - supervised contrastive learning approach. ",This paper proposes a weakly-supervised contrastive learning approach for unsupervised representation learning based on data clustering information. The idea is to use auxiliary information from hashtags (e.g. Instagram image) as a way to augment the representation of a cluster with similar representations and dissimilar representations. The authors show that self-supervision representations with direct downstream labels are more robust to auxiliary-information-infused representations compared to supervised representations that are trained with only indirect supervision signals. They also show that the proposed approach outperforms baseline representation learning methods that do not use auxiliary data information in their approach. The paper also shows that the approach can also be used to learn more robustly to the presence or absence of auxiliary data in an unsuper supervised representation learning approach.   ,This paper proposes a weakly-supervised contrastive learning approach for unsupervised representation learning based on data clustering information. The idea is to use auxiliary information from hashtags (e.g. Instagram image) as a way to augment the representation of a cluster with similar representations and dissimilar representations. The authors show that self-supervision representations with direct downstream labels are more robust to auxiliary-information-infused representations compared to supervised representations that are trained with only indirect supervision signals. They also show that the proposed approach outperforms baseline representation learning methods that do not use auxiliary data information in their approach. The paper also shows that the approach can also be used to learn more robustly to the presence or absence of auxiliary data in an unsuper supervised representation learning approach.   
3247,SP:0a92fcc52970201de4a66b1e76c93dbea9dfd3f1,"Recovering sparse parameters PART-OF machine learning. observational data USED-FOR Recovering sparse parameters. algorithms USED-FOR problem. path - following algorithm USED-FOR PLISA. recovery accuracy EVALUATE-FOR PLISA. empirical Rademacher complexity EVALUATE-FOR PLISA. empirical Rademacher complexity EVALUATE-FOR generalization ability. generalization ability EVALUATE-FOR PLISA. PLISA USED-FOR sparse estimation problems. stability CONJUNCTION convergence. convergence CONJUNCTION stability. generalization ability CONJUNCTION algorithmic properties. algorithmic properties CONJUNCTION generalization ability. convergence FEATURE-OF unrolled algorithm. stability FEATURE-OF unrolled algorithm. convergence HYPONYM-OF algorithmic properties. stability HYPONYM-OF algorithmic properties. techniques USED-FOR learning - based algorithms. OtherScientificTerm are hyperparameters, and problem distribution of interest. Generic are they, and analysis. Method are Provable Learning - based Iterative Sparse recovery Algorithm, and generalization analysis. ","Recovering sparse parameters in machine learning from observational data is an important problem. The authors propose two algorithms to solve this problem. One is the Provable Learning-based Iterative Sparse recovery Algorithm (PLISA) and the other is a path-following algorithm. PLISA improves the recovery accuracy and generalization ability of the empirical Rademacher complexity of recovering sparse parameters. The main contribution of the paper is that PLISA is a provable learning-based iterative iterative recovery algorithm.   The authors provide theoretical analysis of the generalization performance of PLISA for sparse estimation problems where the hyperparameters are provable and the problem distribution of interest is known. The generalization analysis is based on the assumption that the recovery of the parameters is provable, and the authors show that the recovered parameters are provably non-trivial, and that they can be recovered with high probability. They also provide theoretical analyses of the stability and convergence of the unrolled algorithm. Finally, the authors propose techniques to improve the generalizability of the learned algorithms and the stability of the algorithms. ","Recovering sparse parameters in machine learning from observational data is an important problem. The authors propose two algorithms to solve this problem. One is the Provable Learning-based Iterative Sparse recovery Algorithm (PLISA) and the other is a path-following algorithm. PLISA improves the recovery accuracy and generalization ability of the empirical Rademacher complexity of recovering sparse parameters. The main contribution of the paper is that PLISA is a provable learning-based iterative iterative recovery algorithm.   The authors provide theoretical analysis of the generalization performance of PLISA for sparse estimation problems where the hyperparameters are provable and the problem distribution of interest is known. The generalization analysis is based on the assumption that the recovery of the parameters is provable, and the authors show that the recovered parameters are provably non-trivial, and that they can be recovered with high probability. They also provide theoretical analyses of the stability and convergence of the unrolled algorithm. Finally, the authors propose techniques to improve the generalizability of the learned algorithms and the stability of the algorithms. "
3263,SP:5064eda9ba27060af15e81b2b317b2e4558b0ac4,"robot control CONJUNCTION game AI. game AI CONJUNCTION robot control. unified homogeneous action space FEATURE-OF hybrid action space. discretization USED-FOR unified homogeneous action space. Hybrid Action Representation ( HyAR ) USED-FOR compact and decodable latent representation space. compact and decodable latent representation space USED-FOR hybrid action space. embedding table CONJUNCTION conditional Variational Auto - Encoder ( VAE ). conditional Variational Auto - Encoder ( VAE ) CONJUNCTION embedding table. HyAR USED-FOR latent space. embedding table USED-FOR HyAR. unsupervised environmental dynamics prediction USED-FOR action representation. DRL algorithms USED-FOR representation space. action space FEATURE-OF hybrid action embeddings. discrete - continuous action space USED-FOR HyAR. HyAR COMPARE baselines. baselines COMPARE HyAR. baselines USED-FOR high - dimensional action spaces. OtherScientificTerm are Discrete - continuous hybrid action space, discrete or continuous action space, approximation difficulties, discrete action, and continuous parameter. Method are Reinforcement Learning ( RL ), and RL algorithms. Task is hybrid action RL. ","This paper proposes Hybrid Action Representation (HyAR), a novel approach to learn a compact and decodable latent representation space for discrete-continuous hybrid action spaces. The authors propose a discretization that allows for a unified homogeneous action space, which can be used for robot control and game AI.    The paper proposes to use an embedding table and a conditional Variational Auto-Encoder (VAE) to learn the latent space of a discrete or continuous action space. The action representation is learned via unsupervised environmental dynamics prediction. Reinforcement Learning (RL) algorithms are used to learn this representation space, and DRL algorithms are also used to train the representation space.  The main contribution of the paper is that HyAR learns a latent space that is decoupled from the discrete action space and decouples it from the approximation difficulties of learning a discrete action from a continuous parameter. The paper shows that the learned action space of the hybrid action embeddings in the action space can be decomposed into a discrete and continuous one, and that RL algorithms can be trained to learn such decoupling.  Experiments show that the proposed HyAR outperforms baselines for learning high-dimensional action spaces, and is able to generalize well to hybrid action RL. ","This paper proposes Hybrid Action Representation (HyAR), a novel approach to learn a compact and decodable latent representation space for discrete-continuous hybrid action spaces. The authors propose a discretization that allows for a unified homogeneous action space, which can be used for robot control and game AI.    The paper proposes to use an embedding table and a conditional Variational Auto-Encoder (VAE) to learn the latent space of a discrete or continuous action space. The action representation is learned via unsupervised environmental dynamics prediction. Reinforcement Learning (RL) algorithms are used to learn this representation space, and DRL algorithms are also used to train the representation space.  The main contribution of the paper is that HyAR learns a latent space that is decoupled from the discrete action space and decouples it from the approximation difficulties of learning a discrete action from a continuous parameter. The paper shows that the learned action space of the hybrid action embeddings in the action space can be decomposed into a discrete and continuous one, and that RL algorithms can be trained to learn such decoupling.  Experiments show that the proposed HyAR outperforms baselines for learning high-dimensional action spaces, and is able to generalize well to hybrid action RL. "
3279,SP:5128bf712f6b197de113c7a371b4bec36f978eca,SGEM USED-FOR general non - convex stochastic optimization problems. Stochastic Gradient USED-FOR general non - convex stochastic optimization problems. Stochastic Gradient USED-FOR SGEM. AEGD method USED-FOR SGEM. energy CONJUNCTION momentum. momentum CONJUNCTION energy. energy USED-FOR SGEM. momentum PART-OF SGEM. energydependent convergence rates CONJUNCTION regret bound. regret bound CONJUNCTION energydependent convergence rates. regret bound USED-FOR online convex setting. energydependent convergence rates FEATURE-OF nonconvex stochastic setting. unconditional energy stability property FEATURE-OF SGEM. threshold USED-FOR energy variable. SGEM COMPARE AEGD. AEGD COMPARE SGEM. SGDM USED-FOR deep neural networks. SGEM USED-FOR deep neural networks. SGEM COMPARE SGDM. SGDM COMPARE SGEM. Method is Adaptive Gradient Descent. ,"This paper studies the general non-convex stochastic gradient descent (SGEM) problem in the online convex setting. In particular, the authors consider the setting where the gradient of SGEM converges to the stationary point of a non-smooth convex function. In this setting, SGEM can be viewed as an extension of the Adaptive Gradient Descent (AGD) algorithm. The authors show that SGEM has the same unconditional energy stability property as AEGD, and provide a regret bound for the online setting.  ","This paper studies the general non-convex stochastic gradient descent (SGEM) problem in the online convex setting. In particular, the authors consider the setting where the gradient of SGEM converges to the stationary point of a non-smooth convex function. In this setting, SGEM can be viewed as an extension of the Adaptive Gradient Descent (AGD) algorithm. The authors show that SGEM has the same unconditional energy stability property as AEGD, and provide a regret bound for the online setting.  "
3295,SP:11f49b0a975be87769be29e85d7e3924699cf2c9,"non - autoregressive ( NAR ) approaches USED-FOR inference. NAR models COMPARE AR counterparts. AR counterparts COMPARE NAR models. training CONJUNCTION inference. inference CONJUNCTION training. multiple datasets EVALUATE-FOR NAR models. NAR EVALUATE-FOR CMLMC. raw data USED-FOR CMLMC. multiple datasets EVALUATE-FOR AR. multiple datasets EVALUATE-FOR CMLMC. Metric is human - level accuracy. Method are AR framework, and distillation. OtherScientificTerm is raw data without distillation. ","This paper proposes a new non-autoregressive (NAR) approaches for inference and training. The authors show that NAR models outperform their AR counterparts in terms of human-level accuracy on multiple datasets. The main contribution of the paper is that the authors propose a new AR framework, called CMLMC, which is able to learn from raw data without distillation. Experiments show that the proposed method outperforms the state-of-the-art NAR methods on multiple benchmarks.   ","This paper proposes a new non-autoregressive (NAR) approaches for inference and training. The authors show that NAR models outperform their AR counterparts in terms of human-level accuracy on multiple datasets. The main contribution of the paper is that the authors propose a new AR framework, called CMLMC, which is able to learn from raw data without distillation. Experiments show that the proposed method outperforms the state-of-the-art NAR methods on multiple benchmarks.   "
3311,SP:96f8ac3c6163e56d8ae1954a162bae01e6b58a0a,Ultra - low power local signal processing USED-FOR edge applications. always - on devices USED-FOR edge applications. limited power budget FEATURE-OF domain. spiking neural networks USED-FOR Neuromorphic processors. computational power FEATURE-OF Neuromorphic processors. limited power budget FEATURE-OF Neuromorphic processors. spiking neural dynamics COMPARE dilated temporal convolutions. dilated temporal convolutions COMPARE spiking neural dynamics. WaveSense HYPONYM-OF spiking neural network. WaveNet architecture USED-FOR spiking neural network. neural dynamics CONJUNCTION fixed time - constants. fixed time - constants CONJUNCTION neural dynamics. fixed time - constants CONJUNCTION feed - forward architecture. feed - forward architecture CONJUNCTION fixed time - constants. WaveSense USED-FOR neuromorphic implementation. feed - forward architecture USED-FOR WaveSense. fixed time - constants USED-FOR WaveSense. neural dynamics USED-FOR WaveSense. datasets USED-FOR keyword - spotting. keyword - spotting EVALUATE-FOR model. datasets EVALUATE-FOR model. CNNs CONJUNCTION LSTMs. LSTMs CONJUNCTION CNNs. network COMPARE spiking neural networks. spiking neural networks COMPARE network. network COMPARE artificial neural networks. artificial neural networks COMPARE network. LSTMs HYPONYM-OF artificial neural networks. CNNs HYPONYM-OF artificial neural networks. ,"This paper studies the problem of ultra-low power local signal processing for edge applications on always-on devices with limited power budget. Neuromorphic processors with spiking neural networks have limited computational power, and the authors propose WaveSense, which is a new neuromorphic implementation based on the WaveNet architecture. WaveSense uses neural dynamics and fixed time-constants instead of dilated temporal convolutions. The authors show that WaveSense is able to achieve state-of-the-art performance on standard datasets for keyword-spuriousing and keyword-spotting. They also show that the proposed network outperforms spiking networks and other artificial neural networks, such as CNNs and LSTMs. ","This paper studies the problem of ultra-low power local signal processing for edge applications on always-on devices with limited power budget. Neuromorphic processors with spiking neural networks have limited computational power, and the authors propose WaveSense, which is a new neuromorphic implementation based on the WaveNet architecture. WaveSense uses neural dynamics and fixed time-constants instead of dilated temporal convolutions. The authors show that WaveSense is able to achieve state-of-the-art performance on standard datasets for keyword-spuriousing and keyword-spotting. They also show that the proposed network outperforms spiking networks and other artificial neural networks, such as CNNs and LSTMs. "
3327,SP:7f20a2e4e95f857140b87b0730360b3ff2f371f4,"machine learning USED-FOR social applications. machine learning USED-FOR injustice. algorithms USED-FOR high - confidence behavioral guarantees. Shifty algorithms HYPONYM-OF algorithms. algorithms USED-FOR demographic shift ’s challenges. Shifty HYPONYM-OF technique. real - world dataset of university entrance exams EVALUATE-FOR Shifty. algorithm USED-FOR models. high - confidence fairness guarantees FEATURE-OF algorithm. Method is machine learning algorithms. OtherScientificTerm are unfair behavior, and demographic shift. Generic are approaches, and methods. Metric is fairness assurances. ","This paper studies the problem of fairness in machine learning for social applications. The authors propose two algorithms, called Shifty algorithms, that provide high-confidence behavioral guarantees for machine learning algorithms that are sensitive to unfair behavior in the face of demographic shift. They show that these algorithms are effective at addressing demographic shift’s challenges, and that their technique, Shifty, can be applied to a wide range of social applications where fairness assurances are not available. They evaluate Shifty on a real-world dataset of university entrance exams, and show that Shifty outperforms existing approaches. They also show that the proposed algorithm can be used to train models that have high-confident fairness guarantees. ","This paper studies the problem of fairness in machine learning for social applications. The authors propose two algorithms, called Shifty algorithms, that provide high-confidence behavioral guarantees for machine learning algorithms that are sensitive to unfair behavior in the face of demographic shift. They show that these algorithms are effective at addressing demographic shift’s challenges, and that their technique, Shifty, can be applied to a wide range of social applications where fairness assurances are not available. They evaluate Shifty on a real-world dataset of university entrance exams, and show that Shifty outperforms existing approaches. They also show that the proposed algorithm can be used to train models that have high-confident fairness guarantees. "
3343,SP:94f097921bee5fdc10ec2e7c901b2ddb876d9d41,"Stochastic dual dynamic programming ( SDDP ) USED-FOR multi - stage stochastic optimization. Stochastic dual dynamic programming ( SDDP ) USED-FOR modeling real - world process optimization tasks. worst - case complexity EVALUATE-FOR SDDP. trainable neural model USED-FOR problem instances. trainable neural model USED-FOR piece - wise linear value function. trainable neural model USED-FOR SDDP. intrinsic low - dimension space FEATURE-OF piece - wise linear value function. SDDP CONJUNCTION reinforcement learning algorithms. reinforcement learning algorithms CONJUNCTION SDDP. solution quality EVALUATE-FOR competitors. ν - SDDP COMPARE competitors. competitors COMPARE ν - SDDP. problem solving cost EVALUATE-FOR ν - SDDP. reinforcement learning algorithms HYPONYM-OF competitors. SDDP HYPONYM-OF competitors. solution quality EVALUATE-FOR ν - SDDP. synthetic and real - world process optimization problems EVALUATE-FOR ν - SDDP. OtherScientificTerm are decision variables, and successive problems. Material is low dimensional problems. Method is SDDP solver. Task is optimization. ","This paper proposes Stochastic dual dynamic programming (SDDP) for multi-stage stochastic optimization, which is a general framework for modeling real-world process optimization tasks. SDDP has been shown to have the worst-case complexity of $O(\sqrt{T})$ for low dimensional problems, and the authors show that SDDP can be used to reduce the problem solving cost to $O(1/T)$. SDDP uses a trainable neural model to learn a piece-wise linear value function in an intrinsic low-dimension space, which can then be used as a training objective for the SDDP solver. The authors also show that the training objective can be combined with existing reinforcement learning algorithms to improve the performance of SDDP and improve the solution quality compared to competitors such as SDDP.  The authors evaluate the effectiveness of the proposed ν-SDDP on both synthetic and real world process optimization problems, showing that the proposed SDDP is able to achieve the best performance in terms of the number of decision variables, and that the optimization is more efficient.   ","This paper proposes Stochastic dual dynamic programming (SDDP) for multi-stage stochastic optimization, which is a general framework for modeling real-world process optimization tasks. SDDP has been shown to have the worst-case complexity of $O(\sqrt{T})$ for low dimensional problems, and the authors show that SDDP can be used to reduce the problem solving cost to $O(1/T)$. SDDP uses a trainable neural model to learn a piece-wise linear value function in an intrinsic low-dimension space, which can then be used as a training objective for the SDDP solver. The authors also show that the training objective can be combined with existing reinforcement learning algorithms to improve the performance of SDDP and improve the solution quality compared to competitors such as SDDP.  The authors evaluate the effectiveness of the proposed ν-SDDP on both synthetic and real world process optimization problems, showing that the proposed SDDP is able to achieve the best performance in terms of the number of decision variables, and that the optimization is more efficient.   "
3359,SP:3d9f5132f9ec3807dbca78462a459fd123a09b24,protocol USED-FOR private next - token prediction. protocol USED-FOR privacy violations. language models USED-FOR privacy violations. private corpus USED-FOR language models. relaxation of group differentially private prediction USED-FOR SUBMIX. data - dependent privacy accounting mechanism USED-FOR it. it USED-FOR data - extraction attacks. data - dependent privacy accounting mechanism USED-FOR data - extraction attacks. data - dependent privacy accounting mechanism USED-FOR SUBMIX. SUBMIX HYPONYM-OF protocol. transformer - based models USED-FOR next - token predictions. GPT-2 HYPONYM-OF transformer - based models. Generic is model. Method is language model. ,"This paper proposes a new protocol for private next-token prediction based on the relaxation of group differentially private prediction. The protocol, called SUBMIX, aims to prevent privacy violations in language models trained on a private corpus. Specifically, it uses a data-dependent privacy accounting mechanism to make it more robust to data-extraction attacks. Experiments are conducted on transformer-based models (e.g., GPT-2) for next-Token predictions and show that the proposed model is more robust than existing methods.","This paper proposes a new protocol for private next-token prediction based on the relaxation of group differentially private prediction. The protocol, called SUBMIX, aims to prevent privacy violations in language models trained on a private corpus. Specifically, it uses a data-dependent privacy accounting mechanism to make it more robust to data-extraction attacks. Experiments are conducted on transformer-based models (e.g., GPT-2) for next-Token predictions and show that the proposed model is more robust than existing methods."
3375,SP:7f524d186ea939309c7eeb843c62b6a4b4cfbc8a,unsupervised method USED-FOR OOD samples. classification model USED-FOR k - NN density estimate. k - NN density estimate USED-FOR unsupervised method. k - NN density estimator COMPARE OOD detection method. OOD detection method COMPARE k - NN density estimator. Label Smoothed Embedding Hypothesis HYPONYM-OF label smoothing. label smoothing USED-FOR model. proposal COMPARE OOD baselines. OOD baselines COMPARE proposal. k - NN density estimation USED-FOR OOD examples. finite - sample high - probability statistical results USED-FOR k - NN density estimation. Material is indistribution samples. ,"This paper proposes an unsupervised method to detect out-of-distribution (OOD) samples using a k-NN density estimate from a classification model. The proposed k-NN density estimator is based on finite-sample high-probability statistical results, and the authors show that their k-NCD density estimation outperforms the state of the art OOD detection method by a large margin. The authors also propose a new model based on label smoothing, the so-called Label Smoothed Embedding Hypothesis. The proposal is empirically shown to outperform OOD baselines in the presence of indistribution samples.","This paper proposes an unsupervised method to detect out-of-distribution (OOD) samples using a k-NN density estimate from a classification model. The proposed k-NN density estimator is based on finite-sample high-probability statistical results, and the authors show that their k-NCD density estimation outperforms the state of the art OOD detection method by a large margin. The authors also propose a new model based on label smoothing, the so-called Label Smoothed Embedding Hypothesis. The proposal is empirically shown to outperform OOD baselines in the presence of indistribution samples."
3391,SP:aafbd6ada14cc59a272fe4bf95fac71fa18e57ab,"continuous time domain FEATURE-OF stochastic differential equations. stochastic differential equations USED-FOR Diffusion - based methods. denoising score matching USED-FOR models. denoising score matching framework USED-FOR representation learning. GANs CONJUNCTION VAEs. VAEs CONJUNCTION GANs. VAEs USED-FOR representations. GANs USED-FOR representations. denoising score matching objective USED-FOR diffusion - based representation learning. approach USED-FOR infinite - dimensional latent code. infinite - dimensional latent code USED-FOR state - of - the - art models. semi - supervised image classification EVALUATE-FOR state - of - the - art models. adversarial training USED-FOR diffusionbased models. adversarial training USED-FOR sample quality. smaller noise scales FEATURE-OF approximation of the prior. sampling speed EVALUATE-FOR adversarial training. approximation of the prior USED-FOR adversarial training. Method are non - adversarial generative model, and multi - scale denoising autoencoders. OtherScientificTerm are supervised signal, and latent codes. Task is denoising. Generic is representation. ","Diffusion-based methods are based on stochastic differential equations in the continuous time domain. The authors propose a denoising score matching objective for diffusion-based representation learning, where the goal is to learn a non-adversarial generative model that is robust to the noise in the supervised signal. The paper proposes a new approach to learn an infinite-dimensional latent code, which can be used to improve the sample quality of the representation learned by GANs and VAEs.   The paper also proposes a novel denoizing score matching framework for representation learning. The proposed approach can be applied to multi-scale denoised autoencoders, and the authors show that the proposed approach is able to achieve state-of-the-art performance on semi-supervised image classification.  The authors also show that adversarial training for diffusionbased models can improve sample quality through the use of an approximation of the prior at smaller noise scales, which improves the sampling speed. The main contribution of the paper is that the authors propose an approach to denoise the latent codes, which is more computationally efficient. ","Diffusion-based methods are based on stochastic differential equations in the continuous time domain. The authors propose a denoising score matching objective for diffusion-based representation learning, where the goal is to learn a non-adversarial generative model that is robust to the noise in the supervised signal. The paper proposes a new approach to learn an infinite-dimensional latent code, which can be used to improve the sample quality of the representation learned by GANs and VAEs.   The paper also proposes a novel denoizing score matching framework for representation learning. The proposed approach can be applied to multi-scale denoised autoencoders, and the authors show that the proposed approach is able to achieve state-of-the-art performance on semi-supervised image classification.  The authors also show that adversarial training for diffusionbased models can improve sample quality through the use of an approximation of the prior at smaller noise scales, which improves the sampling speed. The main contribution of the paper is that the authors propose an approach to denoise the latent codes, which is more computationally efficient. "
3407,SP:8cfc837d5c10d539bbd098df7134c42e4830ba25,"Goal - conditioned reinforcement learning ( RL ) USED-FOR tasks. navigation CONJUNCTION manipulation. manipulation CONJUNCTION navigation. expert demonstrations CONJUNCTION reward shaping. reward shaping CONJUNCTION expert demonstrations. offline data CONJUNCTION expert demonstrations. expert demonstrations CONJUNCTION offline data. planning USED-FOR curriculum of intermediate states. algorithm USED-FOR distant goal - reaching task. planning USED-FOR algorithm. M - step USED-FOR goal - conditioned policy. E - step CONJUNCTION M - step. M - step CONJUNCTION E - step. expectation maximization USED-FOR goal - conditioned policies. goal - conditioned RL CONJUNCTION graph search. graph search CONJUNCTION goal - conditioned RL. prior methods COMPARE ours. ours COMPARE prior methods. planning USED-FOR ours. goal - conditioned RL USED-FOR prior methods. graph search USED-FOR prior methods. method COMPARE prior methods. prior methods COMPARE method. it USED-FOR tasks. graph search USED-FOR methods. Method are Classifier - Planning ( C - Planning ), and graph planning. Generic is policy. ","Goal-conditioned reinforcement learning (RL) is a popular approach for tasks that require offline data, expert demonstrations, and reward shaping. The authors propose a new algorithm called Classifier-Plans (C-Planning) that uses planning to learn a curriculum of intermediate states to guide the algorithm to a distant goal-reaching task. The algorithm is based on the idea of graph planning, where the policy is trained to maximize the expected return of the next state. The E-step and M-step of the algorithm are used to learn the goal-conditioning policy, and the expectation maximization is used to train goal- conditioned policies. Experiments show that the proposed method outperforms prior methods that do not use graph search, and that it can be applied to a wide range of tasks, including navigation, manipulation, and exploration.    The paper is well-written, well-motivated, and easy to follow. The idea of using graph planning is interesting. However, there is a lack of comparison with prior methods based on prior methods using goal-conditional RL and graph search. ","Goal-conditioned reinforcement learning (RL) is a popular approach for tasks that require offline data, expert demonstrations, and reward shaping. The authors propose a new algorithm called Classifier-Plans (C-Planning) that uses planning to learn a curriculum of intermediate states to guide the algorithm to a distant goal-reaching task. The algorithm is based on the idea of graph planning, where the policy is trained to maximize the expected return of the next state. The E-step and M-step of the algorithm are used to learn the goal-conditioning policy, and the expectation maximization is used to train goal- conditioned policies. Experiments show that the proposed method outperforms prior methods that do not use graph search, and that it can be applied to a wide range of tasks, including navigation, manipulation, and exploration.    The paper is well-written, well-motivated, and easy to follow. The idea of using graph planning is interesting. However, there is a lack of comparison with prior methods based on prior methods using goal-conditional RL and graph search. "
3423,SP:ef3193842e06d4a6edb8a6a86ea5bc97ee5eaa4a,"regularization technique USED-FOR deep neural networks. Mixup HYPONYM-OF regularization technique. mixup USED-FOR k - mixup. Wasserstein metric FEATURE-OF interpolation. interpolation HYPONYM-OF displacement interpolation. mixup USED-FOR k - mixup case. k - mixup USED-FOR cluster and manifold structures. network architectures CONJUNCTION benchmark datasets. benchmark datasets CONJUNCTION network architectures. mixup HYPONYM-OF data augmentation approach. data augmentation approach USED-FOR models. beta distribution USED-FOR Averaging weights. mixup USED-FOR Perturbations. embedded manifold USED-FOR distributions. α USED-FOR procedure. fully - connected network USED-FOR binary classification. synthetic datasets USED-FOR binary classification. 1 - mixup CONJUNCTION 32 - mixup regularization. 32 - mixup regularization CONJUNCTION 1 - mixup. synthetic datasets USED-FOR fully - connected network. k - mixup USED-FOR local structure. blur CONJUNCTION contrast. contrast CONJUNCTION blur. displacement interpolation USED-FOR optimal transport. global cluster CONJUNCTION manifold support structure. manifold support structure CONJUNCTION global cluster. k - mixup USED-FOR perturbed training datasets. manifold support structure FEATURE-OF perturbed training datasets. global cluster FEATURE-OF perturbed training datasets. Metric are generalization, adversarial robustness, and robustness. Generic is It. OtherScientificTerm are local distributional structure, clusters, data manifold, and discrete distributions. Task is regularization. Method is mixup regularization. ","This paper proposes a new regularization technique for deep neural networks called Mixup. Mixup is a popular regularization method for improving generalization and adversarial robustness. It is based on the observation that the local distributional structure of the training data is highly correlated with the clusters and the data manifold. The authors propose to use mixup to regularize the k-mixup case, which is a variant of displacement interpolation (i.e. interpolation on the Wasserstein metric).   The authors show that k-Mixup can be applied to both cluster and manifold structures. Perturbations induced by mixup are robust to perturbations caused by a data augmentation approach (e.g., mixup). Averaging weights from the beta distribution is used as a regularizer. The paper also shows that the optimal transport is obtained by using displacement interpolations.  The paper further shows that a fully-connected network trained on synthetic datasets for binary classification is robust to blur and contrast, and robustness to adversarial attacks is also improved.    Finally, the paper shows that mixup regularization can be used to improve the generalization performance of models trained on different network architectures and different benchmark datasets. The main contribution of the paper is that the authors propose a new procedure based on α. The idea is to mix the two distributions on the embedded manifold, and then apply k-mixtureup to each of the two discrete distributions.  Experiments are conducted on a few synthetic datasets, and on a fully connected networks trained on the synthetic datasets.  Results show that the benefits of mixup on perturbed training datasets are that have a global cluster or a manifold support structure, and that the benefit from k- mixup is not due to the local structure. ","This paper proposes a new regularization technique for deep neural networks called Mixup. Mixup is a popular regularization method for improving generalization and adversarial robustness. It is based on the observation that the local distributional structure of the training data is highly correlated with the clusters and the data manifold. The authors propose to use mixup to regularize the k-mixup case, which is a variant of displacement interpolation (i.e. interpolation on the Wasserstein metric).   The authors show that k-Mixup can be applied to both cluster and manifold structures. Perturbations induced by mixup are robust to perturbations caused by a data augmentation approach (e.g., mixup). Averaging weights from the beta distribution is used as a regularizer. The paper also shows that the optimal transport is obtained by using displacement interpolations.  The paper further shows that a fully-connected network trained on synthetic datasets for binary classification is robust to blur and contrast, and robustness to adversarial attacks is also improved.    Finally, the paper shows that mixup regularization can be used to improve the generalization performance of models trained on different network architectures and different benchmark datasets. The main contribution of the paper is that the authors propose a new procedure based on α. The idea is to mix the two distributions on the embedded manifold, and then apply k-mixtureup to each of the two discrete distributions.  Experiments are conducted on a few synthetic datasets, and on a fully connected networks trained on the synthetic datasets.  Results show that the benefits of mixup on perturbed training datasets are that have a global cluster or a manifold support structure, and that the benefit from k- mixup is not due to the local structure. "
3439,SP:0fe6a9848026e5f6436a380199e27a9ad26cffed,lightweight network USED-FOR embeddings. nonlinear classification layer USED-FOR lightweight network. nonlinear classification layer USED-FOR embeddings. nonlinearity USED-FOR representation ( embedding ) learning. deep networks USED-FOR representation ( embedding ) learning. embeddings USED-FOR linear classifier. linear classifier USED-FOR they. nonlinearity USED-FOR deep networks. embedding vector space FEATURE-OF nonlinear classifiers. limited - capacity backbone USED-FOR network. nonlinear kernelized classification layer USED-FOR deep networks. classification layer USED-FOR nonlinear classifier. radial kernel functions USED-FOR nonlinear classifier. embeddings FEATURE-OF radial kernel functions. radial kernel functions USED-FOR classification layer. layer USED-FOR model - efficient classifiers. layer USED-FOR computer vision and natural language processing tasks. ,"This paper proposes a novel nonlinear classification layer for a lightweight network to learn embeddings. The nonlinearity in representation (embedding) learning in deep networks can be seen as a result of deep networks with a nonlinear kernelized classification layer. The authors show that nonlinear classifiers in the embedding vector space can be learned by a single network with a limited-capacity backbone, and that they can be trained with a linear classifier in the same way as a deep classifier with the same number of layers.   The authors also show that a classification layer with radial kernel functions that are non-linear in their embedding and can be used to learn nonlinearclassifiers.  The proposed layer can be applied to a wide range of computer vision and natural language processing tasks, and the authors demonstrate that the proposed layer leads to model-efficient classifiers.","This paper proposes a novel nonlinear classification layer for a lightweight network to learn embeddings. The nonlinearity in representation (embedding) learning in deep networks can be seen as a result of deep networks with a nonlinear kernelized classification layer. The authors show that nonlinear classifiers in the embedding vector space can be learned by a single network with a limited-capacity backbone, and that they can be trained with a linear classifier in the same way as a deep classifier with the same number of layers.   The authors also show that a classification layer with radial kernel functions that are non-linear in their embedding and can be used to learn nonlinearclassifiers.  The proposed layer can be applied to a wide range of computer vision and natural language processing tasks, and the authors demonstrate that the proposed layer leads to model-efficient classifiers."
3455,SP:01ee8ec81619784788eb0ce9785098e437d17a7c,Graph Neural Networks ( GNNs ) USED-FOR node representations. nodal features CONJUNCTION graph structure. graph structure CONJUNCTION nodal features. fairness - aware data augmentation frameworks USED-FOR intrinsic bias. nodal features CONJUNCTION graph structure. graph structure CONJUNCTION nodal features. nodal features USED-FOR fairness - aware data augmentation frameworks. graph structure USED-FOR fairness - aware data augmentation frameworks. schemes USED-FOR GNN - based learning mechanisms. schemes USED-FOR fairness. fairness EVALUATE-FOR GNN - based learning mechanisms. node classification CONJUNCTION link prediction. link prediction CONJUNCTION node classification. real networks USED-FOR graph contrastive learning. real networks USED-FOR node classification. real networks USED-FOR link prediction. statistical parity CONJUNCTION equal opportunity. equal opportunity CONJUNCTION statistical parity. augmentation strategies COMPARE contrastive methods. contrastive methods COMPARE augmentation strategies. augmentation strategies USED-FOR fairness. statistical parity FEATURE-OF fairness. Method is Node representation learning. Material is graphs. Generic is representations. ,"Graph Neural Networks (GNNs) are used to learn node representations for graphs. Node representation learning has been a hot topic in recent years, and the authors show that existing fairness-aware data augmentation frameworks rely on nodal features and graph structure to mitigate intrinsic bias. The authors propose two schemes to improve the fairness of existing GNN-based learning mechanisms. The first is based on real networks for graph contrastive learning. The second uses real networks to improve node classification and link prediction. They show that the proposed augmentation strategies outperform existing contrastive methods for fairness in terms of statistical parity and equal opportunity. ","Graph Neural Networks (GNNs) are used to learn node representations for graphs. Node representation learning has been a hot topic in recent years, and the authors show that existing fairness-aware data augmentation frameworks rely on nodal features and graph structure to mitigate intrinsic bias. The authors propose two schemes to improve the fairness of existing GNN-based learning mechanisms. The first is based on real networks for graph contrastive learning. The second uses real networks to improve node classification and link prediction. They show that the proposed augmentation strategies outperform existing contrastive methods for fairness in terms of statistical parity and equal opportunity. "
3471,SP:7739dc9e37f7f1384f87d2e60281e5bb27fece99,"observational data USED-FOR estimating treatment effects. instrumental variable ( IV ) USED-FOR two - stage regression. 2SLS HYPONYM-OF two - stage regression. IVs CONJUNCTION confounders. confounders CONJUNCTION IVs. nonlinear IV regression variants USED-FOR confounding bias. confounders USED-FOR nonlinear IV regression variants. balancing USED-FOR treatment effect estimation. bias - variance trade - off FEATURE-OF imbalanced treatment distributions. balanced confounders representation USED-FOR treatment effect estimation. nonlinear IV methods USED-FOR confounding. balanced representation of confounders USED-FOR confounder balancing. treatment regression PART-OF modules. modules PART-OF CB - IV algorithm. outcome regression PART-OF CB - IV algorithm. treatment regression PART-OF CB - IV algorithm. confounder balancing USED-FOR treatment effect estimation. IV regression USED-FOR treatment effect estimation. confounder balancing USED-FOR IV regression. multiplicative assumption COMPARE additive separability assumption. additive separability assumption COMPARE multiplicative assumption. multiplicative assumption FEATURE-OF CB - IV algorithm. IV regression CONJUNCTION confounder balancing methods. confounder balancing methods CONJUNCTION IV regression. CB - IV algorithm USED-FOR treatment effect estimation. CB - IV algorithm COMPARE state - of - the - art methods. state - of - the - art methods COMPARE CB - IV algorithm. state - of - the - art methods USED-FOR treatment effect estimation. confounder balancing methods USED-FOR treatment effect estimation. CB - IV algorithm COMPARE confounder balancing methods. confounder balancing methods COMPARE CB - IV algorithm. IV regression HYPONYM-OF CB - IV algorithm. confounder balancing methods HYPONYM-OF state - of - the - art methods. IV regression HYPONYM-OF state - of - the - art methods. OtherScientificTerm are unmeasured confounders, additive separability of noise, and observed confounders. Generic are they, and second stage. Material is linear setting. ","This paper considers the problem of estimating treatment effects from observational data with unmeasured confounders. The authors propose a two-stage regression with instrumental variable (IV) and confounder-free (influential variable-free) regression, which is a variant of 2SLS, a well-known two-step regression algorithm for two-armed bandits.   The authors show that nonlinear IV regression variants of the two stages are biased towards confounding bias due to the additive separability of noise in the IVs, and that the confounding bias can be alleviated by balancing the treatment effect estimation using a balanced representation of confoundering variables.  They also show that the bias-variance trade-off between imbalanced treatment distributions and treatment effects can be reduced by balancing.  The main contribution of the paper is the development of a new algorithm, the CB-IV algorithm, which consists of two modules: treatment regression and outcome regression. In the treatment regression module, the authors propose to use the two-stages of two-SLS to mitigate the confounding.  In the first stage, the treatment is treated as a linear setting, and the unbalanced treatment distribution is used as an instrumental variable in the second stage.  Experiments are conducted to show that, under the assumption that the treatment distributions are balanced, the proposed CB- IV algorithm outperforms state-of-the-art methods (i.e., IV regression, confoundER balancing methods) on IV regression with IV regression under the multiplicative assumption, and on the outcome regression, and also shows that the proposed by the authors outperforms the state- of-the art of the art methods in terms of treatment effects estimation. ","This paper considers the problem of estimating treatment effects from observational data with unmeasured confounders. The authors propose a two-stage regression with instrumental variable (IV) and confounder-free (influential variable-free) regression, which is a variant of 2SLS, a well-known two-step regression algorithm for two-armed bandits.   The authors show that nonlinear IV regression variants of the two stages are biased towards confounding bias due to the additive separability of noise in the IVs, and that the confounding bias can be alleviated by balancing the treatment effect estimation using a balanced representation of confoundering variables.  They also show that the bias-variance trade-off between imbalanced treatment distributions and treatment effects can be reduced by balancing.  The main contribution of the paper is the development of a new algorithm, the CB-IV algorithm, which consists of two modules: treatment regression and outcome regression. In the treatment regression module, the authors propose to use the two-stages of two-SLS to mitigate the confounding.  In the first stage, the treatment is treated as a linear setting, and the unbalanced treatment distribution is used as an instrumental variable in the second stage.  Experiments are conducted to show that, under the assumption that the treatment distributions are balanced, the proposed CB- IV algorithm outperforms state-of-the-art methods (i.e., IV regression, confoundER balancing methods) on IV regression with IV regression under the multiplicative assumption, and on the outcome regression, and also shows that the proposed by the authors outperforms the state- of-the art of the art methods in terms of treatment effects estimation. "
3487,SP:fdb68c39fce254b73310a3101b2fe97ba47e69fe,"stochastic gradient descent steps USED-FOR models. MAML objective COMPARE non - adaptive learning ( NAL ). non - adaptive learning ( NAL ) COMPARE MAML objective. MAML COMPARE NAL. NAL COMPARE MAML. easy and hard tasks PART-OF linear regression setting. MAML COMPARE NAL. NAL COMPARE MAML. hardness EVALUATE-FOR tasks. MAML USED-FOR hard tasks. Method are gradient descent, and two - layer neural networks. OtherScientificTerm is easy tasks optimal solutions. Task is few - shot image classification. ","This paper studies the problem of few-shot learning in the setting of two-layer neural networks. In particular, the authors consider a linear regression setting, where the objective function is a linear function of the number of training samples and the size of the training set. The authors show that the MAML algorithm is a generalization of the non-adaptive learning (NAL) algorithm. They also show that NAL can be seen as a special case of NAL.   ","This paper studies the problem of few-shot learning in the setting of two-layer neural networks. In particular, the authors consider a linear regression setting, where the objective function is a linear function of the number of training samples and the size of the training set. The authors show that the MAML algorithm is a generalization of the non-adaptive learning (NAL) algorithm. They also show that NAL can be seen as a special case of NAL.   "
3503,SP:e8143c7880c16ee9ce7a544e0fd80f001b1b4f9f,"astrophysics CONJUNCTION remote sensing. remote sensing CONJUNCTION astrophysics. Sparse Blind Source Separation ( BSS ) USED-FOR applications. remote sensing HYPONYM-OF applications. astrophysics HYPONYM-OF applications. Proximal Alternating Linearized Minimization ( PALM ) algorithm HYPONYM-OF sparse BSS methods. PALM hyperparameters CONJUNCTION variables. variables CONJUNCTION PALM hyperparameters. PALM hyperparameters USED-FOR Unrolling PALM. data - driven knowledge USED-FOR Unrolling PALM. Learned PALM ( LPALM ) algorithm USED-FOR semi - blind source separation. algorithm COMPARE PALM. PALM COMPARE algorithm. LPALM USED-FOR astrophysical multispectral imaging. cumbersome hyperparameter FEATURE-OF PALM. LPALM COMPARE PALM. PALM COMPARE LPALM. separation quality EVALUATE-FOR algorithm. unrolled source separation methods USED-FOR semi - blind setting. LPALM COMPARE unrolled source separation methods. unrolled source separation methods COMPARE LPALM. LPALM USED-FOR semi - blind setting. OtherScientificTerm are hyperparameter choice, and variable mixing matrices. Method are algorithm unfolding / unrolling, and unrolled algorithms. Task is real - world applications. ","This paper proposes a new method for semi-blind source separation based on the Proximal Alternating Linearized Minimization (PALM) algorithm. The authors propose a new algorithm called Learned PALM (LPALM), which learns the hyperparameters of the proximal alternating linearized minimization algorithm. They show that the learned algorithm outperforms the original PALM algorithm in terms of separation quality. They also show that LPALM can be applied to the semi-blind setting. ","This paper proposes a new method for semi-blind source separation based on the Proximal Alternating Linearized Minimization (PALM) algorithm. The authors propose a new algorithm called Learned PALM (LPALM), which learns the hyperparameters of the proximal alternating linearized minimization algorithm. They show that the learned algorithm outperforms the original PALM algorithm in terms of separation quality. They also show that LPALM can be applied to the semi-blind setting. "
3519,SP:7716315001949ab88c8a216302fe51bae872fc87,"transformers USED-FOR language modeling. power - law relationship FEATURE-OF transformers. memory CONJUNCTION computation. computation CONJUNCTION memory. attention module USED-FOR Legendre Memory Unit based model. implicit self - attention HYPONYM-OF attention module. loss EVALUATE-FOR transformers. transformers COMPARE LSTMs. LSTMs COMPARE transformers. model COMPARE transformers. transformers COMPARE model. model COMPARE transformers. transformers COMPARE model. model COMPARE LSTMs. LSTMs COMPARE model. transformers COMPARE LSTMs. LSTMs COMPARE transformers. loss EVALUATE-FOR model. global self - attention USED-FOR architecture. OtherScientificTerm are model size, and sequence length. Metric is computational and memory requirements. ","This paper studies the power-law relationship between transformers and LSTMs in the context of language modeling. The authors propose a Legendre Memory Unit based model with an attention module (implicit self-attention, which is a variant of the Legendre memory unit based model proposed in [1] and [2]). The authors show that transformers with the proposed loss outperform LSTM in terms of loss, model size, and computational and memory requirements. They also show that the proposed model outperforms transformers without memory or computation.    The authors also propose a new architecture based on global self attention, which can be used to reduce model size and reduce the number of layers. They show that this architecture can be applied to any architecture with the same input sequence length, and that it can be combined with existing transformers.","This paper studies the power-law relationship between transformers and LSTMs in the context of language modeling. The authors propose a Legendre Memory Unit based model with an attention module (implicit self-attention, which is a variant of the Legendre memory unit based model proposed in [1] and [2]). The authors show that transformers with the proposed loss outperform LSTM in terms of loss, model size, and computational and memory requirements. They also show that the proposed model outperforms transformers without memory or computation.    The authors also propose a new architecture based on global self attention, which can be used to reduce model size and reduce the number of layers. They show that this architecture can be applied to any architecture with the same input sequence length, and that it can be combined with existing transformers."
3535,SP:832f422b3554e89702e13c8c5690ee26f2289e3b,Generative adversarial networks ( GANs ) USED-FOR image generation. photo - realistic quality EVALUATE-FOR Generative adversarial networks ( GANs ). LatentKeypointGAN HYPONYM-OF two - stage GAN. internal conditioning FEATURE-OF space keypoints. appearance embeddings PART-OF keypoints. domain knowledge CONJUNCTION supervision signals. supervision signals CONJUNCTION domain knowledge. network architectures CONJUNCTION training schemes. training schemes CONJUNCTION network architectures. LatentKeypointGAN USED-FOR interpretable latent space. re - positioning USED-FOR LatentKeypointGAN. generating portraits HYPONYM-OF keypoint embeddings. GAN - based method USED-FOR unsupervised keypoint detection. Material is image content. OtherScientificTerm is spatial and appearance factors. ,"Generative adversarial networks (GANs) have recently been shown to achieve photo-realistic quality for image generation. This paper proposes a two-stage GAN called LatentKeypointGAN, which aims to improve the quality of the generated images. The key idea is to learn a set of space keypoints with internal conditioning on the image content. These keypoints are represented as appearance embeddings (e.g. generating portraits) that are shared across different spatial and appearance factors. The authors show that the latent space generated by LatentkeypointGAN can be used to learn an interpretable latent space, which is then used to train a GAN-based method for unsupervised keypoint detection. The paper also shows that the re-positioning of keypoints can be further improved by the use of different network architectures and training schemes. ","Generative adversarial networks (GANs) have recently been shown to achieve photo-realistic quality for image generation. This paper proposes a two-stage GAN called LatentKeypointGAN, which aims to improve the quality of the generated images. The key idea is to learn a set of space keypoints with internal conditioning on the image content. These keypoints are represented as appearance embeddings (e.g. generating portraits) that are shared across different spatial and appearance factors. The authors show that the latent space generated by LatentkeypointGAN can be used to learn an interpretable latent space, which is then used to train a GAN-based method for unsupervised keypoint detection. The paper also shows that the re-positioning of keypoints can be further improved by the use of different network architectures and training schemes. "
3551,SP:9206ae6e31077569313838504ef6daa89ad3b59c,"layer normalization USED-FOR deep fully - connected neural networks. mean field formalism USED-FOR deep fully - connected neural networks. initialization scheme CONJUNCTION activation function. activation function CONJUNCTION initialization scheme. normalization techniques USED-FOR problems. method USED-FOR residual networks. method USED-FOR initialization variances. Task is non - perturbative analysis of signal propagation. OtherScientificTerm are depth, gradient explosion, and representation shrinkage. Method is fully - connected architecture. ","This paper proposes a non-perturbative analysis of signal propagation in deep fully-connected neural networks with layer normalization based on the mean field formalism. The authors show that the depth of the network is not the cause of gradient explosion, but of representation shrinkage, which is caused by the initialization scheme and the activation function. They also show that normalization techniques can be used to alleviate these problems. Finally, the authors propose a method to regularize the initialization variances of residual networks, and show that this method can also be applied to residual networks. ","This paper proposes a non-perturbative analysis of signal propagation in deep fully-connected neural networks with layer normalization based on the mean field formalism. The authors show that the depth of the network is not the cause of gradient explosion, but of representation shrinkage, which is caused by the initialization scheme and the activation function. They also show that normalization techniques can be used to alleviate these problems. Finally, the authors propose a method to regularize the initialization variances of residual networks, and show that this method can also be applied to residual networks. "
3567,SP:2177be818b5843c580c787f1b2d725154846feb6,"optimal step sizes USED-FOR stochastic gradient descent. line searches USED-FOR step sizes. line searches PART-OF optimization. step sizes FEATURE-OF full - batch loss. line - search method USED-FOR full - batch loss. parabola USED-FOR line - search method. parabolas USED-FOR Learning rates. approach COMPARE SGD. SGD COMPARE approach. line search approaches USED-FOR Deep Learning across models. approach COMPARE line search approaches. line search approaches COMPARE approach. SGD COMPARE line search approaches. line search approaches COMPARE SGD. piece - wise constant learning rate schedule USED-FOR approach. validation and test accuracy EVALUATE-FOR batch sizes. piece - wise constant learning rate schedule USED-FOR SGD. validation and test accuracy EVALUATE-FOR approach. Task is Deep Learning. OtherScientificTerm are inherent noise, noisy update step directions, and optimal update step size. ","This paper studies the problem of finding optimal step sizes for stochastic gradient descent (SGD) in the presence of inherent noise. The authors propose a line-search method for finding the optimal full-batch loss with different step sizes. They show that line searches can be used to find step sizes that are close to the optimal in the absence of noisy update step directions. Learning rates are approximated by parabolas, and the authors show that the optimal update step size can be found by using a parabola. They also show that their approach outperforms SGD with a piece-wise constant learning rate schedule in terms of validation and test accuracy for different batch sizes.   The authors also compare their approach with other line search approaches for Deep Learning across models and show that SGD is more robust to noisy update steps than other recent state-of-the-art line search methods.","This paper studies the problem of finding optimal step sizes for stochastic gradient descent (SGD) in the presence of inherent noise. The authors propose a line-search method for finding the optimal full-batch loss with different step sizes. They show that line searches can be used to find step sizes that are close to the optimal in the absence of noisy update step directions. Learning rates are approximated by parabolas, and the authors show that the optimal update step size can be found by using a parabola. They also show that their approach outperforms SGD with a piece-wise constant learning rate schedule in terms of validation and test accuracy for different batch sizes.   The authors also compare their approach with other line search approaches for Deep Learning across models and show that SGD is more robust to noisy update steps than other recent state-of-the-art line search methods."
3583,SP:62233782f9046c85617d9ccfe8427eae7d1c9da7,"Noise - contrastive estimation ( NCE ) HYPONYM-OF statistically consistent method. statistically consistent method USED-FOR unnormalized probabilistic models. noise distribution USED-FOR NCE. noise distribution USED-FOR NCE. exponential loss USED-FOR eNCE. eNCE HYPONYM-OF NCE. exponential loss USED-FOR NCE. OtherScientificTerm are flat ) loss landscape, and exponential family. Method is normalized gradient descent. ","This paper proposes Noise-contrastive estimation (NCE), a statistically consistent method for unnormalized probabilistic models. NCE is based on the assumption that the noise distribution of the NCE depends on the underlying noise distribution, and that the (flat) loss landscape of the exponential family is well-studied. The authors show that eNCE, a variant of NCE based on an exponential loss, is a special case of the proposed NCE. They also show that normalized gradient descent converges to an exponential family. ","This paper proposes Noise-contrastive estimation (NCE), a statistically consistent method for unnormalized probabilistic models. NCE is based on the assumption that the noise distribution of the NCE depends on the underlying noise distribution, and that the (flat) loss landscape of the exponential family is well-studied. The authors show that eNCE, a variant of NCE based on an exponential loss, is a special case of the proposed NCE. They also show that normalized gradient descent converges to an exponential family. "
3599,SP:ceba6c1421b2d03863007fdaf029b8b946519c1b,"Privacy HYPONYM-OF distributed machine learning. distributed SGD algorithm USED-FOR model. noisy information USED-FOR differential privacy ( DP ). parameter - server architecture FEATURE-OF distributed SGD algorithm. DP CONJUNCTION BR. BR CONJUNCTION DP. convergence FEATURE-OF distributed SGD. Byzantine faults FEATURE-OF distributed SGD. ( α, f)-Byzantine resilience USED-FOR those. ( α, f)-BR USED-FOR approximate convergence guarantee. hyperparameter optimization USED-FOR guarantee. approaches USED-FOR DP. approaches USED-FOR BR. DP CONJUNCTION BR. BR CONJUNCTION DP. DP CONJUNCTION BR. BR CONJUNCTION DP. Method is learning algorithm. Metric is learning accuracy. ","Privacy is an important problem in distributed machine learning, especially when the model is trained with a distributed SGD algorithm with noisy information. This paper studies the problem of differential privacy (DP) under the assumption that the learning algorithm is not affected by the parameter-server architecture. The authors show that under certain assumptions, the convergence of the distributedSGD with Byzantine faults is guaranteed. They also provide an approximate convergence guarantee based on the (alpha, f)-Byzantine resilience of the algorithm, which is similar to those of previous works. They further show that hyperparameter optimization can be used to further improve the guarantee. Finally, they show that existing approaches for DP and BR can be extended to DP, BR, and DP.   ","Privacy is an important problem in distributed machine learning, especially when the model is trained with a distributed SGD algorithm with noisy information. This paper studies the problem of differential privacy (DP) under the assumption that the learning algorithm is not affected by the parameter-server architecture. The authors show that under certain assumptions, the convergence of the distributedSGD with Byzantine faults is guaranteed. They also provide an approximate convergence guarantee based on the (alpha, f)-Byzantine resilience of the algorithm, which is similar to those of previous works. They further show that hyperparameter optimization can be used to further improve the guarantee. Finally, they show that existing approaches for DP and BR can be extended to DP, BR, and DP.   "
3615,SP:bc783f0c829f90931535e63687d13172879631b3,code editing USED-FOR query code snippet. support exemplars USED-FOR query code snippet. editing exemplar USED-FOR editorial pattern. common pattern USED-FOR code editing. support exemplars USED-FOR common pattern. deep learning approach USED-FOR code editing problem. them USED-FOR query code snippet editing. support exemplars USED-FOR edit representations. edit representations PART-OF learning approach. multi - extent similarities ensemble USED-FOR query code snippet editing. language - specific grammar USED-FOR abstract syntax trees. similarities measurement USED-FOR collective tree representations. collective tree representations USED-FOR query and support sample matching. method COMPARE non - composition baselines. non - composition baselines COMPARE method. C # and Python datasets EVALUATE-FOR method. Task is computer source code editing. Material is support and query code snippets. Method is similarity - ranking error estimator. ,"This paper tackles the problem of computer source code editing, where both support and query code snippets are available. The authors propose a deep learning approach to solve the code editing problem by learning a common pattern between the support exemplars of a query code snippet and the edit exemplars for a given common pattern for code editing. Each editing exemplar corresponds to an editorial pattern, and the authors use them to learn edit representations from a set of support exemplar and query exemplars, and use them for the task of code editing for queries. The proposed learning approach is a learning approach that combines edit representations learned from the edit representations of the query and support code snippets, and a multi-extent similarities ensemble for the problem. In particular, the authors propose to learn abstract syntax trees with a language-specific grammar, and to use a similarity-ranking error estimator. The similarity measurement is used to learn the collective tree representations of all the support examples and the query examples, which are then used to compute the similarity between queries and support examples. The query and query sample matching is then performed by using the proposed method on C# and Python datasets, and compared to non-compositional baselines.  ","This paper tackles the problem of computer source code editing, where both support and query code snippets are available. The authors propose a deep learning approach to solve the code editing problem by learning a common pattern between the support exemplars of a query code snippet and the edit exemplars for a given common pattern for code editing. Each editing exemplar corresponds to an editorial pattern, and the authors use them to learn edit representations from a set of support exemplar and query exemplars, and use them for the task of code editing for queries. The proposed learning approach is a learning approach that combines edit representations learned from the edit representations of the query and support code snippets, and a multi-extent similarities ensemble for the problem. In particular, the authors propose to learn abstract syntax trees with a language-specific grammar, and to use a similarity-ranking error estimator. The similarity measurement is used to learn the collective tree representations of all the support examples and the query examples, which are then used to compute the similarity between queries and support examples. The query and query sample matching is then performed by using the proposed method on C# and Python datasets, and compared to non-compositional baselines.  "
3631,SP:ca0c4bdb02f7d939fb6de38b6b446ced4b5984a0,text CONJUNCTION music. music CONJUNCTION text. deep generative models USED-FOR realistic sequence data. text HYPONYM-OF realistic sequence data. music HYPONYM-OF realistic sequence data. high - level structure USED-FOR generative process. local coherence CONJUNCTION global coherence. global coherence CONJUNCTION local coherence. global coherence EVALUATE-FOR models. local coherence EVALUATE-FOR models. approach USED-FOR global structure. relational constraints FEATURE-OF global structure. model USED-FOR realistic data. model USED-FOR relational constraints. model PART-OF generative model. model PART-OF generative model. program synthesis algorithm USED-FOR relational constraints. constraint data USED-FOR generative model. approach COMPARE state - of - the - art. state - of - the - art COMPARE approach. approach USED-FOR high - level structure. state - of - the - art USED-FOR high - level structure. capturing high - level structure EVALUATE-FOR approach. low - level structure EVALUATE-FOR approach. OtherScientificTerm is measures of music. Generic is constraints. ,"This paper proposes a new approach to learn a high-level structure in deep generative models for realistic sequence data, such as text and music. The authors show that models trained with local coherence and global coherence are able to capture the local and global structures of the data. They also show that a model trained on realistic data can capture the relational constraints of relational constraints, which are measures of music and text coherence. The generative process can be seen as learning a model that captures this relational constraints through a program synthesis algorithm. The constraints are learned by training a generative model on the constraint data. The proposed approach is compared to state-of-the-art methods for capturing high level structure, and shows that the proposed approach outperforms them in capturing the low-level structures.  ","This paper proposes a new approach to learn a high-level structure in deep generative models for realistic sequence data, such as text and music. The authors show that models trained with local coherence and global coherence are able to capture the local and global structures of the data. They also show that a model trained on realistic data can capture the relational constraints of relational constraints, which are measures of music and text coherence. The generative process can be seen as learning a model that captures this relational constraints through a program synthesis algorithm. The constraints are learned by training a generative model on the constraint data. The proposed approach is compared to state-of-the-art methods for capturing high level structure, and shows that the proposed approach outperforms them in capturing the low-level structures.  "
3647,SP:692ae0c583a1585eff1a7d9c0d3b51b7879611cc,"biological systems CONJUNCTION combinatorial optimization. combinatorial optimization CONJUNCTION biological systems. particle physics CONJUNCTION biological systems. biological systems CONJUNCTION particle physics. scaling problems FEATURE-OF set - to - hypergraph tasks. run - time complexity HYPONYM-OF scaling problems. training method USED-FOR iterative refinement. efficiency CONJUNCTION constant memory usage. constant memory usage CONJUNCTION efficiency. contributions PART-OF set - to - hypergraph model. model COMPARE state - of - the - art. state - of - the - art COMPARE model. Task is set - to - hypergraph prediction. OtherScientificTerm are hyperedges, and positive edges. Metric are memory requirements, and asymptotic memory scaling. ","This paper studies the problem of set-to-hypergraph prediction, which is an important problem in particle physics, biological systems, and combinatorial optimization. In particular, scaling problems such as run-time complexity and memory requirements are of high importance in set- to-hyperGraph tasks. This paper proposes a new training method for iterative refinement of hyperedges, where the hyperedge is the set of nodes that have the most positive edges. The authors show that the proposed training method can achieve asymptotic memory scaling, which leads to a trade-off between efficiency and constant memory usage. In addition, the authors also show that contributions of the proposed model can be incorporated into the training of a standard set to hypergraph model. The proposed model is shown to outperform the state-of-the-art.   ","This paper studies the problem of set-to-hypergraph prediction, which is an important problem in particle physics, biological systems, and combinatorial optimization. In particular, scaling problems such as run-time complexity and memory requirements are of high importance in set- to-hyperGraph tasks. This paper proposes a new training method for iterative refinement of hyperedges, where the hyperedge is the set of nodes that have the most positive edges. The authors show that the proposed training method can achieve asymptotic memory scaling, which leads to a trade-off between efficiency and constant memory usage. In addition, the authors also show that contributions of the proposed model can be incorporated into the training of a standard set to hypergraph model. The proposed model is shown to outperform the state-of-the-art.   "
3663,SP:e3481fb6d8d1aa45d6ed4a454e781f5a2c30c57e,"biases FEATURE-OF models. post - processing method USED-FOR models. deep embeddings PART-OF pre - trained model. shallow neural network USED-FOR It. Ethical Module HYPONYM-OF shallow neural network. methodology COMPARE bias mitigation. bias mitigation COMPARE methodology. Method is deep learning algorithms. OtherScientificTerm are representation power, von Mises - Fisher loss, and latent space. Task is gender bias in facial recognition. ","This paper proposes a post-processing method to improve the performance of models that are sensitive to biases in deep learning algorithms. It is based on the idea that deep embeddings in a pre-trained model can be biased. The authors propose a shallow neural network, called the Ethical Module, to mitigate this issue. The representation power of the deep embedding is reduced by minimizing the von Mises-Fisher loss. The paper shows that the proposed methodology outperforms the state-of-the-art bias mitigation on a number of datasets, including gender bias in facial recognition. ","This paper proposes a post-processing method to improve the performance of models that are sensitive to biases in deep learning algorithms. It is based on the idea that deep embeddings in a pre-trained model can be biased. The authors propose a shallow neural network, called the Ethical Module, to mitigate this issue. The representation power of the deep embedding is reduced by minimizing the von Mises-Fisher loss. The paper shows that the proposed methodology outperforms the state-of-the-art bias mitigation on a number of datasets, including gender bias in facial recognition. "
3679,SP:3fb5dcc8b8fb731e09c14b16480cada1c7ccfaa7,"new class data USED-FOR KD loss. phase model USED-FOR old class knowledge. free image stream USED-FOR placebo data. placebo data USED-FOR KD loss. Google Images HYPONYM-OF free image stream. ImageNet-1k CONJUNCTION ImageNet - Subset. ImageNet - Subset CONJUNCTION ImageNet-1k. supervision CONJUNCTION memory budget. memory budget CONJUNCTION supervision. memory budget FEATURE-OF old class exemplars. CIL methods EVALUATE-FOR method. higher - resolution benchmarks EVALUATE-FOR top - performing CIL methods. ImageNet-1k HYPONYM-OF higher - resolution benchmarks. ImageNet - Subset HYPONYM-OF higher - resolution benchmarks. Task are class - incremental learning ( CIL ), and learning of new classes. Method are knowledge distillation ( KD ), evaluation function, and reinforcement learning algorithm. Material are old - class data, and image stream. OtherScientificTerm are class overlap, placebos, and pseudo CIL tasks. Generic is function. ","This paper tackles the problem of knowledge distillation (KD) in class-incremental learning (CIL), where the goal is to distill knowledge from old-class data to new-class samples without any class overlap. The paper proposes a novel evaluation function, called knowledge distillation (DK), which is based on a reinforcement learning algorithm. The KD loss is trained on the new class data, while the old class knowledge is used to train a phase model to learn from the new data. The authors propose to use a placebo data from a free image stream (e.g., Google Images) as the KD loss. The idea is that the evaluation function can be seen as a function of the amount of overlap between the old and new classes, and that the learning of new classes can be regarded as a learning of the old ones.    The paper shows that KD loss can be trained on placebos (i.e., a set of placebos that are not used in the training of the new classes) and on pseudo CIL tasks, where the placebos are generated from the same image stream as in the original CIL task. The proposed method is evaluated on top-performing CIL methods on two higher-resolution benchmarks (ImageNet-1k and ImageNet-Subset) and is shown to outperform a number of existing methods. It is also shown that the performance of old class exemplars trained with supervision and a memory budget can be improved. ","This paper tackles the problem of knowledge distillation (KD) in class-incremental learning (CIL), where the goal is to distill knowledge from old-class data to new-class samples without any class overlap. The paper proposes a novel evaluation function, called knowledge distillation (DK), which is based on a reinforcement learning algorithm. The KD loss is trained on the new class data, while the old class knowledge is used to train a phase model to learn from the new data. The authors propose to use a placebo data from a free image stream (e.g., Google Images) as the KD loss. The idea is that the evaluation function can be seen as a function of the amount of overlap between the old and new classes, and that the learning of new classes can be regarded as a learning of the old ones.    The paper shows that KD loss can be trained on placebos (i.e., a set of placebos that are not used in the training of the new classes) and on pseudo CIL tasks, where the placebos are generated from the same image stream as in the original CIL task. The proposed method is evaluated on top-performing CIL methods on two higher-resolution benchmarks (ImageNet-1k and ImageNet-Subset) and is shown to outperform a number of existing methods. It is also shown that the performance of old class exemplars trained with supervision and a memory budget can be improved. "
3695,SP:506e0a888c03a955b708464eed3670c04baf4912,"approach USED-FOR modeling discrete structure. Energy - based Models ( EBMs ) USED-FOR modeling discrete structure. inference CONJUNCTION learning of EBM. learning of EBM CONJUNCTION inference. Energy - based Models ( EBMs ) USED-FOR approach. inference USED-FOR EBM. sampling from discrete distributions USED-FOR it. Markov Chain Monte Carlo ( MCMC ) USED-FOR sampling. informed proposal USED-FOR Markov Chain Monte Carlo ( MCMC ). local updates FEATURE-OF informed proposal. energy changes USED-FOR it. composition of local moves USED-FOR path auxiliary algorithm. sampling CONJUNCTION inference. inference CONJUNCTION sampling. inference CONJUNCTION learning. learning CONJUNCTION inference. path auxiliary algorithms COMPARE generic samplers. generic samplers COMPARE path auxiliary algorithms. generic samplers USED-FOR sampling. generic samplers USED-FOR discrete models. path auxiliary algorithms USED-FOR discrete models. discrete models USED-FOR sampling. discrete models USED-FOR inference. generic samplers USED-FOR inference. high dimensional discrete data USED-FOR deep EBMs. OtherScientificTerm are discrete distributions, evaluation of energy function, and linearization of the energy function. Generic is algorithm. ","This paper proposes a new approach to modeling discrete structure using Energy-based Models (EBMs). The key idea of the approach is to combine inference and learning of EBM with sampling from discrete distributions. The sampling is done using Markov Chain Monte Carlo (MCMC) with an informed proposal that is based on local updates of the energy function. The authors propose a path auxiliary algorithm based on the composition of local moves, where the energy changes are computed as a function of the evaluation of energy function and the number of local updates. The algorithm is shown to be computationally efficient. The paper also shows that path auxiliary algorithms are more efficient than generic samplers for sampling and inference, and that deep EBMs can be trained on high dimensional discrete data. Finally, the authors show that the sampling of discrete models from discrete models using discrete models with path auxiliary methods outperforms generic sampler and inference. ","This paper proposes a new approach to modeling discrete structure using Energy-based Models (EBMs). The key idea of the approach is to combine inference and learning of EBM with sampling from discrete distributions. The sampling is done using Markov Chain Monte Carlo (MCMC) with an informed proposal that is based on local updates of the energy function. The authors propose a path auxiliary algorithm based on the composition of local moves, where the energy changes are computed as a function of the evaluation of energy function and the number of local updates. The algorithm is shown to be computationally efficient. The paper also shows that path auxiliary algorithms are more efficient than generic samplers for sampling and inference, and that deep EBMs can be trained on high dimensional discrete data. Finally, the authors show that the sampling of discrete models from discrete models using discrete models with path auxiliary methods outperforms generic sampler and inference. "
3711,SP:4b466277aa5561a80c48d5e72559de4ce95f228b,"Discovery and learning of an underlying spatiotemporal hierarchy PART-OF machine learning. sequential data USED-FOR Discovery and learning of an underlying spatiotemporal hierarchy. layerwise representations USED-FOR hierarchical generative models. Variational Predictive Routing ( VPR ) HYPONYM-OF neural probabilistic inference system. neural probabilistic inference system USED-FOR latent representations of video features. temporal hierarchy FEATURE-OF latent representations of video features. hierarchical renewal process USED-FOR continuous data. VPR USED-FOR organisation of representations. event detection mechanism USED-FOR VPR. organisation of representations PART-OF model. latent hierarchy USED-FOR organisation of representations. system USED-FOR event detection mechanism. latent representations USED-FOR event detection mechanism. VPR USED-FOR event boundaries. VPR USED-FOR timeagnostic rollouts. video datasets EVALUATE-FOR VPR. framework USED-FOR model - based reinforcement learning. approach USED-FOR framework. approach USED-FOR model - based reinforcement learning. neuroscience USED-FOR approach. OtherScientificTerm are spatiotemporal hierarchy, temporal dynamics, spatiotemporal features, hierarchy, and flexible and informative state - space rollouts. ","This paper proposes Variational Predictive Routing (VPR), a method for learning a spatiotemporal hierarchy in a sequential data. The idea is to learn a hierarchical generative model for a sequence of video frames, where each frame is represented as a layer-wise latent variable. This latent variable is then used to train a neural network to predict the next layer of the latent variable, which is used to learn an event detection mechanism. The proposed method is evaluated on a number of video datasets.  ","This paper proposes Variational Predictive Routing (VPR), a method for learning a spatiotemporal hierarchy in a sequential data. The idea is to learn a hierarchical generative model for a sequence of video frames, where each frame is represented as a layer-wise latent variable. This latent variable is then used to train a neural network to predict the next layer of the latent variable, which is used to learn an event detection mechanism. The proposed method is evaluated on a number of video datasets.  "
3727,SP:459ef2e6bd7638020955dbb4d8ae1098619f7b95,global features USED-FOR methods. re - ranking process USED-FOR global features. it USED-FOR accurate and semantic local information. spatial and channel attention CONJUNCTION intermediate supervision. intermediate supervision CONJUNCTION spatial and channel attention. convolutional neural networks COMPARE RANSAC algorithm. RANSAC algorithm COMPARE convolutional neural networks. it USED-FOR UGALR. spatial and channel attention USED-FOR it. intermediate supervision USED-FOR it. RANSAC algorithm USED-FOR local feature matching. spatial and channel attention USED-FOR accurate and semantic local information. convolutional neural networks USED-FOR local feature matching. Oxford and Paris datasets EVALUATE-FOR approach. Task is Image retrieval. OtherScientificTerm is features. Method is local feature learning. Metric is memory consumption. ,"Image retrieval is a challenging problem, and existing methods rely on global features. However, the re-ranking process in such methods is expensive and time-consuming. This paper proposes a local feature learning approach to tackle this problem. The key idea is to use convolutional neural networks instead of the standard RANSAC algorithm, and use it to improve UGALR by using spatial and channel attention and intermediate supervision to capture accurate and semantic local information. Experiments on Oxford and Paris datasets show that the proposed approach achieves state-of-the-art performance. The paper is well-written and well-motivated, and the experiments are well-designed. The results are interesting and interesting, and show that local feature matching can be achieved with convolution neural networks without memory consumption.   ","Image retrieval is a challenging problem, and existing methods rely on global features. However, the re-ranking process in such methods is expensive and time-consuming. This paper proposes a local feature learning approach to tackle this problem. The key idea is to use convolutional neural networks instead of the standard RANSAC algorithm, and use it to improve UGALR by using spatial and channel attention and intermediate supervision to capture accurate and semantic local information. Experiments on Oxford and Paris datasets show that the proposed approach achieves state-of-the-art performance. The paper is well-written and well-motivated, and the experiments are well-designed. The results are interesting and interesting, and show that local feature matching can be achieved with convolution neural networks without memory consumption.   "
3743,SP:487cc308a1e8ee078c54b2158bcae47e920e73f8,"computer vision CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION computer vision. Multitask learning USED-FOR applications domains. computer vision HYPONYM-OF applications domains. reinforcement learning HYPONYM-OF applications domains. algorithm USED-FOR negative transfer. it USED-FOR gradient magnitudes. RotoGrad USED-FOR negative transfer. RotoGrad HYPONYM-OF algorithm. multi - label classification CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION multi - label classification. RotoGrad COMPARE methods. methods COMPARE RotoGrad. CelebA CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION CelebA. RotoGrad USED-FOR complex problems. methods USED-FOR complex problems. NYUv2 dataset USED-FOR computer vision tasks. CelebA USED-FOR multi - label classification. computer vision tasks HYPONYM-OF complex problems. multi - label classification HYPONYM-OF complex problems. OtherScientificTerm are shared network parameters, gradient magnitude, gradient directions, and training convergence. Method is Pytorch implementation. ","Multitask learning is an important problem in many applications domains such as computer vision and reinforcement learning. In this paper, the authors propose an algorithm called RotoGrad, which aims to address the problem of negative transfer between different tasks with shared network parameters. The algorithm is based on the Pytorch implementation of the gradient magnitude, and the authors show that it is able to adaptively adjust the gradient magnitudes for different tasks in order to improve the gradient directions. The authors also show that the proposed algorithm can be applied to complex problems such as multi-label classification and computer vision tasks on the NYUv2 dataset. They show that under certain conditions, the proposed method can outperform existing methods on complex problems including multi-labels classification, multi-class classification, and several other complex problems.   ","Multitask learning is an important problem in many applications domains such as computer vision and reinforcement learning. In this paper, the authors propose an algorithm called RotoGrad, which aims to address the problem of negative transfer between different tasks with shared network parameters. The algorithm is based on the Pytorch implementation of the gradient magnitude, and the authors show that it is able to adaptively adjust the gradient magnitudes for different tasks in order to improve the gradient directions. The authors also show that the proposed algorithm can be applied to complex problems such as multi-label classification and computer vision tasks on the NYUv2 dataset. They show that under certain conditions, the proposed method can outperform existing methods on complex problems including multi-labels classification, multi-class classification, and several other complex problems.   "
3759,SP:050cd8319d84a1bd8c2ccb930ba69b33c8fb6e60,soft neuron association USED-FOR pre - trained networks. soft neuron association USED-FOR Layer - wise model fusion. optimal transport USED-FOR Layer - wise model fusion. networks USED-FOR OTFusion. model fusion framework USED-FOR neural networks. CLAFusion HYPONYM-OF model fusion framework. cross - layer alignment USED-FOR model fusion framework. cross - layer alignment USED-FOR heterogeneous neural networks. unbalanced assignment problem USED-FOR cross - layer alignment problem. dynamic programming USED-FOR cross - layer alignment problem. cross - layer alignment USED-FOR framework. framework USED-FOR layer - wise model fusion. number of layers of neural networks USED-FOR framework. CLAFusion USED-FOR fused network. finetuning process USED-FOR it. finetuning process USED-FOR residual networks. residual networks EVALUATE-FOR it. CIFAR10 dataset EVALUATE-FOR residual networks. CIFAR10 dataset EVALUATE-FOR it. model compression CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION model compression. knowledge distillation USED-FOR teacher - student setting. Material is heterogeneous data. OtherScientificTerm is retraining. ,"Layer-wise model fusion is based on soft neuron association between pre-trained networks and their weights via optimal transport. The authors propose a new model fusion framework called CLAFusion to fuse neural networks with heterogeneous data. OTFusion is used to fuse two networks with the same number of layers. The proposed framework uses cross-layer alignment to fuse heterogeneous neural networks by solving the cross-layton alignment problem, which is an unbalanced assignment problem. This is solved using dynamic programming. The framework can be applied to layer-wide model fusion for any number of neural networks, and it can be used to apply the finetuning process to any residual networks. The fused network can be trained with CLAFusions.  The authors evaluate it on the CIFAR10 dataset, and show that it achieves state-of-the-art performance in terms of model compression and knowledge distillation in the teacher-student setting. They also show that the proposed framework is robust to the number of neurons and the amount of retraining.  ","Layer-wise model fusion is based on soft neuron association between pre-trained networks and their weights via optimal transport. The authors propose a new model fusion framework called CLAFusion to fuse neural networks with heterogeneous data. OTFusion is used to fuse two networks with the same number of layers. The proposed framework uses cross-layer alignment to fuse heterogeneous neural networks by solving the cross-layton alignment problem, which is an unbalanced assignment problem. This is solved using dynamic programming. The framework can be applied to layer-wide model fusion for any number of neural networks, and it can be used to apply the finetuning process to any residual networks. The fused network can be trained with CLAFusions.  The authors evaluate it on the CIFAR10 dataset, and show that it achieves state-of-the-art performance in terms of model compression and knowledge distillation in the teacher-student setting. They also show that the proposed framework is robust to the number of neurons and the amount of retraining.  "
3775,SP:f764eae15cd083fdb4eb2af09ac64c2d878a454f,"generalization EVALUATE-FOR deep networks. supervised learning USED-FOR deep networks. implicit regularization FEATURE-OF overparameterized deep networks. stochastic gradient descent USED-FOR implicit regularization. SGD USED-FOR supervised learning. SGD USED-FOR implicit regularization. regularizer USED-FOR degenerate solutions. implicit regularization USED-FOR temporal difference learning. regularizer COMPARE supervised learning case. supervised learning case COMPARE regularizer. representations USED-FOR state - action pairs. bootstrapping USED-FOR deep network value function. deep network value function USED-FOR feature representations. bootstrapping USED-FOR feature representations. explicit regularizer USED-FOR implicit regularizer. DR3 HYPONYM-OF explicit regularizer. D4RL domains CONJUNCTION robotic manipulation. robotic manipulation CONJUNCTION D4RL domains. Atari 2600 games CONJUNCTION D4RL domains. D4RL domains CONJUNCTION Atari 2600 games. DR3 USED-FOR unlearning. DR3 USED-FOR robotic manipulation. performance CONJUNCTION stability. stability CONJUNCTION performance. unlearning CONJUNCTION D4RL domains. D4RL domains CONJUNCTION unlearning. offline RL methods COMPARE DR3. DR3 COMPARE offline RL methods. unlearning CONJUNCTION robotic manipulation. robotic manipulation CONJUNCTION unlearning. Atari 2600 games FEATURE-OF unlearning. stability EVALUATE-FOR DR3. images USED-FOR robotic manipulation. performance EVALUATE-FOR DR3. Method are overparameterization, and deep reinforcement learning ( RL ) methods. OtherScientificTerm are parsimonious solutions, degenerate feature representations, and Bellman backup. Task is offline deep RL setting. ","This paper studies the implicit regularization of overparameterized deep networks in the context of generalization in deep networks. In particular, the authors show that stochastic gradient descent (SGD) can be used as an implicit regularizer for the overparametrized deep networks, and that this regularizer is more robust to degenerate solutions than SGD in supervised learning. The authors also show that the degenerate feature representations learned by SGD can be replaced by an explicit regularizer called DR3, which is a regularizer that encourages parsimonious solutions to be learned in the presence of degenerate features. This regularizer has the advantage of being more robust than the supervised learning case, as the representations learned for state-action pairs in the offline deep RL setting are more likely to be degenerate.    The authors further show that DR3 is able to learn feature representations with bootstrapping, where the deep network value function is approximated by the bootstrapped version of the feature representations. DR3 improves performance on unlearning in Atari 2600 games, D4RL domains, robotic manipulation on images, and deep reinforcement learning (RL) methods in general. The performance and stability of DR3 are improved over offline RL methods that do not use DR3. ","This paper studies the implicit regularization of overparameterized deep networks in the context of generalization in deep networks. In particular, the authors show that stochastic gradient descent (SGD) can be used as an implicit regularizer for the overparametrized deep networks, and that this regularizer is more robust to degenerate solutions than SGD in supervised learning. The authors also show that the degenerate feature representations learned by SGD can be replaced by an explicit regularizer called DR3, which is a regularizer that encourages parsimonious solutions to be learned in the presence of degenerate features. This regularizer has the advantage of being more robust than the supervised learning case, as the representations learned for state-action pairs in the offline deep RL setting are more likely to be degenerate.    The authors further show that DR3 is able to learn feature representations with bootstrapping, where the deep network value function is approximated by the bootstrapped version of the feature representations. DR3 improves performance on unlearning in Atari 2600 games, D4RL domains, robotic manipulation on images, and deep reinforcement learning (RL) methods in general. The performance and stability of DR3 are improved over offline RL methods that do not use DR3. "
3791,SP:6fd793b27123bf80504e2ad5957455b7ec311612,RLSVI USED-FOR posterior samples. algorithm USED-FOR deep RL. HyperDQN HYPONYM-OF algorithm. non - linear neural network USED-FOR Q - values. base model HYPONYM-OF non - linear neural network. meta model HYPONYM-OF probabilistic hypermodel. probabilistic hypermodel USED-FOR method. hypermodel USED-FOR approximate posterior samples. Q - value functions USED-FOR exploratory action sequences. RLSVI USED-FOR exploration. posterior samples USED-FOR Q - value function. Atari suite EVALUATE-FOR HyperDQN. Atari suite EVALUATE-FOR DQN. HyperDQN COMPARE DQN. DQN COMPARE HyperDQN. maximum human - normalized score EVALUATE-FOR DQN. maximum human - normalized score EVALUATE-FOR HyperDQN. HyperDQN COMPARE exploration bonus and randomized exploration methods. exploration bonus and randomized exploration methods COMPARE HyperDQN. HyperDQN USED-FOR SuperMarioBros. exploration bonus and randomized exploration methods USED-FOR SuperMarioBros. Method is exploration method. OtherScientificTerm is feature. Generic is models. Metric is efficiency. ,"This paper proposes HyperDQN, an algorithm for deep RL that uses RLSVI to generate posterior samples from a non-linear neural network (i.e., a base model) to approximate Q-values. The proposed method is based on a probabilistic hypermodel, called a meta model, which is a nonlinear combination of a base and a hypermodel. The hypermodel is used to generate approximate posterior samples for the Q-value function, which are then used to train a hyper model for the exploration method.  The authors show that the proposed method outperforms DQN on the Atari suite and achieves the maximum human-normalized score. The authors also show that RLSVi can be used for exploration, and that the posterior samples generated from posterior samples can be efficiently used to compute Q-valued functions for the exploratory action sequences.   The paper also shows that HyperQQN outperforms the exploration bonus and randomized exploration methods for SuperMarioBros, and outperforms a number of existing exploration bonus methods. The main contribution of the paper is that the authors propose to use probabilistically trained models for exploration. The paper is well-written and easy to follow.","This paper proposes HyperDQN, an algorithm for deep RL that uses RLSVI to generate posterior samples from a non-linear neural network (i.e., a base model) to approximate Q-values. The proposed method is based on a probabilistic hypermodel, called a meta model, which is a nonlinear combination of a base and a hypermodel. The hypermodel is used to generate approximate posterior samples for the Q-value function, which are then used to train a hyper model for the exploration method.  The authors show that the proposed method outperforms DQN on the Atari suite and achieves the maximum human-normalized score. The authors also show that RLSVi can be used for exploration, and that the posterior samples generated from posterior samples can be efficiently used to compute Q-valued functions for the exploratory action sequences.   The paper also shows that HyperQQN outperforms the exploration bonus and randomized exploration methods for SuperMarioBros, and outperforms a number of existing exploration bonus methods. The main contribution of the paper is that the authors propose to use probabilistically trained models for exploration. The paper is well-written and easy to follow."
3807,SP:b428383660928374c953f659ea1e05852dbdcd6e,"representation learning USED-FOR model. image classification CONJUNCTION recommender systems. recommender systems CONJUNCTION image classification. representation learning USED-FOR downstream tasks. downstream tasks EVALUATE-FOR model. recommender systems HYPONYM-OF real - world scenarios. image classification HYPONYM-OF real - world scenarios. cause, effect and spurious correlated variables PART-OF representation. hypothetical causal graph USED-FOR mutual information measures. mutual information measures USED-FOR learning procedure. learning procedure USED-FOR causal representation. hypothetical causal graph USED-FOR learning procedure. observational data USED-FOR causal representation. reduced sample complexity CONJUNCTION generalization ability. generalization ability CONJUNCTION reduced sample complexity. counterfactual loss PART-OF optimization. reduced sample complexity EVALUATE-FOR causality - inspired learning. generalization ability EVALUATE-FOR causality - inspired learning. adversarial attacks CONJUNCTION distribution shift. distribution shift CONJUNCTION adversarial attacks. causal representations USED-FOR models. adversarial attacks USED-FOR models. approach USED-FOR causal representations. approach USED-FOR models. Method is learning approaches. OtherScientificTerm is features. Metric is generalizability. ",This paper proposes a new representation learning method for learning causal representations of the causal structure of the data. The proposed method is based on the idea that causal representations can be learned from observational data and can be used to improve the generalization performance of the model. The authors propose a new counterfactual loss function that is designed to encourage the model to learn a causal representation that is more robust to spurious correlations in the observational data. Experiments on image classification and recommender systems demonstrate the effectiveness of the proposed method.  ,This paper proposes a new representation learning method for learning causal representations of the causal structure of the data. The proposed method is based on the idea that causal representations can be learned from observational data and can be used to improve the generalization performance of the model. The authors propose a new counterfactual loss function that is designed to encourage the model to learn a causal representation that is more robust to spurious correlations in the observational data. Experiments on image classification and recommender systems demonstrate the effectiveness of the proposed method.  
3823,SP:1258c05a80a17949b50e6dae13deea1d2235f456,Federated learning HYPONYM-OF distributed learning scheme. edge devices USED-FOR model. training USED-FOR edge devices. gradient compression CONJUNCTION distillation. distillation CONJUNCTION gradient compression. gradient compression HYPONYM-OF compact formats. distillation HYPONYM-OF compact formats. progressive training framework USED-FOR federated learning. ProgFed HYPONYM-OF progressive training framework. It COMPARE models. models COMPARE It. asymptotic rate EVALUATE-FOR ProgFed. ResNet CONJUNCTION ConvNets. ConvNets CONJUNCTION ResNet. VGG CONJUNCTION ResNet. ResNet CONJUNCTION VGG. ConvNets CONJUNCTION U - nets. U - nets CONJUNCTION ConvNets. computation CONJUNCTION communication costs. communication costs CONJUNCTION computation. simple classification CONJUNCTION medical image segmentation. medical image segmentation CONJUNCTION simple classification. communication costs EVALUATE-FOR converged models. VGG CONJUNCTION ConvNets. ConvNets CONJUNCTION VGG. training approach USED-FOR converged models. tasks EVALUATE-FOR training approach. tasks HYPONYM-OF architectures. medical image segmentation HYPONYM-OF tasks. simple classification HYPONYM-OF tasks. computation EVALUATE-FOR training approach. communication costs EVALUATE-FOR training approach. VGG HYPONYM-OF architectures. ConvNets HYPONYM-OF architectures. ResNet HYPONYM-OF architectures. U - nets HYPONYM-OF architectures. approach COMPARE compression. compression COMPARE approach. OtherScientificTerm is limited network bandwidth. Generic is full models. ,"This paper proposes a federated learning scheme, called ProgFed, which is a distributed learning scheme where the model is trained on edge devices with limited network bandwidth. The authors propose a progressive training framework, called Progressive Fed, to accelerate the training of edge devices during training. It outperforms existing models in terms of asymptotic rate, and is able to learn compact formats (e.g. gradient compression and distillation). The authors also show that their training approach can be used to train converged models with lower computation and communication costs. The training approach is evaluated on three different architectures (VGG, ResNet, ConvNets and U-nets) and three different tasks (simple classification, medical image segmentation). The results show that the proposed approach outperforms compression in all three cases, and can be applied to full models.","This paper proposes a federated learning scheme, called ProgFed, which is a distributed learning scheme where the model is trained on edge devices with limited network bandwidth. The authors propose a progressive training framework, called Progressive Fed, to accelerate the training of edge devices during training. It outperforms existing models in terms of asymptotic rate, and is able to learn compact formats (e.g. gradient compression and distillation). The authors also show that their training approach can be used to train converged models with lower computation and communication costs. The training approach is evaluated on three different architectures (VGG, ResNet, ConvNets and U-nets) and three different tasks (simple classification, medical image segmentation). The results show that the proposed approach outperforms compression in all three cases, and can be applied to full models."
3839,SP:8cdaa6e0dafd750ebdb5d7a4c1987a042400662f,"adversarial attacks FEATURE-OF Deep neural networks. Adversarial training USED-FOR model. adversarial Rademacher complexity FEATURE-OF adversarial training. two - layer neural networks USED-FOR adversarial Rademacher complexity. adversarial Rademacher complexity FEATURE-OF deep neural networks. Rademacher complexity EVALUATE-FOR neural nets. product of weight norms PART-OF bound. adversarially trained weight norms COMPARE trained weight norms. trained weight norms COMPARE adversarially trained weight norms. Generic are models, and method. Task is adversarial settings. OtherScientificTerm is layer. ","This paper studies the Rademacher complexity of deep neural networks under adversarial attacks. Deep neural networks have been shown to be robust to a variety of types of attacks. Adversarial training is a common technique to improve the robustness of a model. This paper studies two-layer neural networks, and shows that adversarial training has the same adversarial Rademachacher complexity as the standard adversarial robust training. The authors also show that two-layered neural networks can achieve a similar adversarial complexity to the standard robust training in adversarial settings. The main contribution of this paper is to show that the robust performance of neural nets can be improved by reducing the Radmacher complexity.   The authors propose a novel method to do so. They show that adversarially trained weight norms are asymptotically similar to the trained weight norm of the original layer. The bound is based on the product of weight norms between the original and adversarial layers. ","This paper studies the Rademacher complexity of deep neural networks under adversarial attacks. Deep neural networks have been shown to be robust to a variety of types of attacks. Adversarial training is a common technique to improve the robustness of a model. This paper studies two-layer neural networks, and shows that adversarial training has the same adversarial Rademachacher complexity as the standard adversarial robust training. The authors also show that two-layered neural networks can achieve a similar adversarial complexity to the standard robust training in adversarial settings. The main contribution of this paper is to show that the robust performance of neural nets can be improved by reducing the Radmacher complexity.   The authors propose a novel method to do so. They show that adversarially trained weight norms are asymptotically similar to the trained weight norm of the original layer. The bound is based on the product of weight norms between the original and adversarial layers. "
3855,SP:925d6bb051e9b384669fb695085b678c11f7c11a,Estimation of ( differential ) entropy CONJUNCTION mutual information. mutual information CONJUNCTION Estimation of ( differential ) entropy. estimators USED-FOR differential entropy. approach USED-FOR KNIFE - based estimators. KNIFE - based estimators USED-FOR mutual information. neural networks USED-FOR real - world tasks. it USED-FOR neural networks. high - dimensional synthetic data EVALUATE-FOR method. visual domain adaptation CONJUNCTION textual fair classification. textual fair classification CONJUNCTION visual domain adaptation. textual fair classification CONJUNCTION textual fine - tuning. textual fine - tuning CONJUNCTION textual fair classification. tasks EVALUATE-FOR KNIFE - based estimation. textual fine - tuning EVALUATE-FOR KNIFE - based estimation. textual fine - tuning HYPONYM-OF tasks. visual domain adaptation HYPONYM-OF tasks. textual fair classification HYPONYM-OF tasks. Method is KNIFE. ,"This paper proposes a new approach to estimate the differential entropy and mutual information between two sets of data points. The proposed method, called Knife, is based on the idea of estimating the (differential) entropy and the mutual information. The authors show that the proposed approach can be used to improve the performance of existing KNIFE-based estimators for differential entropy. They also show that it can be applied to train neural networks for real-world tasks. The method is evaluated on high-dimensional synthetic data and is shown to perform well on a number of tasks, including visual domain adaptation, textual fair classification, and textual fine-tuning.","This paper proposes a new approach to estimate the differential entropy and mutual information between two sets of data points. The proposed method, called Knife, is based on the idea of estimating the (differential) entropy and the mutual information. The authors show that the proposed approach can be used to improve the performance of existing KNIFE-based estimators for differential entropy. They also show that it can be applied to train neural networks for real-world tasks. The method is evaluated on high-dimensional synthetic data and is shown to perform well on a number of tasks, including visual domain adaptation, textual fair classification, and textual fine-tuning."
3871,SP:d2f3beac855f0d72c13552fecb2bdb9d42195df3,"ε - greedy CONJUNCTION softmax. softmax CONJUNCTION ε - greedy. action - value methods USED-FOR reinforcement learning. Soft - greedy operators USED-FOR exploration. exploration USED-FOR action - value methods. ε - greedy HYPONYM-OF Soft - greedy operators. softmax HYPONYM-OF Soft - greedy operators. resmax HYPONYM-OF soft - greedy operator. It USED-FOR coverage of the state - space. It USED-FOR exploration. it COMPARE softmax. softmax COMPARE it. non - expansion USED-FOR it. exploration hyperparameter USED-FOR non - expansion. mellowmax HYPONYM-OF non - expansion. state - action specific temperature USED-FOR softmax policy. resmax COMPARE softmax. softmax COMPARE resmax. resmax COMPARE ε - greedy. ε - greedy COMPARE resmax. ε - greedy CONJUNCTION softmax. softmax CONJUNCTION ε - greedy. Generic is operators. OtherScientificTerm are suboptimality gap, and overemphasizing sub - optimal actions. Task is learning. Material is tabular and deep RL. ","Soft-greedy operators such as resmax and softmax have been widely used for exploration in action-value methods in reinforcement learning. However, these operators suffer from the suboptimality gap, which is a result of overemphasizing sub-optimal actions. This paper proposes a new soft-rewarder, called mellowmax, that is a variant of resmax. It allows for better coverage of the state-space, and it is more robust to non-expansion (e.g., mellowmin) than softmax. The authors also propose an exploration hyperparameter that encourages non-excess expansion. The paper also proposes a softmax policy with a state-action specific temperature. Experiments on tabular and deep RL show that the proposed resmax outperforms softmax, and the authors also show that it can be used in combination with softmax to improve the performance. ","Soft-greedy operators such as resmax and softmax have been widely used for exploration in action-value methods in reinforcement learning. However, these operators suffer from the suboptimality gap, which is a result of overemphasizing sub-optimal actions. This paper proposes a new soft-rewarder, called mellowmax, that is a variant of resmax. It allows for better coverage of the state-space, and it is more robust to non-expansion (e.g., mellowmin) than softmax. The authors also propose an exploration hyperparameter that encourages non-excess expansion. The paper also proposes a softmax policy with a state-action specific temperature. Experiments on tabular and deep RL show that the proposed resmax outperforms softmax, and the authors also show that it can be used in combination with softmax to improve the performance. "
3887,SP:792ae8808aa6902758146aef1548c975492b833c,"learnability FEATURE-OF deep learning models. concept USED-FOR model. concept USED-FOR learnability. learnability lock USED-FOR model. learnability lock USED-FOR learnability. learnability EVALUATE-FOR model. learnability FEATURE-OF dataset. universal transformation function USED-FOR class - wise perturbation. class - wise perturbation USED-FOR learnability lock. inverse transformation USED-FOR learnability. visual classification tasks EVALUATE-FOR method. Task are information technology, learnability attack, and preventing unauthorized exploitation. Method are deep learning, commercial models, adversarial invertible transformation, and machine learning models. OtherScientificTerm are digital formats, and visual features. Material is image. Generic is models. ","This paper studies the problem of learnability of deep learning models. The authors propose a new concept called learnability lock, which is the concept that a model with a certain class-wise perturbation will not be able to learn a given dataset. They show that this learnability is a result of a class-level transformation function that is invertible. They also show that a universal transformation function can be used as a universal function to enforce the learnability on any dataset. Finally, they show that the inverse transformation can also be used to enforce learnability. ","This paper studies the problem of learnability of deep learning models. The authors propose a new concept called learnability lock, which is the concept that a model with a certain class-wise perturbation will not be able to learn a given dataset. They show that this learnability is a result of a class-level transformation function that is invertible. They also show that a universal transformation function can be used as a universal function to enforce the learnability on any dataset. Finally, they show that the inverse transformation can also be used to enforce learnability. "
3903,SP:9af10703605e620e563241e2602a50b629f3d37a,"Graph Neural Networks ( GNNs ) USED-FOR modeling relational data. node or edge features FEATURE-OF graph. features PART-OF real - world applications. approach USED-FOR missing features. approach USED-FOR graph machine learning applications. approach USED-FOR diffusion - type differential equation. graph FEATURE-OF diffusion - type differential equation. minimization of the Dirichlet energy USED-FOR approach. Feature Propagation HYPONYM-OF algorithm. approach COMPARE methods. methods COMPARE approach. missing features EVALUATE-FOR approach. common node - classification benchmarks EVALUATE-FOR approach. common node - classification benchmarks EVALUATE-FOR methods. nodes CONJUNCTION edges. edges CONJUNCTION nodes. edges PART-OF GPU. nodes FEATURE-OF graph. edges PART-OF graph. Generic are they, and equation. Material is social networks. ","Graph Neural Networks (GNNs) are an important technique for modeling relational data, but they can be problematic when there are missing node or edge features in the graph. In real-world applications, these features are often missing in the form of missing features. This paper proposes an approach to learning missing features in graph machine learning applications by solving a diffusion-type differential equation on the graph, based on the minimization of the Dirichlet energy. The authors propose an algorithm called Feature Propagation, which is a simple and effective algorithm. The proposed approach outperforms existing methods on common node-classification benchmarks with missing features on a number of common graph-level tasks.    The main contribution of this paper is that the authors propose a new approach to solving the problem of learning the diffusion of a graph with nodes and edges in a GPU, which can be used to solve a diffusion equation on a graph. This is an important problem in social networks, where the graph is a collection of nodes or edges, and the nodes are connected to each other. ","Graph Neural Networks (GNNs) are an important technique for modeling relational data, but they can be problematic when there are missing node or edge features in the graph. In real-world applications, these features are often missing in the form of missing features. This paper proposes an approach to learning missing features in graph machine learning applications by solving a diffusion-type differential equation on the graph, based on the minimization of the Dirichlet energy. The authors propose an algorithm called Feature Propagation, which is a simple and effective algorithm. The proposed approach outperforms existing methods on common node-classification benchmarks with missing features on a number of common graph-level tasks.    The main contribution of this paper is that the authors propose a new approach to solving the problem of learning the diffusion of a graph with nodes and edges in a GPU, which can be used to solve a diffusion equation on a graph. This is an important problem in social networks, where the graph is a collection of nodes or edges, and the nodes are connected to each other. "
3919,SP:cbaa3f1379fa99159899d79ccb479c0187403aca,"limited labeled data USED-FOR model. heuristics USED-FOR sample selection strategies. integer optimization problem USED-FOR core set. discrete Wasserstein distance FEATURE-OF unlabeled pool. Generalized Benders Decomposition algorithm USED-FOR problem. unlabeled pool USED-FOR unsupervised learning. unsupervised learning USED-FOR latent features. latent features USED-FOR strategy. optimization approach COMPARE baselines. baselines COMPARE optimization approach. data sets EVALUATE-FOR optimization approach. optimization approach COMPARE them. them COMPARE optimization approach. data sets EVALUATE-FOR baselines. them USED-FOR low budget regime. optimization approach USED-FOR low budget regime. Method are Active learning, and deep learning. OtherScientificTerm is unlabeled data pool. ","Active learning is an important problem in deep learning, where the goal is to learn a model with limited labeled data. The paper proposes two heuristics to improve sample selection strategies. The first is to use an integer optimization problem to find a core set of samples from an unlabeled data pool, and then to use the Generalized Benders Decomposition algorithm to solve the problem. The second strategy uses the learned latent features from the unsupervised learning to select samples from the unlabeling pool based on the discrete Wasserstein distance. The optimization approach is evaluated on two data sets, and compared with several baselines and shows that the optimization approach outperforms them in the low budget regime. ","Active learning is an important problem in deep learning, where the goal is to learn a model with limited labeled data. The paper proposes two heuristics to improve sample selection strategies. The first is to use an integer optimization problem to find a core set of samples from an unlabeled data pool, and then to use the Generalized Benders Decomposition algorithm to solve the problem. The second strategy uses the learned latent features from the unsupervised learning to select samples from the unlabeling pool based on the discrete Wasserstein distance. The optimization approach is evaluated on two data sets, and compared with several baselines and shows that the optimization approach outperforms them in the low budget regime. "
3935,SP:4c72923f78ca6590dc11e10d1a2403076a583718,"manual inspection USED-FOR genome reconstruction. approach USED-FOR assembling genomes. method USED-FOR approach. method USED-FOR assembling genomes. geometric deep learning USED-FOR genome assembly. geometric deep learning USED-FOR assembly graph. genomic sequence USED-FOR assembly graph. graph convolutional network USED-FOR genome. dataset USED-FOR graph convolutional network. human genomic data USED-FOR graph convolutional network. human genomic data USED-FOR dataset. greedy search algorithm COMPARE greedy search. greedy search COMPARE greedy search algorithm. greedy search algorithm USED-FOR graph topology. greedy search algorithm USED-FOR model. graph machine learning algorithms USED-FOR de novo genome assembly problem. graph machine learning algorithms COMPARE human handcrafted techniques. human handcrafted techniques COMPARE graph machine learning algorithms. Material is human DNA sequence. OtherScientificTerm are telomere, and graph. Metric is assembly speed. Generic is it. Method is de novo assemblers. ","This paper proposes a method for assembling genomes using a method based on geometric deep learning for genome assembly. The idea is to train a graph convolutional network on a dataset based on human genomic data, where a human DNA sequence is represented as a graph, and a genomic sequence is used to construct an assembly graph from the genomic sequence. This graph is then used for manual inspection for genome reconstruction. The authors propose a greedy search algorithm to find the best graph topology, which is faster than the traditional greedy search. They also propose a dataset for training the graph neural network on which the dataset is constructed from human genomic DNA. They show that the proposed graph machine learning algorithms for the de novo genome assembly problem outperform human handcrafted techniques, and that the model can be trained using a single sample from the dataset. The paper is well-written and well-motivated, and the paper is clearly written. However, the paper suffers from a lack of comparison with the state-of-the-art in terms of assembly speed, which makes it difficult to judge the quality of the paper.  The paper also suffers from the lack of comparisons with the best of the best in the field, which may be a limitation of the work.  In addition to the issues with the paper, there are also questions about the performance of the proposed model, and whether the proposed method can be applied to a more complex task (e.g. de-novo assembly of a telomere).    The authors also have questions about how to train the graph network and how to select the best de novevo assemblers.  ","This paper proposes a method for assembling genomes using a method based on geometric deep learning for genome assembly. The idea is to train a graph convolutional network on a dataset based on human genomic data, where a human DNA sequence is represented as a graph, and a genomic sequence is used to construct an assembly graph from the genomic sequence. This graph is then used for manual inspection for genome reconstruction. The authors propose a greedy search algorithm to find the best graph topology, which is faster than the traditional greedy search. They also propose a dataset for training the graph neural network on which the dataset is constructed from human genomic DNA. They show that the proposed graph machine learning algorithms for the de novo genome assembly problem outperform human handcrafted techniques, and that the model can be trained using a single sample from the dataset. The paper is well-written and well-motivated, and the paper is clearly written. However, the paper suffers from a lack of comparison with the state-of-the-art in terms of assembly speed, which makes it difficult to judge the quality of the paper.  The paper also suffers from the lack of comparisons with the best of the best in the field, which may be a limitation of the work.  In addition to the issues with the paper, there are also questions about the performance of the proposed model, and whether the proposed method can be applied to a more complex task (e.g. de-novo assembly of a telomere).    The authors also have questions about how to train the graph network and how to select the best de novevo assemblers.  "
3951,SP:24de906e4289c9073b6c55c747b0913b8df5e053,"catastrophic forgetting FEATURE-OF Continual learning. meta - learning USED-FOR metacontinual learning algorithms. experience replay ( ER ) PART-OF meta - testing. ER USED-FOR meta - testing. ER USED-FOR metatraining. ER USED-FOR continual learning representations. ER PART-OF meta - training. reservoir sampling USED-FOR replay buffer. meta - learned Predictive Sample Selection USED-FOR replay buffer. meta - learned Predictive Sample Selection COMPARE reservoir sampling. reservoir sampling COMPARE meta - learned Predictive Sample Selection. method COMPARE state - of - the - art. state - of - the - art COMPARE method. clustering structures FEATURE-OF learned representations. Method are online aware meta - learning ( OML ), and OML. Generic is model. Task is online - aware nature of OML. OtherScientificTerm is randomness. ","Continual learning suffers from catastrophic forgetting. Continual learning is an important problem, and meta-learning is a popular approach to improve the performance of metacontinual learning algorithms. In this paper, the authors propose an online aware meta - learning (OML) approach to address this problem. The authors propose to incorporate experience replay (ER) into meta-testing, and use ER in meta-training for continual learning representations. They show that ER can be used for metatraining, and show that the model learns to sample from the replay buffer based on ER. They also show that meta-learned Predictive Sample Selection is more effective at selecting a good replay buffer than reservoir sampling, and that the online-aware nature of OML is beneficial. Finally, they show that their method outperforms the state-of-the-art by a large margin, and they also demonstrate that the learned representations exhibit clustering structures that are more robust to randomness. ","Continual learning suffers from catastrophic forgetting. Continual learning is an important problem, and meta-learning is a popular approach to improve the performance of metacontinual learning algorithms. In this paper, the authors propose an online aware meta - learning (OML) approach to address this problem. The authors propose to incorporate experience replay (ER) into meta-testing, and use ER in meta-training for continual learning representations. They show that ER can be used for metatraining, and show that the model learns to sample from the replay buffer based on ER. They also show that meta-learned Predictive Sample Selection is more effective at selecting a good replay buffer than reservoir sampling, and that the online-aware nature of OML is beneficial. Finally, they show that their method outperforms the state-of-the-art by a large margin, and they also demonstrate that the learned representations exhibit clustering structures that are more robust to randomness. "
3967,SP:3c78454f053f74930979a8054cd7c8a34b6fe63d,"Multi - agent joint Q - learning USED-FOR multi - agent cooperation. Centralized Training with Decentralized Execution ( CTDE ) USED-FOR Multi - agent joint Q - learning. methods USED-FOR multi - agent credit assignment problem. Bellman optimality equation FEATURE-OF joint Q - value. Bellman optimality FEATURE-OF joint Q - value. Q - values USED-FOR joint Q - value. gradient ascent solution USED-FOR problem. ECAQ COMPARE baselines. baselines COMPARE ECAQ. ECAQ USED-FOR credit assignment. Method are centralized training, and deep neural networks. Task are explicit credit assignment problem, and multi - agent cooperation in complex problems. OtherScientificTerm is time horizon. ","Multi-agent joint Q-learning for multi-agent cooperation based on Centralized Training with Decentralized Execution (CTDE) is proposed. The authors propose two methods to solve the multi-agents credit assignment problem, which is an extension of centralized training with centralized execution. The paper considers the problem of learning the Bellman optimality equation of the joint Q - value of the agents, which can be formulated as the explicit credit assignment function. The main contribution of the paper is that the authors propose a gradient ascent solution to solve this problem.   The authors show that under certain assumptions on the Q-values of the agent and the agents’ Q-value, the authors prove that the optimal solution of the problem can be found by solving a simple optimization problem. They also show that the solution of this problem is non-trivial for deep neural networks. Finally, they show that ECAQ can be used to solve credit assignment more efficiently than other baselines.  The paper is well-written and well-motivated, and the authors provide a detailed analysis of the time horizon of their method, and provide a theoretical analysis of their results. They show that their method can be applied to multi-manual cooperation in complex problems, and show that it can solve the problem efficiently. ","Multi-agent joint Q-learning for multi-agent cooperation based on Centralized Training with Decentralized Execution (CTDE) is proposed. The authors propose two methods to solve the multi-agents credit assignment problem, which is an extension of centralized training with centralized execution. The paper considers the problem of learning the Bellman optimality equation of the joint Q - value of the agents, which can be formulated as the explicit credit assignment function. The main contribution of the paper is that the authors propose a gradient ascent solution to solve this problem.   The authors show that under certain assumptions on the Q-values of the agent and the agents’ Q-value, the authors prove that the optimal solution of the problem can be found by solving a simple optimization problem. They also show that the solution of this problem is non-trivial for deep neural networks. Finally, they show that ECAQ can be used to solve credit assignment more efficiently than other baselines.  The paper is well-written and well-motivated, and the authors provide a detailed analysis of the time horizon of their method, and provide a theoretical analysis of their results. They show that their method can be applied to multi-manual cooperation in complex problems, and show that it can solve the problem efficiently. "
3983,SP:0d2b225ac697679d10df25f371b2a718d4949b42,"transductive learning USED-FOR adversarial robustness. defenses COMPARE defense mechanisms. defense mechanisms COMPARE defenses. defenses USED-FOR bilevel optimization problem. test - time input USED-FOR model. threat analysis perspective EVALUATE-FOR defense mechanisms. threat models USED-FOR transductive - learning based defenses. attacking model space USED-FOR bilevel attack objectives. Greedy Model Space Attack ( GMSA ) HYPONYM-OF attack framework. GMSA USED-FOR transductive - learning based defenses. weak instantiations USED-FOR GMSA. Material is NeurIPS 2020. Task are ICML 2020, and adaptive attacks. Method are transductivelearning based defenses, and transductive adversarial training. OtherScientificTerm is AutoAttack. Metric is robustness. ","This paper studies the problem of adversarial robustness in the context of transductive learning. In particular, the authors propose a new attack framework called Greedy Model Space Attack (GMSA), which is motivated by the observation that existing defenses against adversarial attacks can be viewed as a bilevel optimization problem, where the model is trained with test-time input, and the defense mechanisms are evaluated from a threat analysis perspective. The authors show that the existing defenses are more robust to GMSA than existing defense mechanisms. They also show that GMSA can be used as a weak instantiation of AutoAttack, which is an existing attack framework.    The authors also provide a theoretical analysis of GMSA, and show that existing adversarial learning based defenses are not robust to the GMSA. The main contribution of the paper is that the authors provide an analysis of the robustness of existing threat models against transductively-learning based defenses, and demonstrate that adaptive attacks are more powerful than AutoAttack. The paper also shows that the attacking model space in GMSA is more robust than the original one, and that the adversarial perturbations are more effective in the case that the attacker has access to the original model space.  Finally, the paper shows that adversarial training with GMSA leads to better robustness than without it. ","This paper studies the problem of adversarial robustness in the context of transductive learning. In particular, the authors propose a new attack framework called Greedy Model Space Attack (GMSA), which is motivated by the observation that existing defenses against adversarial attacks can be viewed as a bilevel optimization problem, where the model is trained with test-time input, and the defense mechanisms are evaluated from a threat analysis perspective. The authors show that the existing defenses are more robust to GMSA than existing defense mechanisms. They also show that GMSA can be used as a weak instantiation of AutoAttack, which is an existing attack framework.    The authors also provide a theoretical analysis of GMSA, and show that existing adversarial learning based defenses are not robust to the GMSA. The main contribution of the paper is that the authors provide an analysis of the robustness of existing threat models against transductively-learning based defenses, and demonstrate that adaptive attacks are more powerful than AutoAttack. The paper also shows that the attacking model space in GMSA is more robust than the original one, and that the adversarial perturbations are more effective in the case that the attacker has access to the original model space.  Finally, the paper shows that adversarial training with GMSA leads to better robustness than without it. "
3999,SP:e7024cae196fc5eb6a62d289a95d76b532b6a36c,"batch normalization USED-FOR training of neural networks. batch renormalization USED-FOR small minibatches. function class FEATURE-OF inference model. Method are neural networks, and per - example training procedure. OtherScientificTerm are gradient, and identity shortcuts. Generic are approximation, and normalization. Metric are training step computation, and model accuracy. ","This paper studies the effect of batch normalization on the training of neural networks. The authors show that the gradient of the normalization of the weights of a neural network can be approximated as a function of the function class of the training data, and that this approximation is more efficient than the original normalization. They also show that batch renormalization can be applied to small minibatches and that the per-example training procedure can be reduced to a single per-instance training procedure.   The authors also provide a theoretical analysis that shows that the training step computation can be significantly reduced when the inference model is of the same function class as the training dataset and that identity shortcuts can be used to reduce the number of steps needed to compute the gradient.  Finally, the authors provide empirical evidence that normalization can reduce the amount of training steps and improve the model accuracy.","This paper studies the effect of batch normalization on the training of neural networks. The authors show that the gradient of the normalization of the weights of a neural network can be approximated as a function of the function class of the training data, and that this approximation is more efficient than the original normalization. They also show that batch renormalization can be applied to small minibatches and that the per-example training procedure can be reduced to a single per-instance training procedure.   The authors also provide a theoretical analysis that shows that the training step computation can be significantly reduced when the inference model is of the same function class as the training dataset and that identity shortcuts can be used to reduce the number of steps needed to compute the gradient.  Finally, the authors provide empirical evidence that normalization can reduce the amount of training steps and improve the model accuracy."
4015,SP:4aa42984fcb0fd66936d668477b2719ef5c427d4,large - scale pretraining CONJUNCTION adaptation. adaptation CONJUNCTION large - scale pretraining. general domain data USED-FOR large - scale pretraining. large - scale pretraining PART-OF natural language processing. fine - tuning USED-FOR models. trainable parameters USED-FOR downstream tasks. trainable rank decomposition matrices PART-OF Transformer architecture. LoRA HYPONYM-OF Low - Rank Adaptation. GPT-3 175B COMPARE LoRA. LoRA COMPARE GPT-3 175B. LoRA USED-FOR trainable parameters. GPU memory requirement EVALUATE-FOR LoRA. Adam USED-FOR GPT-3 175B. LoRA COMPARE finetuning. finetuning COMPARE LoRA. GPT-2 CONJUNCTION GPT-3. GPT-3 CONJUNCTION GPT-2. RoBERTa EVALUATE-FOR finetuning. model quality EVALUATE-FOR finetuning. model quality EVALUATE-FOR LoRA. Method is fine - tuned models. Metric is training throughput. OtherScientificTerm is inference latency. Task is language model adaptation. ,"This paper studies the problem of large-scale pretraining and adaptation on general domain data, which is a common problem in natural language processing, where fine-tuning is a popular way to fine-tune models. The authors propose a new Low-Rank Adaptation, called LoRA, which extends the Transformer architecture with trainable rank decomposition matrices to reduce the training throughput and inference latency. They show that the GPT-3 175B trained with Adam can be trained with LoRA with the same GPU memory requirement, and that the trainable parameters can be used for other downstream tasks. They also show that LoRA is more efficient than finetuning on RoBERTa, and can improve the model quality in terms of model quality and performance on language model adaptation.   ","This paper studies the problem of large-scale pretraining and adaptation on general domain data, which is a common problem in natural language processing, where fine-tuning is a popular way to fine-tune models. The authors propose a new Low-Rank Adaptation, called LoRA, which extends the Transformer architecture with trainable rank decomposition matrices to reduce the training throughput and inference latency. They show that the GPT-3 175B trained with Adam can be trained with LoRA with the same GPU memory requirement, and that the trainable parameters can be used for other downstream tasks. They also show that LoRA is more efficient than finetuning on RoBERTa, and can improve the model quality in terms of model quality and performance on language model adaptation.   "
4031,SP:b77a00beb0802f47810b03d3c4aa24d92781414f,"CRFs USED-FOR distributions with nonlocal dependencies. nonlocal constraints FEATURE-OF CRFs. global arity constraints HYPONYM-OF nonlocal constraints. CRFs USED-FOR constraints. nonlocal ones HYPONYM-OF constraints. regular - constrained CRF ( RegCCRF ) COMPARE CRF. CRF COMPARE regular - constrained CRF ( RegCCRF ). RegCCRFs COMPARE models. models COMPARE RegCCRFs. constraints PART-OF training. constraints FEATURE-OF decoding. constraints PART-OF RegCCRFs. constrained training COMPARE constrained decoding. constrained decoding COMPARE constrained training. deep neural model USED-FOR semantic role labeling. RegCCRF PART-OF deep neural model. RegCCRF USED-FOR semantic role labeling. dataset EVALUATE-FOR RegCCRF. RegCCRF USED-FOR downstream tasks. Task is structured prediction. OtherScientificTerm are local dependencies, CRF ’s Markov assumption, and output structures. Generic is it. ","This paper considers the problem of structured prediction in the presence of non-local constraints. In particular, the authors consider distributions with nonlocal dependencies, i.e., distributions with local dependencies that are constrained by CRFs. The nonlocal constraints in CRFs are called global arity constraints, and the authors show that CRFs satisfy these constraints, including nonlocal ones. The authors propose a regular-constrained CRF (RegCCRF) that replaces the CRF with a CRF that satisfies the global constraints. RegCCRFs are shown to outperform existing models on a variety of tasks, and it is shown that RegCC RFs are more robust to constraints in training than models that do not require local dependencies.   The authors also show that the constraints in the decoding can be reduced to the constraints of the RegccRFs, and show that constrained training is more effective than constrained decoding. Finally, the paper proposes a deep neural model that incorporates RegCCRRF into the training process, and uses it to perform semantic role labeling on a dataset where the constraints are not present in the original CRF’s Markov assumption, and where the output structures are non-convex. The paper shows that the RegCCRF is able to perform well on this dataset, and can be used for downstream tasks. ","This paper considers the problem of structured prediction in the presence of non-local constraints. In particular, the authors consider distributions with nonlocal dependencies, i.e., distributions with local dependencies that are constrained by CRFs. The nonlocal constraints in CRFs are called global arity constraints, and the authors show that CRFs satisfy these constraints, including nonlocal ones. The authors propose a regular-constrained CRF (RegCCRF) that replaces the CRF with a CRF that satisfies the global constraints. RegCCRFs are shown to outperform existing models on a variety of tasks, and it is shown that RegCC RFs are more robust to constraints in training than models that do not require local dependencies.   The authors also show that the constraints in the decoding can be reduced to the constraints of the RegccRFs, and show that constrained training is more effective than constrained decoding. Finally, the paper proposes a deep neural model that incorporates RegCCRRF into the training process, and uses it to perform semantic role labeling on a dataset where the constraints are not present in the original CRF’s Markov assumption, and where the output structures are non-convex. The paper shows that the RegCCRF is able to perform well on this dataset, and can be used for downstream tasks. "
4047,SP:74c186a96c12adff178264aa84ace8d04dc7d725,"preprocessing steps USED-FOR methods. computational budget EVALUATE-FOR core ” network. normalization CONJUNCTION color space transformation. color space transformation CONJUNCTION normalization. segmentation CONJUNCTION normalization. normalization CONJUNCTION segmentation. face detection CONJUNCTION segmentation. segmentation CONJUNCTION face detection. neural models USED-FOR camera - based physiological measurement. color space transformation CONJUNCTION preprocessing steps. preprocessing steps CONJUNCTION color space transformation. face detection USED-FOR neural models. segmentation HYPONYM-OF neural models. normalization HYPONYM-OF neural models. preprocessing steps PART-OF neural models. EfficientPhys HYPONYM-OF neural models. raw video frames USED-FOR models. accuracy EVALUATE-FOR models. latency EVALUATE-FOR networks. efficiency EVALUATE-FOR light weight network. Task are Camera - based physiological measurement, and replication. Method are endto - end ” models, and transformer or convolutional backbone. Generic is operations. ","The paper addresses the problem of camera-based physiological measurement, where the goal is to capture a person’s physiological measurements in a video.  Camera-based biomedical measurements are typically performed on a large number of sensors, and there is a large amount of data available.    The paper proposes two methods to reduce the number of preprocessing steps required in order to achieve a good performance.  Both methods are based on the observation that “endto-end” models (i.e., “core” network with a computational budget that scales linearly with the size of the dataset) can be easily replicated across multiple sensors.  The authors propose two neural models, called EfficientPhys, that are able to replicate the performance of existing neural models for camera-related physiological measurement (face detection, segmentation, normalization, color space transformation, and preprocessing).  Both models are trained on raw video frames, and the authors show that their models achieve comparable accuracy and latency to existing models.  They also show that the efficiency of a light weight network can be improved by reducing the amount of operations required to achieve the same accuracy.  In addition, the authors propose a “transfer learning” method that can be used to learn a transformer or convolutional backbone, which can be applied to any neural models that have pre processing steps that are commonly used in neural models (e.g., normalization or colorspace transformation).","The paper addresses the problem of camera-based physiological measurement, where the goal is to capture a person’s physiological measurements in a video.  Camera-based biomedical measurements are typically performed on a large number of sensors, and there is a large amount of data available.    The paper proposes two methods to reduce the number of preprocessing steps required in order to achieve a good performance.  Both methods are based on the observation that “endto-end” models (i.e., “core” network with a computational budget that scales linearly with the size of the dataset) can be easily replicated across multiple sensors.  The authors propose two neural models, called EfficientPhys, that are able to replicate the performance of existing neural models for camera-related physiological measurement (face detection, segmentation, normalization, color space transformation, and preprocessing).  Both models are trained on raw video frames, and the authors show that their models achieve comparable accuracy and latency to existing models.  They also show that the efficiency of a light weight network can be improved by reducing the amount of operations required to achieve the same accuracy.  In addition, the authors propose a “transfer learning” method that can be used to learn a transformer or convolutional backbone, which can be applied to any neural models that have pre processing steps that are commonly used in neural models (e.g., normalization or colorspace transformation)."
4063,SP:3003bab6e3f7e2e21cd6cf27ee7d483d877d9fb3,"Structural pruning USED-FOR network architecture. inference speed EVALUATE-FOR Structural pruning. Hardware - Aware Latency Pruning ( HALP ) USED-FOR structural pruning. global resource allocation optimization problem USED-FOR structural pruning. latency reduction potential CONJUNCTION global saliency score. global saliency score CONJUNCTION latency reduction potential. latency lookup table USED-FOR latency reduction potential. latency lookup table USED-FOR global saliency score. HALP USED-FOR accuracy drop. HALP USED-FOR global saliency score. HALP USED-FOR latency reduction potential. latency lookup table USED-FOR HALP. HALP USED-FOR filter importance ranking. latency lookup table USED-FOR filter importance ranking. metrics EVALUATE-FOR pruning. metrics USED-FOR global structural pruning. reward maximization problem USED-FOR global structural pruning. pruning efficacy CONJUNCTION accuracy - efficiency trade - off. accuracy - efficiency trade - off CONJUNCTION pruning efficacy. pruning efficacy EVALUATE-FOR HALP. accuracy - efficiency trade - off EVALUATE-FOR HALP. augmented knapsack solver USED-FOR problem. HALP USED-FOR classification and detection tasks. ImageNet and VOC datasets USED-FOR classification and detection tasks. ImageNet and VOC datasets EVALUATE-FOR HALP. ImageNet USED-FOR ResNet-50/-101 pruning. HALP USED-FOR ResNet-50/-101 pruning. network throughput EVALUATE-FOR HALP. HALP USED-FOR SSD pruning. VOC EVALUATE-FOR SSD pruning. throughput EVALUATE-FOR HALP. HALP COMPARE prior art. prior art COMPARE HALP. Metric is accuracy. OtherScientificTerm are latency, and top-1 accuracy changes. ","This paper proposes Hardware-Aware Latency Pruning (HALP) to improve the efficiency of network architecture pruning and inference speed. Structural pruning aims to reduce the latency of the network architecture while preserving the accuracy. The paper proposes a global resource allocation optimization problem for structural pruning, where the latency reduction potential and the global saliency score are computed from a latency lookup table. HALP uses HALP to compute the latency reduce potential of the latency and the saliency of each layer, and then uses a modified augmented knapsack solver to solve the problem.  The paper also proposes two metrics to evaluate the effectiveness of the proposed pruning. The first metric is the latency-aware latency-reduction potential, and HALP is shown to be able to achieve the accuracy drop when the latency is low. The second metric, the latency lookup score, is used for filter importance ranking, which can be computed from the latency table.   The authors also propose a reward maximization problem for global structural Pruning, and show that HALP outperforms prior work in terms of pruning efficacy and accuracy-efficiency trade-off.  HALP achieves state-of-the-art performance on ResNet-50/-101 pruning on ImageNet and VOC datasets for classification and detection tasks, and outperforms the prior art on SSD pruning (on VOC). HALP also improves the network throughput on VOC, and is able to perform well on classification and detections.  In addition, HALP improves the accuracy of the top-1 accuracy changes, and can be applied to improve network performance.","This paper proposes Hardware-Aware Latency Pruning (HALP) to improve the efficiency of network architecture pruning and inference speed. Structural pruning aims to reduce the latency of the network architecture while preserving the accuracy. The paper proposes a global resource allocation optimization problem for structural pruning, where the latency reduction potential and the global saliency score are computed from a latency lookup table. HALP uses HALP to compute the latency reduce potential of the latency and the saliency of each layer, and then uses a modified augmented knapsack solver to solve the problem.  The paper also proposes two metrics to evaluate the effectiveness of the proposed pruning. The first metric is the latency-aware latency-reduction potential, and HALP is shown to be able to achieve the accuracy drop when the latency is low. The second metric, the latency lookup score, is used for filter importance ranking, which can be computed from the latency table.   The authors also propose a reward maximization problem for global structural Pruning, and show that HALP outperforms prior work in terms of pruning efficacy and accuracy-efficiency trade-off.  HALP achieves state-of-the-art performance on ResNet-50/-101 pruning on ImageNet and VOC datasets for classification and detection tasks, and outperforms the prior art on SSD pruning (on VOC). HALP also improves the network throughput on VOC, and is able to perform well on classification and detections.  In addition, HALP improves the accuracy of the top-1 accuracy changes, and can be applied to improve network performance."
4079,SP:c44d676c09c8e5a70d73b21b507b41a422fec809,"permutation invariance CONJUNCTION multi - objective generation. multi - objective generation CONJUNCTION permutation invariance. GraphEBM HYPONYM-OF molecular graph generation method. GraphEBM USED-FOR permutation invariant and multi - objective molecule generation. energy - based models ( EBMs ) USED-FOR molecular graph generation method. GraphEBM USED-FOR permutation invariant distribution. EBMs CONJUNCTION parameterized permutation - invariant energy function. parameterized permutation - invariant energy function CONJUNCTION EBMs. parameterized permutation - invariant energy function USED-FOR GraphEBM. molecular graphs FEATURE-OF permutation invariant distribution. contrastive divergence USED-FOR energy function. compositional generation USED-FOR drug discovery. compositional generation USED-FOR GraphEBM. Task is molecular graph generation. Method are Langevin dynamics, and learning strategy. OtherScientificTerm is flexible degrees. Generic is method. ","This paper proposes a molecular graph generation method based on energy-based models (EBMs). Specifically, the authors propose GraphEBM, which is an extension of the Langevin dynamics to permutation invariance and multi-objective generation. The authors show that GraphE BM can be used for both permutation-invariant and molecule-level molecular generation.    The authors propose a novel learning strategy for learning the permutation of the energy function of a molecule, which allows for flexible degrees of permutation.  The proposed method is based on the recent development of the contrastive divergence between the learned energy function and the true energy function, which has been used in previous work.  In particular, the proposed graphEBM is able to learn a permutation invariant distribution on molecular graphs, which can be applied to compositional generation for drug discovery. ","This paper proposes a molecular graph generation method based on energy-based models (EBMs). Specifically, the authors propose GraphEBM, which is an extension of the Langevin dynamics to permutation invariance and multi-objective generation. The authors show that GraphE BM can be used for both permutation-invariant and molecule-level molecular generation.    The authors propose a novel learning strategy for learning the permutation of the energy function of a molecule, which allows for flexible degrees of permutation.  The proposed method is based on the recent development of the contrastive divergence between the learned energy function and the true energy function, which has been used in previous work.  In particular, the proposed graphEBM is able to learn a permutation invariant distribution on molecular graphs, which can be applied to compositional generation for drug discovery. "
4095,SP:70e60fa5deef3e3ba77d05d0c3e0e7fbf396aa1d,"approaches USED-FOR program synthesis. neural models USED-FOR combinatorial search algorithms. neural model USED-FOR hands - on search policy. hands - on search policy USED-FOR bottom - up synthesis. search history CONJUNCTION partial program executions. partial program executions CONJUNCTION search history. neural model USED-FOR approach. bottom - up searches USED-FOR data. - policy USED-FOR CROSSBEAM. data USED-FOR - policy. data USED-FOR CROSSBEAM. string manipulation CONJUNCTION logic programming. logic programming CONJUNCTION string manipulation. string manipulation EVALUATE-FOR CROSSBEAM. domains EVALUATE-FOR CROSSBEAM. string manipulation HYPONYM-OF domains. logic programming HYPONYM-OF domains. OtherScientificTerm are search space, search space blowup, and program space. Method is combinatorial search algorithm. Task is structured prediction. Generic is state - of - the - art. ","This paper proposes a new approach to program synthesis, which builds upon existing approaches to combinatorial search algorithms using neural models. The key idea is to use a neural model to learn a hands-on search policy for bottom-up synthesis, where the search history and partial program executions are used to guide the search space. The search space blowup is caused by the fact that there is no structured prediction in the original search space, and the search policy is trained to find a solution that minimizes the loss on the search. The paper proposes CROSSBEAM, an approach that uses the learned neural model as a guide to learn the hands-onset search policy, and then uses this guide to find the optimal solution to the search problem.   The paper shows that the state-of-the-art is the state of the art on two domains: string manipulation and logic programming.  The authors also show that the learned-policy can be used to generate new data from the previous top-up searches, which can be then used to train a new-policy on the new data. The proposed approach is evaluated on two different problems, where it is shown to outperform the previous state- of-the art.","This paper proposes a new approach to program synthesis, which builds upon existing approaches to combinatorial search algorithms using neural models. The key idea is to use a neural model to learn a hands-on search policy for bottom-up synthesis, where the search history and partial program executions are used to guide the search space. The search space blowup is caused by the fact that there is no structured prediction in the original search space, and the search policy is trained to find a solution that minimizes the loss on the search. The paper proposes CROSSBEAM, an approach that uses the learned neural model as a guide to learn the hands-onset search policy, and then uses this guide to find the optimal solution to the search problem.   The paper shows that the state-of-the-art is the state of the art on two domains: string manipulation and logic programming.  The authors also show that the learned-policy can be used to generate new data from the previous top-up searches, which can be then used to train a new-policy on the new data. The proposed approach is evaluated on two different problems, where it is shown to outperform the previous state- of-the art."
4111,SP:daa044ffefe80bae16b014f60061d941ed8c2ba6,minimizing the squared Bellman error USED-FOR Deep Reinforcement Learning. target networks USED-FOR training. lagging parameters USED-FOR target networks. functional regularizer USED-FOR squared Bellman error. target networks COMPARE regularization. regularization COMPARE target networks. up - to - date parameters USED-FOR regularization. Atari environments EVALUATE-FOR target - network based methods. sample efficiency CONJUNCTION performance. performance CONJUNCTION sample efficiency. performance EVALUATE-FOR target - network based methods. sample efficiency EVALUATE-FOR target - network based methods. approach COMPARE squared Bellman error. squared Bellman error COMPARE approach. OtherScientificTerm is fast - changing target Q - values. Method is training method. ,"This paper studies the problem of minimizing the squared Bellman error in Deep Reinforcement Learning. The authors propose a functional regularizer to reduce the squared bellman error. They show that target networks with lagging parameters are more efficient for training. They also show that the target networks trained with this regularization are more robust to fast-changing target Q-values. They further show that regularization with up-to-date parameters is more effective than regularization based on the current training method. Finally, they demonstrate that target-network based methods improve both sample efficiency and performance on Atari environments.    The authors also demonstrate that the proposed approach is more efficient than the standard approach of minimizing the squared Bernoulli error. ","This paper studies the problem of minimizing the squared Bellman error in Deep Reinforcement Learning. The authors propose a functional regularizer to reduce the squared bellman error. They show that target networks with lagging parameters are more efficient for training. They also show that the target networks trained with this regularization are more robust to fast-changing target Q-values. They further show that regularization with up-to-date parameters is more effective than regularization based on the current training method. Finally, they demonstrate that target-network based methods improve both sample efficiency and performance on Atari environments.    The authors also demonstrate that the proposed approach is more efficient than the standard approach of minimizing the squared Bernoulli error. "
4127,SP:dd174014d056a7d2bc86ee99119841eafa62ed52,"neighborhood subgraphs FEATURE-OF hierarchy of local isomorphism. message - passing GNNs COMPARE Weisfeiler Lehman test. Weisfeiler Lehman test COMPARE message - passing GNNs. model COMPARE Weisfeiler Lehman test. Weisfeiler Lehman test COMPARE model. model USED-FOR graph structures. Weisfeiler Lehman test USED-FOR graph structures. GraphSNN HYPONYM-OF neural model. graph learning tasks EVALUATE-FOR model. model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE model. benchmark tasks EVALUATE-FOR state - of - the - art methods. benchmark tasks EVALUATE-FOR model. Method are Graph Neural Networks ( GNNs ), and GNNs. OtherScientificTerm is structural properties of graphs. ","This paper proposes a new type of graph neural network, called GraphSNN, which is based on the Weisfeiler-Lehman test for graph structure. This test is used to measure the similarity between the graph structure of a node and its neighborhood subgraphs. The authors show that this test can be applied to a number of existing graph neural networks (GNNs) and shows that it can be used to evaluate the structural properties of a graph. The paper also shows that GNNs trained with this test outperform existing ones on several graph classification tasks.","This paper proposes a new type of graph neural network, called GraphSNN, which is based on the Weisfeiler-Lehman test for graph structure. This test is used to measure the similarity between the graph structure of a node and its neighborhood subgraphs. The authors show that this test can be applied to a number of existing graph neural networks (GNNs) and shows that it can be used to evaluate the structural properties of a graph. The paper also shows that GNNs trained with this test outperform existing ones on several graph classification tasks."
4143,SP:beb9ba0261e176bfc50e9bf5bed2b6169d388285,"prediction interval ( PI ) method USED-FOR uncertainty quantification. retraining of neural networks ( NNs ) USED-FOR confidence level. retraining of neural networks ( NNs ) USED-FOR PI methods. fine tuning USED-FOR well - calibrated PI. sensitive hyperparameters FEATURE-OF customized loss functions. customized loss functions USED-FOR they. PI3NN method USED-FOR PIs. standard mean squared error loss USED-FOR NNs. root - finding algorithms USED-FOR PIs. root - finding algorithms USED-FOR linear combinations. PI3NN USED-FOR PIs. it USED-FOR crossing issue. OOD samples COMPARE in - distribution samples. in - distribution samples COMPARE OOD samples. initialization scheme USED-FOR PIs. PIs FEATURE-OF OOD samples. initialization scheme USED-FOR OOD samples. initialization scheme USED-FOR in - distribution samples. initialization scheme USED-FOR OOD identification challenge. predictive uncertainty quality CONJUNCTION robustness. robustness CONJUNCTION predictive uncertainty quality. method COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE method. robustness CONJUNCTION OOD samples identification. OOD samples identification CONJUNCTION robustness. OOD samples identification EVALUATE-FOR method. robustness EVALUATE-FOR state - of - the - art approaches. robustness EVALUATE-FOR method. predictive uncertainty quality EVALUATE-FOR state - of - the - art approaches. predictive uncertainty quality EVALUATE-FOR method. OtherScientificTerm are over - confident PIs, confidence levels, and hyperparameters. Method is retraining NNs. ","This paper proposes a new prediction interval (PI) method for uncertainty quantification based on the retraining of neural networks (NNs) to improve the confidence level. Previous PI methods rely on retraining NNs with the standard mean squared error loss, but over-confident PIs are not well-calibrated due to the fact that they use customized loss functions with sensitive hyperparameters. The authors propose a PI3NN method to learn PIs that are well calibrated and robust to cross-entropy between confidence levels. They show that the PI methods can be improved by fine tuning to achieve a more robust and robust PIs.    The main contribution of the paper is to propose a new initialization scheme for the OOD identification challenge, which is based on an initialization scheme that is more robust to OOD samples with PIs than in-distribution samples.  The authors also propose to use root-finding algorithms to find PIs for linear combinations of PIs, and show that it can avoid the crossing issue.  Finally, the authors show that their method outperforms previous state-of-the-art approaches in terms of predictive uncertainty quality, robustness, and OLD samples identification. They also demonstrate that the method does not suffer from the issue of over-confidence in PIs and that it is robust to hyperparameter changes. ","This paper proposes a new prediction interval (PI) method for uncertainty quantification based on the retraining of neural networks (NNs) to improve the confidence level. Previous PI methods rely on retraining NNs with the standard mean squared error loss, but over-confident PIs are not well-calibrated due to the fact that they use customized loss functions with sensitive hyperparameters. The authors propose a PI3NN method to learn PIs that are well calibrated and robust to cross-entropy between confidence levels. They show that the PI methods can be improved by fine tuning to achieve a more robust and robust PIs.    The main contribution of the paper is to propose a new initialization scheme for the OOD identification challenge, which is based on an initialization scheme that is more robust to OOD samples with PIs than in-distribution samples.  The authors also propose to use root-finding algorithms to find PIs for linear combinations of PIs, and show that it can avoid the crossing issue.  Finally, the authors show that their method outperforms previous state-of-the-art approaches in terms of predictive uncertainty quality, robustness, and OLD samples identification. They also demonstrate that the method does not suffer from the issue of over-confidence in PIs and that it is robust to hyperparameter changes. "
4159,SP:4b44a834e2212bacb4c2d9408a81f1efc76a670b,"deep networks USED-FOR functions. classifiers CONJUNCTION detectors. detectors CONJUNCTION classifiers. detectors CONJUNCTION trackers. trackers CONJUNCTION detectors. deep networks USED-FOR classifiers. models USED-FOR applications. trackers HYPONYM-OF functions. classifiers HYPONYM-OF functions. detectors HYPONYM-OF functions. learning algorithms USED-FOR slow adaptation. gradient descent HYPONYM-OF learning algorithms. learning algorithms USED-FOR model. Meta - learning USED-FOR adaptation. metalearning USED-FOR online problems. meta - learning USED-FOR online setting. known ground - truth task boundaries FEATURE-OF discrete notion of tasks. discrete notion of tasks USED-FOR they. discrete boundaries PART-OF real - world settings. ground truth knowledge FEATURE-OF task boundaries. FOML COMPARE online learning methods. online learning methods COMPARE FOML. Rainbow - MNIST, and CIFAR100 datasets EVALUATE-FOR online learning methods. Rainbow - MNIST, and CIFAR100 datasets EVALUATE-FOR FOML. OtherScientificTerm are input distributions, and environmental conditions. Method are intelligent system, and Fully Online MetaLearning ( FOML ) algorithm. Task is complex and high - dimensional problems. Generic is methods. ","This paper proposes a meta-learning algorithm for online learning of complex and high-dimensional problems where the input distributions are discrete. The authors propose the Fully Online MetaLearning (FOML) algorithm, which learns a model of an intelligent system that is able to adapt to new environments and environmental conditions in an online setting. This is achieved by using deep networks to learn functions such as classifiers, detectors, and trackers, and applying the learned models to a variety of applications. Meta-learning is used to guide the adaptation of a model to a new environment, and learning algorithms such as gradient descent are used to prevent slow adaptation. The paper also proposes metalearning for online problems, which is an extension of existing methods. FOML outperforms existing online learning methods on the Rainbow-MNIST, and CIFAR100 datasets. The main contribution of the paper is that they use a discrete notion of tasks that has known ground-truth task boundaries, which can be used to learn a model that is robust to changes in discrete boundaries in real-world settings. They also show that the task boundaries have ground truth knowledge.  ","This paper proposes a meta-learning algorithm for online learning of complex and high-dimensional problems where the input distributions are discrete. The authors propose the Fully Online MetaLearning (FOML) algorithm, which learns a model of an intelligent system that is able to adapt to new environments and environmental conditions in an online setting. This is achieved by using deep networks to learn functions such as classifiers, detectors, and trackers, and applying the learned models to a variety of applications. Meta-learning is used to guide the adaptation of a model to a new environment, and learning algorithms such as gradient descent are used to prevent slow adaptation. The paper also proposes metalearning for online problems, which is an extension of existing methods. FOML outperforms existing online learning methods on the Rainbow-MNIST, and CIFAR100 datasets. The main contribution of the paper is that they use a discrete notion of tasks that has known ground-truth task boundaries, which can be used to learn a model that is robust to changes in discrete boundaries in real-world settings. They also show that the task boundaries have ground truth knowledge.  "
4175,SP:fbae35cb171b3a3eb7c5d4bc83881ed7c4a70aae,"chemical science and engineering task USED-FOR applications. structural design of functional molecules HYPONYM-OF chemical science and engineering task. molecular optimization HYPONYM-OF structural design of functional molecules. drug discovery HYPONYM-OF applications. Deep generative models CONJUNCTION combinatorial optimization methods. combinatorial optimization methods CONJUNCTION Deep generative models. knowledge network USED-FOR discrete chemical structures. knowledge network USED-FOR differentiable scaffolding tree ( DST ). DST USED-FOR gradient - based optimization. chemical graph structure USED-FOR gradient - based optimization. Method are brute - force enumeration, graph neural network ( GNN ), and gradient - based molecular optimizations. OtherScientificTerm are molecule structures, locally differentiable ones, derivatives, graph parameters, and domain experts. ","This paper tackles the problem of molecular optimization, which is an important chemical science and engineering task for applications such as drug discovery and structural design of functional molecules. Deep generative models and combinatorial optimization methods have been widely used in recent years, but there has been a lack of work on gradient-based molecular optimizations. This paper proposes to use a graph neural network (GNN) to learn molecule structures that are locally differentiable ones. The authors propose a differentiable scaffolding tree (DST) that uses a knowledge network to learn discrete chemical structures, which are then used to guide the gradient of a molecule to be optimized. The DST can be used to learn a chemical graph structure, which can then be used as a guide to guide gradient -based optimization.    The authors show that the knowledge network is able to learn the derivatives of the derivatives and the graph parameters, and that the derivatives can be learned locally and globally. The paper also shows that the DST is robust to domain experts, and can be trained without brute-force enumeration. ","This paper tackles the problem of molecular optimization, which is an important chemical science and engineering task for applications such as drug discovery and structural design of functional molecules. Deep generative models and combinatorial optimization methods have been widely used in recent years, but there has been a lack of work on gradient-based molecular optimizations. This paper proposes to use a graph neural network (GNN) to learn molecule structures that are locally differentiable ones. The authors propose a differentiable scaffolding tree (DST) that uses a knowledge network to learn discrete chemical structures, which are then used to guide the gradient of a molecule to be optimized. The DST can be used to learn a chemical graph structure, which can then be used as a guide to guide gradient -based optimization.    The authors show that the knowledge network is able to learn the derivatives of the derivatives and the graph parameters, and that the derivatives can be learned locally and globally. The paper also shows that the DST is robust to domain experts, and can be trained without brute-force enumeration. "
4191,SP:61b59899cf6ae442d9f8f5226e79708a4280cfb2,"diseases CONJUNCTION lab tests. lab tests CONJUNCTION diseases. patient representation USED-FOR sequential information. drug - lab interactions CONJUNCTION diagnosis - lab interactions. diagnosis - lab interactions CONJUNCTION drug - lab interactions. graphs USED-FOR drug - lab interactions. graphs USED-FOR diagnosis - lab interactions. real - world datasets EVALUATE-FOR solution. prediction errors EVALUATE-FOR solution. Method are Personalized medical systems, and knowledge - augmented approach. OtherScientificTerm is lab test responses. ","Personalized medical systems are becoming more and more popular in the medical community. However, there is a lack of understanding of the relationship between diseases and lab tests. This paper proposes a knowledge-augmented approach to address this problem. The idea is to use patient representation to capture sequential information from both the patient and the lab test responses. The proposed solution is evaluated on two real-world datasets, and the results show that the proposed solution achieves better prediction errors. The authors also show that graphs can be used to capture drug-lab interactions and diagnosis-lab interaction. ","Personalized medical systems are becoming more and more popular in the medical community. However, there is a lack of understanding of the relationship between diseases and lab tests. This paper proposes a knowledge-augmented approach to address this problem. The idea is to use patient representation to capture sequential information from both the patient and the lab test responses. The proposed solution is evaluated on two real-world datasets, and the results show that the proposed solution achieves better prediction errors. The authors also show that graphs can be used to capture drug-lab interactions and diagnosis-lab interaction. "
4207,SP:8623cebb515c4a736427449b46ad2cdf8b806b77,"Single domain generalization ( SDG ) HYPONYM-OF domain generalization. diversity of source domain USED-FOR robust model. adversarial data augmentation strategy USED-FOR SDG methods. OS - SDG USED-FOR model. CrossMatch approach USED-FOR SDG methods. SDG methods USED-FOR identifying unknown classes. multi - binary classifier USED-FOR SDG methods. adversarial data augmentation strategy USED-FOR CrossMatch. model USED-FOR unknown class identification. multibinary classifiers CONJUNCTION model. model CONJUNCTION multibinary classifiers. consistency regularization USED-FOR auxiliary samples. consistency regularization USED-FOR model. SDG methods USED-FOR model. CrossMatch USED-FOR SDG methods. benchmark datasets EVALUATE-FOR CrossMatch. SDG methods USED-FOR OS - SDG setting. benchmark datasets EVALUATE-FOR SDG methods. OtherScientificTerm are label space, source label space, target domains, and unknown classes. Task is real - world applications. ","This paper studies the problem of domain generalization (SDG) in the setting where the target domain is not the same as the label space, but the source label space is different from the target domains. The authors propose the CrossMatch approach to tackle this problem. In particular, the authors propose to use the diversity of source domain to train a robust model that can generalize to unseen classes. They also propose a novel adversarial data augmentation strategy to improve the performance of existing SDG methods for identifying unknown classes.    The authors show that CrossMatch can be applied to any SDG method that uses a multi-binary classifier. In the OS-SDG setting, they show that the model trained with CrossMatch is able to generalize better than a model trained without CrossMatch. They further show that a consistency regularization is applied to the auxiliary samples generated by CrossMatch to encourage the model to generalise to unknown class identification.  They also show that using CrossMatch improves the performance on standard benchmark datasets and real-world applications.  Finally, they demonstrate that the proposed CrossMatch generalizes better than other SDG algorithms that do not use CrossMatch on the same benchmark datasets.","This paper studies the problem of domain generalization (SDG) in the setting where the target domain is not the same as the label space, but the source label space is different from the target domains. The authors propose the CrossMatch approach to tackle this problem. In particular, the authors propose to use the diversity of source domain to train a robust model that can generalize to unseen classes. They also propose a novel adversarial data augmentation strategy to improve the performance of existing SDG methods for identifying unknown classes.    The authors show that CrossMatch can be applied to any SDG method that uses a multi-binary classifier. In the OS-SDG setting, they show that the model trained with CrossMatch is able to generalize better than a model trained without CrossMatch. They further show that a consistency regularization is applied to the auxiliary samples generated by CrossMatch to encourage the model to generalise to unknown class identification.  They also show that using CrossMatch improves the performance on standard benchmark datasets and real-world applications.  Finally, they demonstrate that the proposed CrossMatch generalizes better than other SDG algorithms that do not use CrossMatch on the same benchmark datasets."
4223,SP:126f8ffb855aa22eda4d681a499953879ed3679e,Trust - region methods USED-FOR policy optimization. policy optimization USED-FOR reinforcement learning. Kullback - Leibler divergence USED-FOR Trust - region methods. Wasserstein policy optimization ( WPO ) CONJUNCTION Sinkhorn policy optimization ( SPO ). Sinkhorn policy optimization ( SPO ) CONJUNCTION Wasserstein policy optimization ( WPO ). Wasserstein and Sinkhorn trust regions FEATURE-OF policy optimization. parametric distribution class FEATURE-OF policy. Lagrangian duality USED-FOR close - form policy updates. SPO COMPARE WPO. WPO COMPARE SPO. monotonic performance improvement FEATURE-OF WPO. robotic locomotion tasks EVALUATE-FOR approaches. tabular domains CONJUNCTION robotic locomotion tasks. robotic locomotion tasks CONJUNCTION tabular domains. SPO COMPARE policy gradient methods. policy gradient methods COMPARE SPO. tabular domains EVALUATE-FOR approaches. approaches COMPARE policy gradient methods. policy gradient methods COMPARE approaches. sample insufficiency FEATURE-OF WPO. OtherScientificTerm is policy distribution. Method is entropic regularizer. ,"This paper proposes two Trust-region methods based on the Kullback-Leibler divergence for policy optimization in reinforcement learning. The authors extend the Wasserstein and Sinkhorn trust regions to policy optimization to the case where the policy distribution is a parametric distribution class and the entropic regularizer is a Lagrangian duality. They show that the proposed approaches, called Wassersteins policy optimization (WPO) and Sinkshorn policy optimization(SPO), achieve monotonic performance improvement over WPO. They also show that SPO achieves better sample insufficiency than WPO in the case of close-form policy updates due to the use of Lagrangians. They evaluate their approaches on tabular domains and robotic locomotion tasks and show that their approaches outperform existing policy gradient methods. ","This paper proposes two Trust-region methods based on the Kullback-Leibler divergence for policy optimization in reinforcement learning. The authors extend the Wasserstein and Sinkhorn trust regions to policy optimization to the case where the policy distribution is a parametric distribution class and the entropic regularizer is a Lagrangian duality. They show that the proposed approaches, called Wassersteins policy optimization (WPO) and Sinkshorn policy optimization(SPO), achieve monotonic performance improvement over WPO. They also show that SPO achieves better sample insufficiency than WPO in the case of close-form policy updates due to the use of Lagrangians. They evaluate their approaches on tabular domains and robotic locomotion tasks and show that their approaches outperform existing policy gradient methods. "
4239,SP:999eacf6500c87205584a3256d7ca45b3016fb1c,"forgetting USED-FOR learning. learning trajectories FEATURE-OF artificial neural networks. forget - and - relearn USED-FOR learning trajectories. relearning step USED-FOR features. forgetting step USED-FOR undesirable information. forget - and - relearn framework USED-FOR iterative training algorithms. image classification FEATURE-OF iterative training algorithms. forgetting operations USED-FOR algorithms. iterative training PART-OF neural networks. OtherScientificTerm are Forgetting, and disproportionate forgetting of undesirable information. Task is human and machine learning. Generic is model. ","This paper studies the problem of forgetting in the context of learning trajectories of artificial neural networks. The authors propose a forget-and-reluar approach to tackle the issue of forgetting that is prevalent in both human and machine learning. They show that the forgetting of undesirable information during the relearning step can lead to disproportionate forgetting of desirable information, and propose a new forgetting step to remove the undesirable information from the features. They also show that iterative training in neural networks can be seen as a form of iterative learning in which the forgetting happens during the iterative stage of the training process.    The authors also provide a theoretical analysis of the forgetting-and--relearn framework for a number of popular iterative pretraining algorithms for image classification, and show that these algorithms can be understood as a combination of different forgetting operations. The paper also shows that the model is able to learn a good balance between forgetting and relearn. ","This paper studies the problem of forgetting in the context of learning trajectories of artificial neural networks. The authors propose a forget-and-reluar approach to tackle the issue of forgetting that is prevalent in both human and machine learning. They show that the forgetting of undesirable information during the relearning step can lead to disproportionate forgetting of desirable information, and propose a new forgetting step to remove the undesirable information from the features. They also show that iterative training in neural networks can be seen as a form of iterative learning in which the forgetting happens during the iterative stage of the training process.    The authors also provide a theoretical analysis of the forgetting-and--relearn framework for a number of popular iterative pretraining algorithms for image classification, and show that these algorithms can be understood as a combination of different forgetting operations. The paper also shows that the model is able to learn a good balance between forgetting and relearn. "
4255,SP:2789859517b6624730b14a7e010444a72d3dd3ed,"sufficient coverage CONJUNCTION policy. policy CONJUNCTION sufficient coverage. RL agents COMPARE agents. agents COMPARE RL agents. offline - online manner USED-FOR RL agents. Method are Batch RL, batch RL agents, and batch RL. OtherScientificTerm are data - collection process, and agent. Task is offline - online setting. Generic is setting. ","This paper proposes a new setting called Batch RL, where the data-collecting process is offline-online and the agent is trained in an offline-offline setting. The authors show that RL agents trained in this offline-augmented manner outperform existing batch RL agents. They also show that in this setting, RL agents can be trained in a more efficient way than existing agents in the online setting, and that the agent can learn a sufficient coverage and a good policy.   ","This paper proposes a new setting called Batch RL, where the data-collecting process is offline-online and the agent is trained in an offline-offline setting. The authors show that RL agents trained in this offline-augmented manner outperform existing batch RL agents. They also show that in this setting, RL agents can be trained in a more efficient way than existing agents in the online setting, and that the agent can learn a sufficient coverage and a good policy.   "
4271,SP:76625a25e770415599a34122110d61cb3b7e614c,learning USED-FOR domain shift. episodic training procedure USED-FOR learning. episodic training procedure USED-FOR domain generalization ( DG ). learning USED-FOR domain generalization ( DG ). episodic training procedure USED-FOR domain shift. Y - discrepancy USED-FOR domain shift. source - domain samples USED-FOR Y - discrepancy. ERM CONJUNCTION domain - invariant learning. domain - invariant learning CONJUNCTION ERM. PAC - style generalization bound USED-FOR discrepancyoptimal meta - learning. PAC - style generalization bound COMPARE DG bounds. DG bounds COMPARE PAC - style generalization bound. domain - invariant learning HYPONYM-OF DG bounds. ERM HYPONYM-OF DG bounds. computational complexity EVALUATE-FOR discrepancy - optimal meta - learning. classification EVALUATE-FOR discrepancy - optimal meta - learning. classification CONJUNCTION computational complexity. computational complexity CONJUNCTION classification. bilevel optimization algorithm USED-FOR DG. DomainBed EVALUATE-FOR algorithm. DG benchmarks EVALUATE-FOR algorithm. ,"This paper studies the problem of domain generalization (DG) in the context of meta-learning, where the goal is to generalize well across different domains. The authors propose a novel method to tackle this problem, which is based on the Y-divergence between source-domain and target-domain samples. They show that the proposed method, Y-Divergence, is a generalization bound that is more robust to domain shift than existing generalization bounds. They also provide a theoretical analysis of the generalization performance of the proposed algorithm. ","This paper studies the problem of domain generalization (DG) in the context of meta-learning, where the goal is to generalize well across different domains. The authors propose a novel method to tackle this problem, which is based on the Y-divergence between source-domain and target-domain samples. They show that the proposed method, Y-Divergence, is a generalization bound that is more robust to domain shift than existing generalization bounds. They also provide a theoretical analysis of the generalization performance of the proposed algorithm. "
4287,SP:6421a9759c766641fd8c128a249f1a9c5699d19c,"deep reinforcement learning USED-FOR two - player games. combinatorial search methods USED-FOR NP - complete domains. SAT CONJUNCTION CSP. CSP CONJUNCTION SAT. deep reinforcement learning USED-FOR combinatorial search methods. Go HYPONYM-OF two - player games. CSP HYPONYM-OF NP - complete domains. SAT HYPONYM-OF NP - complete domains. exponential combinatorial search space FEATURE-OF hard instances. best - first search CONJUNCTION Monte Carlo tree search. Monte Carlo tree search CONJUNCTION best - first search. Monte Carlo tree search HYPONYM-OF search methods. best - first search HYPONYM-OF search methods. methods USED-FOR hard planning instances. policy and value networks USED-FOR DNN - based best - first search. Sokoban domain EVALUATE-FOR DNN - based best - first search. value network USED-FOR policy network. cost distribution FEATURE-OF search algorithms. heavy - tailed runtime distributions FEATURE-OF Sokoban planning instances. abstract tree model USED-FOR tails. policy network USED-FOR search. polynomial scaling FEATURE-OF left heavy tails. random restart strategies USED-FOR DNN - based search. random restart strategies USED-FOR combinatorial solvers. DNN - based search USED-FOR left and right heavy tails. Task is PSPACE - hard planning problems. Method is domain - specific solvers. Generic are specialized solvers, approaches, and model. OtherScientificTerm is exponentially sized sub - trees. ","This paper considers the problem of solving PSPACE-hard planning problems, which is a special case of the NP-complete domains, where the goal is to find a solution to a set of problems that can be solved in a single step. The authors consider combinatorial search methods based on deep reinforcement learning to solve two-player games (Go and Go-like problems) that have been popularly used in the literature, such as SAT and CSP.    The authors show that existing search methods such as best-first search, Monte Carlo tree search, and other methods can be used to solve hard planning instances in an exponential combinatorially search space.  They also show that DNN-based best search on the Sokoban domain, using policy and value networks, is able to find solutions to hard instances in a polynomial number of steps.  The main contribution of the paper is that the authors provide a theoretical analysis of the cost distribution of the search algorithms in the case of heavy-tailed runtime distributions for Sokoben planning instances.  In particular, they show that the cost of a search is proportional to the number of times that the search is performed on the tail of the problem, and that the tails are exponentially sized sub-trees.  This is an interesting finding, and the authors also provide an empirical analysis that shows that a policy network trained with a value network that is trained to predict the return of the policy network during the search can be more robust to heavy tails.  Finally, the authors propose a new abstract tree model to model the tails, and show that this model is more robust than previous approaches, and can be applied to both left and right heavy tails, which are more difficult to solve due to polynomials in the search space, and to the fact that search algorithms with heavy-tail runtime distributions tend to have a similar cost distribution. They also propose random restart strategies to improve the performance of DNN -based search for the left and left heavy tails using the proposed model, and demonstrate that this improves performance over existing combinatorical solvers. ","This paper considers the problem of solving PSPACE-hard planning problems, which is a special case of the NP-complete domains, where the goal is to find a solution to a set of problems that can be solved in a single step. The authors consider combinatorial search methods based on deep reinforcement learning to solve two-player games (Go and Go-like problems) that have been popularly used in the literature, such as SAT and CSP.    The authors show that existing search methods such as best-first search, Monte Carlo tree search, and other methods can be used to solve hard planning instances in an exponential combinatorially search space.  They also show that DNN-based best search on the Sokoban domain, using policy and value networks, is able to find solutions to hard instances in a polynomial number of steps.  The main contribution of the paper is that the authors provide a theoretical analysis of the cost distribution of the search algorithms in the case of heavy-tailed runtime distributions for Sokoben planning instances.  In particular, they show that the cost of a search is proportional to the number of times that the search is performed on the tail of the problem, and that the tails are exponentially sized sub-trees.  This is an interesting finding, and the authors also provide an empirical analysis that shows that a policy network trained with a value network that is trained to predict the return of the policy network during the search can be more robust to heavy tails.  Finally, the authors propose a new abstract tree model to model the tails, and show that this model is more robust than previous approaches, and can be applied to both left and right heavy tails, which are more difficult to solve due to polynomials in the search space, and to the fact that search algorithms with heavy-tail runtime distributions tend to have a similar cost distribution. They also propose random restart strategies to improve the performance of DNN -based search for the left and left heavy tails using the proposed model, and demonstrate that this improves performance over existing combinatorical solvers. "
4303,SP:84c415bc0f120d1997289f91661ff74e7297d3bd,"approach USED-FOR meta - policy. adaptive loss USED-FOR meta - policy. human videos USED-FOR approach. method COMPARE baseline. baseline COMPARE method. vision - based tasks EVALUATE-FOR method. Method are Meta - Imitation Learning, and meta - imitation learning. OtherScientificTerm are human demonstrations, robot demonstrations, robot demonstration, and human imitation behavior. Generic is it. Task are meta - training phase, and data collection. ","This paper proposes a meta-imitation learning method, Meta-Imitation Learning (MIL), where the goal is to learn to imitate human demonstrations from robot demonstrations. The approach is based on human videos, and the authors propose a novel approach to learn a new meta-policy based on an adaptive loss. The key idea is that the robot demonstrations are trained to imitate the human demonstrations, and then the robot demonstration is used to train a new policy based on the learned human imitation behavior. The proposed method is evaluated on vision-based tasks, and is shown to outperform a baseline, and it is also shown that the meta-training phase can be extended to the case where there is no data collection.","This paper proposes a meta-imitation learning method, Meta-Imitation Learning (MIL), where the goal is to learn to imitate human demonstrations from robot demonstrations. The approach is based on human videos, and the authors propose a novel approach to learn a new meta-policy based on an adaptive loss. The key idea is that the robot demonstrations are trained to imitate the human demonstrations, and then the robot demonstration is used to train a new policy based on the learned human imitation behavior. The proposed method is evaluated on vision-based tasks, and is shown to outperform a baseline, and it is also shown that the meta-training phase can be extended to the case where there is no data collection."
4319,SP:fedf5c75e83d6ab41ef9d5daa9054ffe4e424ec2,"Over - parameterized deep networks USED-FOR classification and ranking problems. gradient - based optimizers USED-FOR Over - parameterized deep networks. weight space FEATURE-OF adaptivity. Adam HYPONYM-OF Adaptive optimizers. weight decay ( WD ) CONJUNCTION normal hyper - parameter tuning. normal hyper - parameter tuning CONJUNCTION weight decay ( WD ). adaptive optimizers COMPARE SGD. SGD COMPARE adaptive optimizers. normal hyper - parameter tuning USED-FOR adaptive optimizers. weight decay ( WD ) USED-FOR adaptive optimizers. image classification domain EVALUATE-FOR SGD. image classification domain EVALUATE-FOR adaptive optimizers. generalization performance EVALUATE-FOR SGD. generalization performance EVALUATE-FOR adaptive optimizers. OtherScientificTerm are tuned regularization, network weights, training loss, and train loss. Generic are networks, and network. ","This paper studies the over-parameterized deep networks trained with gradient-based optimizers for classification and ranking problems. Adaptive optimizers, such as Adam, have been shown to improve the generalization performance of these networks when the weights of the network are tuned regularization. This paper shows that the adaptivity in the weight space is a function of the training loss, and that adaptive optimizers trained with weight decay (WD) and normal hyper-parameters tuning are more robust than SGD on the image classification domain. The paper also shows that adaptive optimization performance improves when the network weights are not tuned. ","This paper studies the over-parameterized deep networks trained with gradient-based optimizers for classification and ranking problems. Adaptive optimizers, such as Adam, have been shown to improve the generalization performance of these networks when the weights of the network are tuned regularization. This paper shows that the adaptivity in the weight space is a function of the training loss, and that adaptive optimizers trained with weight decay (WD) and normal hyper-parameters tuning are more robust than SGD on the image classification domain. The paper also shows that adaptive optimization performance improves when the network weights are not tuned. "
4335,SP:819df8d847a99f13ed5efdcabae8b464c12b464b,"model USED-FOR equivariance. edge orientations FEATURE-OF face. edge orientations CONJUNCTION face poses. face poses CONJUNCTION edge orientations. symmetries USED-FOR low and high - level features. face poses FEATURE-OF camera. edge orientations HYPONYM-OF low and high - level features. edge orientations HYPONYM-OF symmetries. equivariant networks USED-FOR partial and full equivariances. Partial G - CNNs HYPONYM-OF equivariant networks. full equivariance FEATURE-OF Partial G - CNNs. Partial G - CNNs COMPARE G - CNNs. G - CNNs COMPARE Partial G - CNNs. discrete groups CONJUNCTION continuous groups. continuous groups CONJUNCTION discrete groups. method USED-FOR discrete groups. method USED-FOR continuous groups. Task are generalization, and natural image classification. Method is group equivariant architectures. OtherScientificTerm are distribution, and rotations. Material is rotated MNIST. Generic is them. ","This paper proposes a new model for learning equivariance for group equivariant architectures. The authors propose to use symmetries between low and high-level features (e.g., edge orientations of the face, face poses of the camera, etc.) to learn a distribution over the group. They show that equivarient networks are able to learn partial and full equivariances, and show that Partial G-CNNs (which they call Partial Equivariant CNNs) are equivariants of the distribution. They also show that their method works for discrete groups as well as continuous groups.    The authors also demonstrate that their model is able to generalize to rotated MNIST, which is an important task for generalization, and they show that natural image classification can be learned in this way.  They also provide a theoretical analysis that shows that their Partial G - CNNs have full equivariances, and that they can be trained to learn them. ","This paper proposes a new model for learning equivariance for group equivariant architectures. The authors propose to use symmetries between low and high-level features (e.g., edge orientations of the face, face poses of the camera, etc.) to learn a distribution over the group. They show that equivarient networks are able to learn partial and full equivariances, and show that Partial G-CNNs (which they call Partial Equivariant CNNs) are equivariants of the distribution. They also show that their method works for discrete groups as well as continuous groups.    The authors also demonstrate that their model is able to generalize to rotated MNIST, which is an important task for generalization, and they show that natural image classification can be learned in this way.  They also provide a theoretical analysis that shows that their Partial G - CNNs have full equivariances, and that they can be trained to learn them. "
4351,SP:0c0ca9df96f1fa2eb8b83a47d0d5964590fef290,Markov chain Monte Carlo ( MCMC ) USED-FOR approximating intractable distributions. Langevin dynamics HYPONYM-OF Markov chain Monte Carlo ( MCMC ). datapoint - wise iterations CONJUNCTION slow convergence. slow convergence CONJUNCTION datapoint - wise iterations. its USED-FOR deep latent variable models. inference model USED-FOR latent variables. ALD USED-FOR scalable inference. large - scale datasets USED-FOR ALD. datapoint - wise iterations USED-FOR it. large - scale datasets USED-FOR scalable inference. MCMC USED-FOR it. stationary distribution USED-FOR ALD. ALD USED-FOR generative modeling. it USED-FOR prior distribution. it USED-FOR latent variable. prior distribution FEATURE-OF latent variable. ALD USED-FOR unconditional distribution. it USED-FOR generative modeling. energy - based model HYPONYM-OF unconditional distribution. ALD USED-FOR deep latent variable model. Langevin autoencoder ( LAE ) HYPONYM-OF deep latent variable model. ALD USED-FOR autoencoder - like posterior inference. LAE USED-FOR autoencoder - like posterior inference. latent space EBM USED-FOR LAE. ALD USED-FOR LAE. ALD COMPARE LD. LD COMPARE ALD. ALD USED-FOR target distributions. toy datasets EVALUATE-FOR ALD. conditional and unconditional cases FEATURE-OF target distributions. SVHN CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION SVHN. CIFAR-10 CONJUNCTION CelebA - HQ. CelebA - HQ CONJUNCTION CIFAR-10. datasets USED-FOR image generation task. image generation task EVALUATE-FOR LAE. SVHN HYPONYM-OF datasets. datasets EVALUATE-FOR LAE. CelebA - HQ HYPONYM-OF datasets. CIFAR-10 HYPONYM-OF datasets. LAE COMPARE non - amortized MCMC methods. non - amortized MCMC methods COMPARE LAE. LAE COMPARE AVI - based methods. AVI - based methods COMPARE LAE. Fréchet,"This paper proposes a new deep latent variable model called Langevin autoencoder (LAE) based on Langevin dynamics, which is a variant of Markov chain Monte Carlo (MCMC) for approximating intractable distributions. The authors show that ALD can be used for scalable inference on large-scale datasets without datapoint-wise iterations and slow convergence, and that its can be applied to deep generative models as well. In particular, ALD is applied to generative modeling in the sense that it uses MCMC to approximate the prior distribution of the latent variable, and then uses the inference model to learn the latent variables. In addition, it can also be used to learn an unconditional distribution (an energy-based model). The authors propose to use ALD to learn a stationary distribution for each latent variable in the latent space, and use it as a prior distribution for the next latent variable.   The authors also propose a deep latent variational auto-encoder (LD) model based on ALD, and show that the ALD-based LAE is able to be used in conjunction with ALD for autoencoders, and can be trained on toy datasets. They also show that LAE outperforms non-amortized MCMC methods on three datasets (SVHN, CIFAR-10, and CelebA-HQ) for an image generation task, and on a Fréchet et al. (2018) dataset for which ALD performs better than LD.  The main contribution of this paper is the introduction of ALD as an autoencing method for generative modelling, and the development of an ALD model for deep latent variables, as well-tuned to the target distributions in both conditional and unconditional cases. The paper also shows that the proposed LAE can be combined with a latent space EBM. ","This paper proposes a new deep latent variable model called Langevin autoencoder (LAE) based on Langevin dynamics, which is a variant of Markov chain Monte Carlo (MCMC) for approximating intractable distributions. The authors show that ALD can be used for scalable inference on large-scale datasets without datapoint-wise iterations and slow convergence, and that its can be applied to deep generative models as well. In particular, ALD is applied to generative modeling in the sense that it uses MCMC to approximate the prior distribution of the latent variable, and then uses the inference model to learn the latent variables. In addition, it can also be used to learn an unconditional distribution (an energy-based model). The authors propose to use ALD to learn a stationary distribution for each latent variable in the latent space, and use it as a prior distribution for the next latent variable.   The authors also propose a deep latent variational auto-encoder (LD) model based on ALD, and show that the ALD-based LAE is able to be used in conjunction with ALD for autoencoders, and can be trained on toy datasets. They also show that LAE outperforms non-amortized MCMC methods on three datasets (SVHN, CIFAR-10, and CelebA-HQ) for an image generation task, and on a Fréchet et al. (2018) dataset for which ALD performs better than LD.  The main contribution of this paper is the introduction of ALD as an autoencing method for generative modelling, and the development of an ALD model for deep latent variables, as well-tuned to the target distributions in both conditional and unconditional cases. The paper also shows that the proposed LAE can be combined with a latent space EBM. "
4367,SP:5631097031c7e599bdeae64366ffa6e4558837c6,hypergraph reasoning USED-FOR large domains. logical rules PART-OF logical reasoning. structured neural network USED-FOR hypergraph reasoning. neural networks CONJUNCTION finite - domain quantification operations. finite - domain quantification operations CONJUNCTION neural networks. SpaLoc USED-FOR grounding of relationships. sparse tensors USED-FOR SpaLoc. sparse tensors USED-FOR grounding of relationships. finite - domain quantification operations USED-FOR SpaLoc. neural networks USED-FOR SpaLoc. sparsification loss USED-FOR SpaLoc model. intermediate layers PART-OF SpaLoc model. sparsification loss USED-FOR intermediate layers. training and inference - time sub - sampling USED-FOR SpaLoc. real - world knowledge graphs HYPONYM-OF large - scale graphs. information loss FEATURE-OF sampled sub - graphs. information - theoretic measure information sufficiency USED-FOR sampling and label calibration paradigm. accuracy CONJUNCTION efficiency. efficiency CONJUNCTION accuracy. accuracy EVALUATE-FOR SpaLoc. efficiency EVALUATE-FOR SpaLoc. synthetic datasets EVALUATE-FOR SpaLoc. real - world knowledge graph reasoning benchmarks EVALUATE-FOR SpaLoc. OtherScientificTerm is grandparent relationship. Method is hypergraph neural networks. ,"This paper proposes a structured neural network for hypergraph reasoning in large domains, where the goal is to learn logical rules that can be applied to large domains. The authors propose to use hypergraph neural networks to learn the grounding of relationships using sparse tensors, and then apply a sparsification loss to the intermediate layers of the proposed SpaLoc model. SpaLoc is trained with neural networks and finite-domain quantification operations, and the authors propose a training and inference-time sub-sampling to improve the accuracy and efficiency of SpaLoc.  The authors also propose a sampling and label calibration paradigm based on an information-theoretic measure information sufficiency for the sampled sub-graphs, which is based on the information loss of sampled subgraphs.    The paper is well-written and well-motivated. The idea is interesting and the paper is clearly written.  However, the paper suffers from a lack of experiments on large-scale graphs (e.g., real-world knowledge graphs, i.e. graphs that contain a large number of entities and a large amount of information). The paper does not provide sufficient experiments on synthetic datasets, and does not compare with state-of-the-art baselines.  In addition, the authors do not provide any experimental evidence that the proposed model is able to generalize well to new domains.  Finally, the proposed method is evaluated on a number of real-life knowledge graph reasoning benchmarks, and it is shown that SpaLoc can generalize to a variety of domains, and that it generalizes well to a new domain. ","This paper proposes a structured neural network for hypergraph reasoning in large domains, where the goal is to learn logical rules that can be applied to large domains. The authors propose to use hypergraph neural networks to learn the grounding of relationships using sparse tensors, and then apply a sparsification loss to the intermediate layers of the proposed SpaLoc model. SpaLoc is trained with neural networks and finite-domain quantification operations, and the authors propose a training and inference-time sub-sampling to improve the accuracy and efficiency of SpaLoc.  The authors also propose a sampling and label calibration paradigm based on an information-theoretic measure information sufficiency for the sampled sub-graphs, which is based on the information loss of sampled subgraphs.    The paper is well-written and well-motivated. The idea is interesting and the paper is clearly written.  However, the paper suffers from a lack of experiments on large-scale graphs (e.g., real-world knowledge graphs, i.e. graphs that contain a large number of entities and a large amount of information). The paper does not provide sufficient experiments on synthetic datasets, and does not compare with state-of-the-art baselines.  In addition, the authors do not provide any experimental evidence that the proposed model is able to generalize well to new domains.  Finally, the proposed method is evaluated on a number of real-life knowledge graph reasoning benchmarks, and it is shown that SpaLoc can generalize to a variety of domains, and that it generalizes well to a new domain. "
4383,SP:9657121b01c51f78c00d06b47d3e8d678dd85d54,"top - k classification accuracy EVALUATE-FOR machine learning. probability distribution USED-FOR k. top-1 CONJUNCTION top-5 accuracy. top-5 accuracy CONJUNCTION top-1. ImageNet EVALUATE-FOR models. top-5 accuracy EVALUATE-FOR models. top-1 EVALUATE-FOR models. Task is differentiable sorting and ranking. Metric are top-5 accuracies, and top-1 accuracy. Method is ImageNet models. ","This paper studies the problem of differentiable sorting and ranking. The goal is to improve the top-k classification accuracy in machine learning. The authors propose to use a probability distribution over the top k points of the probability distribution for k, and then compute the difference between top-1 and top-5 accuracies. The paper shows that models trained on ImageNet with the same number of training samples achieve similar or slightly better top 1/k accuracy than models trained with a large number of samples.  The paper also shows that for ImageNet models trained from scratch, the top 1 accuracy matches the top 5 accuracy.","This paper studies the problem of differentiable sorting and ranking. The goal is to improve the top-k classification accuracy in machine learning. The authors propose to use a probability distribution over the top k points of the probability distribution for k, and then compute the difference between top-1 and top-5 accuracies. The paper shows that models trained on ImageNet with the same number of training samples achieve similar or slightly better top 1/k accuracy than models trained with a large number of samples.  The paper also shows that for ImageNet models trained from scratch, the top 1 accuracy matches the top 5 accuracy."
4399,SP:cb3188f435c54a365890e20e4d582c250d919833,"speed CONJUNCTION accuracy. accuracy CONJUNCTION speed. accuracy EVALUATE-FOR method. speed EVALUATE-FOR method. method USED-FOR OT problem. Douglas - Rachford splitting technique USED-FOR method. method USED-FOR approximate regularized problem. entropic regularization USED-FOR methods. algorithm COMPARE Sinkhorn method. Sinkhorn method COMPARE algorithm. method COMPARE Sinkhorn method. Sinkhorn method COMPARE method. iteration complexity EVALUATE-FOR method. linear convergence rate USED-FOR OT problem. primal - dual stopping criterion FEATURE-OF method. computation times CONJUNCTION robustness. robustness CONJUNCTION computation times. robustness EVALUATE-FOR method. computation times EVALUATE-FOR method. OtherScientificTerm are sparse transport plans, and numerical issues. ","This paper proposes a new method for solving the OT problem with sparse transport plans. The proposed method is based on the Douglas-Rachford splitting technique. The method is able to achieve speed and accuracy on both computation and accuracy. The authors show that the method can solve an approximate regularized problem with entropic regularization. The algorithm is shown to have a linear convergence rate to the solution of OT problem, and the proposed method has the same iteration complexity as the Sinkhorn method, but the method has a primal-dual stopping criterion. The paper also discusses numerical issues with the method, and shows that the algorithm can achieve faster computation times and robustness.  ","This paper proposes a new method for solving the OT problem with sparse transport plans. The proposed method is based on the Douglas-Rachford splitting technique. The method is able to achieve speed and accuracy on both computation and accuracy. The authors show that the method can solve an approximate regularized problem with entropic regularization. The algorithm is shown to have a linear convergence rate to the solution of OT problem, and the proposed method has the same iteration complexity as the Sinkhorn method, but the method has a primal-dual stopping criterion. The paper also discusses numerical issues with the method, and shows that the algorithm can achieve faster computation times and robustness.  "
4415,SP:9a087cc734a3e7f3ab848bef5e2eff37fe40f303,"distribution of distributions USED-FOR Federated learning data. generalization studies USED-FOR federated learning. framework USED-FOR performance gaps. dataset synthesis strategy USED-FOR realistic simulations of generalization. OtherScientificTerm are meta - distribution, local data distributions, out - of - sample gap, and participation gap. Material are natural and synthetic federated datasets, and naturally - partitioned data. Method is semantic synthesis strategy. ","This paper studies the distribution of distributions for Federated learning data. The authors propose a meta-distribution for federated learning, where the local data distributions are partitioned according to the out-of-sample gap and the participation gap. They also propose a new dataset synthesis strategy to generate realistic simulations of generalization. They show that the proposed framework can reduce performance gaps between natural and synthetic federated datasets. They further propose a semantic synthesis strategy, which can be applied to naturally-partitioned data. ","This paper studies the distribution of distributions for Federated learning data. The authors propose a meta-distribution for federated learning, where the local data distributions are partitioned according to the out-of-sample gap and the participation gap. They also propose a new dataset synthesis strategy to generate realistic simulations of generalization. They show that the proposed framework can reduce performance gaps between natural and synthetic federated datasets. They further propose a semantic synthesis strategy, which can be applied to naturally-partitioned data. "
4431,SP:da0e8c89f343abfe500eb4c1968e418c2fb52ef6,"generalization EVALUATE-FOR LMs. self - supervision objectives USED-FOR LMs. supervised finetuning USED-FOR pre - trained language models ( PLMs ). model capacity CONJUNCTION data size. data size CONJUNCTION model capacity. pre - training techniques USED-FOR PLMs. PLMs USED-FOR few - shot setting. PLMs USED-FOR few - shot tasks. PLMs USED-FOR zero - shot setting. BERT family USED-FOR models. IMDB dataset CONJUNCTION Amazon dataset. Amazon dataset CONJUNCTION IMDB dataset. IMDB dataset HYPONYM-OF datasets. Amazon dataset HYPONYM-OF datasets. PLMs USED-FOR language understanding tasks. zero - shot setting FEATURE-OF PLMs. GLUE HYPONYM-OF language understanding tasks. Method are deep learning, language models ( LMs ), and prompt - based learning. OtherScientificTerm are manually / automatically created prompts, and manually created prompts. Metric is accuracy. ","This paper studies the generalization performance of pre-trained language models (PLMs) in the few-shot setting, where the goal is to improve the performance of LMs trained with self-supervision objectives. In this setting, the authors show that PLMs trained on manually/automatically created prompts are able to generalize better than those trained with pre-training techniques that do not rely on manually created prompts. The authors also show that the performance improvement is due to the use of supervised finetuning, which is a common technique in deep learning.    The authors first show that pre-trainable language models can generalize well to a wide range of tasks, and that LMs can be trained on a wide variety of tasks with different levels of model capacity and data size. They then show that, in the zero-shot learning setting, PLMs can generalise well to language understanding tasks on a few language understanding benchmarks (IMDB dataset, Amazon dataset, and GLUE) using PLMs that are trained with different pre-learning techniques. The models are based on the BERT family, and the authors further show that models trained with prompt-based learning outperform models that are not trained with prompts at all.  Finally, they show that training PLMs on a set of PLMs with prompts is beneficial in a single-task setting, in that the accuracy of the model is not affected by the number of prompts, but rather by the amount of prompts.","This paper studies the generalization performance of pre-trained language models (PLMs) in the few-shot setting, where the goal is to improve the performance of LMs trained with self-supervision objectives. In this setting, the authors show that PLMs trained on manually/automatically created prompts are able to generalize better than those trained with pre-training techniques that do not rely on manually created prompts. The authors also show that the performance improvement is due to the use of supervised finetuning, which is a common technique in deep learning.    The authors first show that pre-trainable language models can generalize well to a wide range of tasks, and that LMs can be trained on a wide variety of tasks with different levels of model capacity and data size. They then show that, in the zero-shot learning setting, PLMs can generalise well to language understanding tasks on a few language understanding benchmarks (IMDB dataset, Amazon dataset, and GLUE) using PLMs that are trained with different pre-learning techniques. The models are based on the BERT family, and the authors further show that models trained with prompt-based learning outperform models that are not trained with prompts at all.  Finally, they show that training PLMs on a set of PLMs with prompts is beneficial in a single-task setting, in that the accuracy of the model is not affected by the number of prompts, but rather by the amount of prompts."
4447,SP:9817dccb1a121058b23a2ef825ed339cf8b53674,Attention mechanism USED-FOR tasks. sharpener module USED-FOR attention mechanism. it USED-FOR representation. alignment FEATURE-OF attention. real - world scene text recognition datasets EVALUATE-FOR approach. approach COMPARE ones. ones COMPARE approach. approach COMPARE soft and hard attention. soft and hard attention COMPARE approach. soft and hard attention HYPONYM-OF ones. ,This paper proposes a new attention mechanism for tasks where the attention mechanism is trained with a sharpener module. The key idea is that it can be used to improve the alignment between the representation of the input and the output of the attention. Experiments on real-world scene text recognition datasets show that the proposed approach outperforms existing ones such as soft and hard attention. ,This paper proposes a new attention mechanism for tasks where the attention mechanism is trained with a sharpener module. The key idea is that it can be used to improve the alignment between the representation of the input and the output of the attention. Experiments on real-world scene text recognition datasets show that the proposed approach outperforms existing ones such as soft and hard attention. 
4463,SP:3913ed3b3cf6494368e3be6cacb637ff85f80ee6,"Learning USED-FOR combinatorial optimization problems. operations research solvers CONJUNCTION heuristics. heuristics CONJUNCTION operations research solvers. vehicle routing problem HYPONYM-OF combinatorial optimization problems. apriori given number of available vehicles USED-FOR complex assignment problem. bounded fleet size FEATURE-OF logistic service providers. post - processing scheme USED-FOR supervised approach. method COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE method. Method are deep reinforcement learning approaches, learning - based approaches, and supervised deep learning framework. Generic is them. OtherScientificTerm is apriori fixed number of available vehicles. ","This paper proposes a new deep reinforcement learning approach for solving combinatorial optimization problems. The authors consider the vehicle routing problem, where the goal is to find a route that maximizes the number of available vehicles in a given set of vehicles. This is a well-studied problem that has been studied in the literature for a number of years, and the authors propose a novel approach to solve it. The proposed approach is based on a supervised deep learning framework, where a learning algorithm is trained to solve the problem.  ","This paper proposes a new deep reinforcement learning approach for solving combinatorial optimization problems. The authors consider the vehicle routing problem, where the goal is to find a route that maximizes the number of available vehicles in a given set of vehicles. This is a well-studied problem that has been studied in the literature for a number of years, and the authors propose a novel approach to solve it. The proposed approach is based on a supervised deep learning framework, where a learning algorithm is trained to solve the problem.  "
4479,SP:594a813c0d0baa66738b9c8331370f861ad3c416,"Existing methods USED-FOR variables. clustering effect HYPONYM-OF observed graph structure. observed graph structure HYPONYM-OF variables. clustering effect HYPONYM-OF variables. causal relationship FEATURE-OF variables. link prediction method USED-FOR graph learning. counterfactual inference USED-FOR link prediction method. counterfactual inference USED-FOR graph learning. It USED-FOR counterfactual links. It USED-FOR representations. observed and counterfactual links USED-FOR representations. benchmark datasets EVALUATE-FOR graph learning method. link prediction EVALUATE-FOR graph learning method. Task is graph - based applications. Generic is it. Method are causal models, and graph representations. OtherScientificTerm is global graph structural properties. ","This paper proposes a link prediction method for graph learning based on counterfactual inference. Existing methods rely on two variables: the observed graph structure (clustering effect) and the causal relationship between two variables, which is not suitable for many graph-based applications. The authors propose to combine these two variables and propose a new graph learning method. It learns representations based on both the observed and counterfactually links, and then uses it to train a model that predicts the link between two nodes in a graph. This is a natural extension of the link prediction work of [1].    [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14]  [15]   The authors show that the proposed link learning method achieves state-of-the-art link prediction performance on several benchmark datasets. They also show that their method is able to learn causal models that are robust to global graph structural properties, and that the graph representations learned by the proposed method can be used to learn to predict links between nodes.","This paper proposes a link prediction method for graph learning based on counterfactual inference. Existing methods rely on two variables: the observed graph structure (clustering effect) and the causal relationship between two variables, which is not suitable for many graph-based applications. The authors propose to combine these two variables and propose a new graph learning method. It learns representations based on both the observed and counterfactually links, and then uses it to train a model that predicts the link between two nodes in a graph. This is a natural extension of the link prediction work of [1].    [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14]  [15]   The authors show that the proposed link learning method achieves state-of-the-art link prediction performance on several benchmark datasets. They also show that their method is able to learn causal models that are robust to global graph structural properties, and that the graph representations learned by the proposed method can be used to learn to predict links between nodes."
4495,SP:48a7e50451b887f55be17b2662aa11ce18791cc1,"unsupervised feature selection methods USED-FOR ranking features. second - order covariance matrix CONJUNCTION first - order data matrix. first - order data matrix CONJUNCTION second - order covariance matrix. sparse attention matrix USED-FOR second - order relations between features. graph segmentation USED-FOR feature selection. attention matrix USED-FOR relational graph. SOFT COMPARE state - of - the - art methods. state - of - the - art methods COMPARE SOFT. SOFT COMPARE method. method COMPARE SOFT. Task are Unsupervised feature selection, and unsupervised feature selection. OtherScientificTerm is external guidance information. Method is knowledge contrastive disTillation ( SOFT ) model. ","This paper studies the problem of unsupervised feature selection methods for ranking features in the presence of external guidance information. The authors propose a novel knowledge contrastive disTillation (SOFT) model, which aims to improve the performance of existing state-of-the-art features selection methods. Unsupervised features selection is an important problem in many real-world applications, and the authors propose to use a sparse attention matrix to capture the second-order relations between features, which is then used for feature selection via graph segmentation. The attention matrix is used to represent a relational graph, where each node in the graph is represented as a pair of features, and each feature is associated with a second order covariance matrix and a first-order data matrix. Experiments show that the proposed SOFT outperforms the current state of the art methods in terms of performance, and that the method is more robust to external guidance than existing methods.","This paper studies the problem of unsupervised feature selection methods for ranking features in the presence of external guidance information. The authors propose a novel knowledge contrastive disTillation (SOFT) model, which aims to improve the performance of existing state-of-the-art features selection methods. Unsupervised features selection is an important problem in many real-world applications, and the authors propose to use a sparse attention matrix to capture the second-order relations between features, which is then used for feature selection via graph segmentation. The attention matrix is used to represent a relational graph, where each node in the graph is represented as a pair of features, and each feature is associated with a second order covariance matrix and a first-order data matrix. Experiments show that the proposed SOFT outperforms the current state of the art methods in terms of performance, and that the method is more robust to external guidance than existing methods."
4511,SP:14bcae11aeede63f28d1b80c05ed18a01d3e3f3c,"Multimodal variational autoencoders ( VAEs ) USED-FOR joint distribution. vision CONJUNCTION language. language CONJUNCTION vision. heterogeneous data USED-FOR joint distribution. language HYPONYM-OF heterogeneous data. vision HYPONYM-OF heterogeneous data. idiosyncratic representations PART-OF recognition model. mixtures CONJUNCTION factorisations. factorisations CONJUNCTION mixtures. mixtures USED-FOR idiosyncratic representations. MEME COMPARE baselines. baselines COMPARE MEME. metrics EVALUATE-FOR partial and complete observation schemes. metrics EVALUATE-FOR MEME. metrics EVALUATE-FOR baselines. representations COMPARE approaches. approaches COMPARE representations. mutual supervision USED-FOR representations. OtherScientificTerm are modalities, and relatedness between data. Generic are alternative, and formulation. Method are Mutually supErvised Multimodal VAE ( MEME ), and semisupervised VAEs. Material is partiallyobserved data. ","This paper proposes a multimodal variational autoencoders (VAEs) for learning a joint distribution over heterogeneous data (e.g., vision and language). The authors propose an alternative to the traditional multi-modal VAE, called Mutually supErvised MultimodAL VAE (MEME). The key idea is to learn a representation of the modalities that is invariant to the relatedness between data. This formulation is based on semisupervised VAEs. The authors show that this representation can be learned from partiallyobserved data. They also show that the resulting recognition model is robust to idiosyncratic representations from the heterogeneous representations of vision, language, and mixtures of mixtures and factorisations. Experiments show that MEME outperforms the baselines on a variety of metrics for both partial and complete observation schemes. The representations learned with mutual supervision are also shown to be more robust than approaches that do not rely on mutual supervision. ","This paper proposes a multimodal variational autoencoders (VAEs) for learning a joint distribution over heterogeneous data (e.g., vision and language). The authors propose an alternative to the traditional multi-modal VAE, called Mutually supErvised MultimodAL VAE (MEME). The key idea is to learn a representation of the modalities that is invariant to the relatedness between data. This formulation is based on semisupervised VAEs. The authors show that this representation can be learned from partiallyobserved data. They also show that the resulting recognition model is robust to idiosyncratic representations from the heterogeneous representations of vision, language, and mixtures of mixtures and factorisations. Experiments show that MEME outperforms the baselines on a variety of metrics for both partial and complete observation schemes. The representations learned with mutual supervision are also shown to be more robust than approaches that do not rely on mutual supervision. "
4527,SP:e834a52cadebe5f125ce491273b4ad1146beae3f,"Reinforcement Learning agent USED-FOR directed behaviors. Intrinsic Motivation USED-FOR Reinforcement Learning agent. options USED-FOR simple tabular cases. Deep Explore Options USED-FOR complex visual problems. Explore Options PART-OF Deep Reinforcement Learning paradigm. unrelated intrinsic rewards USED-FOR Deep Explore Options. J - PER HYPONYM-OF transitionselection algorithm. interest of multiple agents USED-FOR transitionselection algorithm. intrinsic reward learning USED-FOR auxiliary task. architecture USED-FOR shared representation. Atari Suite FEATURE-OF hard and easy exploration games. Atari Suite EVALUATE-FOR Deep Explore Options. hard and easy exploration games EVALUATE-FOR Deep Explore Options. they COMPARE weighted sum of rewards. weighted sum of rewards COMPARE they. weighted sum of rewards COMPARE baselines. baselines COMPARE weighted sum of rewards. they COMPARE baselines. baselines COMPARE they. intrinsic rewards USED-FOR they. OtherScientificTerm are sparse or noisy rewards, intrinsic and extrinsic rewards, interesting behaviors, high dimensional spaces, and exploitative or exploratory behaviors. Method is intrinsically motivated agent. Task is exploration. ","This paper proposes an approach to explore options in deep reinforcement learning (DRL) that is motivated by intrinsic motivation. The approach is based on the idea of exploring options in a tabular setting, where the agent is given a set of options and the goal is to maximize the return of the options. The authors propose a method to select the best options from the set of available options, which they call Deep Explore Options. They show that the proposed method outperforms baselines on a number of Atari games.","This paper proposes an approach to explore options in deep reinforcement learning (DRL) that is motivated by intrinsic motivation. The approach is based on the idea of exploring options in a tabular setting, where the agent is given a set of options and the goal is to maximize the return of the options. The authors propose a method to select the best options from the set of available options, which they call Deep Explore Options. They show that the proposed method outperforms baselines on a number of Atari games."
4543,SP:41578dd1a4bdb043b3d68afa5f9cebb3e14f3907,method USED-FOR Hamiltonian dynamical systems. stiffness FEATURE-OF dynamical system. SANN USED-FOR stiff and nonstiff portions. stiffness - aware index USED-FOR SANN. classification CONJUNCTION resampling technique. resampling technique CONJUNCTION classification. time integration strategies USED-FOR dynamical characteristics. dynamical characteristics FEATURE-OF Hamiltonian vector fields. step size adaptation USED-FOR dynamical characteristics. resampling technique USED-FOR time integration strategies. classification USED-FOR time integration strategies. step size adaptation HYPONYM-OF time integration strategies. three - body problem CONJUNCTION billiard model. billiard model CONJUNCTION three - body problem. billiard model EVALUATE-FOR SANN. complex physical systems EVALUATE-FOR SANN. three - body problem HYPONYM-OF complex physical systems. billiard model HYPONYM-OF complex physical systems. SANN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE SANN. energy EVALUATE-FOR SANN. accuracy EVALUATE-FOR state - of - the - art methods. accuracy EVALUATE-FOR SANN. Method is stiffness - aware neural network ( SANN ). ,"This paper proposes a new method for learning Hamiltonian dynamical systems. The proposed method, stiffness-aware neural network (SANN), is based on the idea that the stiffness of a dynamical system should be considered. SANN is trained to distinguish between stiff and nonstiff portions of a Hamiltonian system by using a stiff-aware index. The authors propose two time integration strategies based on classification and resampling technique to learn the dynamical characteristics of the Hamiltonian vector fields. Experiments on complex physical systems such as the three-body problem and the billiard model show that SANN outperforms state-of-the-art methods in terms of energy and accuracy. ","This paper proposes a new method for learning Hamiltonian dynamical systems. The proposed method, stiffness-aware neural network (SANN), is based on the idea that the stiffness of a dynamical system should be considered. SANN is trained to distinguish between stiff and nonstiff portions of a Hamiltonian system by using a stiff-aware index. The authors propose two time integration strategies based on classification and resampling technique to learn the dynamical characteristics of the Hamiltonian vector fields. Experiments on complex physical systems such as the three-body problem and the billiard model show that SANN outperforms state-of-the-art methods in terms of energy and accuracy. "
4559,SP:bfb0a059eeb6f40a18fbd20c0eec5037a64ca09e,"language models USED-FOR tasks. generating realistic text CONJUNCTION synthesizing computer programs. synthesizing computer programs CONJUNCTION generating realistic text. generating realistic text HYPONYM-OF tasks. synthesizing computer programs HYPONYM-OF tasks. they USED-FOR tasks. unbounded multi - step computation USED-FOR tasks. models USED-FOR multistep computations. Transformers USED-FOR multi - step computations. intermediate computation steps PART-OF scratchpad ”. language models USED-FOR multi - step computations. scratchpads USED-FOR language models. OtherScientificTerm are computer programs, integers, and programs. Method is intermediate computations. ","This paper proposes to use language models to tackle two tasks: generating realistic text and synthesizing computer programs. The authors show that language models trained on scratchpads are able to perform multi-step computations, and that they can be used for both tasks with unbounded multi-stage computation. Transformers are used to perform multistep computations in this paper, and the paper shows that the models can be trained to solve multisteps computations. The paper also shows that a “scratchpad” (i.e., a set of intermediate computation steps in a sequence of integers) can be decomposed into “Scratchpads”, where each scratchpad consists of multiple intermediate computations for each program. The programs are decomposed as follows: (1) a program is decomposed in a series of programs, (2) each program is represented as a sequence, (3) all programs are represented as strings, and (4) each string is represented by an integer.","This paper proposes to use language models to tackle two tasks: generating realistic text and synthesizing computer programs. The authors show that language models trained on scratchpads are able to perform multi-step computations, and that they can be used for both tasks with unbounded multi-stage computation. Transformers are used to perform multistep computations in this paper, and the paper shows that the models can be trained to solve multisteps computations. The paper also shows that a “scratchpad” (i.e., a set of intermediate computation steps in a sequence of integers) can be decomposed into “Scratchpads”, where each scratchpad consists of multiple intermediate computations for each program. The programs are decomposed as follows: (1) a program is decomposed in a series of programs, (2) each program is represented as a sequence, (3) all programs are represented as strings, and (4) each string is represented by an integer."
4575,SP:e6c1a8b4bba287455dc9cf145b6bd1f04e2148a9,"deep networks USED-FOR adversarial attacks. methods USED-FOR adversarial perturbations. deep image generators CONJUNCTION optimization objective. optimization objective CONJUNCTION deep image generators. deep image generators USED-FOR feature - level adversarial perturbations. optimization objective USED-FOR feature - level adversarial perturbations. them USED-FOR targeted feature - level attacks. they USED-FOR targeted feature - level attacks. ImageNet scale FEATURE-OF targeted feature - level attacks. them USED-FOR copy / paste ” adversaries. OtherScientificTerm are feature - class associations, natural objects, and targeted misclassification. Method is featurefool attacks. Generic is attacks. Material is natural image. ","This paper studies the problem of adversarial attacks on deep networks. The authors propose two methods to generate adversarial perturbations that can be used to fool feature-class associations. The main idea is that featurefool attacks can be seen as “copy/paste” attacks, where a natural image is replaced by a copy of the same image, but the target image is not the same as the original image. The paper shows that these attacks are effective in the presence of natural objects, and that they can also be used for targeted feature-level attacks on ImageNet scale.  The paper also shows that deep image generators and an optimization objective can be combined to generate feature-levels adversarial examples, which can be exploited to fool the featurefools.  Finally, the paper shows how to use them to generate “Copy/paste ” adversaries, and how to apply them to target targeted features of a target image.  ","This paper studies the problem of adversarial attacks on deep networks. The authors propose two methods to generate adversarial perturbations that can be used to fool feature-class associations. The main idea is that featurefool attacks can be seen as “copy/paste” attacks, where a natural image is replaced by a copy of the same image, but the target image is not the same as the original image. The paper shows that these attacks are effective in the presence of natural objects, and that they can also be used for targeted feature-level attacks on ImageNet scale.  The paper also shows that deep image generators and an optimization objective can be combined to generate feature-levels adversarial examples, which can be exploited to fool the featurefools.  Finally, the paper shows how to use them to generate “Copy/paste ” adversaries, and how to apply them to target targeted features of a target image.  "
4591,SP:873618263dc4246a39c44d0abfecfb5f688817e3,Simulated annealing ( SA ) HYPONYM-OF stochastic global optimisation technique. stochastic global optimisation technique USED-FOR discrete and continuous variable problems. neighbour proposal distribution CONJUNCTION temperature annealing schedule. temperature annealing schedule CONJUNCTION neighbour proposal distribution. SA optimiser USED-FOR problem. temperature annealing schedule HYPONYM-OF handpicked components. handpicked components USED-FOR SA optimiser. neighbour proposal distribution HYPONYM-OF handpicked components. policy USED-FOR solution quality. policy USED-FOR proposal distribution. Knapsack problem CONJUNCTION Bin Packing problem. Bin Packing problem CONJUNCTION Knapsack problem. Bin Packing problem CONJUNCTION Travelling Salesperson problem. Travelling Salesperson problem CONJUNCTION Bin Packing problem. Rosenbrock ’s function CONJUNCTION Knapsack problem. Knapsack problem CONJUNCTION Rosenbrock ’s function. Neural SA COMPARE SA baselines. SA baselines COMPARE Neural SA. hand - selected parameters USED-FOR SA baselines. hand - selected parameters USED-FOR Neural SA. problems EVALUATE-FOR Neural SA. problems EVALUATE-FOR SA baselines. proposal distribution USED-FOR Neural SA. Travelling Salesperson problem HYPONYM-OF problems. Rosenbrock ’s function HYPONYM-OF problems. Bin Packing problem HYPONYM-OF problems. Knapsack problem HYPONYM-OF problems. solution quality CONJUNCTION wall clock time. wall clock time CONJUNCTION solution quality. Neural SA COMPARE solvers. solvers COMPARE Neural SA. wall clock time EVALUATE-FOR Neural SA. solution quality EVALUATE-FOR Neural SA. wall clock time EVALUATE-FOR solvers. solution quality EVALUATE-FOR solvers. Method is SA. OtherScientificTerm is computational budget. ,"This paper proposes Simulated annealing (SA) which is a stochastic global optimisation technique for discrete and continuous variable problems. In particular, the proposed SA optimiser is based on two handpicked components: a neighbour proposal distribution and a temperature annealed schedule. The proposal distribution is learned using a policy that optimizes the solution quality. The proposed Neural SA is evaluated on three problems: Rosenbrock’s function, Knapsack problem, Bin Packing problem, and the Travelling Salesperson problem. The paper shows that Neural SA with hand-selected parameters outperforms the state-of-the-art SA baselines on all three problems and achieves better solution quality and wall clock time compared to existing solvers. ","This paper proposes Simulated annealing (SA) which is a stochastic global optimisation technique for discrete and continuous variable problems. In particular, the proposed SA optimiser is based on two handpicked components: a neighbour proposal distribution and a temperature annealed schedule. The proposal distribution is learned using a policy that optimizes the solution quality. The proposed Neural SA is evaluated on three problems: Rosenbrock’s function, Knapsack problem, Bin Packing problem, and the Travelling Salesperson problem. The paper shows that Neural SA with hand-selected parameters outperforms the state-of-the-art SA baselines on all three problems and achieves better solution quality and wall clock time compared to existing solvers. "
4607,SP:cae31f7436920eb3946e3f5bca0ac88a73d7c3ec,"policy changes of agents USED-FOR learning process. measurement indicators USED-FOR non - stationarity. notion USED-FOR non - stationarity. non - stationarity FEATURE-OF policy sequence. δ - stationarity measurement HYPONYM-OF notion. trust - region constraint FEATURE-OF joint policy. policy factorization USED-FOR policy divergence. trust - region constraints FEATURE-OF factorized policies. mean - field approximation HYPONYM-OF policy factorization. computational complexity EVALUATE-FOR it. pairwise Markov random field USED-FOR joint policy. MAMT HYPONYM-OF Multi - Agent Mirror descent policy algorithm. end - to - end manner USED-FOR trust - region of the local policies. Trust region decomposition USED-FOR Multi - Agent Mirror descent policy algorithm. MAMT USED-FOR non - stationarity problem. MAMT USED-FOR joint policies ’ divergence. method COMPARE baselines. baselines COMPARE method. complexity FEATURE-OF cooperative tasks. cooperative tasks EVALUATE-FOR baselines. cooperative tasks EVALUATE-FOR method. complexity EVALUATE-FOR baselines. complexity EVALUATE-FOR method. Task is Non - stationarity. Generic is algorithms. Metric is KL - divergence. OtherScientificTerm are trust - region decomposition dilemma, joint policy divergence, and δ - stationarity. Method is message passing. ","Non-stationarity is one of the most important measurement indicators for measuring the non-stability of policy changes of agents during the learning process. This paper proposes a new notion called the δ-stigma measurement, which is a notion that measures the nonstationarity of a policy sequence. The authors propose two algorithms to compute the KL-divergence between the joint policy and a joint policy that satisfies the trust-region constraint.   The first algorithm, MAMT, is a Multi-Agent Mirror descent policy algorithm that is based on Trust region decomposition. The key idea of the algorithm is to factorize joint policy into a pairwise Markov Markov random field, and then use the policy factorization (i.e., mean-field approximation) to compute a policy divergence between two factorized policies that satisfy trust-regions constraints.  The authors show that the proposed algorithm, which they refer to as ‘trust-region decomposition dilemma’, is computationally efficient as it reduces the computational complexity of learning the joint policies’ divergence.  In addition, the authors propose an end-to-end manner to decompose the trust region of the local policies into a set of sub-policies that are shared across agents. The proposed method, called ‘MAMT’ (multi-agent mirror descent), is a multi-agent variant of the Multi-agent Mirror descent (MARS) algorithm.  MARS is a variant of MARS, which uses the message passing between two agents to learn a single-agent policies. MARS solves a non-stationary problem by using MAMMARS to compute joint policy divergence. The paper shows that MAMARS outperforms several baselines in terms of complexity on cooperative tasks. The method also outperforms baselines that do not use message passing.","Non-stationarity is one of the most important measurement indicators for measuring the non-stability of policy changes of agents during the learning process. This paper proposes a new notion called the δ-stigma measurement, which is a notion that measures the nonstationarity of a policy sequence. The authors propose two algorithms to compute the KL-divergence between the joint policy and a joint policy that satisfies the trust-region constraint.   The first algorithm, MAMT, is a Multi-Agent Mirror descent policy algorithm that is based on Trust region decomposition. The key idea of the algorithm is to factorize joint policy into a pairwise Markov Markov random field, and then use the policy factorization (i.e., mean-field approximation) to compute a policy divergence between two factorized policies that satisfy trust-regions constraints.  The authors show that the proposed algorithm, which they refer to as ‘trust-region decomposition dilemma’, is computationally efficient as it reduces the computational complexity of learning the joint policies’ divergence.  In addition, the authors propose an end-to-end manner to decompose the trust region of the local policies into a set of sub-policies that are shared across agents. The proposed method, called ‘MAMT’ (multi-agent mirror descent), is a multi-agent variant of the Multi-agent Mirror descent (MARS) algorithm.  MARS is a variant of MARS, which uses the message passing between two agents to learn a single-agent policies. MARS solves a non-stationary problem by using MAMMARS to compute joint policy divergence. The paper shows that MAMARS outperforms several baselines in terms of complexity on cooperative tasks. The method also outperforms baselines that do not use message passing."
4623,SP:989b58167a15ae4fafbe27ff534d327991b6c4d7,"correlated audio and visual information PART-OF Video recordings of speech. self - supervised representation learning framework USED-FOR audio - visual speech. AV - HuBERT USED-FOR audio - visual speech representation. audio - visual speech representation USED-FOR lip - reading and automatic speech recognition. AV - HuBERT COMPARE state - of - the - art approach. state - of - the - art approach COMPARE AV - HuBERT. labeled data USED-FOR AV - HuBERT. transcribed video data USED-FOR state - of - the - art approach. WER EVALUATE-FOR AV - HuBERT. labeled data USED-FOR LRS3. audio - visual representation USED-FOR audio - only speech recognition. benchmark USED-FOR audio - only speech recognition. benchmark EVALUATE-FOR audio - visual representation. relative WER reduction EVALUATE-FOR audio - visual representation. Method are speech representation, and self - training. Material is multi - stream video input. OtherScientificTerm is multimodal hidden units. Metric is lip - reading WER. ","This paper proposes a self-supervised representation learning framework for audio-visual speech. Video recordings of speech contain correlated audio and visual information. The authors propose AV-HuBERT to learn an audio - visual speech representation that can be used for lip-reading and automatic speech recognition.    The authors show that AV-huBERT achieves better WER than the state-of-the-art approach that uses transcribed video data.  The main contribution of the paper is that the speech representation is learned from multi-stream video input, and that the multimodal hidden units are learned in an end-to-end manner. The self-training is done in an unsupervised manner, and the authors show the performance of AV-uBERT on LRS3 with labeled data. The paper also shows that audio-only speech recognition with audio-video representation achieves better relative WER reduction than the standard benchmark for the task. The results are particularly strong for lip reading, where the lip-readers achieve a lip-texting WER of 0.5. ","This paper proposes a self-supervised representation learning framework for audio-visual speech. Video recordings of speech contain correlated audio and visual information. The authors propose AV-HuBERT to learn an audio - visual speech representation that can be used for lip-reading and automatic speech recognition.    The authors show that AV-huBERT achieves better WER than the state-of-the-art approach that uses transcribed video data.  The main contribution of the paper is that the speech representation is learned from multi-stream video input, and that the multimodal hidden units are learned in an end-to-end manner. The self-training is done in an unsupervised manner, and the authors show the performance of AV-uBERT on LRS3 with labeled data. The paper also shows that audio-only speech recognition with audio-video representation achieves better relative WER reduction than the standard benchmark for the task. The results are particularly strong for lip reading, where the lip-readers achieve a lip-texting WER of 0.5. "
4639,SP:7c9eb8aa4a4dcb5965157d860e812d81654e3aa7,combinatorial optimisation USED-FOR real - world applications. logistics CONJUNCTION natural sciences. natural sciences CONJUNCTION logistics. natural sciences FEATURE-OF combinatorial optimisation. graphs USED-FOR combinatorial optimisation. pre - solved instances USED-FOR problems. graph neural networks ( GNNs ) USED-FOR decision step. ECORD HYPONYM-OF RL algorithm. pre - processing step USED-FOR GNN. recurrent unit USED-FOR fast - acting exploratory phase. SOTA USED-FOR RL algorithms. ECORD USED-FOR RL algorithms. RL algorithms USED-FOR Maximum Cut problem. ECORD USED-FOR SOTA. SOTA USED-FOR Maximum Cut problem. Maximum Cut problem EVALUATE-FOR ECORD. scalability EVALUATE-FOR ECORD. speed EVALUATE-FOR ECORD. nearest competitor COMPARE ECORD. ECORD COMPARE nearest competitor. optimality gap EVALUATE-FOR ECORD. Method is Reinforcement learning ( RL ). Generic is it. OtherScientificTerm is wall - clock time. ,"This paper proposes a new RL algorithm, called ECORD, for combinatorial optimisation in real-world applications in logistics and natural sciences. Reinforcement learning (RL) is an important problem in many applications, but it can be slow and expensive due to wall-clock time. In this paper, the authors propose to use graph neural networks (GNNs) to speed up the decision step by using a pre-processing step to train a GNN, and then use a recurrent unit to perform a fast-acting exploratory phase. The authors show that using graphs is a natural choice to perform combinatorially optimisation, and can be applied to a variety of problems with pre-solved instances. ECORD is shown to outperform existing RL algorithms on the Maximum Cut problem using SOTA, and it is shown that ECORD can also outperform SOTA in terms of speed and scalability. In addition, ECORD shows that it is faster than the nearest competitor and has a smaller optimality gap. ","This paper proposes a new RL algorithm, called ECORD, for combinatorial optimisation in real-world applications in logistics and natural sciences. Reinforcement learning (RL) is an important problem in many applications, but it can be slow and expensive due to wall-clock time. In this paper, the authors propose to use graph neural networks (GNNs) to speed up the decision step by using a pre-processing step to train a GNN, and then use a recurrent unit to perform a fast-acting exploratory phase. The authors show that using graphs is a natural choice to perform combinatorially optimisation, and can be applied to a variety of problems with pre-solved instances. ECORD is shown to outperform existing RL algorithms on the Maximum Cut problem using SOTA, and it is shown that ECORD can also outperform SOTA in terms of speed and scalability. In addition, ECORD shows that it is faster than the nearest competitor and has a smaller optimality gap. "
4655,SP:f741d980c9c560a21298e947f1605dcbab7ceeac,"Discrete latent variables USED-FOR generation process of real world data. discrete latents USED-FOR Variational Autoencoders ( VAEs ). discrete VAEs COMPARE ones. ones COMPARE discrete VAEs. direct discrete optimization USED-FOR encoding model. direct discrete optimization USED-FOR discrete nature of the latents. reparameterization trick CONJUNCTION amortization. amortization CONJUNCTION reparameterization trick. sampling approximation CONJUNCTION reparameterization trick. reparameterization trick CONJUNCTION sampling approximation. amortization HYPONYM-OF VAE mechanisms. reparameterization trick HYPONYM-OF VAE mechanisms. sampling approximation HYPONYM-OF VAE mechanisms. truncated posteriors CONJUNCTION evolutionary algorithms. evolutionary algorithms CONJUNCTION truncated posteriors. variational setting USED-FOR Discrete optimization. approach USED-FOR evolutionary algorithms. evolutionary algorithms USED-FOR Discrete optimization. truncated posteriors USED-FOR variational setting. truncated posteriors USED-FOR Discrete optimization. decoder network USED-FOR latent states. gradient ascent USED-FOR network weights. gradient ascent USED-FOR discrete variational method. binary latents FEATURE-OF VAEs. discrete variational method USED-FOR VAEs. amortized training COMPARE direct discrete optimization. direct discrete optimization COMPARE amortized training. direct discrete optimization USED-FOR large neural networks. amortized training USED-FOR large neural networks. smaller networks USED-FOR direct optimization. direct optimization COMPARE zero - shot ’ learning. zero - shot ’ learning COMPARE direct optimization. large supervised neural networks COMPARE VAEs. VAEs COMPARE large supervised neural networks. sampling - based approximation CONJUNCTION reparameterization. reparameterization CONJUNCTION sampling - based approximation. approach USED-FOR training of VAEs. sampling - based approximation USED-FOR training of VAEs. direct optimization USED-FOR VAEs. direct optimization USED-FOR denoising. VAEs USED-FOR denoising. they COMPARE non - generative approaches. non - generative approaches COMPARE they. VAEs COMPARE non - generative approaches. non - generative approaches COMPARE VAEs. Method are VAE training, and small","This paper proposes a discrete variational autoencoder (VAE) method for learning discrete latent variables for the generation process of real world data. The authors propose Variational Autoencoders (VAEs) with discrete latents, which they call discrete VAEs. They show that discrete VAE training is more computationally efficient than previous ones, and that direct discrete optimization of the encoding model can be more efficient than directly optimizing the discrete nature of the latents. Discrete optimization in the variational setting is based on truncated posteriors and evolutionary algorithms, and the authors propose an approach based on gradient ascent to optimize the network weights. They also show that VAEs with binary latents can be trained with a discrete variant of VAE, and they show that the decoder network is able to learn the latent states of the VAE. They further show that direct optimization of VAEs using direct optimization can be used to train smaller networks more efficiently than amortized training for large neural networks, and direct optimization for denoising can be performed on smaller networks. Finally, the authors show that their approach can be applied to the training of large VAEs, and show that they outperform non-generative approaches in terms of performance.    The authors also provide a theoretical analysis that shows that the VAEs trained with the discrete variable can be denoised using a discrete VAational method, which is a generalization of gradient ascent. The paper also shows that a VAE trained with direct optimization performs better than VAEs when the number of latent variables is small, and shows that their sampling-based approximation, reparameterization, amortization, and reparametrization are effective for training VAEs in general. ","This paper proposes a discrete variational autoencoder (VAE) method for learning discrete latent variables for the generation process of real world data. The authors propose Variational Autoencoders (VAEs) with discrete latents, which they call discrete VAEs. They show that discrete VAE training is more computationally efficient than previous ones, and that direct discrete optimization of the encoding model can be more efficient than directly optimizing the discrete nature of the latents. Discrete optimization in the variational setting is based on truncated posteriors and evolutionary algorithms, and the authors propose an approach based on gradient ascent to optimize the network weights. They also show that VAEs with binary latents can be trained with a discrete variant of VAE, and they show that the decoder network is able to learn the latent states of the VAE. They further show that direct optimization of VAEs using direct optimization can be used to train smaller networks more efficiently than amortized training for large neural networks, and direct optimization for denoising can be performed on smaller networks. Finally, the authors show that their approach can be applied to the training of large VAEs, and show that they outperform non-generative approaches in terms of performance.    The authors also provide a theoretical analysis that shows that the VAEs trained with the discrete variable can be denoised using a discrete VAational method, which is a generalization of gradient ascent. The paper also shows that a VAE trained with direct optimization performs better than VAEs when the number of latent variables is small, and shows that their sampling-based approximation, reparameterization, amortization, and reparametrization are effective for training VAEs in general. "
4671,SP:deb189d37bd51b92762ce259a106d9a9e9d81ea4,reward - based task EVALUATE-FOR approach. action - prediction USED-FOR methods. Controlled Effect Network ( CEN ) HYPONYM-OF unsupervised method. counterfactual measures of blame USED-FOR unsupervised method. it USED-FOR controlled effects. it PART-OF exploration method. CEN USED-FOR intrinsic motivator. it COMPARE action - prediction models. action - prediction models COMPARE it. CEN COMPARE action - prediction models. action - prediction models COMPARE CEN. Task is Identifying controllable aspects of the environment. Method is reinforcement learning agents. ,"Identifying controllable aspects of the environment is an important problem for reinforcement learning agents. Previous methods rely on action-prediction, which can be problematic when the reward-based task is difficult to control. This paper proposes an unsupervised method called Controlled Effect Network (CEN), which uses counterfactual measures of blame to guide the exploration of an environment. The approach is evaluated on a reward-free and reward-constrained reinforcement learning task. The authors show that CEN can be used as an intrinsic motivator and that it can be integrated into an exploration method to learn controlled effects. They also show that it outperforms existing action-predictive models when CEN is used in combination with other exploration methods. ","Identifying controllable aspects of the environment is an important problem for reinforcement learning agents. Previous methods rely on action-prediction, which can be problematic when the reward-based task is difficult to control. This paper proposes an unsupervised method called Controlled Effect Network (CEN), which uses counterfactual measures of blame to guide the exploration of an environment. The approach is evaluated on a reward-free and reward-constrained reinforcement learning task. The authors show that CEN can be used as an intrinsic motivator and that it can be integrated into an exploration method to learn controlled effects. They also show that it outperforms existing action-predictive models when CEN is used in combination with other exploration methods. "
4687,SP:ea18d57904e25fd09ed0f6c9972029d78779a8a6,"they USED-FOR networks. neural architecture search CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION neural architecture search. neural architecture search HYPONYM-OF model compression techniques. knowledge distillation HYPONYM-OF model compression techniques. network pruning HYPONYM-OF model compression technique. it USED-FOR SR networks. filter pruning USED-FOR residual blocks. regularization FEATURE-OF pruned structure. regularization USED-FOR structure - regularized pruning ( SRP ). filters USED-FOR unimportant filters. residual USED-FOR layers. filters USED-FOR layers. SRP USED-FOR image SR networks. lightweight network SRPN - L CONJUNCTION SRPN. SRPN CONJUNCTION lightweight network SRPN - L. SRP USED-FOR lightweight network SRPN - L. SRPN - L CONJUNCTION SRPN. SRPN CONJUNCTION SRPN - L. OtherScientificTerm are moderate model size, and pruned filters. Generic is network. Method are L2 regularization, and lightweight and larger image SR networks. ","This paper proposes a new model compression technique called network pruning, which is an extension of existing model compression techniques such as neural architecture search and knowledge distillation, where they prune the networks to reduce the size of the networks. The authors propose structure-regularized pruning (SRP), which uses a regularization on the pruned structure to encourage the network to have a moderate model size. They show that it can be applied to SR networks and that it works well for both lightweight and larger image SR networks. They also show that filter pruning can be used to prune residual blocks in the residual blocks, and that filters that are pruned to remove unimportant filters are more likely to be used as residual for the remaining layers.  The authors show that SRP is able to improve the performance of existing imageSR networks on lightweight network SRPN-L, SRPN, and SRPN with L2 regularization.  They also demonstrate that pruned filters are able to achieve better performance when the number of filters is small.  ","This paper proposes a new model compression technique called network pruning, which is an extension of existing model compression techniques such as neural architecture search and knowledge distillation, where they prune the networks to reduce the size of the networks. The authors propose structure-regularized pruning (SRP), which uses a regularization on the pruned structure to encourage the network to have a moderate model size. They show that it can be applied to SR networks and that it works well for both lightweight and larger image SR networks. They also show that filter pruning can be used to prune residual blocks in the residual blocks, and that filters that are pruned to remove unimportant filters are more likely to be used as residual for the remaining layers.  The authors show that SRP is able to improve the performance of existing imageSR networks on lightweight network SRPN-L, SRPN, and SRPN with L2 regularization.  They also demonstrate that pruned filters are able to achieve better performance when the number of filters is small.  "
4703,SP:0dee45001ae9600f485614dfe6874a516ac01db5,"model USED-FOR few - shot learning methods. sparsely labeled novel category data USED-FOR model. abundantly labeled base category data USED-FOR model. abundantly labeled base category data USED-FOR few - shot learning methods. framework USED-FOR large domain shift. framework USED-FOR few - shot learning. contrastive loss FEATURE-OF base category data. feature extracting backbone USED-FOR framework. contrastive loss USED-FOR feature extracting backbone. masking module USED-FOR target domain classification. backbone USED-FOR features. backbone USED-FOR classifier. it EVALUATE-FOR framework. cross - domain few - shot learning benchmark EVALUATE-FOR it. cross - domain few - shot learning benchmark EVALUATE-FOR framework. framework COMPARE cross - domain methods. cross - domain methods COMPARE framework. framework COMPARE meta - learning approaches. meta - learning approaches COMPARE framework. meta - learning approaches COMPARE cross - domain methods. cross - domain methods COMPARE meta - learning approaches. Generic is methods. OtherScientificTerm are distant domain categories, and supervision. ","This paper proposes a novel model for few-shot learning from sparsely labeled novel category data. The model is based on the assumption that there is a large amount of abundantly labeled base category data in the target domain, and that the model is trained on a large number of examples from the source domain. The authors propose a novel framework to tackle the problem of large domain shift, which is common in existing methods. The proposed framework is built on top of a feature extracting backbone that is trained with contrastive loss on the source and target domains. The backbone is trained to extract features from distant domain categories, and a masking module is used for target domain classification. This backbone is then used to train a classifier on the feature extracted from the backbone. The framework is evaluated on the cross-domain few-set learning benchmark, and it is shown to outperform existing meta-learning approaches, and is also shown to generalize well to unseen domains.  ","This paper proposes a novel model for few-shot learning from sparsely labeled novel category data. The model is based on the assumption that there is a large amount of abundantly labeled base category data in the target domain, and that the model is trained on a large number of examples from the source domain. The authors propose a novel framework to tackle the problem of large domain shift, which is common in existing methods. The proposed framework is built on top of a feature extracting backbone that is trained with contrastive loss on the source and target domains. The backbone is trained to extract features from distant domain categories, and a masking module is used for target domain classification. This backbone is then used to train a classifier on the feature extracted from the backbone. The framework is evaluated on the cross-domain few-set learning benchmark, and it is shown to outperform existing meta-learning approaches, and is also shown to generalize well to unseen domains.  "
4719,SP:92aa611d71a8da597358330d84fddbb90de2cf4f,"gradient descent USED-FOR generalisation. networks USED-FOR gradient descent. infinite width networks CONJUNCTION finite width networks. finite width networks CONJUNCTION infinite width networks. Bayesian inference USED-FOR infinite width networks. gradient descent USED-FOR finite width networks. error COMPARE chance. chance COMPARE error. gradient descent USED-FOR functions. implicit biases of architecture CONJUNCTION gradient descent. gradient descent CONJUNCTION implicit biases of architecture. gradient descent PART-OF generalisation. implicit biases of architecture PART-OF generalisation. Method are neural networks, and network architecture. OtherScientificTerm are implicit bias of architecture, architecture, NNGP posterior, and minimum a posteriori functions. Metric is average test error. Generic is function. ","This paper studies the effect of gradient descent on the generalisation performance of neural networks. The authors show that gradient descent converges to a function that depends on the implicit bias of architecture and on the number of layers in the network. They show that for networks with infinite width, the average test error is the same as for infinite width networks with Bayesian inference, and for finite width networks, the error is lower than that of chance. They also show that the generalization performance depends on both the implicit biases of architecture as well as gradient descent.    The paper also shows that for functions that depend on the function, gradient descent converge to the minimum a posteriori function. This is a result that is consistent with the observation that the NNGP posterior of a function is a function of the network architecture, and that the error of the function is the sum of the gradient of the weights of the architecture and the weight of the functions of the inputs. ","This paper studies the effect of gradient descent on the generalisation performance of neural networks. The authors show that gradient descent converges to a function that depends on the implicit bias of architecture and on the number of layers in the network. They show that for networks with infinite width, the average test error is the same as for infinite width networks with Bayesian inference, and for finite width networks, the error is lower than that of chance. They also show that the generalization performance depends on both the implicit biases of architecture as well as gradient descent.    The paper also shows that for functions that depend on the function, gradient descent converge to the minimum a posteriori function. This is a result that is consistent with the observation that the NNGP posterior of a function is a function of the network architecture, and that the error of the function is the sum of the gradient of the weights of the architecture and the weight of the functions of the inputs. "
4735,SP:a0e3cf719a95bbc5aad2f663ba5a3169c316ee9b,"large - scale pre - trained multilingual representations USED-FOR cross - lingual transfer methods. transfer EVALUATE-FOR cross - lingual transfer methods. XTREME benchmark EVALUATE-FOR X - Mixup. X - Mixup COMPARE baselines. baselines COMPARE X - Mixup. text understanding tasks EVALUATE-FOR baselines. XTREME benchmark EVALUATE-FOR baselines. text understanding tasks EVALUATE-FOR X - Mixup. OtherScientificTerm are cross - lingual representation discrepancy, and representation discrepancy. Task is cross - lingual transfer. Generic is representations. ","This paper studies the problem of large-scale pre-trained multilingual representations for cross-lingual transfer methods with the goal of improving the transfer performance of existing cross-limbilingual transfer methods. The authors propose X-Mixup, which aims to reduce the cross-linguistic representation discrepancy between the source and target languages. X-mixup is evaluated on the XTREME benchmark, where it is shown to outperform several baselines on text understanding tasks. The paper also shows that the representation discrepancy can be reduced to zero when the number of languages is small.   The authors also show that the representations learned by X-Mixedup are transferable to other languages. ","This paper studies the problem of large-scale pre-trained multilingual representations for cross-lingual transfer methods with the goal of improving the transfer performance of existing cross-limbilingual transfer methods. The authors propose X-Mixup, which aims to reduce the cross-linguistic representation discrepancy between the source and target languages. X-mixup is evaluated on the XTREME benchmark, where it is shown to outperform several baselines on text understanding tasks. The paper also shows that the representation discrepancy can be reduced to zero when the number of languages is small.   The authors also show that the representations learned by X-Mixedup are transferable to other languages. "
4751,SP:19f8cd8f0c274b6141ba097d2ebb6d18af0986fd,"defenses USED-FOR attacks. robust algorithms USED-FOR heterogeneous datasets. bucketing scheme USED-FOR robust algorithms. bucketing CONJUNCTION robust algorithms. robust algorithms CONJUNCTION bucketing. robust algorithms USED-FOR attacks. bucketing USED-FOR attacks. guaranteed convergence FEATURE-OF non - iid Byzantine robust problem. Method are Byzantine robust distributed or federated learning, and machine learning model. Generic are algorithm, problem, and approach. OtherScientificTerm are heterogeneous ( non - iid ), and realistic assumptions. ","This paper studies the problem of Byzantine robust distributed or federated learning, where the goal is to defend against attacks on the machine learning model. The authors propose a new algorithm, called Byzantine robust bucketing, to address this problem. They show that under some realistic assumptions, robust algorithms for heterogeneous (non-iid) datasets are guaranteed to be robust to attacks under the proposed bucketing scheme. They also show that the bucketing and robust algorithms can be applied to heterogeneous datasets, and that the robust algorithms are robust to such attacks under bucketing. Finally, they prove a guaranteed convergence of the non-iID Byzantine robust problem under realistic assumptions. ","This paper studies the problem of Byzantine robust distributed or federated learning, where the goal is to defend against attacks on the machine learning model. The authors propose a new algorithm, called Byzantine robust bucketing, to address this problem. They show that under some realistic assumptions, robust algorithms for heterogeneous (non-iid) datasets are guaranteed to be robust to attacks under the proposed bucketing scheme. They also show that the bucketing and robust algorithms can be applied to heterogeneous datasets, and that the robust algorithms are robust to such attacks under bucketing. Finally, they prove a guaranteed convergence of the non-iID Byzantine robust problem under realistic assumptions. "
4767,SP:4d63513b9a1b9b9fc44a69b3d5679a8f48eb95e7,disentanglement CONJUNCTION multi - task learning. multi - task learning CONJUNCTION disentanglement. hard parameter sharing USED-FOR multi - task learning. hard parameter sharing USED-FOR disentanglement. automatically generated supervised tasks USED-FOR neural networks. automatically generated supervised tasks USED-FOR representations. neural networks USED-FOR representations. disentanglement PART-OF multi - task neural network training. metrics EVALUATE-FOR disentanglement. Method is disentangled representations. Generic is they. Task is multi - task learning setting. ,"This paper studies the relationship between disentanglement and multi-task learning in the context of hard parameter sharing in multi-tasks. The authors show that the disentangled representations learned by neural networks trained on automatically generated supervised tasks can be used to learn representations that are transferable across tasks. They also show that disentangling representations can be learned using neural networks that have been trained on a large number of tasks, and that they can be shared across different tasks. The paper also shows that the performance of disentangler is improved when the number of task-specific parameters is shared across tasks, which is a common practice in the multi-target learning setting.   The authors also provide a set of metrics to evaluate the performance on disentangle the representations learned during multi- task neural network training, and show that hard parameters sharing can improve the performance. ","This paper studies the relationship between disentanglement and multi-task learning in the context of hard parameter sharing in multi-tasks. The authors show that the disentangled representations learned by neural networks trained on automatically generated supervised tasks can be used to learn representations that are transferable across tasks. They also show that disentangling representations can be learned using neural networks that have been trained on a large number of tasks, and that they can be shared across different tasks. The paper also shows that the performance of disentangler is improved when the number of task-specific parameters is shared across tasks, which is a common practice in the multi-target learning setting.   The authors also provide a set of metrics to evaluate the performance on disentangle the representations learned during multi- task neural network training, and show that hard parameters sharing can improve the performance. "
4783,SP:9851adb72e2918780f661f83f7da06eb866787be,Certifying Robust Policies ( CROP ) USED-FOR reinforcement learning. framework USED-FOR Certifying Robust Policies ( CROP ). framework USED-FOR state level robustness certification. adversarial state perturbations FEATURE-OF reinforcement learning. robustness CONJUNCTION lower bound of cumulative rewards. lower bound of cumulative rewards CONJUNCTION robustness. per - state actions CONJUNCTION lower bound of cumulative rewards. lower bound of cumulative rewards CONJUNCTION per - state actions. robustness CONJUNCTION per - state actions. per - state actions CONJUNCTION robustness. per - state actions HYPONYM-OF robustness certification criteria. lower bound of cumulative rewards HYPONYM-OF robustness certification criteria. robustness HYPONYM-OF robustness certification criteria. policy USED-FOR robustness. Gaussian noise USED-FOR Q - functions. Q - functions USED-FOR policy. policy USED-FOR local smoothing algorithm. Gaussian noise USED-FOR policy. global smoothing algorithm USED-FOR robustness. robustness FEATURE-OF finite - horizon cumulative reward. global smoothing algorithm USED-FOR finite - horizon cumulative reward. adversarial state perturbations FEATURE-OF finite - horizon cumulative reward. tight certification bounds FEATURE-OF reward. adaptive search USED-FOR tight certification bounds. adaptive search USED-FOR local smoothing approach. adversarial training CONJUNCTION regularization. regularization CONJUNCTION adversarial training. methods USED-FOR empirically robust RL. RL robustness certification framework EVALUATE-FOR methods. regularization HYPONYM-OF methods. adversarial training HYPONYM-OF methods. adversarial training USED-FOR empirically robust RL. Atari games EVALUATE-FOR methods. RegCVX CONJUNCTION RadialRL. RadialRL CONJUNCTION RegCVX. RegPGD CONJUNCTION RegCVX. RegCVX CONJUNCTION RegPGD. certified robustness EVALUATE-FOR RadialRL. adversarial attacks EVALUATE-FOR algorithms. OtherScientificTerm is cumulative rewards. Generic is trajectory. ,"Certifying Robust Policies (CROP) is a framework for state level robustness certification for reinforcement learning under adversarial state perturbations. The paper proposes three robustness certified criteria: robustness to cumulative rewards, per-state actions, and lower bound of cumulative rewards. The robustness is certified by a policy trained with Gaussian noise on the Q-functions of the policy, and the policy is then used to train a local smoothing algorithm to improve the robustness of a finite-horizon cumulative reward against adversarial states perturbation. The authors also propose a global smoothing method to improve robustness in the presence of adversarial attacks.  The authors show that the proposed local smooth approach can achieve tight certification bounds on the reward using an adaptive search. The proposed methods are evaluated on a number of Atari games, and compared with other methods for empirically robust RL (adversarial training, regularization, and adversarial regularization). RegPGD, RegCVX, and RadialRL are shown to achieve certified robustness on the Atari games.    The paper is well-written and well-motivated. ","Certifying Robust Policies (CROP) is a framework for state level robustness certification for reinforcement learning under adversarial state perturbations. The paper proposes three robustness certified criteria: robustness to cumulative rewards, per-state actions, and lower bound of cumulative rewards. The robustness is certified by a policy trained with Gaussian noise on the Q-functions of the policy, and the policy is then used to train a local smoothing algorithm to improve the robustness of a finite-horizon cumulative reward against adversarial states perturbation. The authors also propose a global smoothing method to improve robustness in the presence of adversarial attacks.  The authors show that the proposed local smooth approach can achieve tight certification bounds on the reward using an adaptive search. The proposed methods are evaluated on a number of Atari games, and compared with other methods for empirically robust RL (adversarial training, regularization, and adversarial regularization). RegPGD, RegCVX, and RadialRL are shown to achieve certified robustness on the Atari games.    The paper is well-written and well-motivated. "
4799,SP:78da3c97182ec1baf6a131740bf7c91a9afb2fd2,"approach USED-FOR conformal prediction. conformal prediction USED-FOR model uncertainty. calibrated candidate set USED-FOR conformal prediction. in - silico screening USED-FOR drug discovery. coverage CONJUNCTION precision. precision CONJUNCTION coverage. natural language processing CONJUNCTION computer vision. computer vision CONJUNCTION natural language processing. computer vision CONJUNCTION computational chemistry. computational chemistry CONJUNCTION computer vision. natural language processing FEATURE-OF classification tasks. computational chemistry FEATURE-OF classification tasks. classification tasks EVALUATE-FOR approach. computer vision HYPONYM-OF classification tasks. OtherScientificTerm are coverage property, conformal sets, noisy candidates, predicted conformal sets, and user - specified tolerance. Task is large - scale settings. Generic are constraint, and algorithm. Metric is true positive rate. ","This paper proposes a novel approach to conformal prediction for model uncertainty based on a calibrated candidate set. Conformal prediction is defined as the prediction of a set of candidate sets that satisfy a coverage property, i.e., the set of candidates that cover the entire training set. The paper shows that conformal sets are robust to noisy candidates, and that the coverage property of conformal predictions is robust to noise in the training data.  The paper also shows that in large-scale settings, the proposed approach is able to achieve a true positive rate of 0.5% in the presence of noisy candidates. This is achieved by imposing a constraint on the number of candidate set that are allowed to be in the conformal set, which is a constraint that ensures that the number (number of) conformal pairs is equal to the coverage and the precision of the candidate sets. This constraint is enforced in a user-specified tolerance. The proposed algorithm is tested on classification tasks in natural language processing, computer vision, and computational chemistry, and is applied to drug discovery through in-silico screening. ","This paper proposes a novel approach to conformal prediction for model uncertainty based on a calibrated candidate set. Conformal prediction is defined as the prediction of a set of candidate sets that satisfy a coverage property, i.e., the set of candidates that cover the entire training set. The paper shows that conformal sets are robust to noisy candidates, and that the coverage property of conformal predictions is robust to noise in the training data.  The paper also shows that in large-scale settings, the proposed approach is able to achieve a true positive rate of 0.5% in the presence of noisy candidates. This is achieved by imposing a constraint on the number of candidate set that are allowed to be in the conformal set, which is a constraint that ensures that the number (number of) conformal pairs is equal to the coverage and the precision of the candidate sets. This constraint is enforced in a user-specified tolerance. The proposed algorithm is tested on classification tasks in natural language processing, computer vision, and computational chemistry, and is applied to drug discovery through in-silico screening. "
4815,SP:b126d2f3c397633745c8833e22ace93a2470e963,complexity EVALUATE-FOR functions. neural network USED-FOR functions. unit - length curve USED-FOR network. random initialization USED-FOR ReLU networks. higher moments of the length distortion CONJUNCTION distortion of higher - dimensional volumes. distortion of higher - dimensional volumes CONJUNCTION higher moments of the length distortion. upper bounds USED-FOR higher moments of the length distortion. upper bounds USED-FOR distortion of higher - dimensional volumes. OtherScientificTerm is curve of outputs. Metric is expected length distortion. ,"This paper studies the complexity of functions defined by a neural network in terms of the curve of outputs. The authors show that a network with a unit-length curve is equivalent to a ReLU network with random initialization. They also provide upper bounds on the expected length distortion of the network, and show that ReLU networks with a random initialization have higher moments of the length distortion and higher moments for distortion of higher-dimensional volumes. ","This paper studies the complexity of functions defined by a neural network in terms of the curve of outputs. The authors show that a network with a unit-length curve is equivalent to a ReLU network with random initialization. They also provide upper bounds on the expected length distortion of the network, and show that ReLU networks with a random initialization have higher moments of the length distortion and higher moments for distortion of higher-dimensional volumes. "
4831,SP:b3b6d0512edfca461ea295ee8665f7f226c45d57,"methods USED-FOR policies. safety constraints CONJUNCTION sparse rewards. sparse rewards CONJUNCTION safety constraints. learning policies PART-OF reinforcement learning ( RL ) problems. Behavioral priors USED-FOR RL. Behavioral priors USED-FOR policy primitives. behavioral priors USED-FOR safe policy learning. SAFEty skill pRiors ( SAFER ) HYPONYM-OF behavioral prior learning algorithm. policy learning USED-FOR complex control tasks. behavioral prior learning algorithm USED-FOR policy learning. SAFER USED-FOR safety variable. contrastive training USED-FOR SAFER. offline data USED-FOR safety requirements. safe and unsafe data USED-FOR contrastive training. abstract actions USED-FOR safe primitive skills. offline data USED-FOR safety variable. SAFER USED-FOR safe and successful policy. safety variable CONJUNCTION abstract action. abstract action CONJUNCTION safety variable. SAFER USED-FOR inference stage. safety variable USED-FOR safety skills. safety skills USED-FOR SAFER. safety skills USED-FOR safe and successful policy. SAFER USED-FOR policies. baseline methods USED-FOR policies. SAFER COMPARE baseline methods. baseline methods COMPARE SAFER. SAFER USED-FOR safety. policies CONJUNCTION safety. safety CONJUNCTION policies. baseline methods USED-FOR safety. Material is offline datasets. OtherScientificTerm are unsafe behavior, and undesirable behaviors. Task is complex safety - critical robotic grasping tasks. ","This paper proposes a safety-aware policy learning algorithm called SAFEty skill pRiors (SAFER) that learns a set of safety primitive skills that can be used to guide policy learning in the presence of unsafe environments. The safety primitive skill is learned by contrastive learning, where the safety skill is used as a prior over the policy. The paper shows that SAFER is able to learn safe primitive skills in the absence of offline data. The authors also show that the proposed algorithm can be combined with existing RL algorithms to improve the safety of the learned policies.","This paper proposes a safety-aware policy learning algorithm called SAFEty skill pRiors (SAFER) that learns a set of safety primitive skills that can be used to guide policy learning in the presence of unsafe environments. The safety primitive skill is learned by contrastive learning, where the safety skill is used as a prior over the policy. The paper shows that SAFER is able to learn safe primitive skills in the absence of offline data. The authors also show that the proposed algorithm can be combined with existing RL algorithms to improve the safety of the learned policies."
4847,SP:a5dadb3ecc3caed3b9d9a68eda0d48a53c2d1ce2,"multi - branch restoration model USED-FOR restoration tasks. Human Visual System USED-FOR multi - branch restoration model. Retinal Ganglion Cells HYPONYM-OF Human Visual System. deraindrop CONJUNCTION deblurring. deblurring CONJUNCTION deraindrop. image dehazing CONJUNCTION deraindrop. deraindrop CONJUNCTION image dehazing. datasets EVALUATE-FOR multi - branch architecture. CMFNet HYPONYM-OF multi - branch architecture. deblurring HYPONYM-OF datasets. deraindrop HYPONYM-OF datasets. image dehazing HYPONYM-OF datasets. pretrained models USED-FOR restoration tasks. Task are Image restoration, and autonomous cars. Method is learning based restoration methods. OtherScientificTerm is generalization. ","This paper proposes a multi-branch restoration model inspired by the Human Visual System (i.e., Retinal Ganglion Cells) to tackle restoration tasks in autonomous cars. Image restoration is an important problem that is often overlooked in learning based restoration methods. The authors propose a new multi- branch architecture called CMFNet, which is evaluated on three datasets: image dehazing, deraindrop and deblurring. The results show that pretrained models are able to achieve state-of-the-art performance on the restoration tasks, and that the generalization is robust to the number of branches. ","This paper proposes a multi-branch restoration model inspired by the Human Visual System (i.e., Retinal Ganglion Cells) to tackle restoration tasks in autonomous cars. Image restoration is an important problem that is often overlooked in learning based restoration methods. The authors propose a new multi- branch architecture called CMFNet, which is evaluated on three datasets: image dehazing, deraindrop and deblurring. The results show that pretrained models are able to achieve state-of-the-art performance on the restoration tasks, and that the generalization is robust to the number of branches. "
4863,SP:263b386beee44b0b45b6f6dc3cf80d020500be62,"FL USED-FOR data heterogeneity. Personalized federated learning ( PFL ) USED-FOR data heterogeneity. FL USED-FOR Personalized federated learning ( PFL ). FL CONJUNCTION PFL. PFL CONJUNCTION FL. unlabeled clients USED-FOR model. hypernetwork module CONJUNCTION encoder module. encoder module CONJUNCTION hypernetwork module. approach USED-FOR IT - PFLHN. encoder module USED-FOR approach. hypernetwork module USED-FOR approach. encoder module USED-FOR IT - PFLHN. hypernetwork module USED-FOR IT - PFLHN. hypernetwork USED-FOR personalized model. client representation PART-OF hypernetwork. benchmark datasets EVALUATE-FOR IT - PFL - HN. IT - PFL - HN COMPARE FL and PFL methods. FL and PFL methods COMPARE IT - PFL - HN. benchmark datasets EVALUATE-FOR FL and PFL methods. multi - task learning CONJUNCTION domain adaptation. domain adaptation CONJUNCTION multi - task learning. multi - task learning USED-FOR it. Method are Federated learning ( FL ), personalized models, prediction service, and encoder network. Material is labeled data. OtherScientificTerm are unlabeled data, large domain shift, data privacy, and differential privacy. Generic is learning setup. Metric is generalization error. ","This paper studies the problem of federated learning (FL) and personalized learning (PFL) in the context of data heterogeneity. In FL and PFL, the model is trained on unlabeled clients, and the goal is to train personalized models that are robust to data heterogeneity between clients. However, in FL, there is a large amount of labeled data, and there are many clients with large domain shift. The paper proposes an approach called IT-PFLHN, which extends the approach of FL to Personalized Federated learning with data heterogeneity by introducing a hypernetwork module and an encoder module. The hypernetwork is used to learn a personalized model for each client, which is then used to train the prediction service. The encoder network is used for the task-specific case, while the hypernetwork consists of a client representation and the client representation of the other clients. The proposed approach is evaluated on several benchmark datasets, and compared to FL and standard PFL methods. The results show that the proposed approach outperforms the state-of-the-art on multi-task learning and domain adaptation, and that it is more robust to multi-tasks than other FL methods. In addition, the paper shows that the learning setup does not suffer from generalization error and that data privacy is preserved.   ","This paper studies the problem of federated learning (FL) and personalized learning (PFL) in the context of data heterogeneity. In FL and PFL, the model is trained on unlabeled clients, and the goal is to train personalized models that are robust to data heterogeneity between clients. However, in FL, there is a large amount of labeled data, and there are many clients with large domain shift. The paper proposes an approach called IT-PFLHN, which extends the approach of FL to Personalized Federated learning with data heterogeneity by introducing a hypernetwork module and an encoder module. The hypernetwork is used to learn a personalized model for each client, which is then used to train the prediction service. The encoder network is used for the task-specific case, while the hypernetwork consists of a client representation and the client representation of the other clients. The proposed approach is evaluated on several benchmark datasets, and compared to FL and standard PFL methods. The results show that the proposed approach outperforms the state-of-the-art on multi-task learning and domain adaptation, and that it is more robust to multi-tasks than other FL methods. In addition, the paper shows that the learning setup does not suffer from generalization error and that data privacy is preserved.   "
4879,SP:960d0a63a82593f6e72275b65f0501f0469d1924,"classification PART-OF selfsupervised learning. conditional diffusion based generative model ( RCDM ) USED-FOR representations. self - supervised models USED-FOR representations. model COMPARE generative models. generative models COMPARE model. generation quality COMPARE generative models. generative models COMPARE generation quality. generation quality EVALUATE-FOR model. tool USED-FOR self - supervised models. SSL projector embedding USED-FOR tasks. classifications HYPONYM-OF tasks. SSL model USED-FOR image manipulation. inherent structure USED-FOR image manipulation. SSL model USED-FOR inherent structure. supervised representation CONJUNCTION SSL representation. SSL representation CONJUNCTION supervised representation. Method are neural networks, SSL ( backbone ) representation, and SSL representations. Generic is representation. OtherScientificTerm is conditioning. ","This paper proposes a conditional diffusion based generative model (RCDM) to learn representations that can be used for classification in selfsupervised learning. The authors show that the proposed model outperforms other generative models in terms of generation quality. The paper also proposes a tool to train self-supervised models to learn such representations. The key idea is to learn a SSL (backbone) representation of the input image, and then use the SSL projector embedding for various tasks (e.g. classifications, image manipulation, etc.). The SSL model learns the inherent structure of the image manipulation using the SSL model, and the representation is then used as a representation for the downstream tasks.  The authors demonstrate that the learned SSL representations are more interpretable than the supervised representation and the SSL representation, and that the generated images are more likely to have inherent structure.   The paper is well-written and easy to follow. ","This paper proposes a conditional diffusion based generative model (RCDM) to learn representations that can be used for classification in selfsupervised learning. The authors show that the proposed model outperforms other generative models in terms of generation quality. The paper also proposes a tool to train self-supervised models to learn such representations. The key idea is to learn a SSL (backbone) representation of the input image, and then use the SSL projector embedding for various tasks (e.g. classifications, image manipulation, etc.). The SSL model learns the inherent structure of the image manipulation using the SSL model, and the representation is then used as a representation for the downstream tasks.  The authors demonstrate that the learned SSL representations are more interpretable than the supervised representation and the SSL representation, and that the generated images are more likely to have inherent structure.   The paper is well-written and easy to follow. "
4895,SP:398899e6c86b4a2a17dfa5c2f4478811f4331c1d,Fp sketch HYPONYM-OF well - celebrated streaming algorithm. well - celebrated streaming algorithm USED-FOR frequency moments estimation. DP baselines COMPARE non - private baseline. non - private baseline COMPARE DP baselines. Fp sketch COMPARE DP baselines. DP baselines COMPARE Fp sketch. Fp sketch COMPARE non - private baseline. non - private baseline COMPARE Fp sketch. logarithmic factor USED-FOR non - private baseline. polylogarithmic space USED-FOR Fp sketch. differential privacy guarantee FEATURE-OF accuracy. differential privacy guarantee FEATURE-OF Fp sketch. accuracy EVALUATE-FOR Fp sketch. OtherScientificTerm is evaluation code. ,"This paper proposes a new streaming algorithm for frequency moments estimation, called Fp sketch, which is a well-celebrated streaming algorithm. Compared to DP baselines, the proposed non-private baseline is based on a logarithmic factor, and the authors show that Fp sketched in polylogarithmically space has a better differential privacy guarantee on the accuracy. The authors also provide an evaluation code for the proposed algorithm. ","This paper proposes a new streaming algorithm for frequency moments estimation, called Fp sketch, which is a well-celebrated streaming algorithm. Compared to DP baselines, the proposed non-private baseline is based on a logarithmic factor, and the authors show that Fp sketched in polylogarithmically space has a better differential privacy guarantee on the accuracy. The authors also provide an evaluation code for the proposed algorithm. "
4911,SP:3253b13851b5a3b5e3c8c6e24891db05903a4e57,"paradigm USED-FOR diverse strategies. Reward - Switching Policy Optimization ( RSPO ) USED-FOR diverse strategies. diverse strategies USED-FOR complex RL environments. Reward - Switching Policy Optimization ( RSPO ) HYPONYM-OF paradigm. trajectory - based novelty measurement USED-FOR optimization process. RSPO USED-FOR extrinsic and intrinsic rewards. trajectory - based novelty measurement USED-FOR RSPO. trajectory - based novelty measurement USED-FOR extrinsic and intrinsic rewards. policy optimization USED-FOR RSPO. extrinsic rewards USED-FOR policy optimization. extrinsic rewards USED-FOR RSPO. intrinsic diversity reward USED-FOR exploration. RSPO USED-FOR exploration. RSPO USED-FOR trajectories. policies USED-FOR trajectories. intrinsic diversity reward USED-FOR RSPO. single - agent particle - world tasks CONJUNCTION MuJoCo continuous control. MuJoCo continuous control CONJUNCTION single - agent particle - world tasks. multi - agent stag - hunt games CONJUNCTION StarCraftII challenges. StarCraftII challenges CONJUNCTION multi - agent stag - hunt games. RSPO USED-FOR strategies. MuJoCo continuous control CONJUNCTION multi - agent stag - hunt games. multi - agent stag - hunt games CONJUNCTION MuJoCo continuous control. single - agent particle - world tasks CONJUNCTION multi - agent stag - hunt games. multi - agent stag - hunt games CONJUNCTION single - agent particle - world tasks. OtherScientificTerm are learning policy, and sampled trajectory. ","This paper proposes a new paradigm called Reward-Switching Policy Optimization (RSPO) to learn diverse strategies in complex RL environments. RSPO optimizes both extrinsic and intrinsic rewards based on a trajectory-based novelty measurement that is used to guide the optimization process. The intrinsic diversity reward encourages exploration, while the extrinsiative reward encourages the learning policy to explore the state-action pairs that are more likely to be explored by the sampled trajectory. The authors show that by using policy optimization to optimize the intrinsic reward, RSPOP can learn trajectories that are diverse across different policies. They demonstrate the effectiveness of the proposed strategies using experiments on single-agent particle-world tasks, MuJoCo continuous control, multi-agent stag-hunt games, and StarCraftII challenges. ","This paper proposes a new paradigm called Reward-Switching Policy Optimization (RSPO) to learn diverse strategies in complex RL environments. RSPO optimizes both extrinsic and intrinsic rewards based on a trajectory-based novelty measurement that is used to guide the optimization process. The intrinsic diversity reward encourages exploration, while the extrinsiative reward encourages the learning policy to explore the state-action pairs that are more likely to be explored by the sampled trajectory. The authors show that by using policy optimization to optimize the intrinsic reward, RSPOP can learn trajectories that are diverse across different policies. They demonstrate the effectiveness of the proposed strategies using experiments on single-agent particle-world tasks, MuJoCo continuous control, multi-agent stag-hunt games, and StarCraftII challenges. "
4927,SP:e3ab3aa87ab023bd9949b99a17d4b6e26c1473c0,"Diffusion models HYPONYM-OF generative models. GANs COMPARE autoregressive models. autoregressive models COMPARE GANs. sample quality CONJUNCTION autoregressive models. autoregressive models CONJUNCTION sample quality. autoregressive models USED-FOR likelihood scores. GANs HYPONYM-OF generative models. autoregressive models HYPONYM-OF generative models. sample quality EVALUATE-FOR GANs. method USED-FOR fast samplers. fast samplers USED-FOR diffusion model. flexible non - Markovian samplers USED-FOR diffusion models. Generalized Gaussian Diffusion Models ( GGDM ) HYPONYM-OF flexible non - Markovian samplers. degrees of freedom FEATURE-OF GGDM samplers. sample quality scores USED-FOR GGDM samplers. gradient descent USED-FOR sample quality scores. sample quality scores USED-FOR degrees of freedom. gradient descent USED-FOR GGDM samplers. reparametrization trick CONJUNCTION gradient rematerialization. gradient rematerialization CONJUNCTION reparametrization trick. sampling process USED-FOR optimization procedure. reparametrization trick USED-FOR optimization procedure. gradient rematerialization USED-FOR optimization procedure. DDSS USED-FOR unconditional image generation. datasets EVALUATE-FOR DDSS. FID scores HYPONYM-OF datasets. fine - tuning CONJUNCTION re - training. re - training CONJUNCTION fine - tuning. method CONJUNCTION pre - trained diffusion model. pre - trained diffusion model CONJUNCTION method. Generic is model. OtherScientificTerm are LSUN, and inference steps. Method is DDPM / DDIM baselines. ","Diffusion models are one of the most commonly used generative models in the literature, and the authors show that GANs have better sample quality than autoregressive models in terms of likelihood scores. They also show that diffusion models can be trained with flexible non-Markovian samplers, such as Generalized Gaussian Diffusion Models (GGDM). The authors propose a method to train fast sampler of the diffusion model with fast sampliers. They show that the GGDM sampler has higher degrees of freedom than the DDPM/DDIM baselines, and that the sample quality scores obtained by gradient descent can be used to train GGDM Samplers. The authors also propose a sampling process to speed up the optimization procedure by using the reparametrization trick and gradient rematerialization. They demonstrate that DDSS is able to achieve state-of-the-art performance for unconditional image generation on two datasets (FID scores and LSUN). They also demonstrate that the proposed method can be combined with a pre-trained diffusion model, and fine-tuning and re-training can be performed in parallel. ","Diffusion models are one of the most commonly used generative models in the literature, and the authors show that GANs have better sample quality than autoregressive models in terms of likelihood scores. They also show that diffusion models can be trained with flexible non-Markovian samplers, such as Generalized Gaussian Diffusion Models (GGDM). The authors propose a method to train fast sampler of the diffusion model with fast sampliers. They show that the GGDM sampler has higher degrees of freedom than the DDPM/DDIM baselines, and that the sample quality scores obtained by gradient descent can be used to train GGDM Samplers. The authors also propose a sampling process to speed up the optimization procedure by using the reparametrization trick and gradient rematerialization. They demonstrate that DDSS is able to achieve state-of-the-art performance for unconditional image generation on two datasets (FID scores and LSUN). They also demonstrate that the proposed method can be combined with a pre-trained diffusion model, and fine-tuning and re-training can be performed in parallel. "
4943,SP:7a7506f2b5500a573c0cfb8b0822e5ea725c886a,Large Language Models ( LLMs ) USED-FOR factual information. embedding layer PART-OF LLMs. lightweight models HYPONYM-OF P - Adapters. embedding layer PART-OF lightweight models. continuous prompts USED-FOR LLM. LLM embeddings USED-FOR continuous prompts. LLM embeddings USED-FOR They. Mixture of Experts ( MoE ) models USED-FOR continuous prompts. classifier USED-FOR natural language prompts. classifier USED-FOR They. human - annotated data USED-FOR classifier. P - Adapters COMPARE MoE models. MoE models COMPARE P - Adapters. MoE models USED-FOR factual information. P - Adapters USED-FOR factual information. consistency EVALUATE-FOR baseline. PAdapters COMPARE baseline. baseline COMPARE PAdapters. precision CONJUNCTION consistency. consistency CONJUNCTION precision. consistency EVALUATE-FOR PAdapters. natural language queries USED-FOR baseline. precision EVALUATE-FOR PAdapters. LLM ’s embeddings USED-FOR natural language prompt. OtherScientificTerm is continuous ones. Method is P - Adapter. ,"This paper proposes P-Adapters, lightweight models that replace the embedding layer in standard LLMs with continuous ones. They leverage the LLM embeddings of a given LLM with continuous prompts from continuous prompts generated by Mixture of Experts (MoE) models. They also use a classifier trained on human-annotated data to generate natural language prompts. The authors show that P-adapters outperform MoE models for factual information from Large Language Models (LLMs) in terms of precision and consistency, and outperform a baseline trained on natural language queries. The paper also shows that PAdapters can be used to generate a natural language prompt from LLM’s embeddeddings. ","This paper proposes P-Adapters, lightweight models that replace the embedding layer in standard LLMs with continuous ones. They leverage the LLM embeddings of a given LLM with continuous prompts from continuous prompts generated by Mixture of Experts (MoE) models. They also use a classifier trained on human-annotated data to generate natural language prompts. The authors show that P-adapters outperform MoE models for factual information from Large Language Models (LLMs) in terms of precision and consistency, and outperform a baseline trained on natural language queries. The paper also shows that PAdapters can be used to generate a natural language prompt from LLM’s embeddeddings. "
4959,SP:35cdf71f027cc5168b55cc34c64bfb2f3087d6f5,"one - shot classification COMPARE CCTS. CCTS COMPARE one - shot classification. catastrophic forgetting CONJUNCTION over fitting. over fitting CONJUNCTION catastrophic forgetting. unclear distribution division FEATURE-OF continual learning task. continual learning task USED-FOR CCTS. Adaptive model training policy USED-FOR CCTS. Adaptive multi - distribution extraction policy PART-OF adaptability. fixed rules CONJUNCTION prior knowledge. prior knowledge CONJUNCTION fixed rules. ACCTS USED-FOR data distributions. data distributions USED-FOR time series evolution. time series evolution CONJUNCTION model change. model change CONJUNCTION time series evolution. method COMPARE baselines. baselines COMPARE method. real - world datasets EVALUATE-FOR method. Task is real - world applications. OtherScientificTerm are vital signs, features, multi - distribution form, independent identically distributed premise, and fixed division rule. Generic are concept, models, and model. Method are Continuous Classification of Time Series ( CCTS ), and Adaptive importance - based replay policy. ","This paper proposes a new continual learning method for continuous classification of time series. The proposed method, called Adaptive Importance-based Continual Classification of Time Series (ACCTS), is based on the idea of multi-distribution multi-class learning, which is an important problem in real-world applications. In particular, the authors consider the problem of catastrophic forgetting and overfitting in the continual learning setting. The authors propose a new method that adapts the model training policy to adapt to the changing distribution of the time series in a continual learning fashion.  ","This paper proposes a new continual learning method for continuous classification of time series. The proposed method, called Adaptive Importance-based Continual Classification of Time Series (ACCTS), is based on the idea of multi-distribution multi-class learning, which is an important problem in real-world applications. In particular, the authors consider the problem of catastrophic forgetting and overfitting in the continual learning setting. The authors propose a new method that adapts the model training policy to adapt to the changing distribution of the time series in a continual learning fashion.  "
4975,SP:d9b74b749aa465496763d3a3a9bf3a53e800587e,internal representations of past inputs USED-FOR language models. approximate kNN lookup USED-FOR language modeling. benchmarks CONJUNCTION tasks. tasks CONJUNCTION benchmarks. approximate kNN lookup PART-OF memory. tasks EVALUATE-FOR language modeling. benchmarks EVALUATE-FOR language modeling. generic webtext ( C4 ) HYPONYM-OF tasks. books ( PG-19 ) HYPONYM-OF tasks. books ( PG-19 ) HYPONYM-OF benchmarks. generic webtext ( C4 ) HYPONYM-OF benchmarks. theorems USED-FOR model. Method is Language models. ,"This paper proposes to use internal representations of past inputs to improve the performance of language models. Language models have been shown to perform well on many benchmarks and tasks (e.g., generic webtext (C4) and books (PG-19)) using language modeling based on approximate kNN lookup in memory. This paper proposes a new model based on these theorems and shows that the proposed model performs well on these tasks.","This paper proposes to use internal representations of past inputs to improve the performance of language models. Language models have been shown to perform well on many benchmarks and tasks (e.g., generic webtext (C4) and books (PG-19)) using language modeling based on approximate kNN lookup in memory. This paper proposes a new model based on these theorems and shows that the proposed model performs well on these tasks."
4991,SP:7a1bbf86c3fdb8738aa826ca330493e857d050ba,"MLMs USED-FOR probability distribution. masked language modeling ( MLM ) objective USED-FOR models. energy - based sequence models USED-FOR MLMs. MLMs USED-FOR energy parametrizations. Metropolis – Hastings Monte Carlo algorithm USED-FOR tractable sampling scheme. masked conditionals USED-FOR masked language models. energybased models USED-FOR open - ended unconditional generation. approach COMPARE undirected generation approaches. undirected generation approaches COMPARE approach. stationary distribution FEATURE-OF Markov chain. Method are parametrizations, and sampling algorithm. Task is machine translation. ",This paper proposes a new masked language modeling (MLM) objective for training models with a masked language modelling (MLMs) objective. The authors propose to use energy-based sequence models to train MLMs to learn the probability distribution over the energy parametrizations of a sequence of words. They show that the MLMs trained with such MLMs are able to learn to learn such the energy parameters of the sequence. They also propose a tractable sampling scheme based on the Metropolis–Hastings Monte Carlo algorithm.    The authors also show that using masked conditionals to train the masked language models can be used to learn more generalizable and tractable parametrized language models. They demonstrate that the proposed approach is more tractable than existing undirected generation approaches and can be applied to open-ended unconditional generation based on energybased models. The paper also shows that the stationary distribution of a Markov chain with respect to a stationary distribution can be learned using the proposed sampling algorithm. They further show that their approach can be extended to machine translation. ,This paper proposes a new masked language modeling (MLM) objective for training models with a masked language modelling (MLMs) objective. The authors propose to use energy-based sequence models to train MLMs to learn the probability distribution over the energy parametrizations of a sequence of words. They show that the MLMs trained with such MLMs are able to learn to learn such the energy parameters of the sequence. They also propose a tractable sampling scheme based on the Metropolis–Hastings Monte Carlo algorithm.    The authors also show that using masked conditionals to train the masked language models can be used to learn more generalizable and tractable parametrized language models. They demonstrate that the proposed approach is more tractable than existing undirected generation approaches and can be applied to open-ended unconditional generation based on energybased models. The paper also shows that the stationary distribution of a Markov chain with respect to a stationary distribution can be learned using the proposed sampling algorithm. They further show that their approach can be extended to machine translation. 
5007,SP:011626ba4fafee13d4a30e3f13c1df5b7071a7f1,"data augmentation USED-FOR deep neural networks. deep neural networks USED-FOR NLP tasks. data augmentation USED-FOR NLP tasks. labeled samples USED-FOR It. low - data or classimbalanced regimes HYPONYM-OF labeled samples. parameter tuning CONJUNCTION inherent randomness. inherent randomness CONJUNCTION parameter tuning. inherent randomness USED-FOR augmentation techniques. parameter tuning USED-FOR augmentation techniques. reward function USED-FOR policy. augmentation USED-FOR NLP tasks. augmentation strategy USED-FOR task. learning data augmentation policy USED-FOR augmentation strategy. reward function USED-FOR augmentation policy. learning - based augmentation COMPARE augmentation schemes. augmentation schemes COMPARE learning - based augmentation. augmentations USED-FOR task. text classification tasks EVALUATE-FOR augmentation schemes. text classification tasks EVALUATE-FOR learning - based augmentation. augmentation policy USED-FOR tasks. method USED-FOR low - data and class - imbalanced regimes. OtherScientificTerm are informative training signals, and semantic similarity. Method are data augmentation policy, and sample re - weighting scheme. Generic is model. ","This paper studies the problem of data augmentation in deep neural networks for NLP tasks. It focuses on the problem where the number of labeled samples (in low-data or classimbalanced regimes) is limited (i.e., there are not enough informative training signals) and the data augmentation policy is biased. The authors propose a sample re-weighting scheme to improve the performance of the model. They show that learning-based augmentation outperforms other augmentation schemes that rely on parameter tuning and inherent randomness. They also show that the augmentation strategy for a given task can be learned from a learning data augmentmentation policy. Finally, they propose an augmentation policy that uses a reward function that encourages the policy to learn a good augmentation for a specific task. They evaluate their method on text classification tasks and show that their method outperforms existing augmentation techniques on a number of different augmentations for the same task.  ","This paper studies the problem of data augmentation in deep neural networks for NLP tasks. It focuses on the problem where the number of labeled samples (in low-data or classimbalanced regimes) is limited (i.e., there are not enough informative training signals) and the data augmentation policy is biased. The authors propose a sample re-weighting scheme to improve the performance of the model. They show that learning-based augmentation outperforms other augmentation schemes that rely on parameter tuning and inherent randomness. They also show that the augmentation strategy for a given task can be learned from a learning data augmentmentation policy. Finally, they propose an augmentation policy that uses a reward function that encourages the policy to learn a good augmentation for a specific task. They evaluate their method on text classification tasks and show that their method outperforms existing augmentation techniques on a number of different augmentations for the same task.  "
5023,SP:69d41a862ea189f72d4e8af2854e27b95a91fa41,"Meta - learning USED-FOR offline reinforcement learning ( OMRL ). augmented state USED-FOR task identity. intra - task attention mechanism CONJUNCTION inter - task contrastive learning objectives. inter - task contrastive learning objectives CONJUNCTION intra - task attention mechanism. sparse reward CONJUNCTION distribution shift. distribution shift CONJUNCTION sparse reward. sparse reward USED-FOR task representation learning. distribution shift USED-FOR task representation learning. FOCAL HYPONYM-OF SOTA OMRL algorithms. meta - RL benchmarks EVALUATE-FOR prior algorithms. Method are RL algorithms, and context - based encoder. ","Meta-learning is a popular technique for offline reinforcement learning (OMRL) where the task identity is learned from an augmented state. However, existing RL algorithms do not take into account the fact that the context-based encoder may not be well-suited for the task at hand. In this paper, the authors propose to use an intra-task attention mechanism and inter-task contrastive learning objectives to improve the performance of offline RL algorithms. The authors show that sparse reward and distribution shift can improve the task representation learning in the presence of sparse reward. They also show that the SOTA OMRL algorithms (e.g., FOCAL) outperform prior algorithms on meta-RL benchmarks.","Meta-learning is a popular technique for offline reinforcement learning (OMRL) where the task identity is learned from an augmented state. However, existing RL algorithms do not take into account the fact that the context-based encoder may not be well-suited for the task at hand. In this paper, the authors propose to use an intra-task attention mechanism and inter-task contrastive learning objectives to improve the performance of offline RL algorithms. The authors show that sparse reward and distribution shift can improve the task representation learning in the presence of sparse reward. They also show that the SOTA OMRL algorithms (e.g., FOCAL) outperform prior algorithms on meta-RL benchmarks."
5039,SP:ed86c60850d5c8302dcf1c2167db303e778fe681,belief state FEATURE-OF partially observable Markov system. parametric sequential generative modeling methods USED-FOR problem setting. methods USED-FOR belief state modeling. belief state modeling USED-FOR multi - agent settings. policies PART-OF belief model. inference - time improvement framework USED-FOR parametric sequential generative modeling methods. belief fine - tuning ( BFT ) HYPONYM-OF inference - time improvement framework. approximate dynamic programming USED-FOR model parameters. fine - tuning USED-FOR model parameters. BFT USED-FOR model parameters. fine - tuning FEATURE-OF approximate dynamic programming. fine - tuning USED-FOR BFT. approximate dynamic programming USED-FOR BFT. accuracy EVALUATE-FOR belief model. It USED-FOR belief model. it USED-FOR model. accuracy EVALUATE-FOR It. belief model USED-FOR BFT. BFT USED-FOR approximate public belief state search. imperfect - information games FEATURE-OF approximate public belief state search. Method is dynamics model. OtherScientificTerm is specialization. ,"This paper proposes an inference-time improvement framework called belief fine-tuning (BFT) to improve the performance of parametric sequential generative modeling methods for the problem setting of learning a belief state in a partially observable Markov system. Previous methods for belief state modeling in multi-agent settings are limited to the case where the dynamics model is learned in a single agent and the policies in the belief model are learned over multiple agents. BFT uses approximate dynamic programming to fine-tune the model parameters during training, and it can be applied to any model. It improves the accuracy of a belief model trained with the same number of agents. The authors show that BFT can improve the accuracy in the case of approximate public belief state search in imperfect-information games using BFT.   ","This paper proposes an inference-time improvement framework called belief fine-tuning (BFT) to improve the performance of parametric sequential generative modeling methods for the problem setting of learning a belief state in a partially observable Markov system. Previous methods for belief state modeling in multi-agent settings are limited to the case where the dynamics model is learned in a single agent and the policies in the belief model are learned over multiple agents. BFT uses approximate dynamic programming to fine-tune the model parameters during training, and it can be applied to any model. It improves the accuracy of a belief model trained with the same number of agents. The authors show that BFT can improve the accuracy in the case of approximate public belief state search in imperfect-information games using BFT.   "
5055,SP:6150725599c10f0e26f0d7cb1fc04b5b227a4456,"accuracy loss CONJUNCTION slow training runtime. slow training runtime CONJUNCTION accuracy loss. accuracy loss EVALUATE-FOR methods. sparse matrices USED-FOR sparsity mask. fixed structure FEATURE-OF sparse matrices. products of butterfly matrices HYPONYM-OF fixed structure. hardware USED-FOR butterfly ( block and flat ). attention CONJUNCTION MLP. MLP CONJUNCTION attention. fixed sparsity pattern USED-FOR network layers. method USED-FOR network layers. Pixelated Butterfly USED-FOR method. flat block butterfly and low - rank matrices USED-FOR fixed sparsity pattern. fixed sparsity pattern USED-FOR Pixelated Butterfly. fixed sparsity pattern USED-FOR method. flat block butterfly and low - rank matrices USED-FOR method. attention HYPONYM-OF network layers. MLP HYPONYM-OF network layers. Pixelated Butterfly COMPARE butterfly. butterfly COMPARE Pixelated Butterfly. Pixelated Butterfly USED-FOR training. ImageNet classification and WikiText-103 language modeling tasks EVALUATE-FOR sparse models. dense MLP - Mixer CONJUNCTION Vision Transformer. Vision Transformer CONJUNCTION dense MLP - Mixer. sparse models COMPARE dense MLP - Mixer. dense MLP - Mixer COMPARE sparse models. Vision Transformer CONJUNCTION GPT-2 medium. GPT-2 medium CONJUNCTION Vision Transformer. sparse models COMPARE GPT-2 medium. GPT-2 medium COMPARE sparse models. sparse models COMPARE Vision Transformer. Vision Transformer COMPARE sparse models. ImageNet classification and WikiText-103 language modeling tasks EVALUATE-FOR dense MLP - Mixer. ImageNet classification and WikiText-103 language modeling tasks EVALUATE-FOR GPT-2 medium. accuracy EVALUATE-FOR sparse models. Method are Overparameterized neural networks, Sparse model training, and model components. Metric are computational cost, and accuracy – efficiency tradeoffs. OtherScientificTerm is butterfly matrices. ","This paper studies the problem of overparameterized neural networks. Sparse model training has been a hot topic in recent years, but the computational cost has been prohibitively expensive due to the large number of sparse matrices required to achieve a good accuracy loss and slow training runtime. The authors propose two methods to improve the accuracy loss of these methods. The first method, Pixelated Butterfly, uses a fixed sparsity mask over a fixed structure (product of products of butterfly matrices) of the matrices, which is the hardware for butterfly (block and flat) matrices. The second method uses the same method to sparsify the network layers (attention, MLP, etc.) by using a flat block butterfly and low-rank matrices to obtain a more efficient method.   The authors show that the proposed method is able to achieve comparable accuracy to the original butterfly, but with much less computational cost. They also show that their method can be applied to other network layers such as attention and MLP as well. They show that dense MLP-Mixer, Vision Transformer, and GPT-2 medium outperform sparse models on ImageNet classification and WikiText-103 language modeling tasks. The paper also shows that the method can also be used to speed up training when the number of model components is small.  The paper is well-written and well-motivated, and the paper is clearly written. However, there is a lack of discussion about the accuracy – efficiency tradeoffs, and it is not clear to me whether the paper has made any contributions to the community. I would appreciate it if the authors could provide more discussion about why their method works, and how the fixed structure of the dense matrices is a good choice, and if the paper can provide a more detailed analysis of their method. I think the paper could benefit from a more in-depth discussion about how the flat and low rank matrices are used, and whether the flat matrices can be used as a basis for the method. Also, it would be good to see more discussion of the effect of the size of the network on the accuracy of the sparse models.","This paper studies the problem of overparameterized neural networks. Sparse model training has been a hot topic in recent years, but the computational cost has been prohibitively expensive due to the large number of sparse matrices required to achieve a good accuracy loss and slow training runtime. The authors propose two methods to improve the accuracy loss of these methods. The first method, Pixelated Butterfly, uses a fixed sparsity mask over a fixed structure (product of products of butterfly matrices) of the matrices, which is the hardware for butterfly (block and flat) matrices. The second method uses the same method to sparsify the network layers (attention, MLP, etc.) by using a flat block butterfly and low-rank matrices to obtain a more efficient method.   The authors show that the proposed method is able to achieve comparable accuracy to the original butterfly, but with much less computational cost. They also show that their method can be applied to other network layers such as attention and MLP as well. They show that dense MLP-Mixer, Vision Transformer, and GPT-2 medium outperform sparse models on ImageNet classification and WikiText-103 language modeling tasks. The paper also shows that the method can also be used to speed up training when the number of model components is small.  The paper is well-written and well-motivated, and the paper is clearly written. However, there is a lack of discussion about the accuracy – efficiency tradeoffs, and it is not clear to me whether the paper has made any contributions to the community. I would appreciate it if the authors could provide more discussion about why their method works, and how the fixed structure of the dense matrices is a good choice, and if the paper can provide a more detailed analysis of their method. I think the paper could benefit from a more in-depth discussion about how the flat and low rank matrices are used, and whether the flat matrices can be used as a basis for the method. Also, it would be good to see more discussion of the effect of the size of the network on the accuracy of the sparse models."
5071,SP:136e31054a55abca840f6478491972023c2296cb,"score matching USED-FOR data distribution. formulation USED-FOR controllable generation. class center PART-OF forward and reverse process. class center USED-FOR conditional diffusion probabilistic model. faster sampling USED-FOR method. inception score CONJUNCTION FID score. FID score CONJUNCTION inception score. state - of - the - art methods COMPARE conditional image generation. conditional image generation COMPARE state - of - the - art methods. framework COMPARE state - of - the - art methods. state - of - the - art methods COMPARE framework. CIFAR-10 USED-FOR conditional image generation. FID score EVALUATE-FOR conditional image generation. inception score EVALUATE-FOR conditional image generation. Method are Score - based generative models, and diffusion probabilistic models. OtherScientificTerm are Markov chain, and class clustering phenomenon. ",This paper proposes a new generative model for conditional diffusion probabilistic models. Score-based generative models are a general class of models that can be seen as a Markov chain. The authors propose a new formulation for controllable generation based on score matching between the data distribution and the class center of a conditional diffusion model. The class center in the forward and reverse process is the class of the data. The proposed method is based on faster sampling and is able to avoid the class clustering phenomenon. Experiments on CIFAR-10 show that the proposed framework outperforms state-of-the-art methods in terms of inception score and FID score for conditional image generation.,This paper proposes a new generative model for conditional diffusion probabilistic models. Score-based generative models are a general class of models that can be seen as a Markov chain. The authors propose a new formulation for controllable generation based on score matching between the data distribution and the class center of a conditional diffusion model. The class center in the forward and reverse process is the class of the data. The proposed method is based on faster sampling and is able to avoid the class clustering phenomenon. Experiments on CIFAR-10 show that the proposed framework outperforms state-of-the-art methods in terms of inception score and FID score for conditional image generation.
5087,SP:fc2196f1f4ecd864398fed6640ff3f8b19870763,"fixed domain - invariant features CONJUNCTION common hypotheses. common hypotheses CONJUNCTION fixed domain - invariant features. generalization EVALUATE-FOR domain generalization ( DG ) approaches. generalization EVALUATE-FOR prediction tasks. label - informative features USED-FOR label prediction task. label - informative features USED-FOR latent sub - spaces. DG benchmarks EVALUATE-FOR it. DG benchmarks EVALUATE-FOR method. Metric is generalization capacity. Generic are assumption, and approaches. OtherScientificTerm are invariant hypothesis, and sub - spaces. Method is LASSO. ","This paper studies the generalization performance of domain generalization (DG) approaches. The authors propose a new method, called LASSO, to improve the performance of existing DG approaches by learning a set of latent sub-spaces with label-informative features for each label prediction task. They show that the proposed method outperforms existing methods in terms of generalization capacity on a number of benchmark datasets.","This paper studies the generalization performance of domain generalization (DG) approaches. The authors propose a new method, called LASSO, to improve the performance of existing DG approaches by learning a set of latent sub-spaces with label-informative features for each label prediction task. They show that the proposed method outperforms existing methods in terms of generalization capacity on a number of benchmark datasets."
5103,SP:6e8e5bdeb77e3cafe1975da8411fb65118955d14,"kernel thinning ( KT ) algorithm USED-FOR probability distribution. kernel thinning ( KT ) algorithm COMPARE independent sampling. independent sampling COMPARE kernel thinning ( KT ) algorithm. reproducing kernel Hilbert space ( RKHS ) USED-FOR independent sampling. KT USED-FOR RKHS. kernel CONJUNCTION distribution. distribution CONJUNCTION kernel. KT USED-FOR kernel. KT USED-FOR dimension - free guarantees. dimension - free guarantees FEATURE-OF kernel. inverse multiquadric CONJUNCTION sinc. sinc CONJUNCTION inverse multiquadric. Gaussian CONJUNCTION inverse multiquadric. inverse multiquadric CONJUNCTION Gaussian. target KT COMPARE square - root KT. square - root KT COMPARE target KT. maximum mean discrepancy ( MMD ) guarantees EVALUATE-FOR square - root KT. target KT HYPONYM-OF analytic kernels. maximum mean discrepancy ( MMD ) guarantees EVALUATE-FOR target KT. sinc HYPONYM-OF analytic kernels. Gaussian HYPONYM-OF analytic kernels. inverse multiquadric HYPONYM-OF analytic kernels. Laplace CONJUNCTION Matérn. Matérn CONJUNCTION Laplace. fractional power kernel USED-FOR KT. Laplace HYPONYM-OF non - smooth kernels. Matérn HYPONYM-OF non - smooth kernels. MMD guarantees CONJUNCTION individual function guarantees. individual function guarantees CONJUNCTION MMD guarantees. KT USED-FOR target and power kernels. individual function guarantees FEATURE-OF target KT. target KT CONJUNCTION KT+. KT+ CONJUNCTION target KT. integration error EVALUATE-FOR target KT. integration error EVALUATE-FOR KT+. OtherScientificTerm are square - root kernel, square - roots, and differential equation posteriors. ","This paper proposes a novel kernel thinning (KT) algorithm for estimating the probability distribution of a kernel in the reproducing kernel Hilbert space (RKHS). Unlike independent sampling in reproducing kernels, the kernel of a square-root kernel can be represented as a fractional power kernel. The authors show that KT can approximate the RKHS using a fraction of the square-roots of the kernel and the corresponding distribution. They show that using KT yields dimension-free guarantees for the kernel. They also show that for analytic kernels (Gaussian, inverse multiquadric, sinc) and non-smooth kernels (Laplace, Matérn, etc.), the proposed target KT achieves maximum mean discrepancy (MMD) guarantees, which is better than square-regret KT, which achieves MMD guarantees for all analytic kernels. Finally, they show that the integration error of target KT and KT+ is the same as that of square-resolutions and square-versus, and KT can be used to approximate both target and power kernels.   The authors also provide individual function guarantees for target KT, showing that KT is able to approximate the differential equation posteriors of both the target and the power kernels, and that KT+ achieves a lower integration error than square root KT.","This paper proposes a novel kernel thinning (KT) algorithm for estimating the probability distribution of a kernel in the reproducing kernel Hilbert space (RKHS). Unlike independent sampling in reproducing kernels, the kernel of a square-root kernel can be represented as a fractional power kernel. The authors show that KT can approximate the RKHS using a fraction of the square-roots of the kernel and the corresponding distribution. They show that using KT yields dimension-free guarantees for the kernel. They also show that for analytic kernels (Gaussian, inverse multiquadric, sinc) and non-smooth kernels (Laplace, Matérn, etc.), the proposed target KT achieves maximum mean discrepancy (MMD) guarantees, which is better than square-regret KT, which achieves MMD guarantees for all analytic kernels. Finally, they show that the integration error of target KT and KT+ is the same as that of square-resolutions and square-versus, and KT can be used to approximate both target and power kernels.   The authors also provide individual function guarantees for target KT, showing that KT is able to approximate the differential equation posteriors of both the target and the power kernels, and that KT+ achieves a lower integration error than square root KT."
5119,SP:645c3f1864aa843d4899fc2406f694b5aab8460d,"Combinatorial optimization USED-FOR real - world problems. open - source benchmark suite USED-FOR NP - hard MAXIMUM INDEPENDENT SET problem. weighted and unweighted variants FEATURE-OF open - source benchmark suite. benchmark suite USED-FOR guided tree search algorithm. graph convolution network USED-FOR tree search. code quality CONJUNCTION extensibility. extensibility CONJUNCTION code quality. graph convolution network USED-FOR solution structure. random values USED-FOR graph convolution network. extensibility EVALUATE-FOR algorithm. code quality EVALUATE-FOR algorithm. graph kernelization HYPONYM-OF algorithmic techniques. algorithmic techniques USED-FOR tree search. tree search implementations COMPARE solvers. solvers COMPARE tree search implementations. competitive solution quality EVALUATE-FOR GNN. GNN USED-FOR solver. reinforcement learning USED-FOR solver. Method are graph neural networks ( GNNs ), machine learning - based solvers, and classical algorithmic solvers. OtherScientificTerm are NP - hard problems, and problem - specific solution structure. Generic is suite. ","Combinatorial optimization is an important problem in real-world problems, and graph neural networks (GNNs) have recently become popular as machine learning-based solvers. This paper presents an open-source benchmark suite for the NP-hard MAXIMUM INDEPENDENT SET problem, with both weighted and unweighted variants of the benchmark suite. The authors propose a guided tree search algorithm based on this benchmark suite, and show that a graph convolution network can be used as a guide for tree search to learn a solution structure from a set of random values. The algorithm is shown to improve the code quality and extensibility of the algorithm. The paper also shows that tree search with algorithmic techniques (e.g. graph kernelization) can be more efficient than classical algorithmic solvers, and that the tree search implementations can be competitive with existing solvers when the problem-specific solution structure is well defined. Finally, the authors show that the competitive solution quality of a GNN trained with reinforcement learning can be achieved when the solver is trained with the GNN. ","Combinatorial optimization is an important problem in real-world problems, and graph neural networks (GNNs) have recently become popular as machine learning-based solvers. This paper presents an open-source benchmark suite for the NP-hard MAXIMUM INDEPENDENT SET problem, with both weighted and unweighted variants of the benchmark suite. The authors propose a guided tree search algorithm based on this benchmark suite, and show that a graph convolution network can be used as a guide for tree search to learn a solution structure from a set of random values. The algorithm is shown to improve the code quality and extensibility of the algorithm. The paper also shows that tree search with algorithmic techniques (e.g. graph kernelization) can be more efficient than classical algorithmic solvers, and that the tree search implementations can be competitive with existing solvers when the problem-specific solution structure is well defined. Finally, the authors show that the competitive solution quality of a GNN trained with reinforcement learning can be achieved when the solver is trained with the GNN. "
5135,SP:155ecd17d264a084b014abdfd0362146d8fb07e0,Quantization USED-FOR compressing Convolutional Neural Networks ( CNNs ). computational resources USED-FOR compressing Convolutional Neural Networks ( CNNs ). semantic segmentation CONJUNCTION depth prediction. depth prediction CONJUNCTION semantic segmentation. prediction accuracy EVALUATE-FOR networks. depth prediction HYPONYM-OF image - to - image tasks. semantic segmentation HYPONYM-OF image - to - image tasks. approach USED-FOR activation maps compression. activation maps compression USED-FOR 1 × 1 convolutions. 1 × 1 convolutions PART-OF CNNs. compression ratios CONJUNCTION computational savings. computational savings CONJUNCTION compression ratios. computational savings CONJUNCTION low bit quantization rates. low bit quantization rates CONJUNCTION computational savings. low bit quantization rates EVALUATE-FOR WCC. computational savings EVALUATE-FOR WCC. accuracy EVALUATE-FOR WCC. compression ratios EVALUATE-FOR WCC. hardware - friendly Haar - wavelet transform USED-FOR image compression. convolution PART-OF compressed activation map. 1× 1 convolution PART-OF network architecture. 1× 1 convolution USED-FOR WCC. network architecture USED-FOR WCC. WCC CONJUNCTION light quantization. light quantization CONJUNCTION WCC. Method is Convolutional Neural Networks ( CNNs ). OtherScientificTerm is quantization. Metric is compression rates. ,"Quantization is a popular technique for compressing Convolutional Neural Networks (CNNs) to save computational resources. However, quantization can be expensive and can be prohibitively expensive for image-to-image tasks such as semantic segmentation and depth prediction. This paper proposes a new approach to activation maps compression for 1 × 1 convolutions in CNNs. The authors show that the proposed approach, called WCC, can achieve better compression ratios, computational savings, and low bit quantization rates. WCC is based on the hardware-friendly Haar-wavelet transform for image compression, which can be applied to any network architecture that includes 1×1 convolution in the network architecture. The paper also shows that WCC can be combined with light quantization to achieve similar compression rates, while achieving comparable accuracy and lower compression ratios. ","Quantization is a popular technique for compressing Convolutional Neural Networks (CNNs) to save computational resources. However, quantization can be expensive and can be prohibitively expensive for image-to-image tasks such as semantic segmentation and depth prediction. This paper proposes a new approach to activation maps compression for 1 × 1 convolutions in CNNs. The authors show that the proposed approach, called WCC, can achieve better compression ratios, computational savings, and low bit quantization rates. WCC is based on the hardware-friendly Haar-wavelet transform for image compression, which can be applied to any network architecture that includes 1×1 convolution in the network architecture. The paper also shows that WCC can be combined with light quantization to achieve similar compression rates, while achieving comparable accuracy and lower compression ratios. "
5151,SP:004865e6affad32403b7965493a53c8a7ffdda0a,"accelerated learning dynamics USED-FOR correlated and coarse correlated equilibria. normal - form games FEATURE-OF correlated and coarse correlated equilibria. sequential and simultaneous moves CONJUNCTION imperfect information. imperfect information CONJUNCTION sequential and simultaneous moves. imperfect information FEATURE-OF extensive - form games. sequential and simultaneous moves FEATURE-OF extensive - form games. no - regret learning dynamics USED-FOR extensive - form correlated equilibrium ( EFCE ). O(T 3/4)-approximate EFCE FEATURE-OF correlated distribution of play. structured Markov chain USED-FOR refined perturbation analysis. refined perturbation analysis USED-FOR stability of certain fixed point strategies. Task is learning in games. Method are accelerated dynamics, and framework of -regret. OtherScientificTerm is prior rate. ","This paper considers the problem of learning in games with correlated and coarse correlated equilibria in normal-form games with accelerated learning dynamics. In particular, the authors consider the case where there are sequential and simultaneous moves and imperfect information, and extensive-forms games with sequential and concurrent moves. They show that the no-regret learning dynamics converges to an extensive-form correlated equilibrium (EFCE) with O(T 3/4)-approximate EFCE for a correlated distribution of play. They also provide a refined perturbation analysis based on a structured Markov chain to show the stability of certain fixed point strategies. The authors also provide an analysis of the prior rate of the accelerated dynamics, which is based on the framework of -regret.","This paper considers the problem of learning in games with correlated and coarse correlated equilibria in normal-form games with accelerated learning dynamics. In particular, the authors consider the case where there are sequential and simultaneous moves and imperfect information, and extensive-forms games with sequential and concurrent moves. They show that the no-regret learning dynamics converges to an extensive-form correlated equilibrium (EFCE) with O(T 3/4)-approximate EFCE for a correlated distribution of play. They also provide a refined perturbation analysis based on a structured Markov chain to show the stability of certain fixed point strategies. The authors also provide an analysis of the prior rate of the accelerated dynamics, which is based on the framework of -regret."
5167,SP:ee545ff83df4d7ff256ac61fbe0eb0765f52f1d5,"discrete actions COMPARE continuous actions. continuous actions COMPARE discrete actions. maximum of the action - value function USED-FOR dynamic programmingbased methods. Action Quantization from Demonstrations ( AQuaDem ) USED-FOR discretization of continuous action spaces. method USED-FOR discretization of continuous action spaces. priors of demonstrations USED-FOR discretization of continuous action spaces. discrete action deep RL algorithm USED-FOR continuous control problem. RL CONJUNCTION RL. RL CONJUNCTION RL. demonstrations CONJUNCTION RL. RL CONJUNCTION demonstrations. play data USED-FOR RL. Imitation Learning EVALUATE-FOR method. demonstrations USED-FOR RL. setups EVALUATE-FOR method. Imitation Learning HYPONYM-OF setups. RL HYPONYM-OF setups. RL HYPONYM-OF setups. human data COMPARE synthetic data. synthetic data COMPARE human data. human data USED-FOR setups. AQuaDem COMPARE continuous control methods. continuous control methods COMPARE AQuaDem. hard manipulation tasks EVALUATE-FOR continuous control methods. hard manipulation tasks EVALUATE-FOR AQuaDem. sample efficiency EVALUATE-FOR continuous control methods. sample efficiency EVALUATE-FOR AQuaDem. Task are Reinforcement Learning ( RL ), exploration problems, and exploration problem. OtherScientificTerm is action space. ","This paper proposes Action Quantization from Demonstrations (AQuaDem) to learn a discretization of continuous action spaces from the priors of demonstrations. Reinforcement Learning (RL) and RL with demonstrations are two of the most popular exploration problems in the RL literature. However, discrete actions are more expensive to learn than continuous actions, and the maximum of the action-value function can be prohibitively expensive for dynamic programmingbased methods. This paper proposes a discrete action deep RL algorithm to solve the continuous control problem. The exploration problem is formulated as an exploration problem, where the goal is to find the optimal action in the action space. The proposed method is evaluated on three different setups: (1) Imitation Learning, (2) RL, (3) RL with play data, and (4) RL using demonstrations. In all three setups, human data is used instead of synthetic data. The experiments show that AQuADem outperforms existing continuous control methods on hard manipulation tasks, and achieves better sample efficiency. ","This paper proposes Action Quantization from Demonstrations (AQuaDem) to learn a discretization of continuous action spaces from the priors of demonstrations. Reinforcement Learning (RL) and RL with demonstrations are two of the most popular exploration problems in the RL literature. However, discrete actions are more expensive to learn than continuous actions, and the maximum of the action-value function can be prohibitively expensive for dynamic programmingbased methods. This paper proposes a discrete action deep RL algorithm to solve the continuous control problem. The exploration problem is formulated as an exploration problem, where the goal is to find the optimal action in the action space. The proposed method is evaluated on three different setups: (1) Imitation Learning, (2) RL, (3) RL with play data, and (4) RL using demonstrations. In all three setups, human data is used instead of synthetic data. The experiments show that AQuADem outperforms existing continuous control methods on hard manipulation tasks, and achieves better sample efficiency. "
5183,SP:4b39279b98d6aa311bb49dd1384925f9d6f66c2d,domain generalization USED-FOR robust model. domain generalization USED-FOR semantic segmentation. labeled synthetic ( source ) data USED-FOR robust model. channelwise mean USED-FOR style features. adversarial style augmentation ( AdvStyle ) approach USED-FOR hard stylized images. style feature USED-FOR AdvStyle. adversarial training USED-FOR it. adversarial style feature USED-FOR adversarial image. adversarial image USED-FOR robust model training. AdvStyle USED-FOR models. synthetic - to - real semantic segmentation benchmarks EVALUATE-FOR AdvStyle. AdvStyle USED-FOR model. AdvStyle USED-FOR domain generalized image classification. datasets EVALUATE-FOR AdvStyle. OtherScientificTerm is image style variation. Task is overfitting. ,"This paper proposes a new adversarial style augmentation (AdvStyle) approach to improve domain generalization for semantic segmentation by improving the robust model performance on labeled synthetic (source) data with image style variation. AdvStyle augments hard stylized images with a style feature and uses channelwise mean to learn the style features. Then, it uses adversarial training to improve the robustness of the model trained on the adversarial image generated by the style feature. Experiments show that AdvStyle improves the robust performance of models trained on synthetic-to-real semantically segmentation benchmarks. In addition, AdvStyle is also applied to domain generalized image classification, where it improves the performance of a model trained with AdvStyle on two datasets. ","This paper proposes a new adversarial style augmentation (AdvStyle) approach to improve domain generalization for semantic segmentation by improving the robust model performance on labeled synthetic (source) data with image style variation. AdvStyle augments hard stylized images with a style feature and uses channelwise mean to learn the style features. Then, it uses adversarial training to improve the robustness of the model trained on the adversarial image generated by the style feature. Experiments show that AdvStyle improves the robust performance of models trained on synthetic-to-real semantically segmentation benchmarks. In addition, AdvStyle is also applied to domain generalized image classification, where it improves the performance of a model trained with AdvStyle on two datasets. "
5199,SP:4a2e6d70b383e4941e0bc44e7e82972b22e26792,"rigid, dramatic gestures USED-FOR recognition. rigid, dramatic gestures USED-FOR systems. neuromorphic gesture analysis system USED-FOR event - based gesture data. high temporal resolution FEATURE-OF neuromorphic gesture analysis system. high temporal resolution FEATURE-OF event - based gesture data. latent space representation USED-FOR similarity of mid - air gesture data. Dynamic Vision Sensor ( DVS ) USED-FOR event - based data. it USED-FOR sparse, noisy inputs. it USED-FOR interpretable latent space representation. interpretable latent space representation USED-FOR sparse, noisy inputs. DVSGesture dataset EVALUATE-FOR Hybrid GuidedVAE. classification accuracy EVALUATE-FOR Hybrid GuidedVAE. T - SNE plots USED-FOR interpretable latent space representation. neuromorphic hardware USED-FOR model. Method is mid - air gesture recognition systems. Generic are they, approach, and algorithm. ","This paper proposes a neuromorphic gesture analysis system with high temporal resolution for event-based gesture data from a Dynamic Vision Sensor (DVS) with rigid, dramatic gestures for recognition. The authors propose a latent space representation to capture the similarity of mid-air gesture data, which is a common problem in mid-range gesture recognition systems. The proposed approach, Hybrid GuidedVAE, is trained on the DVSGesture dataset and it learns an interpretable latent space space representation from T-SNE plots, and it is able to handle sparse, noisy inputs. The model is trained using neuromorphic hardware, and the authors show that the proposed algorithm achieves state-of-the-art classification accuracy. ","This paper proposes a neuromorphic gesture analysis system with high temporal resolution for event-based gesture data from a Dynamic Vision Sensor (DVS) with rigid, dramatic gestures for recognition. The authors propose a latent space representation to capture the similarity of mid-air gesture data, which is a common problem in mid-range gesture recognition systems. The proposed approach, Hybrid GuidedVAE, is trained on the DVSGesture dataset and it learns an interpretable latent space space representation from T-SNE plots, and it is able to handle sparse, noisy inputs. The model is trained using neuromorphic hardware, and the authors show that the proposed algorithm achieves state-of-the-art classification accuracy. "
5215,SP:2e66468a6b94177e54b0052b97713ee63902c278,"accuracy EVALUATE-FOR neuron - based networks. tabular data USED-FOR Deep learning. Internet of Things ( IoT ) CONJUNCTION drone. drone CONJUNCTION Internet of Things ( IoT ). drone CONJUNCTION Natural User Interface ( NUI ) application. Natural User Interface ( NUI ) application CONJUNCTION drone. annealing mechanism USED-FOR S - HTE inference. S - HTE USED-FOR internal representations. ferns USED-FOR S - HTE. classification and regression benchmark EVALUATE-FOR accuracy. OtherScientificTerm are computational capacity, deep learning capabilities, and neurons. Method are deep learning methods, and PyTorch implementation. Generic is it. Metric is computational complexity. ","Deep learning on tabular data has been a hot topic in recent years, but the computational capacity has been limited. This paper aims to improve the accuracy of neuron-based networks. The authors propose a new annealing mechanism for S-HTE inference based on ferns, and show that it improves the accuracy on a classification and regression benchmark. They also show that S-HETE can be applied to the Internet of Things (IoT), drone, and a Natural User Interface (NUI) application.    The paper is well-written and well-motivated, and the PyTorch implementation is intuitive and easy to follow. However, there are several issues with the paper: (1) deep learning capabilities are limited, (2) the paper does not compare with other deep learning methods, and (3) the authors do not provide a clear explanation of the computational complexity.  The authors also do not explain how S-ETE is able to learn the internal representations of neurons, and whether it can be used as a way to reduce the number of neurons.","Deep learning on tabular data has been a hot topic in recent years, but the computational capacity has been limited. This paper aims to improve the accuracy of neuron-based networks. The authors propose a new annealing mechanism for S-HTE inference based on ferns, and show that it improves the accuracy on a classification and regression benchmark. They also show that S-HETE can be applied to the Internet of Things (IoT), drone, and a Natural User Interface (NUI) application.    The paper is well-written and well-motivated, and the PyTorch implementation is intuitive and easy to follow. However, there are several issues with the paper: (1) deep learning capabilities are limited, (2) the paper does not compare with other deep learning methods, and (3) the authors do not provide a clear explanation of the computational complexity.  The authors also do not explain how S-ETE is able to learn the internal representations of neurons, and whether it can be used as a way to reduce the number of neurons."
5231,SP:b238db9252d83a13438bb747d70e635bb9945958,undirected stateonly experience USED-FOR learning value functions. tabular Q - learning USED-FOR value function. tabular Q - learning USED-FOR discrete Markov decision processes ( MDPs ). refinement of the action space FEATURE-OF value function. offline RL method USED-FOR value functions. Latent Action Q - learning HYPONYM-OF offline RL method. state - only experience USED-FOR value functions. Latent Action Q - learning ( LAQ ) USED-FOR value functions. discrete latent actions USED-FOR Q - learning. latent - variable future prediction model USED-FOR Q - learning. latent - variable future prediction model USED-FOR discrete latent actions. Q - learning USED-FOR Latent Action Q - learning ( LAQ ). Q - learning USED-FOR value functions. LAQ USED-FOR value functions. value functions CONJUNCTION value functions. value functions CONJUNCTION value functions. ground truth actions USED-FOR value functions. Value functions USED-FOR acquisition of goal - directed behavior. Value functions USED-FOR domain - specific low - level controllers. LAQ USED-FOR acquisition of goal - directed behavior. LAQ USED-FOR Value functions. imitation learning oracles CONJUNCTION competing methods. competing methods CONJUNCTION imitation learning oracles. alternatives CONJUNCTION imitation learning oracles. imitation learning oracles CONJUNCTION alternatives. 2D grid world CONJUNCTION 3D visual navigation. 3D visual navigation CONJUNCTION 2D grid world. LAQ COMPARE alternatives. alternatives COMPARE LAQ. environments EVALUATE-FOR LAQ. LAQ COMPARE competing methods. competing methods COMPARE LAQ. LAQ CONJUNCTION imitation learning oracles. imitation learning oracles CONJUNCTION LAQ. 3D visual navigation HYPONYM-OF environments. 2D grid world HYPONYM-OF environments. ,"This paper proposes a new offline RL method called Latent Action Q-learning (LAQ) for learning value functions from undirected stateonly experience. The paper proposes tabular Q-learning to learn a value function from discrete Markov decision processes (MDPs) using a refinement of the action space. The main idea of LAQ is to use a latent-variable future prediction model to learn discrete latent actions, and then to use Q - learning to learn value functions based on discrete actions.    The paper shows that LAQ learns value functions using state-only experience, and that the value functions can be learned from ground truth actions. Value functions are learned for domain-specific low-level controllers, and LAQ can be used for the acquisition of goal-directed behavior.  In experiments on two environments, a 2D grid world and 3D visual navigation, LAQ outperforms existing imitation learning oracles and competing methods. ","This paper proposes a new offline RL method called Latent Action Q-learning (LAQ) for learning value functions from undirected stateonly experience. The paper proposes tabular Q-learning to learn a value function from discrete Markov decision processes (MDPs) using a refinement of the action space. The main idea of LAQ is to use a latent-variable future prediction model to learn discrete latent actions, and then to use Q - learning to learn value functions based on discrete actions.    The paper shows that LAQ learns value functions using state-only experience, and that the value functions can be learned from ground truth actions. Value functions are learned for domain-specific low-level controllers, and LAQ can be used for the acquisition of goal-directed behavior.  In experiments on two environments, a 2D grid world and 3D visual navigation, LAQ outperforms existing imitation learning oracles and competing methods. "
5247,SP:108ebe9045a9e2b8b5aba8352733782462db8a81,"large models USED-FOR deep learning applications. low - latency and high - bandwidth interconnect USED-FOR distributed training algorithms. distributed training algorithms USED-FOR models. GPU clusters USED-FOR models. model parallelism USED-FOR large models. SWARM Parallelism1 HYPONYM-OF model - parallel training algorithm. model - parallel training algorithm USED-FOR swarms of poorly connected, heterogeneous unreliable devices. SWARM USED-FOR temporary randomized pipelines. compression - aware architecture modifications USED-FOR approach. network throughput FEATURE-OF swarm of preemptible T4 GPUs. shared parameters USED-FOR large Transformer language model. swarm of preemptible T4 GPUs USED-FOR large Transformer language model. OtherScientificTerm are dedicated GPU clusters, distributed training setups, and preemptible ” instances. Generic is setups. ","This paper proposes a new model parallelism method for large-scale training of deep learning models. The proposed method, called SWARM Parallelism1, is an extension of the existing SWARM algorithm, which is used to train a model on a large number of GPUs in a distributed fashion. The authors show that the proposed method is able to achieve competitive performance on a variety of tasks, and that it can be applied to a wide range of hardware architectures.   ","This paper proposes a new model parallelism method for large-scale training of deep learning models. The proposed method, called SWARM Parallelism1, is an extension of the existing SWARM algorithm, which is used to train a model on a large number of GPUs in a distributed fashion. The authors show that the proposed method is able to achieve competitive performance on a variety of tasks, and that it can be applied to a wide range of hardware architectures.   "
5263,SP:91d2f094d5481651b554f58aecc2a6207057a47c,"Offline reinforcement learning USED-FOR policies. fixed dataset USED-FOR real - world applications. fixed dataset USED-FOR Offline reinforcement learning. fixed dataset USED-FOR policies. behavior policy COMPARE policy. policy COMPARE behavior policy. transition dynamics PART-OF offline experiences. offline training CONJUNCTION online tuning. online tuning CONJUNCTION offline training. online data USED-FOR agent policy. deployment efficiency CONJUNCTION sample efficiency. sample efficiency CONJUNCTION deployment efficiency. online transition correction ( OTC ) USED-FOR biased transition dynamics. offline and online experiences USED-FOR online transition correction ( OTC ). sampling probabilities USED-FOR online transition correction ( OTC ). distances USED-FOR similarity between transitions. transition similarity USED-FOR adaptive rank - based prioritization. embedding - based and valuebased distance HYPONYM-OF distances. OTC USED-FOR agent policies. OTC USED-FOR online tuning. agent policies USED-FOR online tuning. data efficiency EVALUATE-FOR OTC. OTC COMPARE baselines. baselines COMPARE OTC. tasks EVALUATE-FOR baselines. tasks EVALUATE-FOR OTC. Task is offline decentralized multi - agent reinforcement learning. OtherScientificTerm are online execution, value estimates, uncoordinated and suboptimal policies, and transition bias. Material is online experiences. "," Offline reinforcement learning with policies on a fixed dataset is an important problem in real-world applications, but offline reinforcement learning in decentralized multi-agent reinforcement learning is challenging due to the lack of online execution and the high cost of offline experiences. The authors propose online transition correction (OTC) to address the issue of biased transition dynamics in offline experiences, where the behavior policy is more likely to be biased than the policy learned from online experiences. To address this issue, the authors propose an adaptive rank-based prioritization based on the transition dynamics of the offline and online experiences, which uses the sampling probabilities of the online and offline experiences to compute the transition similarity between transitions. They also propose two new distances, the embedding-based and valuebased distance, to measure the similarity between transition dynamics and value estimates. The paper shows that OTC improves deployment efficiency and sample efficiency in offline training and online tuning. OTC is shown to improve the performance of agent policies trained with OTC in online tuning, and to improve data efficiency in the presence of uncoordinated and suboptimal policies. The transition bias is also alleviated by the use of adaptive rank based prioritization. "," Offline reinforcement learning with policies on a fixed dataset is an important problem in real-world applications, but offline reinforcement learning in decentralized multi-agent reinforcement learning is challenging due to the lack of online execution and the high cost of offline experiences. The authors propose online transition correction (OTC) to address the issue of biased transition dynamics in offline experiences, where the behavior policy is more likely to be biased than the policy learned from online experiences. To address this issue, the authors propose an adaptive rank-based prioritization based on the transition dynamics of the offline and online experiences, which uses the sampling probabilities of the online and offline experiences to compute the transition similarity between transitions. They also propose two new distances, the embedding-based and valuebased distance, to measure the similarity between transition dynamics and value estimates. The paper shows that OTC improves deployment efficiency and sample efficiency in offline training and online tuning. OTC is shown to improve the performance of agent policies trained with OTC in online tuning, and to improve data efficiency in the presence of uncoordinated and suboptimal policies. The transition bias is also alleviated by the use of adaptive rank based prioritization. "
5279,SP:d0e650d568214481b07a0452ec606ccbf6d05410,"computational footprint EVALUATE-FOR Deep Neural Networks ( DNNs ) training. 4 - bit quantization USED-FOR methods. computational footprint EVALUATE-FOR training process. loss gradients HYPONYM-OF neural gradients. unbiased quantization USED-FOR quantized neural network training. logarithmic unbiased quantization ( LUQ ) method USED-FOR forward and backward phase. high precision fine - tuning CONJUNCTION variance reduction method. variance reduction method CONJUNCTION high precision fine - tuning. ImageNet FEATURE-OF ResNet50. method USED-FOR low precision format. OtherScientificTerm are intermediate neural layers, multiplications, and multiplier. Generic is it. Method is 4 - bit training. ","This paper studies the computational footprint of Deep Neural Networks (DNNs) training with 4-bit quantization. The authors propose two methods for quantizing intermediate neural layers. The first method, called unbiased quantization (LUQ), aims to reduce the computational cost of the training process by reducing the number of multiplications in the forward and backward phase of training. The second method, dubbed unbiased quantized neural network training (UNQ), is based on the logarithmic biased quantization of neural gradients (i.e., the loss gradients of the weights of the intermediate layers). The authors show empirically that the proposed method can be applied to any low precision format and that it can be combined with existing methods for high precision fine-tuning and variance reduction method. Experiments on ResNet50 on ImageNet show that the method is able to achieve state-of-the-art performance on the low-precision format.    The authors also show that UNQ can also be used for 4- bit training. ","This paper studies the computational footprint of Deep Neural Networks (DNNs) training with 4-bit quantization. The authors propose two methods for quantizing intermediate neural layers. The first method, called unbiased quantization (LUQ), aims to reduce the computational cost of the training process by reducing the number of multiplications in the forward and backward phase of training. The second method, dubbed unbiased quantized neural network training (UNQ), is based on the logarithmic biased quantization of neural gradients (i.e., the loss gradients of the weights of the intermediate layers). The authors show empirically that the proposed method can be applied to any low precision format and that it can be combined with existing methods for high precision fine-tuning and variance reduction method. Experiments on ResNet50 on ImageNet show that the method is able to achieve state-of-the-art performance on the low-precision format.    The authors also show that UNQ can also be used for 4- bit training. "
5295,SP:f2862d1f987164ed6c3c375cd8962e57c369373b,"Polythetic classifications COMPARE monothetic classifications. monothetic classifications COMPARE Polythetic classifications. features USED-FOR monothetic classifications. shared patterns of features USED-FOR Polythetic classifications. threshold meta - learners USED-FOR functions. embedding dimension USED-FOR functions. embedding dimension USED-FOR threshold meta - learners. Prototypical Networks HYPONYM-OF threshold meta - learners. attentional classifiers USED-FOR problems. linear embedding dimension USED-FOR attentional classifiers. Matching Networks HYPONYM-OF attentional classifiers. linear embedding dimension USED-FOR problems. attentional models USED-FOR misclassification. selfattention feature - selection mechanism USED-FOR non - discriminative features. approach USED-FOR meta - learning Boolean functions. Material is natural world. OtherScientificTerm are task - relevant features, and task - irrelevant features. Task is meta - learning problems. ","Polythetic classifications based on shared patterns of features are more robust to misclassification than monothetic classifications that rely on features that are not relevant to the task at hand. Polythetic classification is a special case of meta-learning problems where a set of functions are learned by threshold meta-learners, such as Prototypical Networks.    The paper proposes to use the embedding dimension of these functions as a proxy for the number of features in the natural world.  The main idea is to use attentional classifiers (e.g., Matching Networks) to solve these problems with linear embedding dimensions.  This approach is shown to be effective for meta-training Boolean functions.  In addition, the paper shows that attentional models can be used to address the problem of ""misclassification"", i.e., the problem that happens when task-relevant features are not shared across tasks, but task-irrelevant features can be shared across all tasks.  A selfattention feature-selection mechanism is used to select non-discriminative features, which are more likely to be useful for the task.","Polythetic classifications based on shared patterns of features are more robust to misclassification than monothetic classifications that rely on features that are not relevant to the task at hand. Polythetic classification is a special case of meta-learning problems where a set of functions are learned by threshold meta-learners, such as Prototypical Networks.    The paper proposes to use the embedding dimension of these functions as a proxy for the number of features in the natural world.  The main idea is to use attentional classifiers (e.g., Matching Networks) to solve these problems with linear embedding dimensions.  This approach is shown to be effective for meta-training Boolean functions.  In addition, the paper shows that attentional models can be used to address the problem of ""misclassification"", i.e., the problem that happens when task-relevant features are not shared across tasks, but task-irrelevant features can be shared across all tasks.  A selfattention feature-selection mechanism is used to select non-discriminative features, which are more likely to be useful for the task."
5311,SP:e1e513fef25d29e17cdadd1b36d932a8ad8897cd,"multi - agent reinforcement learning USED-FOR emergent communication. continuous acoustic channel USED-FOR Human communication. reinforcement learning USED-FOR continuous communication channel. continuous communication channel USED-FOR emergent language. channel characteristics USED-FOR emerging language. vocoder USED-FOR continuous waveform. noise FEATURE-OF communication channel. continuous signalling USED-FOR language learning. platform USED-FOR continuous signalling. deep reinforcement learning USED-FOR platform. OtherScientificTerm are discrete symbols, lossy continuous channel, continuous signal, and concept combinations. Method are environment and training methodology, and deep Q - learning. Material is messaging environment. ","This paper proposes a multi-agent reinforcement learning for emergent communication via reinforcement learning on a continuous acoustic channel. Human communication is typically done on a discrete acoustic channel with discrete symbols, but this paper proposes to use reinforcement learning to learn a continuous communication channel to learn emergent language. The paper proposes an environment and training methodology where the communication channel is a lossy continuous channel, and each agent is trained to communicate with the other agents in a messaging environment. The emerging language is learned based on channel characteristics, e.g., the number of channels, the amount of noise, and how the emerging language changes when the communication signal changes.   The paper also proposes a vocoder that generates a continuous waveform from the input signal, and a platform for continuous signalling for language learning based on deep reinforcement learning. The platform is trained in a similar way as deep Q-learning, but the communication is done in a continuous signal, with the difference being that the vocoder is trained on the entire communication channel rather than just a small subset of it. The communication channel has to be able to handle noise, which can be controlled through the use of a communication channel that has been tuned to a certain level of noise. The authors also propose a way to train the platform in a way that allows for continuous signals to be more robust to changes in the noise in the communication environment.  The authors show that the platform is able to learn to communicate in a language that is robust to noise in a communication environment, and that it can be trained in an unsupervised way. They also show that their platform can learn concept combinations that are useful for communication.","This paper proposes a multi-agent reinforcement learning for emergent communication via reinforcement learning on a continuous acoustic channel. Human communication is typically done on a discrete acoustic channel with discrete symbols, but this paper proposes to use reinforcement learning to learn a continuous communication channel to learn emergent language. The paper proposes an environment and training methodology where the communication channel is a lossy continuous channel, and each agent is trained to communicate with the other agents in a messaging environment. The emerging language is learned based on channel characteristics, e.g., the number of channels, the amount of noise, and how the emerging language changes when the communication signal changes.   The paper also proposes a vocoder that generates a continuous waveform from the input signal, and a platform for continuous signalling for language learning based on deep reinforcement learning. The platform is trained in a similar way as deep Q-learning, but the communication is done in a continuous signal, with the difference being that the vocoder is trained on the entire communication channel rather than just a small subset of it. The communication channel has to be able to handle noise, which can be controlled through the use of a communication channel that has been tuned to a certain level of noise. The authors also propose a way to train the platform in a way that allows for continuous signals to be more robust to changes in the noise in the communication environment.  The authors show that the platform is able to learn to communicate in a language that is robust to noise in a communication environment, and that it can be trained in an unsupervised way. They also show that their platform can learn concept combinations that are useful for communication."
5327,SP:0e6ff65ba4a3df35947d1b6f4d438612088d90a0,"backdoor attacks FEATURE-OF NLP models. NLP backdoor attacks USED-FOR tasks. NLP models CONJUNCTION tasks. tasks CONJUNCTION NLP models. attacks USED-FOR NLP models. attacks USED-FOR tasks. BadPre HYPONYM-OF task - agnostic backdoor attack. task - agnostic backdoor attack USED-FOR pre - trained NLP models. backdoor USED-FOR pre - trained model. backdoor USED-FOR downstream models. transfer learning process USED-FOR downstream models. approach USED-FOR downstream NLP tasks. Task is downstream language tasks. Method is language models. OtherScientificTerm are model misprediction, and prior information. Generic are attack, malicious model, and strategy. ","This paper studies the problem of backdoor attacks on NLP models. The authors propose a new backdoor attack, called BadPre, that can be applied to any pre-trained NLP model. They show that BadPre can be used to attack any language models on any downstream language tasks. They also show that NLP backdoor attacks are transferrable to other tasks. ","This paper studies the problem of backdoor attacks on NLP models. The authors propose a new backdoor attack, called BadPre, that can be applied to any pre-trained NLP model. They show that BadPre can be used to attack any language models on any downstream language tasks. They also show that NLP backdoor attacks are transferrable to other tasks. "
5343,SP:58d3ecb4a1906251e79ad883aa97cc2502642658,"skill pre - training methods COMPARE RL techniques. RL techniques COMPARE skill pre - training methods. skill learning USED-FOR evolving or expanding environment. evolving environment USED-FOR skill discovery. framework USED-FOR skill discovery. incremental skills COMPARE skill discovery methods. skill discovery methods COMPARE incremental skills. evolving and static environments EVALUATE-FOR incremental skills. skill quality EVALUATE-FOR skill discovery methods. skill quality EVALUATE-FOR incremental skills. Task are Reward - free, unsupervised discovery of skills, hand - designing rewards, task supervision, and discovery - of - incremental - skill. OtherScientificTerm are stationary environments, agent dynamics, and learned skills. Generic are methods, and them. ","This paper studies the problem of reward-free, unsupervised discovery of skills in an evolving or expanding environment. Reward-free is defined as a setting where the agent does not have access to hand-designing rewards, and the goal is to learn a set of skills that can be used to solve a new task. The paper shows that skill pre-training methods are superior to existing RL techniques in this setting, and that skill discovery in this evolving environment is similar to that in stationary environments.   The paper proposes a framework for skill discovery that is based on the observation that skill learning can be applied to either a single or an evolving/expanding environment. The authors show that in the evolving environment, skill discovery can be performed without any task supervision. They also show that the agent dynamics of the agent is able to adapt to the environment and learn new learned skills.  They show that incremental skills learned in both evolving and static environments outperform state-of-the-art skill discovery methods in terms of skill quality, and they show that their methods are more robust to changes in the environment dynamics.  The authors also provide a theoretical analysis of their framework, showing that their method is more robust than existing methods and that it is more stable than them.  Finally, the authors provide some empirical evidence that their framework is more resilient to the change in environment dynamics and that the learned skills are more transferable to new tasks. ","This paper studies the problem of reward-free, unsupervised discovery of skills in an evolving or expanding environment. Reward-free is defined as a setting where the agent does not have access to hand-designing rewards, and the goal is to learn a set of skills that can be used to solve a new task. The paper shows that skill pre-training methods are superior to existing RL techniques in this setting, and that skill discovery in this evolving environment is similar to that in stationary environments.   The paper proposes a framework for skill discovery that is based on the observation that skill learning can be applied to either a single or an evolving/expanding environment. The authors show that in the evolving environment, skill discovery can be performed without any task supervision. They also show that the agent dynamics of the agent is able to adapt to the environment and learn new learned skills.  They show that incremental skills learned in both evolving and static environments outperform state-of-the-art skill discovery methods in terms of skill quality, and they show that their methods are more robust to changes in the environment dynamics.  The authors also provide a theoretical analysis of their framework, showing that their method is more robust than existing methods and that it is more stable than them.  Finally, the authors provide some empirical evidence that their framework is more resilient to the change in environment dynamics and that the learned skills are more transferable to new tasks. "
5359,SP:2c6595408f5ec95537eaf555e5fe3d992b58c222,"Convolutional neural networks USED-FOR features. regular quadrilateral convolution kernels USED-FOR features. regular quadrilateral convolution kernels USED-FOR Convolutional neural networks. small convolution kernels USED-FOR models. relative directions CONJUNCTION logarithmic distances. logarithmic distances CONJUNCTION relative directions. LPSC USED-FOR local spatial structures. LPSC USED-FOR single - layer receptive field. LPSC USED-FOR network architecture. convolutions PART-OF network architecture. convolution USED-FOR LPSC. log - polar space pooling USED-FOR convolution. log - polar space pooling USED-FOR LPSC. tasks EVALUATE-FOR LPSC. OtherScientificTerm are convolution kernel, small local receptive fields, and local receptive field. ","Convolutional neural networks with regular quadrilateral convolution kernels are able to learn features that are similar to the features of small convolution kernel, but with small local receptive fields. The authors show that models with small convolutions kernels can learn features similar to those of models with large convolutions. They show that LPSC is able to capture local spatial structures such as relative directions, logarithmic distances, etc.   The authors also show that a single-layer receptive field can be learned with LPSc, and that the network architecture can be decomposed into a set of convolutions that share the same local receptive field. They further show that the convolution can be trained with a convolution with log-polar space pooling, which is a simple modification of the convolutions in the original network architecture.  The paper is well-written and well-motivated, and the experiments on a variety of tasks demonstrate the effectiveness of the proposed method. ","Convolutional neural networks with regular quadrilateral convolution kernels are able to learn features that are similar to the features of small convolution kernel, but with small local receptive fields. The authors show that models with small convolutions kernels can learn features similar to those of models with large convolutions. They show that LPSC is able to capture local spatial structures such as relative directions, logarithmic distances, etc.   The authors also show that a single-layer receptive field can be learned with LPSc, and that the network architecture can be decomposed into a set of convolutions that share the same local receptive field. They further show that the convolution can be trained with a convolution with log-polar space pooling, which is a simple modification of the convolutions in the original network architecture.  The paper is well-written and well-motivated, and the experiments on a variety of tasks demonstrate the effectiveness of the proposed method. "
5375,SP:7791f96b1eef277a9133975507a750d9e7c6b8ff,"non - vacuous bounds USED-FOR NNs. PAC - Bayes theorem USED-FOR NNs generalization. IIW ’s property USED-FOR deep learning. algorithm USED-FOR approximation of IIW. information complexity EVALUATE-FOR NNs. accuracy CONJUNCTION information complexity. information complexity CONJUNCTION accuracy. accuracy EVALUATE-FOR NNs. PIB HYPONYM-OF NNs. IIW compression CONJUNCTION generalization. generalization CONJUNCTION IIW compression. IIW USED-FOR NNs. overparameterization CONJUNCTION noisy labels. noisy labels CONJUNCTION overparameterization. varying batch sizes CONJUNCTION overparameterization. overparameterization CONJUNCTION varying batch sizes. IIW USED-FOR NNs. MCMC - based algorithm USED-FOR optimal weight posterior. PIB FEATURE-OF optimal weight posterior. Task are ML research, and compressing phase transition. Method are IIW - based information bottleneck, and NNs ’ training. ","This paper provides non-vacuous bounds for the generalization performance of NNs under the PAC-Bayes theorem. In particular, the authors show that under the IIW-based information bottleneck, NNs’ training converges to a stationary point. The authors also provide an algorithm for the approximation of IIW.   The paper is well-written and well-motivated, and the results are interesting. However, there are a few issues that need to be addressed in ML research. First, IIW’s property in deep learning is not well-studied. Second, there is no connection between IIW compression and generalization. Third, the information complexity of the NNs (e.g., PIB) is not known. Finally, the paper does not consider the effect of varying batch sizes, overparameterization, and noisy labels.  The authors propose a MCMC-based algorithm to approximate the optimal weight posterior of the PIB, and show that compressing phase transition is the best way to do so.","This paper provides non-vacuous bounds for the generalization performance of NNs under the PAC-Bayes theorem. In particular, the authors show that under the IIW-based information bottleneck, NNs’ training converges to a stationary point. The authors also provide an algorithm for the approximation of IIW.   The paper is well-written and well-motivated, and the results are interesting. However, there are a few issues that need to be addressed in ML research. First, IIW’s property in deep learning is not well-studied. Second, there is no connection between IIW compression and generalization. Third, the information complexity of the NNs (e.g., PIB) is not known. Finally, the paper does not consider the effect of varying batch sizes, overparameterization, and noisy labels.  The authors propose a MCMC-based algorithm to approximate the optimal weight posterior of the PIB, and show that compressing phase transition is the best way to do so."
5391,SP:a733847ade77ffbf38760fc79da17893dea8d53f,"perturbations USED-FOR attacks. linear separable FEATURE-OF perturbations. linear separability USED-FOR attacks. synthetic perturbations COMPARE deliberately crafted attacks. deliberately crafted attacks COMPARE synthetic perturbations. linear separable data USED-FOR perturbations. imperceptible scale FEATURE-OF they. shortcuts USED-FOR deep models. Method are Indiscriminate data poisoning attacks, and pre - trained feature extractors. OtherScientificTerm are imperceptible perturbations, and normal features. Task is shortcut learning problem. ","Indiscriminate data poisoning attacks are imperceptible perturbations that can be used to fool pre-trained feature extractors. These attacks are motivated by the fact that the perturbation is linear separable, and that they can be applied on an imperceptibly scale. This paper studies the shortcut learning problem, and shows that these attacks can be made with linear separability. The paper also shows that the synthetic perturbed data is more powerful than the deliberately crafted attacks.    The main contribution of this paper is that the authors show that shortcuts to deep models can be learned in a way that is imperceptable to the attacker. The authors also show that the imperceptibility of these perturbative attacks is not due to the normal features of the model, but rather that they are caused by the use of a shortcut learning algorithm.","Indiscriminate data poisoning attacks are imperceptible perturbations that can be used to fool pre-trained feature extractors. These attacks are motivated by the fact that the perturbation is linear separable, and that they can be applied on an imperceptibly scale. This paper studies the shortcut learning problem, and shows that these attacks can be made with linear separability. The paper also shows that the synthetic perturbed data is more powerful than the deliberately crafted attacks.    The main contribution of this paper is that the authors show that shortcuts to deep models can be learned in a way that is imperceptable to the attacker. The authors also show that the imperceptibility of these perturbative attacks is not due to the normal features of the model, but rather that they are caused by the use of a shortcut learning algorithm."
5407,SP:7b50be406138ad01db3ee112899f622637896fe9,Offline policy optimization USED-FOR real - world decisionmaking problems. estimator USED-FOR offline policy evaluation. function approximations USED-FOR value functions. Importance sampling HYPONYM-OF estimator. value functions CONJUNCTION process models. process models CONJUNCTION value functions. function approximations USED-FOR process models. Importance sampling USED-FOR offline policy evaluation. algorithm USED-FOR overfitting. overfitting phenomenon FEATURE-OF importance weighted return. per - state - neighborhood normalization condition USED-FOR algorithm. healthcare - inspired simulator CONJUNCTION logged dataset. logged dataset CONJUNCTION healthcare - inspired simulator. healthcare - inspired simulator EVALUATE-FOR method. logged dataset EVALUATE-FOR method. method COMPARE batch reinforcement learning algorithms. batch reinforcement learning algorithms COMPARE method. overfitting EVALUATE-FOR method. Task is online learning. Generic is approach. ,"Offline policy optimization is an important problem in real-world decisionmaking problems. In this paper, the authors propose a new estimator, Importance sampling, for offline policy evaluation based on function approximations for value functions and process models. The authors also propose an algorithm that alleviates the overfitting phenomenon of the importance weighted return under the per-state-neighborhood normalization condition. The proposed method is evaluated on a healthcare-inspired simulator and a logged dataset, and compared with other batch reinforcement learning algorithms. The results show that the proposed method outperforms the baselines in terms of overfitting.    The paper is well-written, well-motivated, and easy to follow. However, there are a few issues with the presentation and presentation of the paper that prevent me from recommending the paper be accepted as a must-read for online learning. I have a few questions about the details of the approach and the experimental results. ","Offline policy optimization is an important problem in real-world decisionmaking problems. In this paper, the authors propose a new estimator, Importance sampling, for offline policy evaluation based on function approximations for value functions and process models. The authors also propose an algorithm that alleviates the overfitting phenomenon of the importance weighted return under the per-state-neighborhood normalization condition. The proposed method is evaluated on a healthcare-inspired simulator and a logged dataset, and compared with other batch reinforcement learning algorithms. The results show that the proposed method outperforms the baselines in terms of overfitting.    The paper is well-written, well-motivated, and easy to follow. However, there are a few issues with the presentation and presentation of the paper that prevent me from recommending the paper be accepted as a must-read for online learning. I have a few questions about the details of the approach and the experimental results. "
5423,SP:c976752a55b9ff47dc63c95a9fd7b51a81e8a42e,model USED-FOR continual learning. CoLLIE HYPONYM-OF model. CoLLIE USED-FOR continual learning. transformation function USED-FOR language embeddings. language CONJUNCTION images. images CONJUNCTION language. transformation function USED-FOR new language use. transformation function USED-FOR CoLLIE. semantic space FEATURE-OF images. few - shot learning COMPARE model. model COMPARE few - shot learning. it COMPARE model. model COMPARE it. zero - shot EVALUATE-FOR model. continual learning EVALUATE-FOR model. Material is vision. Method is multimodal embedding model. OtherScientificTerm is similar language use. ,"This paper proposes a new model called CoLLIE for continual learning, which is a model that learns a multimodal embedding model across multiple languages and images. The key idea is to use a transformation function to learn language embeddings for both language and images, and then use this transformation function for new language use. The authors show that this model can be used for few-shot learning as well as continual learning. They also show that the images in the semantic space can be transformed into similar language use, and that the model is able to learn from zero-shot to continual learning better than a previous model. ","This paper proposes a new model called CoLLIE for continual learning, which is a model that learns a multimodal embedding model across multiple languages and images. The key idea is to use a transformation function to learn language embeddings for both language and images, and then use this transformation function for new language use. The authors show that this model can be used for few-shot learning as well as continual learning. They also show that the images in the semantic space can be transformed into similar language use, and that the model is able to learn from zero-shot to continual learning better than a previous model. "
5439,SP:d3371b322acfc321ee79a2e1b438d82644872fa4,"captioning models USED-FOR visual data. fluency CONJUNCTION fidelity. fidelity CONJUNCTION fluency. fidelity CONJUNCTION adequacy. adequacy CONJUNCTION fidelity. Fidelity CONJUNCTION Fluency. Fluency CONJUNCTION Fidelity. Visual - Linguistic Adequacy CONJUNCTION Fidelity. Fidelity CONJUNCTION Visual - Linguistic Adequacy. VLAF2 USED-FOR Visual - Linguistic Adequacy. VLAF2 USED-FOR Fidelity. VLAF2 USED-FOR Fluency. linguistics USED-FOR Fluency. linguistics USED-FOR VLAF2. BERT CONJUNCTION CLIP. CLIP CONJUNCTION BERT. intrinsic language knowledge USED-FOR models. nocaps dataset EVALUATE-FOR framework. method COMPARE captioning models. captioning models COMPARE method. method COMPARE SPICE scores of human baseline. SPICE scores of human baseline COMPARE method. caption evaluation metrics EVALUATE-FOR captioning models. caption evaluation metrics EVALUATE-FOR method. fluency CONJUNCTION fidelity. fidelity CONJUNCTION fluency. fidelity CONJUNCTION adequacy. adequacy CONJUNCTION fidelity. model USED-FOR object captions. quantitative and qualitative analysis EVALUATE-FOR model. fluency EVALUATE-FOR model. adequacy EVALUATE-FOR model. fidelity EVALUATE-FOR model. Method are object captioning ( NOC ), image captioning models, object captioning models, and visual / language models. OtherScientificTerm is caption annotations. ","This paper proposes a new benchmark for object captioning (NOC), which is based on the observation that existing image captioning models (e.g., BERT and CLIP) are not general enough to generalize well to visual data. The authors propose VLAF2, which combines Visual-Linguistic Adequacy, Fidelity, Fluency, and Fluency with Linguistic Linguistics. The proposed framework is evaluated on the nocaps dataset, and the authors show that models trained with intrinsic language knowledge can generalize better than models trained without any caption annotations. They also show that the proposed method outperforms the SPICE scores of human baseline on a variety of caption evaluation metrics, and that their method is more robust to changes in fluency, fidelity, and fidelity to object captions.   The authors also provide a quantitative and qualitative analysis that shows that their model performs better in terms of fluency and fidelity in the presence of changes to the caption annotations, and better in the absence of changes in the captioning annotations. The paper also shows that the model is able to learn object captionings that generalize to unseen objects in the scene, which is a significant contribution to the current state of the art in the field.  The paper is well-written and well-motivated, and it is clear that the authors have done a lot of work to improve the state-of-the-art visual/language models.","This paper proposes a new benchmark for object captioning (NOC), which is based on the observation that existing image captioning models (e.g., BERT and CLIP) are not general enough to generalize well to visual data. The authors propose VLAF2, which combines Visual-Linguistic Adequacy, Fidelity, Fluency, and Fluency with Linguistic Linguistics. The proposed framework is evaluated on the nocaps dataset, and the authors show that models trained with intrinsic language knowledge can generalize better than models trained without any caption annotations. They also show that the proposed method outperforms the SPICE scores of human baseline on a variety of caption evaluation metrics, and that their method is more robust to changes in fluency, fidelity, and fidelity to object captions.   The authors also provide a quantitative and qualitative analysis that shows that their model performs better in terms of fluency and fidelity in the presence of changes to the caption annotations, and better in the absence of changes in the captioning annotations. The paper also shows that the model is able to learn object captionings that generalize to unseen objects in the scene, which is a significant contribution to the current state of the art in the field.  The paper is well-written and well-motivated, and it is clear that the authors have done a lot of work to improve the state-of-the-art visual/language models."
5455,SP:9f3b6486662d80350d77a4b060d4a5b8b22a6130,"foundation models USED-FOR representations. representations USED-FOR classification. special - purpose algorithms USED-FOR problems. classifier USED-FOR representations. special - purpose algorithms USED-FOR representations. clustering property FEATURE-OF features. neural collapse HYPONYM-OF clustering property. overparameterized classification networks USED-FOR features. foundation models USED-FOR feature maps. foundation models USED-FOR transfer learning. feature maps USED-FOR transfer learning. Task are few - shot learning problems, and few - shot setting. ","This paper studies few-shot learning problems, where the goal is to learn representations that are transferable to a new task. The authors propose two special-purpose algorithms to solve these problems. First, the authors show that the representations learned by a classifier can be transferred to new tasks. Second, they show that a clustering property (i.e., neural collapse) of the features learned by overparameterized classification networks can be used as a proxy for the transferability of the learned representations to a different task.  The authors also show that transfer learning can be performed using the learned feature maps from foundation models that are trained to learn the representations for classification.   ","This paper studies few-shot learning problems, where the goal is to learn representations that are transferable to a new task. The authors propose two special-purpose algorithms to solve these problems. First, the authors show that the representations learned by a classifier can be transferred to new tasks. Second, they show that a clustering property (i.e., neural collapse) of the features learned by overparameterized classification networks can be used as a proxy for the transferability of the learned representations to a different task.  The authors also show that transfer learning can be performed using the learned feature maps from foundation models that are trained to learn the representations for classification.   "
5471,SP:624c95d9ce1ee4b66274e858e2da22bef6b052c7,"3D scanning USED-FOR Point cloud. tasks USED-FOR point cloud reconstruction. 3D sparse stacked - hourglass network USED-FOR densification and denoising. 3D sparse stacked - hourglass network HYPONYM-OF stages. refinement HYPONYM-OF stages. stages PART-OF deep point cloud reconstruction network. 3D sparse stacked - hourglass network PART-OF deep point cloud reconstruction network. transformers USED-FOR refinement. refinement PART-OF deep point cloud reconstruction network. amplified positional encoding HYPONYM-OF module. module USED-FOR transformer. points ’ distances USED-FOR adaptive refinements. module USED-FOR positional encoding vectors. points ’ distances USED-FOR module. points ’ distances USED-FOR positional encoding vectors. ScanNet CONJUNCTION ICL - NUIM. ICL - NUIM CONJUNCTION ScanNet. ICL - NUIM CONJUNCTION ShapeNetPart datasets. ShapeNetPart datasets CONJUNCTION ICL - NUIM. ICL - NUIM EVALUATE-FOR network. ShapeNetPart datasets EVALUATE-FOR network. ScanNet EVALUATE-FOR network. network USED-FOR real - world and unmet scenes. OtherScientificTerm are discrete voxels, and 3D points. ","Point cloud reconstruction is an important problem in 3D scanning. This paper proposes two tasks for point cloud reconstruction: densification and denoising. Point cloud can be represented as discrete voxels, and densification can be modeled as densities of 3D points. The authors propose a deep pointcloud reconstruction network that consists of two stages: 1) a 3D sparse stacked-hourglass network for densification, and 2) a refinement network that uses transformers for refinement. They also introduce a module called amplified positional encoding, which is a module that can be applied to any transformer. The module is used to learn positional encoding vectors based on the points’ distances for adaptive refinements. The network is evaluated on ScanNet, ICL-NUIM, and ShapeNetPart datasets, and the network is shown to be able to reconstruct real-world and unmet scenes.","Point cloud reconstruction is an important problem in 3D scanning. This paper proposes two tasks for point cloud reconstruction: densification and denoising. Point cloud can be represented as discrete voxels, and densification can be modeled as densities of 3D points. The authors propose a deep pointcloud reconstruction network that consists of two stages: 1) a 3D sparse stacked-hourglass network for densification, and 2) a refinement network that uses transformers for refinement. They also introduce a module called amplified positional encoding, which is a module that can be applied to any transformer. The module is used to learn positional encoding vectors based on the points’ distances for adaptive refinements. The network is evaluated on ScanNet, ICL-NUIM, and ShapeNetPart datasets, and the network is shown to be able to reconstruct real-world and unmet scenes."
5487,SP:34a81ca65131576d4c14332a4e9eb3a4c344cab7,"communicating node features CONJUNCTION feature gradients. feature gradients CONJUNCTION communicating node features. training efficiency CONJUNCTION model scalability. model scalability CONJUNCTION training efficiency. stale features CONJUNCTION stale feature gradients. stale feature gradients CONJUNCTION stale features. convergence rate EVALUATE-FOR GCN training. stale features USED-FOR GCN training. stale feature gradients USED-FOR GCN training. convergence rate EVALUATE-FOR PipeGCN. PipeGCN COMPARE vanilla distributed GCN training. vanilla distributed GCN training COMPARE PipeGCN. convergence rate EVALUATE-FOR vanilla distributed GCN training. smoothing method USED-FOR PipeGCN. PipeGCN COMPARE full - graph training methods. full - graph training methods COMPARE PipeGCN. accuracy EVALUATE-FOR full - graph training methods. training throughput EVALUATE-FOR PipeGCN. accuracy EVALUATE-FOR PipeGCN. Method are Graph Convolutional Networks ( GCNs ), large - scale GCNs, and distributed GCN training. Material is graph - structured data. OtherScientificTerm are partitioned subgraph, GCN layer, communication overhead, intra - partition computation, convergence, and staleness. Metric is theoretical convergence guarantee. ","Graph Convolutional Networks (GCNs) are an important architecture for graph-structured data. However, large-scale GCNs are expensive to train due to the communication overhead between each partitioned subgraph and each GCN layer. In this paper, the authors consider the problem of communicating node features and feature gradients across different partitions of the graph, which can be problematic for training efficiency and model scalability. The authors propose a theoretical convergence guarantee for GCN training under the assumption that each partition is partitioned into multiple subgraphs and that each subgraph has a different number of nodes. They show that the convergence rate of GCN learning with stale features and stale feature gradientients is O(1/\sqrt{T}^T) when the intra-partition computation is intractable, and O(T^T log T) for large-sized partitions.  The authors then propose a new algorithm, called PipeGCN, to address the issue of stale features in GCN and propose a smoothing method to improve the convergence of distributed GCN during training. They also provide a theoretical analysis of their algorithm and show the convergence. Finally, they show that with the proposed smoothing algorithm, Pipe GCN achieves a convergence rate that matches or outperforms the convergence result of vanilla distributed GCNs. They further show that their algorithm can achieve better training throughput and improve the accuracy compared to other full-graph training methods. ","Graph Convolutional Networks (GCNs) are an important architecture for graph-structured data. However, large-scale GCNs are expensive to train due to the communication overhead between each partitioned subgraph and each GCN layer. In this paper, the authors consider the problem of communicating node features and feature gradients across different partitions of the graph, which can be problematic for training efficiency and model scalability. The authors propose a theoretical convergence guarantee for GCN training under the assumption that each partition is partitioned into multiple subgraphs and that each subgraph has a different number of nodes. They show that the convergence rate of GCN learning with stale features and stale feature gradientients is O(1/\sqrt{T}^T) when the intra-partition computation is intractable, and O(T^T log T) for large-sized partitions.  The authors then propose a new algorithm, called PipeGCN, to address the issue of stale features in GCN and propose a smoothing method to improve the convergence of distributed GCN during training. They also provide a theoretical analysis of their algorithm and show the convergence. Finally, they show that with the proposed smoothing algorithm, Pipe GCN achieves a convergence rate that matches or outperforms the convergence result of vanilla distributed GCNs. They further show that their algorithm can achieve better training throughput and improve the accuracy compared to other full-graph training methods. "
5503,SP:8302d49558ee0f16392d623d4e604e92db10d041,"robustness USED-FOR applications. in - distribution test points EVALUATE-FOR deep neural networks. accuracy EVALUATE-FOR deep neural networks. methods USED-FOR test time adaptation. ResNet-50 models CONJUNCTION robust vision transformer model. robust vision transformer model CONJUNCTION ResNet-50 models. approach COMPARE prior augmentation and adaptation strategies. prior augmentation and adaptation strategies COMPARE approach. approach COMPARE model evaluation. model evaluation COMPARE approach. baseline ResNet models CONJUNCTION ResNet-50 models. ResNet-50 models CONJUNCTION baseline ResNet models. ImageNet - C CONJUNCTION ImageNet - R. ImageNet - R CONJUNCTION ImageNet - C. ResNet-50 models CONJUNCTION ImageNet - A distribution shift benchmarks. ImageNet - A distribution shift benchmarks CONJUNCTION ResNet-50 models. OtherScientificTerm are distribution shift, model training process, data augmentations, and model parameters. Task is test time robustification. Metric is model robustness. Generic are assumptions, model, and augmentations. ","This paper studies the problem of test time robustification, i.e., the problem that deep neural networks trained on in-distribution test points are more robust to distribution shift than trained on test points outside of the training distribution. The authors propose two methods for test time adaptation: (1) augmenting the model during the model training process with data augmentations, and (2) adapting the model parameters at test time to be robust to test time distribution shift.  The authors show that their approach outperforms prior augmentation and adaptation strategies in terms of accuracy and robustness in a number of applications. They also show that under certain assumptions, their approach is more robust than prior model evaluation, and that the model robustness does not depend on the number of augmentations used.  Experiments are conducted on ImageNet-C, ImageNet -R, and Image-A distribution shift benchmarks, comparing against baseline ResNet models, ResNet-50 models, and a robust vision transformer model.","This paper studies the problem of test time robustification, i.e., the problem that deep neural networks trained on in-distribution test points are more robust to distribution shift than trained on test points outside of the training distribution. The authors propose two methods for test time adaptation: (1) augmenting the model during the model training process with data augmentations, and (2) adapting the model parameters at test time to be robust to test time distribution shift.  The authors show that their approach outperforms prior augmentation and adaptation strategies in terms of accuracy and robustness in a number of applications. They also show that under certain assumptions, their approach is more robust than prior model evaluation, and that the model robustness does not depend on the number of augmentations used.  Experiments are conducted on ImageNet-C, ImageNet -R, and Image-A distribution shift benchmarks, comparing against baseline ResNet models, ResNet-50 models, and a robust vision transformer model."
5519,SP:a985de5e940ff3a4160b378201b8c02f68d1914a,"RL CONJUNCTION planning. planning CONJUNCTION RL. model USED-FOR planning. model USED-FOR RL. accuracy EVALUATE-FOR they. model CONJUNCTION policy. policy CONJUNCTION model. single objective USED-FOR policy. single objective USED-FOR model. expected return FEATURE-OF lower bound. joint optimization USED-FOR objective mismatch. global lower bound FEATURE-OF expected return. global lower bound FEATURE-OF objective. algorithm ( MnM ) COMPARE GAN. GAN COMPARE algorithm ( MnM ). classifier USED-FOR real and fake transitions. Generic are template, models, it, bound, and algorithm. Metric is MSE. Task is control. Method is RL agent. OtherScientificTerm is policies. ","This paper studies the problem of learning a model for RL and planning from a template. The authors propose a new algorithm (MnM) that learns a model and a policy from a single objective. They show that the model and the policy can be learned from the single objective, and that they can be trained in a way that improves the accuracy of the learned models. The lower bound on the expected return of the lower bound is based on the global lower bound of the objective of the two models, and it is shown that this bound depends on the number of transitions that the RL agent is allowed to explore. The paper also shows that the objective mismatch between the learned model and policy is alleviated by joint optimization, and the authors show that this algorithm (MNM) outperforms the GAN in terms of MSE. They also show that a classifier can be used to distinguish between real and fake transitions, and show that their algorithm can be applied to any RL agent.","This paper studies the problem of learning a model for RL and planning from a template. The authors propose a new algorithm (MnM) that learns a model and a policy from a single objective. They show that the model and the policy can be learned from the single objective, and that they can be trained in a way that improves the accuracy of the learned models. The lower bound on the expected return of the lower bound is based on the global lower bound of the objective of the two models, and it is shown that this bound depends on the number of transitions that the RL agent is allowed to explore. The paper also shows that the objective mismatch between the learned model and policy is alleviated by joint optimization, and the authors show that this algorithm (MNM) outperforms the GAN in terms of MSE. They also show that a classifier can be used to distinguish between real and fake transitions, and show that their algorithm can be applied to any RL agent."
5535,SP:a469fbcdc20b11dff4085b6fbc384e77f33cd37d,"single observations CONJUNCTION observation histories. observation histories CONJUNCTION single observations. single observations USED-FOR behavioral cloning policies. observation histories USED-FOR behavioral cloning policies. human decision making USED-FOR model combination approach. instantaneous observation USED-FOR coarse action. images CONJUNCTION MuJoCo continuous control tasks. MuJoCo continuous control tasks CONJUNCTION images. this COMPARE baselines. baselines COMPARE this. CARLA autonomous driving CONJUNCTION MuJoCo continuous control tasks. MuJoCo continuous control tasks CONJUNCTION CARLA autonomous driving. images USED-FOR CARLA autonomous driving. MuJoCo continuous control tasks EVALUATE-FOR this. CARLA autonomous driving EVALUATE-FOR this. MuJoCo continuous control tasks EVALUATE-FOR baselines. CARLA autonomous driving EVALUATE-FOR baselines. Attention maps of baseline imitation methods CONJUNCTION method. method CONJUNCTION Attention maps of baseline imitation methods. CARLA driving task EVALUATE-FOR method. single observation ( BC - SO ) USED-FOR Behavioral cloning. observation history ( BC - OH ) USED-FOR Behavioral cloning. method USED-FOR coarse - to - fine ” imitator. Method are control policy, and imitation - learned policies. Generic are approach, them, and it. OtherScientificTerm are instantaneous observations, historical information, and visual cues. ","This paper proposes a model combination approach for learning a control policy from a single observation. The approach is based on the observation history of the agent, which is used to learn a set of behavioral cloning policies from single observations and observation histories. The authors show that this approach is more robust than baselines that rely on instantaneous observations. They also show that the learned policy can be fine-tuned using human decision making.    The authors evaluate this approach on CARLA autonomous driving on images and MuJoCo continuous control tasks and show that it outperforms baselines.  Behavioral cloning with single observation (BC-SO) and observation history (BC -OH) is the main contribution of this paper. Attention maps of baseline imitation methods and the proposed method are used to train a “coupled coarse-to-fine” imitator, which learns a coarse action from an instantaneous observation and fine-tunes it using historical information.  The paper also shows that the method can be used to fine-train a coarse-tune a ‘fine’ imitation-learned policies. The method is tested on the CARLA driving task and shows that it can learn a policy that is robust to visual cues. ","This paper proposes a model combination approach for learning a control policy from a single observation. The approach is based on the observation history of the agent, which is used to learn a set of behavioral cloning policies from single observations and observation histories. The authors show that this approach is more robust than baselines that rely on instantaneous observations. They also show that the learned policy can be fine-tuned using human decision making.    The authors evaluate this approach on CARLA autonomous driving on images and MuJoCo continuous control tasks and show that it outperforms baselines.  Behavioral cloning with single observation (BC-SO) and observation history (BC -OH) is the main contribution of this paper. Attention maps of baseline imitation methods and the proposed method are used to train a “coupled coarse-to-fine” imitator, which learns a coarse action from an instantaneous observation and fine-tunes it using historical information.  The paper also shows that the method can be used to fine-train a coarse-tune a ‘fine’ imitation-learned policies. The method is tested on the CARLA driving task and shows that it can learn a policy that is robust to visual cues. "
5551,SP:95c4533b5d1a865c4cc6a54615e7ad6357bdaad1,deep learning models USED-FOR dynamics forecasting. generalization EVALUATE-FOR deep learning models. external forces CONJUNCTION boundary conditions. boundary conditions CONJUNCTION external forces. external forces FEATURE-OF systems. them USED-FOR tasks. DyAd HYPONYM-OF model - based meta - learning method. forecaster USED-FOR shared dynamics. encoder USED-FOR time - invariant hidden features. encoder USED-FOR task. parts PART-OF DyAd. encoder HYPONYM-OF parts. forecaster HYPONYM-OF parts. weak supervision USED-FOR encoder. forecaster PART-OF DyAd. encoder PART-OF DyAd. adaptive instance normalization CONJUNCTION adaptive padding. adaptive padding CONJUNCTION adaptive instance normalization. encoder USED-FOR forecaster. forecaster USED-FOR inference. adaptive padding USED-FOR encoder. adaptive instance normalization USED-FOR encoder. adaptive instance normalization USED-FOR forecaster. generalization error EVALUATE-FOR procedure. model COMPARE approaches. approaches COMPARE model. Generic is They. ,"This paper proposes a meta-learning approach to improve the generalization performance of deep learning models for dynamics forecasting. The authors propose a model-based meta learning method called DyAd, which is based on the idea that systems that are subject to external forces and/or boundary conditions should be able to learn from them and generalize to new tasks. DyAd consists of two parts: (1) an encoder that learns time-invariant hidden features, and (2) a forecaster that learns a shared dynamics between the encoder and the forecaster. The encoder is trained with weak supervision, and the two parts of DyAd share the same encoder for each task. The forecaster uses adaptive instance normalization and adaptive padding to make the encode and forecaster more robust to changes in the environment during inference. The proposed procedure is shown to have a lower generalization error than existing approaches. They also show that the model is able to generalize better than previous approaches.","This paper proposes a meta-learning approach to improve the generalization performance of deep learning models for dynamics forecasting. The authors propose a model-based meta learning method called DyAd, which is based on the idea that systems that are subject to external forces and/or boundary conditions should be able to learn from them and generalize to new tasks. DyAd consists of two parts: (1) an encoder that learns time-invariant hidden features, and (2) a forecaster that learns a shared dynamics between the encoder and the forecaster. The encoder is trained with weak supervision, and the two parts of DyAd share the same encoder for each task. The forecaster uses adaptive instance normalization and adaptive padding to make the encode and forecaster more robust to changes in the environment during inference. The proposed procedure is shown to have a lower generalization error than existing approaches. They also show that the model is able to generalize better than previous approaches."
5567,SP:ec70553cb0c27e5349c1b8cce6bcaa96a83bf050,"Monocular 3D object detection HYPONYM-OF 3D scene understanding. manually annotated 3D box labels USED-FOR LiDAR point clouds. manually annotated 3D box labels USED-FOR monocular 3D detection methods. 2D boxes PART-OF image. 2D boxes USED-FOR RoI LiDAR points. network USED-FOR 3D boxes. 3D box estimates CONJUNCTION RoI LiDAR points. RoI LiDAR points CONJUNCTION 3D box estimates. 3D alignment loss FEATURE-OF 3D box estimates. 3D alignment loss USED-FOR network. method COMPARE fully supervised methods. fully supervised methods COMPARE method. KITTI EVALUATE-FOR method. OtherScientificTerm are ill - posed nature of monocular imagery, 3D box labels, weak supervision, and 3D box label. Task are annotation process, weakly supervised monocular 3D detection, and learning problem. ","Monocular 3D object detection is an important problem in 3D scene understanding due to the ill-posed nature of monocular imagery. In this paper, the authors propose to use manually annotated 3D box labels for LiDAR point clouds to improve the performance of existing 3D detection methods. The authors argue that the annotation process can be seen as a weakly supervised monocular3D detection, where only a small number of 2D boxes in the image are annotated. To solve the learning problem, a network is trained to predict 3D boxes based on the 3D alignment loss between the original image and the trained network, and the corresponding 3Dbox labels. The proposed method is evaluated on KITTI, and compared with fully supervised methods.   ","Monocular 3D object detection is an important problem in 3D scene understanding due to the ill-posed nature of monocular imagery. In this paper, the authors propose to use manually annotated 3D box labels for LiDAR point clouds to improve the performance of existing 3D detection methods. The authors argue that the annotation process can be seen as a weakly supervised monocular3D detection, where only a small number of 2D boxes in the image are annotated. To solve the learning problem, a network is trained to predict 3D boxes based on the 3D alignment loss between the original image and the trained network, and the corresponding 3Dbox labels. The proposed method is evaluated on KITTI, and compared with fully supervised methods.   "
5583,SP:34217c6a8ca43b8eeb9ddc83d6f1f0af05918984,"models USED-FOR natural language processing. rigid subword tokenization algorithms USED-FOR models. model inductive bias USED-FOR subword tokenization. characters USED-FOR latent subword representations. block scoring network USED-FOR GBST. CHARFORMER HYPONYM-OF deep Transformer model. GBST PART-OF deep Transformer model. CHARFORMER COMPARE subword - based models. subword - based models COMPARE CHARFORMER. CHARFORMER COMPARE byte - level baselines. byte - level baselines COMPARE CHARFORMER. multilingual, and noisy text datasets EVALUATE-FOR CHARFORMER. English GLUE CONJUNCTION multilingual, and noisy text datasets. multilingual, and noisy text datasets CONJUNCTION English GLUE. English GLUE EVALUATE-FOR CHARFORMER. byte - level baselines COMPARE subword - based models. subword - based models COMPARE byte - level baselines. Generic is model. OtherScientificTerm is byte level. Metric is competitive quality. ","This paper proposes a new model, called CHARFORMER, to tackle the problem of rigid subword tokenization algorithms for models for natural language processing. The key idea is to introduce a model inductive bias towards subwordtokenization, where the latent subword representations at the byte level are encoded as characters, and the block scoring network (GBST) of a deep Transformer model is used to compute GBST at the subword level. Experiments on English GLUE, multilingual, and noisy text datasets show that the proposed model outperforms other byte-level baselines as well as subword-based models. The competitive quality is also improved.","This paper proposes a new model, called CHARFORMER, to tackle the problem of rigid subword tokenization algorithms for models for natural language processing. The key idea is to introduce a model inductive bias towards subwordtokenization, where the latent subword representations at the byte level are encoded as characters, and the block scoring network (GBST) of a deep Transformer model is used to compute GBST at the subword level. Experiments on English GLUE, multilingual, and noisy text datasets show that the proposed model outperforms other byte-level baselines as well as subword-based models. The competitive quality is also improved."
5599,SP:d26d25f2ef23a89a2c139d0dd87c4c86fddcff5e,"Deep neural networks ( DNNs ) USED-FOR backdoor attacks. backdoor PART-OF DNNs. backdoor trigger USED-FOR DNNs. on - device deployed DNNs HYPONYM-OF real - world applications. adversarial objective USED-FOR backdoor detection. optimization perspective USED-FOR problem. adversarial objective USED-FOR solution. singularity FEATURE-OF adversarial map. adversarial map FEATURE-OF backdoorinfected example. skewed distribution FEATURE-OF solution. adversarial extreme value analysis ( AEVA ) USED-FOR backdoors. backdoors PART-OF black - box neural networks. extreme value analysis of the adversarial map USED-FOR AEVA. monte - carlo gradient estimation USED-FOR extreme value analysis of the adversarial map. approach USED-FOR backdoor attacks. backdoor attacks EVALUATE-FOR approach. black - box hard - label scenarios FEATURE-OF detecting backdoor attacks. Method are backdoor detection methods, and DNN. Material is poisoned training data. OtherScientificTerm are predictive confidence, and adversarial singularity phenomenon. ","This paper studies the problem of detecting backdoor attacks on deep neural networks (DNNs) from the perspective of adversarial attacks. The authors propose a novel adversarial objective for backdoor detection based on extreme value analysis (AEVA) and show that AEVA can be used to identify backdoors in black-box neural networks. AEVA is based on the observation that the singularity of the adversarial map of a backdoorinfected example is correlated with the predictive confidence of the DNN. The paper also shows that adversarial singularity phenomenon is also observed in real-world applications such as on-device deployed DNNs.   The paper proposes an optimization perspective to tackle the problem and proposes a novel solution based on an adversarial loss that maximizes the loss of the solution under a skewed distribution. The proposed approach is tested on a variety of backdoor attacks and shows that the proposed approach outperforms existing backdoor detection methods. In addition, the paper also proposes an alternative approach to detect backdoor attacks in the case that the poisoned training data is not available. The main contribution of the paper is the introduction of an approach based on adversarial extreme values analysis to detect backdoors. The approach relies on monte-carlo gradient estimation and the use of the extreme values of a DNN to find a solution that minimizes the risk of a given backdoor infected example. Experiments show that the approach is effective in detecting backdoors even in black box hard-label scenarios. ","This paper studies the problem of detecting backdoor attacks on deep neural networks (DNNs) from the perspective of adversarial attacks. The authors propose a novel adversarial objective for backdoor detection based on extreme value analysis (AEVA) and show that AEVA can be used to identify backdoors in black-box neural networks. AEVA is based on the observation that the singularity of the adversarial map of a backdoorinfected example is correlated with the predictive confidence of the DNN. The paper also shows that adversarial singularity phenomenon is also observed in real-world applications such as on-device deployed DNNs.   The paper proposes an optimization perspective to tackle the problem and proposes a novel solution based on an adversarial loss that maximizes the loss of the solution under a skewed distribution. The proposed approach is tested on a variety of backdoor attacks and shows that the proposed approach outperforms existing backdoor detection methods. In addition, the paper also proposes an alternative approach to detect backdoor attacks in the case that the poisoned training data is not available. The main contribution of the paper is the introduction of an approach based on adversarial extreme values analysis to detect backdoors. The approach relies on monte-carlo gradient estimation and the use of the extreme values of a DNN to find a solution that minimizes the risk of a given backdoor infected example. Experiments show that the approach is effective in detecting backdoors even in black box hard-label scenarios. "
5615,SP:c6dbca0ed0799b7fec21777606f6f809eb2d8c48,"uncertainty estimation USED-FOR classifier. KLoS HYPONYM-OF Kullback – Leibler divergence criterion. class - probability simplex FEATURE-OF Kullback – Leibler divergence criterion. evidential models USED-FOR secondorder uncertainty representation. distributional information USED-FOR KLoS. KLoS USED-FOR class confusion. class - wise divergence measure EVALUATE-FOR KLoS. in - distribution samples USED-FOR class - wise divergence measure. auxiliary neural network USED-FOR refined criterion. KLoSNet USED-FOR refined criterion. KLoSNet HYPONYM-OF auxiliary neural network. misclassifications CONJUNCTION OOD samples. OOD samples CONJUNCTION misclassifications. KLoSNet USED-FOR OOD samples. KLoSNet USED-FOR misclassifications. measures COMPARE KLoS. KLoS COMPARE measures. Material are OOD training data, OOD data, and OOD dataset. Metric is second - order uncertainty measures. OtherScientificTerm is evidential training objective. ","This paper proposes a new class-wise uncertainty measure for out-of-distribution (OOD) data, called Kullback-Leibler divergence (KLoS). KLoS is based on the idea that the class-probability simplex of a classifier can be approximated by a second-order uncertainty measure. The authors show that KLoLS is more robust to class confusion than other second order uncertainty measures, and that it can be used to improve the class confusion of OOD training data.   The authors also show that the evidential training objective can be further improved by incorporating distributional information into the training objective.  The main contribution of the paper is that the authors propose to use evidential models to learn a secondorder uncertainty representation for OOD data, and then use the uncertainty estimation for classifier training.  In addition, the authors introduce an auxiliary neural network called KloSNet, which is a refined version of the original K LoSNet and is trained on the same OOD dataset.  Experiments are conducted to validate the effectiveness of the proposed method. In particular, they show that under certain conditions, KLoNet is able to achieve a class-wide divergence measure that is more sensitive to class-wise divergence measure on in-distributions samples. They also show empirically that the proposed KLoNet can be applied to improve class confusion and class confusion.  Finally, they demonstrate that the KLoSNet can improve the performance of misclassifications and OOD samples on misclassified and out of distributional OOD datasets. ","This paper proposes a new class-wise uncertainty measure for out-of-distribution (OOD) data, called Kullback-Leibler divergence (KLoS). KLoS is based on the idea that the class-probability simplex of a classifier can be approximated by a second-order uncertainty measure. The authors show that KLoLS is more robust to class confusion than other second order uncertainty measures, and that it can be used to improve the class confusion of OOD training data.   The authors also show that the evidential training objective can be further improved by incorporating distributional information into the training objective.  The main contribution of the paper is that the authors propose to use evidential models to learn a secondorder uncertainty representation for OOD data, and then use the uncertainty estimation for classifier training.  In addition, the authors introduce an auxiliary neural network called KloSNet, which is a refined version of the original K LoSNet and is trained on the same OOD dataset.  Experiments are conducted to validate the effectiveness of the proposed method. In particular, they show that under certain conditions, KLoNet is able to achieve a class-wide divergence measure that is more sensitive to class-wise divergence measure on in-distributions samples. They also show empirically that the proposed KLoNet can be applied to improve class confusion and class confusion.  Finally, they demonstrate that the KLoSNet can improve the performance of misclassifications and OOD samples on misclassified and out of distributional OOD datasets. "
5631,SP:8b4f3916dca4e627931558e14836749bd4a6792f,natural image data USED-FOR CNNs. semi - supervised algorithm USED-FOR linear classifier. datadependent features USED-FOR linear classifier. unlabeled data USED-FOR datadependent features. algorithm USED-FOR CNNs. natural distributional assumptions USED-FOR algorithm. it USED-FOR CNNs. low - dimensional structure FEATURE-OF distribution of patches. low - dimensional manifold USED-FOR patches. dimension of the patch distribution USED-FOR algorithm. Method is Convolutional networks ( CNN ). OtherScientificTerm is lower bound. ,"This paper proposes a semi-supervised algorithm to learn a linear classifier with datadependent features from unlabeled data. The authors consider natural image data as natural distributional assumptions for training CNNs. They show that the algorithm can be used to train CNNs with natural distributionals under natural distribution. The algorithm is based on the dimension of the patch distribution, and it is shown that it can be applied to any class of convolutional networks (CNN). They also provide a lower bound on the number of patches that are needed to train the algorithm. The patches are constructed on a low-dimensional manifold, and the authors show that this is possible due to the low-dimension structure of the distribution of patches.","This paper proposes a semi-supervised algorithm to learn a linear classifier with datadependent features from unlabeled data. The authors consider natural image data as natural distributional assumptions for training CNNs. They show that the algorithm can be used to train CNNs with natural distributionals under natural distribution. The algorithm is based on the dimension of the patch distribution, and it is shown that it can be applied to any class of convolutional networks (CNN). They also provide a lower bound on the number of patches that are needed to train the algorithm. The patches are constructed on a low-dimensional manifold, and the authors show that this is possible due to the low-dimension structure of the distribution of patches."
5647,SP:7f2f354d5cc1030bd97bd716aea8fe1d3af86b25,"face images USED-FOR Face clustering. representation capacity FEATURE-OF Graph Convolutional Networks ( GCN ). GCN - based methods USED-FOR face graphs. feature space FEATURE-OF kNN relations. kNN relations USED-FOR GCN - based methods. clean graphs USED-FOR GCNs. clean graphs USED-FOR algorithm. Ada - NETS HYPONYM-OF algorithm. face features USED-FOR robust features. adaptive neighbour discovery strategy USED-FOR edges. It USED-FOR graph. It USED-FOR noise edges. graph USED-FOR GCNs. clean yet rich edges FEATURE-OF graph. public clustering datasets EVALUATE-FOR Ada - NETS. Ada - NETS COMPARE state - of - the - art methods. state - of - the - art methods COMPARE Ada - NETS. public clustering datasets EVALUATE-FOR state - of - the - art methods. generalization EVALUATE-FOR Ada - NETS. OtherScientificTerm are structure space, and face image. ","Face clustering on face images is an important problem, and many existing methods rely on graph convolutional networks (GCNs). However, the representation capacity of Graph Convolutional Networks (GCN) is limited due to the structure space, which limits the expressive power of GCN-based methods for face graphs. This paper proposes a new algorithm called Ada-NETS, which uses clean graphs to train GCNs. The key idea is to learn kNN relations in the feature space, and then use these relations to learn a graph with clean yet rich edges. This graph can then be used to train a GCN. The paper also proposes an adaptive neighbour discovery strategy to discover the edges that are not present in the original face image. It is shown that the noise edges are removed from the graph, and the face features are used to learn robust features from face features. Experiments on public clustering datasets are conducted to show the effectiveness of the proposed algorithm, and show that Ada-NTS outperforms state-of-the-art methods in terms of generalization.  ","Face clustering on face images is an important problem, and many existing methods rely on graph convolutional networks (GCNs). However, the representation capacity of Graph Convolutional Networks (GCN) is limited due to the structure space, which limits the expressive power of GCN-based methods for face graphs. This paper proposes a new algorithm called Ada-NETS, which uses clean graphs to train GCNs. The key idea is to learn kNN relations in the feature space, and then use these relations to learn a graph with clean yet rich edges. This graph can then be used to train a GCN. The paper also proposes an adaptive neighbour discovery strategy to discover the edges that are not present in the original face image. It is shown that the noise edges are removed from the graph, and the face features are used to learn robust features from face features. Experiments on public clustering datasets are conducted to show the effectiveness of the proposed algorithm, and show that Ada-NTS outperforms state-of-the-art methods in terms of generalization.  "
5663,SP:a3bc8e26f55e78f07de081ca85865afd52b6ae4a,"crossdomain representations USED-FOR direct cross - data evaluation. domain information CONJUNCTION camera IDs. camera IDs CONJUNCTION domain information. It USED-FOR features. demographics information USED-FOR features. demographics information USED-FOR It. domain information HYPONYM-OF demographics information. camera IDs HYPONYM-OF demographics information. distributionally robust optimization ( DRO ) USED-FOR learning robust models. uncertainty set HYPONYM-OF data distributions. convex condition FEATURE-OF KL DRO. convex condition FEATURE-OF overparameterized neural networks. real scenarios FEATURE-OF distribution shifts. change - of - measure technique USED-FOR approach. Unit DRO HYPONYM-OF approach. reweighted dataset USED-FOR Unit DRO. large - scale DG ReID CONJUNCTION cross - domain ReID benchmarks. cross - domain ReID benchmarks CONJUNCTION large - scale DG ReID. Unit DRO COMPARE baselines. baselines COMPARE Unit DRO. cross - domain ReID benchmarks EVALUATE-FOR baselines. large - scale DG ReID EVALUATE-FOR baselines. cross - domain ReID benchmarks EVALUATE-FOR Unit DRO. large - scale DG ReID EVALUATE-FOR Unit DRO. OtherScientificTerm are protected demographic features, and demographics. Method are robust models, and models. ","This paper proposes a new method for cross-data evaluation based on cross-domain representations. It leverages demographics information such as domain information and camera IDs to learn features that are robust to distribution shifts. The authors propose a new approach called distributionally robust optimization (DRO) for learning robust models. The idea is to learn a set of protected demographic features that can be used to train robust models in the presence of distribution shifts in the target domain. The proposed approach, Unit DRO, is based on a reweighted dataset where the data distributions (e.g., the uncertainty set) are re-weighted according to the protected demographics. The paper shows that the convex condition of KL DRO holds for overparameterized neural networks and that the proposed approach can be applied to any change-of-measure technique. Experiments are conducted on large-scale DG ReID and the cross-domains ReID benchmarks, and the proposed method outperforms other baselines.","This paper proposes a new method for cross-data evaluation based on cross-domain representations. It leverages demographics information such as domain information and camera IDs to learn features that are robust to distribution shifts. The authors propose a new approach called distributionally robust optimization (DRO) for learning robust models. The idea is to learn a set of protected demographic features that can be used to train robust models in the presence of distribution shifts in the target domain. The proposed approach, Unit DRO, is based on a reweighted dataset where the data distributions (e.g., the uncertainty set) are re-weighted according to the protected demographics. The paper shows that the convex condition of KL DRO holds for overparameterized neural networks and that the proposed approach can be applied to any change-of-measure technique. Experiments are conducted on large-scale DG ReID and the cross-domains ReID benchmarks, and the proposed method outperforms other baselines."
5679,SP:62c1f734b7f6c6e7d5114da6f37c9e3cdda73a23,"Graph Neural Networks ( GNNs ) USED-FOR molecular property prediction. noise correction loss USED-FOR oversmoothing. noise USED-FOR overfitting. generic architectures USED-FOR quantum chemistry. methods USED-FOR regulariser. Noisy Nodes CONJUNCTION non - spatial architectures. non - spatial architectures CONJUNCTION Noisy Nodes. Open Graph Benchmark ( OGB ) datasets EVALUATE-FOR non - spatial architectures. Open Graph Benchmark ( OGB ) datasets EVALUATE-FOR Noisy Nodes. GNN toolkit USED-FOR 3D molecular property prediction. Method is GNNs. OtherScientificTerm are noise correcting node - level loss, and node latents. ","Graph Neural Networks (GNNs) for molecular property prediction have been a popular topic of interest in recent years. However, there are many issues with GNNs, such as oversmoothing, overfitting, and overfitting to noise. In this paper, the authors propose a noise correction loss that aims to mitigate these issues. The noise correcting node-level loss is based on the observation that the noise in node latents can cause overfitting. The authors also propose a regulariser based on existing methods to mitigate the overfitting issue. Experiments on the Open Graph Benchmark (OGB) datasets show that Noisy Nodes and non-spatial architectures outperform existing methods. The paper also shows that generic architectures can be used for quantum chemistry. Finally, a GNN toolkit is proposed for 3D molecule property prediction. ","Graph Neural Networks (GNNs) for molecular property prediction have been a popular topic of interest in recent years. However, there are many issues with GNNs, such as oversmoothing, overfitting, and overfitting to noise. In this paper, the authors propose a noise correction loss that aims to mitigate these issues. The noise correcting node-level loss is based on the observation that the noise in node latents can cause overfitting. The authors also propose a regulariser based on existing methods to mitigate the overfitting issue. Experiments on the Open Graph Benchmark (OGB) datasets show that Noisy Nodes and non-spatial architectures outperform existing methods. The paper also shows that generic architectures can be used for quantum chemistry. Finally, a GNN toolkit is proposed for 3D molecule property prediction. "
5695,SP:24a1b44f37f8eedbab2047fb84600a322d289f3b,"approaches USED-FOR complex interaction between set elements. ( Set)Transformers HYPONYM-OF approaches. self attention USED-FOR approaches. ( Set)Transformers HYPONYM-OF self attention. inducing - point attention CONJUNCTION optimal transport kernel embedding ( OTKE ). optimal transport kernel embedding ( OTKE ) CONJUNCTION inducing - point attention. mixture distribution FEATURE-OF i.i.d. samples. marginal likelihood maximization CONJUNCTION empirical Bayes. empirical Bayes CONJUNCTION marginal likelihood maximization. balanced assignment constraints FEATURE-OF E - step. OTKE HYPONYM-OF framework. balanced assignment constraints FEATURE-OF single - step EM. single - step EM HYPONYM-OF framework. set embedding CONJUNCTION prior - induced model regularization. prior - induced model regularization CONJUNCTION set embedding. OTKE COMPARE approach. approach COMPARE OTKE. approach USED-FOR set embedding. approach USED-FOR prior - induced model regularization. tasks EVALUATE-FOR approach. Task is set2vec problem. Method are vector representation, set embedding feed - forward network, ExpectationMaximization ( EM ) steps, MAP - EM steps, auto - diff backpropagation, and mixture set data fitting framework. OtherScientificTerm are variable number of feature vectors, mixture, and mixture parameters. Metric are computational overhead, and reduced computational cost. ","This paper considers the set2vec problem, where the set is composed of a variable number of feature vectors, and the goal is to learn a vector representation of the set. Two approaches to learn the complex interaction between set elements, (Set)Transformers and (Set2vec) are proposed. Both approaches are based on self attention (e.g. (Set1vec, Set2vec), (SetTransformers), etc.). The authors propose a set embedding feed-forward network, which is based on ExpectationMaximization (EM) steps. The authors also propose inducing-point attention and optimal transport kernel embedding (OTKE), which is a generalization of the two approaches.   The authors show that the proposed framework, OTKE, is a single-step EM with balanced assignment constraints for the E-step, and MAP-EM steps for the MAP-E-step. They also show that a mixture distribution of i.i.d. samples from the mixture distribution can be learned in a single step, and that the marginal likelihood maximization and empirical Bayes are satisfied.  The proposed approach is evaluated on three tasks, where it is shown that OTKE outperforms the proposed approach in terms of both the computational overhead (i.e. the number of MAP-MAP steps) and the reduced computational cost (due to the use of auto-diff backpropagation).    Finally, the authors also demonstrate that the mixture set data fitting framework can be extended to the case where the mixture parameters are unknown, and show that their approach can be used to learn set embedd and prior-induced model regularization. ","This paper considers the set2vec problem, where the set is composed of a variable number of feature vectors, and the goal is to learn a vector representation of the set. Two approaches to learn the complex interaction between set elements, (Set)Transformers and (Set2vec) are proposed. Both approaches are based on self attention (e.g. (Set1vec, Set2vec), (SetTransformers), etc.). The authors propose a set embedding feed-forward network, which is based on ExpectationMaximization (EM) steps. The authors also propose inducing-point attention and optimal transport kernel embedding (OTKE), which is a generalization of the two approaches.   The authors show that the proposed framework, OTKE, is a single-step EM with balanced assignment constraints for the E-step, and MAP-EM steps for the MAP-E-step. They also show that a mixture distribution of i.i.d. samples from the mixture distribution can be learned in a single step, and that the marginal likelihood maximization and empirical Bayes are satisfied.  The proposed approach is evaluated on three tasks, where it is shown that OTKE outperforms the proposed approach in terms of both the computational overhead (i.e. the number of MAP-MAP steps) and the reduced computational cost (due to the use of auto-diff backpropagation).    Finally, the authors also demonstrate that the mixture set data fitting framework can be extended to the case where the mixture parameters are unknown, and show that their approach can be used to learn set embedd and prior-induced model regularization. "
5711,SP:b4f7b660b84fe7702fbcc8a96c192abc3a64f045,unsupervised feature selection USED-FOR informative features. informative features USED-FOR unknown downstream tasks. unsupervised feature selection USED-FOR unknown downstream tasks. CA setting USED-FOR feature selection. machine learning community USED-FOR feature selection. method USED-FOR feature selection. feature selection USED-FOR CA setting. semi - synthetic dataset CONJUNCTION real - world biomedical datasets. real - world biomedical datasets CONJUNCTION semi - synthetic dataset. state - of - the - art methods USED-FOR unsupervised feature selection scenarios. it COMPARE state - of - the - art methods. state - of - the - art methods COMPARE it. semi - synthetic dataset EVALUATE-FOR method. real - world biomedical datasets EVALUATE-FOR method. Task is contrastive analysis ( CA ) setting. Generic is background dataset. OtherScientificTerm is genes. Material is genomic data. ,"This paper proposes an unsupervised feature selection for learning informative features for unknown downstream tasks in the contrastive analysis (CA) setting. In the CA setting, feature selection is performed in the presence of a background dataset, where the goal is to learn features that are informative for a given set of genes. The paper proposes a method for feature selection in this setting that is well-motivated from the machine learning community. The proposed method is evaluated on a semi-synthetic dataset and two real-world biomedical datasets, and it is shown to outperform state-of-the-art methods in both the two standard and two special cases of unsuper supervised feature selection scenarios, where genomic data is available. ","This paper proposes an unsupervised feature selection for learning informative features for unknown downstream tasks in the contrastive analysis (CA) setting. In the CA setting, feature selection is performed in the presence of a background dataset, where the goal is to learn features that are informative for a given set of genes. The paper proposes a method for feature selection in this setting that is well-motivated from the machine learning community. The proposed method is evaluated on a semi-synthetic dataset and two real-world biomedical datasets, and it is shown to outperform state-of-the-art methods in both the two standard and two special cases of unsuper supervised feature selection scenarios, where genomic data is available. "
5727,SP:bc4f69f23aba2034cbf14cb31bdc7a991806bbf6,"Early stopping USED-FOR over - training neural networks. optimal early stopping time CONJUNCTION model dimension. model dimension CONJUNCTION optimal early stopping time. optimal early stopping USED-FOR double descent ”. early stopping USED-FOR generalization. Method are linear regression models, linear models, and deep neural network. Task is deep learning tasks. Generic is model. OtherScientificTerm is features. ","This paper studies the problem of early stopping for over-training neural networks. In particular, the authors consider linear regression models and show that linear models can be over-stopped in the presence of double-descent. They also show that the optimal early stopping time and the model dimension are the only two factors that prevent double descent. The authors further show that early stopping is beneficial for generalization in deep learning tasks.","This paper studies the problem of early stopping for over-training neural networks. In particular, the authors consider linear regression models and show that linear models can be over-stopped in the presence of double-descent. They also show that the optimal early stopping time and the model dimension are the only two factors that prevent double descent. The authors further show that early stopping is beneficial for generalization in deep learning tasks."
5743,SP:ede87b50cd9c4a6533f17e3e5ddfaaeaaac71dcf,"Policy gradient algorithms USED-FOR reinforcement learning ( RL ) problems. Regularization USED-FOR exploration. Regularization USED-FOR stability. entropy functions USED-FOR stability. entropy functions USED-FOR exploration. entropy functions FEATURE-OF Regularization. quasi - Newton method USED-FOR policy gradient algorithm. entropy regularization USED-FOR quasi - Newton method. algorithm USED-FOR natural policy gradient ( NPG ) algorithm. method USED-FOR policy gradient algorithms. method USED-FOR entropy functions. Newton - type quadratic convergence FEATURE-OF algorithms. quasi - Newton method COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE quasi - Newton method. synthetic and industrial - scale examples EVALUATE-FOR quasi - Newton method. single - digit iterations FEATURE-OF quasi - Newton method. OtherScientificTerm are Shannon entropy, and optimal policy. ","Policy gradient algorithms for reinforcement learning (RL) problems are known to suffer from instability due to the Shannon entropy. Regularization on entropy functions to improve stability and encourage exploration has been shown to be effective. In this paper, the authors propose a quasi-Newton method to improve the stability of the policy gradient algorithm by using entropy regularization. The proposed algorithm is a natural policy gradient (NPG) algorithm. The authors show that the proposed method can be used to regularize the entropy functions of existing policy gradient algorithms, which leads to Newton-type quadratic convergence of the algorithms. The experimental results on synthetic and industrial-scale examples demonstrate that the quasi-newton method achieves state-of-the-art performance with single-digit iterations, and is able to learn an optimal policy. ","Policy gradient algorithms for reinforcement learning (RL) problems are known to suffer from instability due to the Shannon entropy. Regularization on entropy functions to improve stability and encourage exploration has been shown to be effective. In this paper, the authors propose a quasi-Newton method to improve the stability of the policy gradient algorithm by using entropy regularization. The proposed algorithm is a natural policy gradient (NPG) algorithm. The authors show that the proposed method can be used to regularize the entropy functions of existing policy gradient algorithms, which leads to Newton-type quadratic convergence of the algorithms. The experimental results on synthetic and industrial-scale examples demonstrate that the quasi-newton method achieves state-of-the-art performance with single-digit iterations, and is able to learn an optimal policy. "
5759,SP:3535504f7599b1f39239f7cd8e09acd40fa8fdf0,"generalization CONJUNCTION sample efficiency. sample efficiency CONJUNCTION generalization. Text - based games ( TBG ) USED-FOR grounded language understanding. generalization HYPONYM-OF problems. sample efficiency HYPONYM-OF problems. deep reinforcement learning ( RL ) methods USED-FOR TBGs. case - based reasoning USED-FOR general method. on - policy neural agent USED-FOR TBGs. method CONJUNCTION on - policy neural agent. on - policy neural agent CONJUNCTION method. method USED-FOR TBGs. approach COMPARE methods. methods COMPARE approach. out - of - distribution generalization EVALUATE-FOR methods. out - of - distribution generalization EVALUATE-FOR approach. OtherScientificTerm is distributional shifts. Method are deep RL approaches, and case - based reasoner. ","This paper studies the problem of grounded language understanding in Text-based games (TBG). Two problems are considered: generalization and sample efficiency. The authors propose to use deep reinforcement learning (RL) methods to solve TBGs using deep RL approaches. They propose a general method based on case-based reasoning, where an on-policy neural agent is trained to solve a set of TBGs with different distributional shifts. The proposed method is a combination of an existing method and a new method that learns a case-by reasoner. The approach is shown to outperform existing methods on out-of-distribution generalization. ","This paper studies the problem of grounded language understanding in Text-based games (TBG). Two problems are considered: generalization and sample efficiency. The authors propose to use deep reinforcement learning (RL) methods to solve TBGs using deep RL approaches. They propose a general method based on case-based reasoning, where an on-policy neural agent is trained to solve a set of TBGs with different distributional shifts. The proposed method is a combination of an existing method and a new method that learns a case-by reasoner. The approach is shown to outperform existing methods on out-of-distribution generalization. "
5775,SP:9a5dd0148a15dc5b4d2bc6762dfe8a8991f8866c,Pre - trained contextual language models USED-FOR language understanding tasks. sense information USED-FOR multi - sense embeddings. multi - sense embeddings PART-OF skip - gram - like framework. pre - trained language model ( BERT ) USED-FOR two - stage method. approach USED-FOR sense disambiguation mechanism. sense disambiguation mechanism PART-OF model. output layer embeddings PART-OF BERT. distribution over word senses USED-FOR approach. BERT USED-FOR distribution over word senses. output layer embeddings USED-FOR distribution over word senses. method COMPARE multi - sense embeddings. multi - sense embeddings COMPARE method. contextual word similarity CONJUNCTION sense induction tasks. sense induction tasks CONJUNCTION contextual word similarity. sense induction tasks EVALUATE-FOR method. contextual word similarity EVALUATE-FOR method. embedding - based topic model ( ETM ) EVALUATE-FOR multi - sense embedding. multiple benchmark data sets EVALUATE-FOR method. multiple benchmark data sets EVALUATE-FOR multi - sense embeddings. Task is resource - constrained systems. Method is Noncontextual word embeddings. Generic is methods. OtherScientificTerm is polysemy. ,"This paper proposes a two-stage method for learning multi-sense embeddings for resource-constrained systems. Pre-trained contextual language models are commonly used for language understanding tasks, and the authors propose to use the sense information to learn multi-ensembles, which is a skip-gram-like framework. Noncontextual word embedding is also considered.    The authors propose a simple two-step method based on a pre-trained language model (BERT). The first step is to learn a distribution over word senses from the output layer embedding of BERT. The second step is an approach to train a sense disambiguation mechanism in the model.  The proposed method is evaluated on contextual word similarity and sense induction tasks, where the proposed method outperforms the state-of-the-art multi-semmas embedding on multiple benchmark data sets. The authors also evaluate the performance of the proposed approach on the embedding-based topic model (ETM) and show that the proposed methods outperform existing methods in terms of polysemy.","This paper proposes a two-stage method for learning multi-sense embeddings for resource-constrained systems. Pre-trained contextual language models are commonly used for language understanding tasks, and the authors propose to use the sense information to learn multi-ensembles, which is a skip-gram-like framework. Noncontextual word embedding is also considered.    The authors propose a simple two-step method based on a pre-trained language model (BERT). The first step is to learn a distribution over word senses from the output layer embedding of BERT. The second step is an approach to train a sense disambiguation mechanism in the model.  The proposed method is evaluated on contextual word similarity and sense induction tasks, where the proposed method outperforms the state-of-the-art multi-semmas embedding on multiple benchmark data sets. The authors also evaluate the performance of the proposed approach on the embedding-based topic model (ETM) and show that the proposed methods outperform existing methods in terms of polysemy."
5800,SP:e4cdba0fc7cd7f440d4436219f3959d8d5e2ad28,"3D point - clouds CONJUNCTION 2D images. 2D images CONJUNCTION 3D point - clouds. 3D point - clouds HYPONYM-OF visual representations of the physical world. 2D images HYPONYM-OF visual representations of the physical world. computer vision models USED-FOR 2D image and 3D point - cloud understanding. human vision USED-FOR representations. 2D model architectures USED-FOR 3D point - clouds. neural net model USED-FOR images. architecture USED-FOR neural net model. image - pretrained model CONJUNCTION point - cloud model. point - cloud model CONJUNCTION image - pretrained model. 2D convolutional filters CONJUNCTION 3D convolutional filters. 3D convolutional filters CONJUNCTION 2D convolutional filters. finetuning efforts FEATURE-OF models. batch normalization layers USED-FOR models. 3D point - cloud classification EVALUATE-FOR models. task - specific architectures USED-FOR point - cloud models. few - shot classification EVALUATE-FOR FIP. data efficiency EVALUATE-FOR FIP. It USED-FOR point - cloud models. Generic are transfer, and model. Method is inflated imagepretrained models ( FIP ). ","This paper investigates the transferability of computer vision models for 2D image and 3D point-cloud understanding. The authors show that a neural net model trained on the same architecture is able to transfer well to different visual representations of the physical world (e.g. point clouds and 2D images). They also show that the representations learned by human vision are transferable across different 2D model architectures, and can be used to transfer between different types of point clouds.    The authors propose a method called inflated imagepretrained models (FIP) that can transfer the transfer from a single image-pretrained model to a single point cloud model. FIP is based on the observation that the transfer between a 2D convolutional filters (2D filters) and a 3D filters (3D filters). It is shown that FIP can transfer well between point cloud models trained on different task-specific architectures. The paper also shows that models trained with different finetuning efforts are able to generalize well across different tasks, and that models with different batch normalization layers can generalize to different tasks. Finally, the paper shows that the models trained using FIP achieve state-of-the-art performance on 3Dpoint-cloud classification and few-shot classification. ","This paper investigates the transferability of computer vision models for 2D image and 3D point-cloud understanding. The authors show that a neural net model trained on the same architecture is able to transfer well to different visual representations of the physical world (e.g. point clouds and 2D images). They also show that the representations learned by human vision are transferable across different 2D model architectures, and can be used to transfer between different types of point clouds.    The authors propose a method called inflated imagepretrained models (FIP) that can transfer the transfer from a single image-pretrained model to a single point cloud model. FIP is based on the observation that the transfer between a 2D convolutional filters (2D filters) and a 3D filters (3D filters). It is shown that FIP can transfer well between point cloud models trained on different task-specific architectures. The paper also shows that models trained with different finetuning efforts are able to generalize well across different tasks, and that models with different batch normalization layers can generalize to different tasks. Finally, the paper shows that the models trained using FIP achieve state-of-the-art performance on 3Dpoint-cloud classification and few-shot classification. "
5825,SP:dc99c307931ae9c5d4a1b998dc94cfc6ac78d11f,"Autoregressive generative models USED-FOR tasks. sequential data USED-FOR tasks. exposure bias CONJUNCTION long - range coherence. long - range coherence CONJUNCTION exposure bias. method USED-FOR autoregressive generative model. energy - based learning objective USED-FOR method. method USED-FOR exposure bias problem. constraint USED-FOR joint distributions. method USED-FOR temporal coherence. exposure bias problem CONJUNCTION temporal coherence. temporal coherence CONJUNCTION exposure bias problem. constraint USED-FOR method. energy - based models USED-FOR energy scores. autoregressive network USED-FOR energy scores. importance sampling USED-FOR model. language modeling CONJUNCTION neural machine translation. neural machine translation CONJUNCTION language modeling. neural machine translation CONJUNCTION image generation. image generation CONJUNCTION neural machine translation. benchmarks EVALUATE-FOR approach. image generation HYPONYM-OF benchmarks. language modeling HYPONYM-OF benchmarks. neural machine translation HYPONYM-OF benchmarks. Generic are They, and network. Method are chain - style conditional modeling, and MCMC process. OtherScientificTerm is distributions. ","Autoregressive generative models have been shown to perform well on a variety of tasks on sequential data. However, they suffer from two issues: exposure bias and long-range coherence. This paper proposes a method to train an autoregressive model with an energy-based learning objective. The proposed method addresses the exposure bias problem and the temporal coherence by introducing a constraint on the joint distributions of the two distributions. The authors propose to use chain-style conditional modeling, where the energy scores of the joint distribution is computed by an autorgressive network. The energy scores are then used to train a second network to predict the energy of the first network. This second network is trained using importance sampling, which is a variant of the MCMC process.  The authors evaluate the proposed approach on three benchmarks: language modeling, neural machine translation, and image generation. ","Autoregressive generative models have been shown to perform well on a variety of tasks on sequential data. However, they suffer from two issues: exposure bias and long-range coherence. This paper proposes a method to train an autoregressive model with an energy-based learning objective. The proposed method addresses the exposure bias problem and the temporal coherence by introducing a constraint on the joint distributions of the two distributions. The authors propose to use chain-style conditional modeling, where the energy scores of the joint distribution is computed by an autorgressive network. The energy scores are then used to train a second network to predict the energy of the first network. This second network is trained using importance sampling, which is a variant of the MCMC process.  The authors evaluate the proposed approach on three benchmarks: language modeling, neural machine translation, and image generation. "
5850,SP:51e748c55bd4134047098559577fa3f37aa7433a,"adversarial attacks FEATURE-OF deep neural networks ( DNNs ). adversarial training ( AT ) method USED-FOR DNN - based classifier. adversarial training ( AT ) method USED-FOR robustness. robustness EVALUATE-FOR DNN - based classifier. adversarial examples USED-FOR adversarial training ( AT ) method. pointwise adversary USED-FOR worst - case adversarial example. PGD - AT and TRADES HYPONYM-OF AT - based methods. pointwise adversary USED-FOR AT - based methods. unified framework USED-FOR Wasserstein distributional robustness. Wasserstein distributional robustness COMPARE AT methods. AT methods COMPARE Wasserstein distributional robustness. Wasserstein cost function CONJUNCTION risk functions. risk functions CONJUNCTION Wasserstein cost function. AT methods PART-OF framework. distributional robustness AT algorithms COMPARE AT counterparts. AT counterparts COMPARE distributional robustness AT algorithms. Method are deep learning systems, classifier, and distributional robustness AT - based algorithms. OtherScientificTerm is adversarial effects. ","This paper studies the problem of adversarial attacks on deep neural networks (DNNs) and proposes a new adversarial training (AT) method to improve the robustness of a DNN-based classifier against adversarial examples.   The authors propose two new AT-based methods, PGD-AT and TRADES, which use a pointwise adversary to generate a worst-case adversarial example for each classifier. The authors also propose a unified framework for Wasserstein distributional robustness, which is a generalization of existing AT methods.  In this framework, the authors show that under certain assumptions on the WESSERstein cost function and the risk functions, the proposed distributional AT algorithms are more robust than their AT counterparts. They also show that the adversarial effects of a point-wise adversary on the classifier are independent of the number of samples and the size of the class.  Finally, they provide a theoretical analysis of the performance of the distributional adversarial robustness AT - which shows that the proposed method is more robust to adversarial perturbations. ","This paper studies the problem of adversarial attacks on deep neural networks (DNNs) and proposes a new adversarial training (AT) method to improve the robustness of a DNN-based classifier against adversarial examples.   The authors propose two new AT-based methods, PGD-AT and TRADES, which use a pointwise adversary to generate a worst-case adversarial example for each classifier. The authors also propose a unified framework for Wasserstein distributional robustness, which is a generalization of existing AT methods.  In this framework, the authors show that under certain assumptions on the WESSERstein cost function and the risk functions, the proposed distributional AT algorithms are more robust than their AT counterparts. They also show that the adversarial effects of a point-wise adversary on the classifier are independent of the number of samples and the size of the class.  Finally, they provide a theoretical analysis of the performance of the distributional adversarial robustness AT - which shows that the proposed method is more robust to adversarial perturbations. "
5875,SP:f192046ea8ad61bfc8e05a0ddb90a8bd15b4640b,"complex dynamics CONJUNCTION sparse annotations. sparse annotations CONJUNCTION complex dynamics. Unsupervised representation learning USED-FOR multivariate time series. it COMPARE problem. problem COMPARE it. data augmentation techniques USED-FOR contrastive training. time slicing USED-FOR segmentlevel augmentation. Bilinear Temporal - Spectral Fusion ( BTSF ) HYPONYM-OF framework. dropout USED-FOR capturing long - term dependencies. dropout USED-FOR global context. instance - level augmentation USED-FOR capturing long - term dependencies. dropout USED-FOR time series. segment - level augmentation COMPARE instance - level augmentation. instance - level augmentation COMPARE segment - level augmentation. global context CONJUNCTION capturing long - term dependencies. capturing long - term dependencies CONJUNCTION global context. dropout USED-FOR instance - level augmentation. iterative bilinear temporal - spectral fusion module USED-FOR affinities. iterative bilinear temporal - spectral fusion module USED-FOR representations of time series. cross - domain interactions USED-FOR representations of time series. alignment CONJUNCTION uniformity. uniformity CONJUNCTION alignment. BTSF USED-FOR bilinear feature representations. forecasting CONJUNCTION anomaly detection. anomaly detection CONJUNCTION forecasting. classification CONJUNCTION forecasting. forecasting CONJUNCTION classification. anomaly detection HYPONYM-OF tasks. tasks USED-FOR time series. anomaly detection HYPONYM-OF time series. classification HYPONYM-OF time series. forecasting HYPONYM-OF time series. anomaly detection HYPONYM-OF tasks. classification HYPONYM-OF tasks. forecasting HYPONYM-OF tasks. BTSF COMPARE state - of - the - art methods. state - of - the - art methods COMPARE BTSF. BTSF COMPARE them. them COMPARE BTSF. Method are contrastive learning, representation learning framework, augmentation methods, and feature representation. OtherScientificTerm is sampling bias. Task is optimization. ","This paper proposes a new representation learning framework for multivariate time series. Unsupervised representation learning is a challenging problem in multdimensional time series due to complex dynamics and sparse annotations. The authors propose a novel contrastive learning framework, called Bilinear Temporal-Spectral Fusion (BTSF), which is a general framework that can be applied to any data augmentation techniques used in contrastive training.    The authors show that it is more efficient than the original problem when the augmentation methods are well-suited to the feature representation. For instance-level augmentation of time series using dropout in a global context and capturing long-term dependencies, the authors propose to use segment level augmentation based on time slicing, which is similar to previous work, but with a sampling bias.  The main difference is that the authors use dropout for the global context, while previous work used instance-levels.  BTSF is able to learn bilinear feature representations by using an iterative bilinearly temporal-spectral fusion module, which learns affinities between the feature representations of different time series based on cross-domain interactions.  Experiments are performed on a variety of tasks for time series, including classification, forecasting, anomaly detection, and forecasting.  Results show that the proposed method outperforms other state-of-the-art methods and outperforms them in most of the cases. The paper also shows that dropout can be used for instance-wise augmentation, which improves the performance. ","This paper proposes a new representation learning framework for multivariate time series. Unsupervised representation learning is a challenging problem in multdimensional time series due to complex dynamics and sparse annotations. The authors propose a novel contrastive learning framework, called Bilinear Temporal-Spectral Fusion (BTSF), which is a general framework that can be applied to any data augmentation techniques used in contrastive training.    The authors show that it is more efficient than the original problem when the augmentation methods are well-suited to the feature representation. For instance-level augmentation of time series using dropout in a global context and capturing long-term dependencies, the authors propose to use segment level augmentation based on time slicing, which is similar to previous work, but with a sampling bias.  The main difference is that the authors use dropout for the global context, while previous work used instance-levels.  BTSF is able to learn bilinear feature representations by using an iterative bilinearly temporal-spectral fusion module, which learns affinities between the feature representations of different time series based on cross-domain interactions.  Experiments are performed on a variety of tasks for time series, including classification, forecasting, anomaly detection, and forecasting.  Results show that the proposed method outperforms other state-of-the-art methods and outperforms them in most of the cases. The paper also shows that dropout can be used for instance-wise augmentation, which improves the performance. "
5900,SP:ef54840009afb095c67bbbc29a7824c20a375ee8,"model architecture CONJUNCTION batch size. batch size CONJUNCTION model architecture. learning rate USED-FOR deep neural networks. algorithm USED-FOR learning rate. gradient descent USED-FOR learning rate. learning rate CONJUNCTION model weights. model weights CONJUNCTION learning rate. approach USED-FOR learning rate. gradient descent step USED-FOR Learning rate. learning rate FEATURE-OF first and second - order gradients. scheme USED-FOR learning rates. learning rate CONJUNCTION batch size. batch size CONJUNCTION learning rate. it USED-FOR optimizing scheme. optimizing scheme EVALUATE-FOR method. Method are line - search, and neural networks. OtherScientificTerm is weight gradients. ","This paper studies the problem of learning rate for deep neural networks. The authors propose a new algorithm to optimize the learning rate in the context of both the model architecture and the batch size. The proposed approach is based on the observation that gradient descent can be used to optimize both learning rate and the model weights. Learning rate can be optimized by the gradient descent step, but line-search can only be used if the weight gradients of the first and second order are close to each other. This paper proposes a new scheme to optimize learning rates for both first- and second-order gradients, and it is shown that it can also be used as an optimizing scheme. The method is tested on a number of datasets, and the results show that the proposed method outperforms existing methods.","This paper studies the problem of learning rate for deep neural networks. The authors propose a new algorithm to optimize the learning rate in the context of both the model architecture and the batch size. The proposed approach is based on the observation that gradient descent can be used to optimize both learning rate and the model weights. Learning rate can be optimized by the gradient descent step, but line-search can only be used if the weight gradients of the first and second order are close to each other. This paper proposes a new scheme to optimize learning rates for both first- and second-order gradients, and it is shown that it can also be used as an optimizing scheme. The method is tested on a number of datasets, and the results show that the proposed method outperforms existing methods."
5925,SP:263c787361cd6d4443ce516d389c694d0fe44b28,"continual meta - learning method USED-FOR sequential multi - task learning. RL USED-FOR offline meta - learning. prior continual learning CONJUNCTION off - policy meta - reinforcement methods. off - policy meta - reinforcement methods CONJUNCTION prior continual learning. CoMPS COMPARE off - policy meta - reinforcement methods. off - policy meta - reinforcement methods COMPARE CoMPS. CoMPS COMPARE prior continual learning. prior continual learning COMPARE CoMPS. continuous control tasks EVALUATE-FOR off - policy meta - reinforcement methods. continuous control tasks EVALUATE-FOR CoMPS. Method are Prior meta - reinforcement learning algorithms, continual reinforcement learning algorithms, continual meta - policy search ( CoMPS ), and meta - training. Generic are they, and method. ","This paper proposes a continual meta-learning method for sequential multi-task learning. Prior meta-reinforcement learning algorithms have been shown to outperform existing continual reinforcement learning algorithms, but they do not perform well when the number of tasks is limited. The authors propose a new method called continual meta meta-policy search (CoMPS), which is based on the observation that RL can be used for offline meta-training. They show that CoMPS outperforms prior continual learning and other off-policy meta-regime methods on a number of continuous control tasks.","This paper proposes a continual meta-learning method for sequential multi-task learning. Prior meta-reinforcement learning algorithms have been shown to outperform existing continual reinforcement learning algorithms, but they do not perform well when the number of tasks is limited. The authors propose a new method called continual meta meta-policy search (CoMPS), which is based on the observation that RL can be used for offline meta-training. They show that CoMPS outperforms prior continual learning and other off-policy meta-regime methods on a number of continuous control tasks."
5950,SP:2bd729b7aa045bf74e31229c9e76e57af36e804b,"“ backdoor ” poisoning attack USED-FOR classification models. threat model USED-FOR poisoned classifier. threat model USED-FOR poisoned classifier. adversarial examples USED-FOR classifier. human interaction FEATURE-OF smoothed adversarial images. ImageNet CONJUNCTION TrojAI. TrojAI CONJUNCTION ImageNet. ImageNet HYPONYM-OF high - resolution datasets. TrojAI HYPONYM-OF high - resolution datasets. high - resolution datasets EVALUATE-FOR attack. method USED-FOR triggers. approach COMPARE method. method COMPARE approach. approach COMPARE modeling trigger distributions. modeling trigger distributions COMPARE approach. backdoors PART-OF poisoned classifiers. secret backdoor PART-OF poisoned classifiers. Method are backdoored classifiers, and Denoised Smoothing. Generic is procedure. OtherScientificTerm is trigger distributions. ","This paper proposes a new “backdoor” poisoning attack against classification models. The idea is that a threat model trained on a poisoned classifier can be used to fool a different threat model to fool the poisoned classification model. The authors propose a procedure called Denoised Smoothing, where a classifier is first trained on adversarial examples, and then the smoothed adversarial images with human interaction are used as triggers to fool backdoored classifiers. Experiments on two high-resolution datasets (ImageNet and TrojAI) show that the attack is effective on both high-resolution datasets. The paper also shows that the proposed approach is more robust to backdoors in poisoned classifiers that have a secret backdoor. Finally, the authors show that their method is able to learn triggers that are more robust than a previous method that does not rely on modeling trigger distributions. ","This paper proposes a new “backdoor” poisoning attack against classification models. The idea is that a threat model trained on a poisoned classifier can be used to fool a different threat model to fool the poisoned classification model. The authors propose a procedure called Denoised Smoothing, where a classifier is first trained on adversarial examples, and then the smoothed adversarial images with human interaction are used as triggers to fool backdoored classifiers. Experiments on two high-resolution datasets (ImageNet and TrojAI) show that the attack is effective on both high-resolution datasets. The paper also shows that the proposed approach is more robust to backdoors in poisoned classifiers that have a secret backdoor. Finally, the authors show that their method is able to learn triggers that are more robust than a previous method that does not rely on modeling trigger distributions. "
5975,SP:e58ab0e3cff6b18013145a1a99cfa9da0a3d872f,"Generative adversarial networks ( GANs ) USED-FOR content generation tasks. GAN compression methods USED-FOR conditional GANs. distilling unconditional GAN USED-FOR StyleGAN2 architecture. output discrepancy issue FEATURE-OF unconditional GAN distillation. heterogeneous distillation scenario FEATURE-OF knowledge distillation losses. style module USED-FOR semantic information. initialization strategy USED-FOR student model. initialization strategy USED-FOR output consistency. semantic consistency FEATURE-OF teacher and student model. latent - direction - based distillation loss USED-FOR semantic relations. latent space FEATURE-OF semantic relations. latent - direction - based distillation loss USED-FOR semantic consistency. approach USED-FOR StyleGAN2. approach COMPARE GAN distillation methods. GAN distillation methods COMPARE approach. GAN distillation methods USED-FOR distilling StyleGAN2. OtherScientificTerm are computation, resource - constrained devices, latent code, and discrepancy issue. ","Generative adversarial networks (GANs) are commonly used for content generation tasks. However, the computation is expensive to run on resource-constrained devices, and conditional GANs have been widely used as GAN compression methods. This paper studies the output discrepancy issue of unconditional GAN distillation, and proposes a novel distilling approach for the StyleGAN2 architecture. The authors propose a novel knowledge distillation losses in the heterogeneous distillation scenario, where a teacher and student model are trained with a different initialization strategy, and the student model is trained with an initialization strategy that encourages the output consistency of the teacher model. To achieve this, the authors introduce a latent-direction-based distillation loss that encourages semantic relations in the latent space to preserve the semantic consistency between the latent code and the teacher code. The style module is used to encode semantic information from the teacher to the style module. The proposed approach is evaluated on the task of distilling the original GAN from the original latent code, and is shown to outperform the state-of-the-art GANdistillation methods.   ","Generative adversarial networks (GANs) are commonly used for content generation tasks. However, the computation is expensive to run on resource-constrained devices, and conditional GANs have been widely used as GAN compression methods. This paper studies the output discrepancy issue of unconditional GAN distillation, and proposes a novel distilling approach for the StyleGAN2 architecture. The authors propose a novel knowledge distillation losses in the heterogeneous distillation scenario, where a teacher and student model are trained with a different initialization strategy, and the student model is trained with an initialization strategy that encourages the output consistency of the teacher model. To achieve this, the authors introduce a latent-direction-based distillation loss that encourages semantic relations in the latent space to preserve the semantic consistency between the latent code and the teacher code. The style module is used to encode semantic information from the teacher to the style module. The proposed approach is evaluated on the task of distilling the original GAN from the original latent code, and is shown to outperform the state-of-the-art GANdistillation methods.   "
6000,SP:2c2231743fa33b95828c6615263954ce1c05f95d,methodology USED-FOR offline algorithms. online settings FEATURE-OF offline algorithms. multi - task learning model USED-FOR behavioral structures. graphs USED-FOR offline algorithms. synthetic data CONJUNCTION historical stock market data. historical stock market data CONJUNCTION synthetic data. historical stock market data EVALUATE-FOR methodology. synthetic data EVALUATE-FOR methodology. ,"This paper proposes a new methodology for learning offline algorithms in online settings. The authors propose a multi-task learning model to learn behavioral structures from graphs. The methodology is evaluated on synthetic data and historical stock market data.    The paper is well-written, well-motivated, and well-organized. ","This paper proposes a new methodology for learning offline algorithms in online settings. The authors propose a multi-task learning model to learn behavioral structures from graphs. The methodology is evaluated on synthetic data and historical stock market data.    The paper is well-written, well-motivated, and well-organized. "
6025,SP:ee3a21d2fb8a073099aa200129a53c31f3b6561d,"Gaussian Processes ( GPs ) HYPONYM-OF Bayesian models. Bayesian models USED-FOR uncertainty estimates. variational inference USED-FOR inferring q. Sparse GPs CONJUNCTION variational inference. variational inference CONJUNCTION Sparse GPs. Sparse GPs USED-FOR GPs. it COMPARE sparse variational GP approaches. sparse variational GP approaches COMPARE it. neural network USED-FOR inducing points locations. training and prediction times EVALUATE-FOR method. Generic are They, them, model, and they. Method is sparse GP approximations. OtherScientificTerm is latent function. Task are learning tasks, and prediction. ","This paper introduces a new family of Bayesian models, called Gaussian Processes (GPs), which is a family of uncertainty estimates that can be used to improve the training and prediction performance of Bayes. Sparse GPs and variational inference for inferring q are used to approximate GPs. They can be seen as a generalization of sparse GP approximations. The paper shows that it is more efficient than other sparse variational GP approaches, and that it can be trained with a neural network to learn inducing points locations for each latent function. The authors also show that the proposed method can reduce the number of parameters of the model by a factor of at most 1.5. The proposed method is shown to be able to reduce the training time and improve the learning and prediction times for a number of learning tasks, and it is shown that the prediction performance can be improved when the latent function is sparse. ","This paper introduces a new family of Bayesian models, called Gaussian Processes (GPs), which is a family of uncertainty estimates that can be used to improve the training and prediction performance of Bayes. Sparse GPs and variational inference for inferring q are used to approximate GPs. They can be seen as a generalization of sparse GP approximations. The paper shows that it is more efficient than other sparse variational GP approaches, and that it can be trained with a neural network to learn inducing points locations for each latent function. The authors also show that the proposed method can reduce the number of parameters of the model by a factor of at most 1.5. The proposed method is shown to be able to reduce the training time and improve the learning and prediction times for a number of learning tasks, and it is shown that the prediction performance can be improved when the latent function is sparse. "
6050,SP:f20c99b441545047a16ae524cc2e317b2c3787a2,"scientific collaborations CONJUNCTION volunteer computing. volunteer computing CONJUNCTION scientific collaborations. hardest problems PART-OF deep learning. Byzantine tolerance FEATURE-OF distributed training algorithms. algorithms USED-FOR large - scale distributed deep learning. protocol USED-FOR secure ( Byzantinetolerant ) decentralized training. communication efficiency FEATURE-OF protocol. theoretical bounds USED-FOR resistance. Byzantine and Sybil attacks FEATURE-OF resistance. communication overhead FEATURE-OF it. Byzantine attackers FEATURE-OF image classification and language modeling. Generic are systems, and models. Metric is efficiency. OtherScientificTerm are redundant communication, and trusted server. ","This paper studies the problem of Byzantine tolerance in distributed training algorithms, which is one of the hardest problems in deep learning due to the lack of communication between the server and the clients. The authors propose a new protocol for secure (Bithmidinetolerant) decentralized training that improves the communication efficiency of the protocol. They also provide theoretical bounds on the resistance to Byzantine and Sybil attacks. They show that under certain conditions, the systems are robust to these attacks. The paper also shows that Byzantine attackers can attack both image classification and language modeling.   The authors also show that the algorithms for large-scale distributed deep learning are Byzantine-tolerant, and that it is due to a communication overhead that is proportional to the number of clients and the amount of redundant communication between them. The main contribution of the paper is that the models are robust against Byzantine attackers and that the protocol is robust to redundant communication. ","This paper studies the problem of Byzantine tolerance in distributed training algorithms, which is one of the hardest problems in deep learning due to the lack of communication between the server and the clients. The authors propose a new protocol for secure (Bithmidinetolerant) decentralized training that improves the communication efficiency of the protocol. They also provide theoretical bounds on the resistance to Byzantine and Sybil attacks. They show that under certain conditions, the systems are robust to these attacks. The paper also shows that Byzantine attackers can attack both image classification and language modeling.   The authors also show that the algorithms for large-scale distributed deep learning are Byzantine-tolerant, and that it is due to a communication overhead that is proportional to the number of clients and the amount of redundant communication between them. The main contribution of the paper is that the models are robust against Byzantine attackers and that the protocol is robust to redundant communication. "
6075,SP:93894f20ab2593e5237b6972fef9fe63e96af89a,"Smoothed particle hydrodynamics ( SPH ) HYPONYM-OF mesh - free Lagrangian method. astrophysics and engineering applications FEATURE-OF weaklyand strongly compressible turbulence. physics based parameters CONJUNCTION Neural Networks. Neural Networks CONJUNCTION physics based parameters. Neural Networks USED-FOR universal function approximators. Neural Networks USED-FOR SPH informed fluid simulators. physics based parameters USED-FOR SPH informed fluid simulators. forward and adjoint based sensitivity analyses USED-FOR gradient based optimization. learning algorithm USED-FOR mixed mode approach. physics informed learning method USED-FOR inverse problems. physically interpretable parameter space FEATURE-OF inverse problems. time scales CONJUNCTION Reynolds numbers. Reynolds numbers CONJUNCTION time scales. interpretability CONJUNCTION generalizability. generalizability CONJUNCTION interpretability. hierarchy of models USED-FOR physical structure. Reynolds numbers FEATURE-OF generalizability. time scales FEATURE-OF generalizability. OtherScientificTerm are Neural Network parameters, Lagrangian statistics of turbulence, and physical symmetries. Material is training data. ","This paper proposes Smoothed particle hydrodynamics (SPH) which is a mesh-free Lagrangian method for weaklyand strongly compressible turbulence in astrophysics and engineering applications. SPH informed fluid simulators based on physics based parameters and Neural Networks are universal function approximators. Neural Network parameters are used to learn the physics-based parameters of the system. The authors propose a learning algorithm for the mixed mode approach, which is based on the forward and adjoint based sensitivity analyses for gradient based optimization.  The authors also propose a physics informed learning method to solve inverse problems in a physically interpretable parameter space. The main contribution of the paper is that the authors provide a theoretical analysis of the training data, which shows that Neural Networks can be used as universal functions for the training of SPH informativeness and generalizability. The paper also shows that the learning algorithm can be applied to any learning algorithm.    The main contributions of this paper are as follows:  1. A theoretical analysis that shows that a Neural Network can be universal for the learning of the physics in SPH. 2. A new learning algorithm is proposed for the mixing of physics based and neural network parameters. 3. A physics-informed learning method is proposed to solve the inverse problems on time scales and the physical symmetries of turbulence. 4. A hierarchy of models is used to model the physical structure of the data. 5. Experiments show that the proposed method is able to achieve better interpretability and better generalization in terms (in terms of time scales, Reynolds numbers, etc.).","This paper proposes Smoothed particle hydrodynamics (SPH) which is a mesh-free Lagrangian method for weaklyand strongly compressible turbulence in astrophysics and engineering applications. SPH informed fluid simulators based on physics based parameters and Neural Networks are universal function approximators. Neural Network parameters are used to learn the physics-based parameters of the system. The authors propose a learning algorithm for the mixed mode approach, which is based on the forward and adjoint based sensitivity analyses for gradient based optimization.  The authors also propose a physics informed learning method to solve inverse problems in a physically interpretable parameter space. The main contribution of the paper is that the authors provide a theoretical analysis of the training data, which shows that Neural Networks can be used as universal functions for the training of SPH informativeness and generalizability. The paper also shows that the learning algorithm can be applied to any learning algorithm.    The main contributions of this paper are as follows:  1. A theoretical analysis that shows that a Neural Network can be universal for the learning of the physics in SPH. 2. A new learning algorithm is proposed for the mixing of physics based and neural network parameters. 3. A physics-informed learning method is proposed to solve the inverse problems on time scales and the physical symmetries of turbulence. 4. A hierarchy of models is used to model the physical structure of the data. 5. Experiments show that the proposed method is able to achieve better interpretability and better generalization in terms (in terms of time scales, Reynolds numbers, etc.)."
6100,SP:d11b81f9ab414fcf430a03cd70c2d3246b678474,"approach USED-FOR deterministic neural network. accuracy CONJUNCTION uncertainty estimates. uncertainty estimates CONJUNCTION accuracy. entropy maximization regularizer USED-FOR predictive distribution. entropy maximization regularizer USED-FOR approach. embedding space FEATURE-OF predictive distribution. cross - entropy loss USED-FOR approach. entropy FEATURE-OF samples. images USED-FOR convex combination. convex combination USED-FOR synthetically generating between - cluster samples. data - dependent regularization USED-FOR maximum likelihood estimation. data - dependent regularization USED-FOR solution. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. real - world datasets EVALUATE-FOR Mix - MaxEnt. calibrated probabilities USED-FOR in - distribution data. Mix - MaxEnt USED-FOR uncertainty estimates. ResNet and Wide - ResNet architectures USED-FOR real - world datasets. CIFAR-10 HYPONYM-OF real - world datasets. CIFAR-100 HYPONYM-OF real - world datasets. classification accuracy EVALUATE-FOR Mix - MaxEnt. OtherScientificTerm are class clusters, out - of - distribution samples, high entropy regions, entropy barrier, and superficial input perturbations. ","This paper proposes Mix-MaxEnt, an approach to train a deterministic neural network with an entropy maximization regularizer to improve both accuracy and uncertainty estimates. The approach is based on the cross-entropy loss, which encourages the class clusters to share the same embedding space, and the out-of-distribution samples to be clustered in high entropy regions. The authors propose synthetically generating between-cluster samples using a convex combination of images from different clusters. The solution is built upon data-dependent regularization for maximum likelihood estimation, where the entropy of the samples is proportional to the number of samples in the cluster. Mix-maxEnt is evaluated on two real-world datasets with ResNet and Wide-ResNet architectures, CIFAR-10 and CIFar-100, and is shown to improve classification accuracy. The main contribution of the paper is that the solution is able to avoid the entropy barrier in the high-density regions, and that it is robust to superficial input perturbations. It is also shown that Mix-MAXEnt can be used to improve uncertainty estimates by using calibrated probabilities for in-distributions data. ","This paper proposes Mix-MaxEnt, an approach to train a deterministic neural network with an entropy maximization regularizer to improve both accuracy and uncertainty estimates. The approach is based on the cross-entropy loss, which encourages the class clusters to share the same embedding space, and the out-of-distribution samples to be clustered in high entropy regions. The authors propose synthetically generating between-cluster samples using a convex combination of images from different clusters. The solution is built upon data-dependent regularization for maximum likelihood estimation, where the entropy of the samples is proportional to the number of samples in the cluster. Mix-maxEnt is evaluated on two real-world datasets with ResNet and Wide-ResNet architectures, CIFAR-10 and CIFar-100, and is shown to improve classification accuracy. The main contribution of the paper is that the solution is able to avoid the entropy barrier in the high-density regions, and that it is robust to superficial input perturbations. It is also shown that Mix-MAXEnt can be used to improve uncertainty estimates by using calibrated probabilities for in-distributions data. "
6125,SP:365490b872464f00634dc7a50d024fceaf0a61ee,"Generative Adversarial Networks ( GANs ) CONJUNCTION auto - encoder animating images. auto - encoder animating images CONJUNCTION Generative Adversarial Networks ( GANs ). driving videos USED-FOR structure representation. structure representation USED-FOR animation - approaches. modules USED-FOR animation - model. modules USED-FOR extraction of structure information. Latent Image Animator ( LIA ) HYPONYM-OF self - supervised autoencoder. linear combination USED-FOR latent space. model COMPARE state - of - art methods. state - of - art methods COMPARE model. TED - talk datasets EVALUATE-FOR state - of - art methods. TED - talk datasets EVALUATE-FOR model. VoxCeleb EVALUATE-FOR state - of - art methods. VoxCeleb EVALUATE-FOR model. LIA USED-FOR motion of a driving video. landmarks CONJUNCTION region representations. region representations CONJUNCTION landmarks. LIA USED-FOR images. structure representations USED-FOR LIA. region representations HYPONYM-OF structure representations. landmarks HYPONYM-OF structure representations. Material are still images, LIA animation examples, and VoxCeleb dataset. Generic are approaches, and models. OtherScientificTerm are appearance variation, motion, and orthogonal motion directions. Metric are complexity, and generated quality. ","This paper proposes a new approach to learn a structure representation of a driving video, which can then be used to train an auto-encoder to generate an animator that can be applied to any driving video. The proposed approach, called Latent Image Animator (LIA), is a self-supervised autoencoder that learns a latent representation of the driving video using a GAN-based approach. The latent representation is learned by a linear combination of two modules, one that learns the structure representation and the other that learns to predict the motion of the vehicle. The authors show that the proposed approach outperforms the state-of-the-art methods on the TED-TALK and VoxCeleb datasets. ","This paper proposes a new approach to learn a structure representation of a driving video, which can then be used to train an auto-encoder to generate an animator that can be applied to any driving video. The proposed approach, called Latent Image Animator (LIA), is a self-supervised autoencoder that learns a latent representation of the driving video using a GAN-based approach. The latent representation is learned by a linear combination of two modules, one that learns the structure representation and the other that learns to predict the motion of the vehicle. The authors show that the proposed approach outperforms the state-of-the-art methods on the TED-TALK and VoxCeleb datasets. "
6150,SP:86f9f89f84e117c86478b9afaf087f65524f5472,"meta - training tasks USED-FOR meta - learning algorithms. approach USED-FOR tasks. data - adaptive meta - regularization USED-FOR MLTI. generalization EVALUATE-FOR MLTI. pose prediction CONJUNCTION molecule property prediction. molecule property prediction CONJUNCTION pose prediction. molecule property prediction CONJUNCTION medical image classification. medical image classification CONJUNCTION molecule property prediction. image recognition CONJUNCTION pose prediction. pose prediction CONJUNCTION image recognition. MLTI framework COMPARE state - of - the - art strategies. state - of - the - art strategies COMPARE MLTI framework. MLTI framework CONJUNCTION representative meta - learning algorithms. representative meta - learning algorithms CONJUNCTION MLTI framework. datasets EVALUATE-FOR MLTI framework. image recognition HYPONYM-OF datasets. medical image classification HYPONYM-OF datasets. pose prediction HYPONYM-OF datasets. molecule property prediction HYPONYM-OF datasets. Method is Meta - learning. OtherScientificTerm are real - world scenarios, and interpolation. ","This paper proposes a meta-learning approach to improve the generalization performance of meta-training algorithms. Meta-learning is an important problem in many real-world scenarios, where the goal is to learn a good representation of the training data. The authors propose a new approach called data-adaptive meta-regularization (MLTI) to regularize the meta-learners to perform well on a variety of tasks. They show that MLTI improves generalization on a wide range of datasets, including image recognition, pose prediction, molecule property prediction, and medical image classification. They also show that the proposed MLTI framework outperforms state-of-the-art strategies when the number of tasks is small and the interpolation is limited.    The authors also provide a theoretical analysis of the performance of the proposed approach.  The paper is well-written and well-motivated, and the paper is easy to follow. The paper has a good amount of experiments on datasets such as image recognition and pose prediction. It also shows that the presented approach can be used to train representative meta -learning algorithms. ","This paper proposes a meta-learning approach to improve the generalization performance of meta-training algorithms. Meta-learning is an important problem in many real-world scenarios, where the goal is to learn a good representation of the training data. The authors propose a new approach called data-adaptive meta-regularization (MLTI) to regularize the meta-learners to perform well on a variety of tasks. They show that MLTI improves generalization on a wide range of datasets, including image recognition, pose prediction, molecule property prediction, and medical image classification. They also show that the proposed MLTI framework outperforms state-of-the-art strategies when the number of tasks is small and the interpolation is limited.    The authors also provide a theoretical analysis of the performance of the proposed approach.  The paper is well-written and well-motivated, and the paper is easy to follow. The paper has a good amount of experiments on datasets such as image recognition and pose prediction. It also shows that the presented approach can be used to train representative meta -learning algorithms. "
6175,SP:73d577e9c4f4af5e11a9e5bdb583ee0f50a315f5,"approach USED-FOR fairness of downstream predictors. encoding sensitive data USED-FOR fairness of downstream predictors. unfairness FEATURE-OF adversarial predictors. representations USED-FOR sensitive attributes. fairness guarantees FEATURE-OF learned representations. probability density USED-FOR sensitive groups. normalizing flow USED-FOR statistical distance. statistical distance FEATURE-OF latent representations. normalizing flow USED-FOR encoder. maximum unfairness EVALUATE-FOR adversarial downstream predictor. interpretability CONJUNCTION transfer learning. transfer learning CONJUNCTION interpretability. FNF USED-FOR group fairness notions. FNF USED-FOR properties. real - world datasets EVALUATE-FOR properties. transfer learning HYPONYM-OF properties. interpretability HYPONYM-OF properties. real - world datasets EVALUATE-FOR FNF. Method are Fair representation learning, Fair Normalizing Flows ( FNF ), and likelihood computation. ","Fair representation learning is an important problem in machine learning. This paper proposes a novel approach to improve the fairness of downstream predictors by encoding sensitive data. The authors propose Fair Normalizing Flows (FNF), which aims to reduce the unfairness of adversarial predictors. The key idea is to learn representations for sensitive attributes, which are then used to enforce fairness guarantees on the learned representations. To achieve this, the authors propose to use a normalizing flow to compute the statistical distance between the latent representations of the sensitive group and the non-sensitive group. The probability density for sensitive groups is then used as a proxy for the likelihood computation for the sensitive groups, and the encoder is trained with the normalising flow. Experiments on real-world datasets show that FNF improves group fairness notions on three properties: interpretability, transfer learning, and maximum unfairness. ","Fair representation learning is an important problem in machine learning. This paper proposes a novel approach to improve the fairness of downstream predictors by encoding sensitive data. The authors propose Fair Normalizing Flows (FNF), which aims to reduce the unfairness of adversarial predictors. The key idea is to learn representations for sensitive attributes, which are then used to enforce fairness guarantees on the learned representations. To achieve this, the authors propose to use a normalizing flow to compute the statistical distance between the latent representations of the sensitive group and the non-sensitive group. The probability density for sensitive groups is then used as a proxy for the likelihood computation for the sensitive groups, and the encoder is trained with the normalising flow. Experiments on real-world datasets show that FNF improves group fairness notions on three properties: interpretability, transfer learning, and maximum unfairness. "
6200,SP:404d5643327f60f0f06f820033a56081f9e01900,"subgraph patterns on graphs USED-FOR graph - based tasks. graph neural networks ( GNNs ) USED-FOR low - dimensional representation. node - centric message passing mechanism USED-FOR GNNs. they USED-FOR complex structure matching. complex structure matching USED-FOR isomorphism counting. COUNT - GNN USED-FOR subgraph isomorphism counting. GNN USED-FOR subgraph isomorphism counting. COUNT - GNN HYPONYM-OF GNN. edge - centric message passing scheme USED-FOR edge level. COUNT - GNN USED-FOR fine - grained structural information. edge USED-FOR encoding graph structures. graph representation USED-FOR graph level. benchmark datasets EVALUATE-FOR COUNT - GNN. COUNT - GNN COMPARE baselines. baselines COMPARE COUNT - GNN. OtherScientificTerm are graph structures, subgraph isomorphisms, nodes, graph, query graphs, structured query graphs, edges, edge adjacency, and first - class citizens. Material is graph data. Method is backtracking framework. Metric is computational cost. Task are node - oriented tasks, and matching. ","This paper proposes a graph neural networks (GNNs) for learning a low-dimensional representation of subgraph patterns on graphs for graph-based tasks. The authors propose a node-centric message passing mechanism for GNNs, where nodes are represented as subgraph isomorphisms, and query graphs are treated as structured query graphs. They show that they can be used for complex structure matching, which is an important problem for isomorphism counting.   The authors introduce COUNT-GNN, a GNN which is a variant of GNN that uses an edge-centric message passing scheme at the edge level to encode information about the subgraph structure of the graph.  The key idea is that each edge in a query graph can be represented as a subgraph of the query graph, and each edge can be encoded as an edge in the original graph. The edge is then used for encoding graph structures, which are then passed through the GNN.  In the backtracking framework, each edge is encoded as the edge adjacency between two nodes in the query graphs, and then the edge is added to the graph representation at the graph level, and the matching is performed between the node and the edge.  COUNT - GNN is shown to be able to encode fine-grained structural information, and is able to reduce the computational cost for node-oriented tasks.  Experiments on several benchmark datasets show that Count- GNN outperforms several baselines, and that it can be applied to a variety of graph-level tasks. It is also shown that it is more robust to node-level changes in the graph structure, and can be trained to encode first-class citizens.","This paper proposes a graph neural networks (GNNs) for learning a low-dimensional representation of subgraph patterns on graphs for graph-based tasks. The authors propose a node-centric message passing mechanism for GNNs, where nodes are represented as subgraph isomorphisms, and query graphs are treated as structured query graphs. They show that they can be used for complex structure matching, which is an important problem for isomorphism counting.   The authors introduce COUNT-GNN, a GNN which is a variant of GNN that uses an edge-centric message passing scheme at the edge level to encode information about the subgraph structure of the graph.  The key idea is that each edge in a query graph can be represented as a subgraph of the query graph, and each edge can be encoded as an edge in the original graph. The edge is then used for encoding graph structures, which are then passed through the GNN.  In the backtracking framework, each edge is encoded as the edge adjacency between two nodes in the query graphs, and then the edge is added to the graph representation at the graph level, and the matching is performed between the node and the edge.  COUNT - GNN is shown to be able to encode fine-grained structural information, and is able to reduce the computational cost for node-oriented tasks.  Experiments on several benchmark datasets show that Count- GNN outperforms several baselines, and that it can be applied to a variety of graph-level tasks. It is also shown that it is more robust to node-level changes in the graph structure, and can be trained to encode first-class citizens."
6225,SP:5a94f18156ab2949c86de45fcf0de2e16977eebb,"Agnostic Personalized Federated Learning ( APFL ) HYPONYM-OF loosely constrained federated learning. Similarity Matching CONJUNCTION Kernel Factorization ( SimFed ). Kernel Factorization ( SimFed ) CONJUNCTION Similarity Matching. Kernel Factorization ( SimFed ) HYPONYM-OF method. Similarity Matching HYPONYM-OF method. ones USED-FOR personalized knowledge reflection. method USED-FOR task - level similarity. locally learned knowledge USED-FOR method. locally learned knowledge USED-FOR task - level similarity. knowledge collapse CONJUNCTION information loss. information loss CONJUNCTION knowledge collapse. dimensionlaity of parameter space USED-FOR knowledge collapse. information loss FEATURE-OF heterogeneous knowledge. basis vectors PART-OF model parameters. method COMPARE federated learning methods. federated learning methods COMPARE method. singleand multi - domain datasets EVALUATE-FOR method. singleand multi - domain datasets EVALUATE-FOR method. Task is federated learning. OtherScientificTerm are personalized labels, Label Heterogeneity, and Domain Heterogeneity. Generic is they. Method are labeling schemes, and agnostic personalized federated learning. Material is local data. ","This paper proposes Agnostic Personalized Federated Learning (APFL) which is a variant of loosely constrained federated learning, where each client only has access to a small subset of the local data, and the goal is to train a model that is agnostic to the personalized labels of all the clients. The authors propose two labeling schemes: Label Heterogeneity (LH) and Domain Heterogeneous (DH). They show that they are agnostic when the labels are not personalized, but agnostic if they are personalized. They also propose a method called agnostic personalized Federated learning (APHFL) based on two existing ones for personalized knowledge reflection: Similarity Matching and Kernel Factorization (SimFed). The proposed method uses locally learned knowledge to compute the task-level similarity between the local labels and the labels of the clients, and then uses the locally learned labels to train the model parameters. The paper also shows that the knowledge collapse and information loss of heterogeneous knowledge can be reduced due to the dimensionlaity of parameter space, which can be alleviated by replacing the basis vectors in model parameters with local data. Experiments on both singleand multi-domain datasets show that the proposed method outperforms existing federated training methods.   ","This paper proposes Agnostic Personalized Federated Learning (APFL) which is a variant of loosely constrained federated learning, where each client only has access to a small subset of the local data, and the goal is to train a model that is agnostic to the personalized labels of all the clients. The authors propose two labeling schemes: Label Heterogeneity (LH) and Domain Heterogeneous (DH). They show that they are agnostic when the labels are not personalized, but agnostic if they are personalized. They also propose a method called agnostic personalized Federated learning (APHFL) based on two existing ones for personalized knowledge reflection: Similarity Matching and Kernel Factorization (SimFed). The proposed method uses locally learned knowledge to compute the task-level similarity between the local labels and the labels of the clients, and then uses the locally learned labels to train the model parameters. The paper also shows that the knowledge collapse and information loss of heterogeneous knowledge can be reduced due to the dimensionlaity of parameter space, which can be alleviated by replacing the basis vectors in model parameters with local data. Experiments on both singleand multi-domain datasets show that the proposed method outperforms existing federated training methods.   "
6250,SP:97f30bea31eccef6c770fbce1e14fd6d2493a178,"Object Dynamics Distillation Network ( ODDN ) USED-FOR object dynamic representations. velocity HYPONYM-OF object dynamic representations. raw video input USED-FOR object dynamic representations. it USED-FOR dynamic representations of objects. relation module USED-FOR object - pair interactions. relation module USED-FOR dynamic representations of objects. video events reasoning CONJUNCTION video prediction. video prediction CONJUNCTION video events reasoning. video events reasoning EVALUATE-FOR approach. scene representation methods USED-FOR representaions. occlusion CONJUNCTION objects collision. objects collision CONJUNCTION occlusion. object dynamic clues USED-FOR model. segmentation CONJUNCTION reconstruction. reconstruction CONJUNCTION segmentation. scene decomposition quality EVALUATE-FOR reconstruction. scene decomposition quality EVALUATE-FOR segmentation. scene decomposition quality EVALUATE-FOR model. OtherScientificTerm are abstract entities, and physical events. Method are object - centric representations of scenes, and ODDN. Material is static images. Task are object dynamics, and video understanding. ","This paper proposes Object Dynamics Distillation Network (ODDN) to learn object dynamic representations from raw video input (e.g. velocity, occlusion, collision) and abstract entities. The key idea is to distill object-centric representations of scenes from static images and use it to learn dynamic representations of objects using a relation module to model object-pair interactions. The approach is evaluated on video events reasoning and video prediction tasks. The representaions are trained using existing scene representation methods. The model is trained with object dynamic clues (occlusion and objects collision) as well as object dynamics. Experiments show that the model achieves state-of-the-art performance on segmentation, reconstruction and scene decomposition quality. The paper also shows that ODDN is able to capture object-centric interactions between objects in a scene, which is important for video understanding. ","This paper proposes Object Dynamics Distillation Network (ODDN) to learn object dynamic representations from raw video input (e.g. velocity, occlusion, collision) and abstract entities. The key idea is to distill object-centric representations of scenes from static images and use it to learn dynamic representations of objects using a relation module to model object-pair interactions. The approach is evaluated on video events reasoning and video prediction tasks. The representaions are trained using existing scene representation methods. The model is trained with object dynamic clues (occlusion and objects collision) as well as object dynamics. Experiments show that the model achieves state-of-the-art performance on segmentation, reconstruction and scene decomposition quality. The paper also shows that ODDN is able to capture object-centric interactions between objects in a scene, which is important for video understanding. "
6275,SP:ba8e50d1fa9cb824fa3f76c0c691997cd151d760,"Graph neural networks ( GNN ) USED-FOR graph - based learning tasks. nodes USED-FOR task. link / motif prediction HYPONYM-OF nodes. random node features CONJUNCTION node distance features. node distance features CONJUNCTION random node features. slow convergence CONJUNCTION inaccurate prediction. inaccurate prediction CONJUNCTION slow convergence. Laplacian Eigenmap CONJUNCTION Deepwalk. Deepwalk CONJUNCTION Laplacian Eigenmap. positional encoding ( PE ) techniques USED-FOR positional features. PE USED-FOR GNNs. positional features USED-FOR GNNs. Laplacian Eigenmap HYPONYM-OF positional encoding ( PE ) techniques. Deepwalk HYPONYM-OF positional encoding ( PE ) techniques. mathematical analysis USED-FOR PEG. PEG HYPONYM-OF GNN layers. mathematical analysis USED-FOR GNN layers. node features CONJUNCTION positional features. positional features CONJUNCTION node features. node features USED-FOR PEG. positional features USED-FOR PEG. permutation equivariance FEATURE-OF PEG. node features CONJUNCTION rotation equivariance. rotation equivariance CONJUNCTION node features. link prediction EVALUATE-FOR PEG. real - world networks USED-FOR link prediction. generalization EVALUATE-FOR PEG. Generic are they, and solution. Metric is complexity. ","Graph neural networks (GNN) have been widely used in graph-based learning tasks, but they are computationally expensive and slow to train. This paper studies the problem of link/motif prediction, which is a task where nodes in a graph are connected to each other and the task is to predict the next node in the graph. The authors propose to use positional encoding (PE) techniques, such as Laplacian Eigenmap and Deepwalk, to encode positional features in GNNs using PE. The paper provides a mathematical analysis of GNN layers, and proposes a solution called PEG, which combines node features, positional features, and rotation equivariance. They show that PEG is permutation equivariant to node features and node distance features. They also show that the complexity of PEG decreases as the number of nodes increases. They demonstrate that the proposed PEG improves link prediction performance on real-world networks, and shows that the generalization performance is improved.","Graph neural networks (GNN) have been widely used in graph-based learning tasks, but they are computationally expensive and slow to train. This paper studies the problem of link/motif prediction, which is a task where nodes in a graph are connected to each other and the task is to predict the next node in the graph. The authors propose to use positional encoding (PE) techniques, such as Laplacian Eigenmap and Deepwalk, to encode positional features in GNNs using PE. The paper provides a mathematical analysis of GNN layers, and proposes a solution called PEG, which combines node features, positional features, and rotation equivariance. They show that PEG is permutation equivariant to node features and node distance features. They also show that the complexity of PEG decreases as the number of nodes increases. They demonstrate that the proposed PEG improves link prediction performance on real-world networks, and shows that the generalization performance is improved."
6300,SP:cf448479f68c3194c1a9e11729bf70d7cc2ae8fd,non - parallel datasets USED-FOR models. non - parallel datasets USED-FOR text style transfer models. weak supervision USED-FOR style transfer models. LaMer HYPONYM-OF text style transfer framework. large - scale language models USED-FOR text style transfer framework. MLE training CONJUNCTION imitation learning refinement. imitation learning refinement CONJUNCTION MLE training. imitation learning refinement USED-FOR intrinsic parallelism. parallel expressions PART-OF non - parallel datasets. intrinsic parallelism FEATURE-OF data. imitation learning refinement USED-FOR LaMer. MLE training USED-FOR LaMer. scene graphs USED-FOR LaMer. scene graphs USED-FOR parallel expressions. content preservation CONJUNCTION fluency. fluency CONJUNCTION content preservation. transfer accuracy CONJUNCTION content preservation. content preservation CONJUNCTION transfer accuracy. task EVALUATE-FOR model. sentiment & formality transfer EVALUATE-FOR model. sentiment & formality transfer CONJUNCTION political stance transfer. political stance transfer CONJUNCTION sentiment & formality transfer. political stance transfer EVALUATE-FOR model. political stance transfer HYPONYM-OF task. fluency EVALUATE-FOR model. content preservation EVALUATE-FOR model. transfer accuracy EVALUATE-FOR model. model COMPARE models. models COMPARE model. OtherScientificTerm is style - independent information. ,"This paper proposes LaMer, a text style transfer framework based on large-scale language models with weak supervision. The authors show that models trained on non-parallel datasets can achieve similar transfer performance as models trained with non - parallel datasets, but without style-independent information. To achieve this, LaMer uses MLE training and imitation learning refinement to encourage intrinsic parallelism in the data. The parallel expressions of the non-parametric datasets are represented as scene graphs. The model is evaluated on the task of sentiment & formality transfer and political stance transfer, and shows that the model achieves better transfer accuracy, content preservation, and fluency. ","This paper proposes LaMer, a text style transfer framework based on large-scale language models with weak supervision. The authors show that models trained on non-parallel datasets can achieve similar transfer performance as models trained with non - parallel datasets, but without style-independent information. To achieve this, LaMer uses MLE training and imitation learning refinement to encourage intrinsic parallelism in the data. The parallel expressions of the non-parametric datasets are represented as scene graphs. The model is evaluated on the task of sentiment & formality transfer and political stance transfer, and shows that the model achieves better transfer accuracy, content preservation, and fluency. "
6325,SP:8f7b2d1020d9e527118b8fb816760c13b0d0bfcb,"Multi - hop logical reasoning HYPONYM-OF representation learning on knowledge graphs ( KGs ). one - hop link prediction CONJUNCTION logical queries. logical queries CONJUNCTION one - hop link prediction. one - hop link prediction PART-OF It. logical queries PART-OF It. classical, triple - based graphs USED-FOR algorithms. hyper - relational modeling paradigm USED-FOR KGs. key - value pairs FEATURE-OF typed edges. approaches USED-FOR approximate query answering ( QA ). Hyper - relational queries PART-OF real - world KG applications. qualifier pairs USED-FOR approaches. hyper - relational KGs USED-FOR complex queries. multi - hop reasoning problem USED-FOR complex queries. multi - hop reasoning problem USED-FOR hyper - relational KGs. Graph Neural Networks CONJUNCTION query embedding techniques. query embedding techniques CONJUNCTION Graph Neural Networks. method USED-FOR queries. qualifiers USED-FOR QA. query patterns USED-FOR QA. query patterns EVALUATE-FOR qualifiers. Generic is paradigm. OtherScientificTerm are fine - grained context, and hyper - relational conjunctive queries. ","Multi-hop logical reasoning is an important problem in representation learning on knowledge graphs (KGs). It combines one-hop link prediction and logical queries. The paper proposes a hyper-relational modeling paradigm for KGs, where the algorithms are based on classical, triple-based graphs. The authors show that existing approaches for approximate query answering (QA) can be improved by using more qualifier pairs. Hyper-rel relational queries are commonly used in real-world KG applications, but this paradigm is not well suited for fine-grained context. In this paper, the authors propose a method to convert queries into hyper-ridiculous conjunctive queries, where each query is represented as a set of key-value pairs between two typed edges. The proposed method is based on Graph Neural Networks and query embedding techniques, and the authors demonstrate that the proposed method can be used to generate queries that are more likely to be answered by a given query. They also show that the query patterns generated by the proposed QA using the proposed query patterns are more similar to those generated by previous methods.   The authors also demonstrate that their method is able to solve the multi-hop reasoning problem for complex queries in hyper-referential KGs. ","Multi-hop logical reasoning is an important problem in representation learning on knowledge graphs (KGs). It combines one-hop link prediction and logical queries. The paper proposes a hyper-relational modeling paradigm for KGs, where the algorithms are based on classical, triple-based graphs. The authors show that existing approaches for approximate query answering (QA) can be improved by using more qualifier pairs. Hyper-rel relational queries are commonly used in real-world KG applications, but this paradigm is not well suited for fine-grained context. In this paper, the authors propose a method to convert queries into hyper-ridiculous conjunctive queries, where each query is represented as a set of key-value pairs between two typed edges. The proposed method is based on Graph Neural Networks and query embedding techniques, and the authors demonstrate that the proposed method can be used to generate queries that are more likely to be answered by a given query. They also show that the query patterns generated by the proposed QA using the proposed query patterns are more similar to those generated by previous methods.   The authors also demonstrate that their method is able to solve the multi-hop reasoning problem for complex queries in hyper-referential KGs. "
6350,SP:5f8b58424a1a8eeb72217e75189d6f773a298a7a,"DYHPO HYPONYM-OF method. Bayesian optimization USED-FOR gray - box setup. Bayesian optimization USED-FOR technique. surrogate USED-FOR Gaussian Processes. multi - budget information FEATURE-OF acquisition function. acquisition function PART-OF surrogate. learning curve dynamics PART-OF surrogate. DYHPO COMPARE hyperparameter optimization baselines. hyperparameter optimization baselines COMPARE DYHPO. Method are Gray - box hyperparameter optimization techniques, and multibudget search mechanisms. OtherScientificTerm is hyperparameter configurations. ","This paper proposes a new method called DYHPO, which is a generalization of existing Gray-box hyperparameter optimization techniques. The technique is based on Bayesian optimization for the gray-box setup. The authors propose a surrogate for Gaussian Processes that incorporates multi-budget information into the acquisition function. The learning curve dynamics of the surrogate is also incorporated into the surrogate. Experiments are conducted to show that the proposed method outperforms existing hyperparameters optimization baselines. The paper also shows that the surrogate can be used as a surrogate to improve the performance of existing multibudget search mechanisms. ","This paper proposes a new method called DYHPO, which is a generalization of existing Gray-box hyperparameter optimization techniques. The technique is based on Bayesian optimization for the gray-box setup. The authors propose a surrogate for Gaussian Processes that incorporates multi-budget information into the acquisition function. The learning curve dynamics of the surrogate is also incorporated into the surrogate. Experiments are conducted to show that the proposed method outperforms existing hyperparameters optimization baselines. The paper also shows that the surrogate can be used as a surrogate to improve the performance of existing multibudget search mechanisms. "
6375,SP:99d3d94e3af5d2dc7b92c00ac1345d1d2dd0d15b,"learned image compression COMPARE image coding techniques. image coding techniques COMPARE learned image compression. rate - distortion EVALUATE-FOR learned image compression. deterministic inference USED-FOR Gaussian mixture models. methods USED-FOR image compression models. cross - platform consistent manner FEATURE-OF image compression models. Method are non - deterministic calculation, and training and fine - tuning based approaches. Task is decoding. OtherScientificTerm are post - training quantization, and entropy parameters. ","This paper studies the rate-distortion of learned image compression compared to standard image coding techniques. The authors argue that the non-deterministic calculation of the quantization of the image compression is the main reason for the performance degradation in decoding and post-training quantization. They propose two training and fine-tuning based approaches to address this issue. The main contribution of the paper is to use deterministic inference to train Gaussian mixture models. The paper also proposes two methods to train image compression models in a cross-platform consistent manner, where the entropy parameters are shared across platforms. ","This paper studies the rate-distortion of learned image compression compared to standard image coding techniques. The authors argue that the non-deterministic calculation of the quantization of the image compression is the main reason for the performance degradation in decoding and post-training quantization. They propose two training and fine-tuning based approaches to address this issue. The main contribution of the paper is to use deterministic inference to train Gaussian mixture models. The paper also proposes two methods to train image compression models in a cross-platform consistent manner, where the entropy parameters are shared across platforms. "
6400,SP:85d0df515e9e555f3ea1c21d607304dfaeae69c0,"nanoscale resolution FEATURE-OF imaging and analysis of cellular ultrastructure. fully unsupervised Noise Reconstruction and Removal Network USED-FOR denoising scanning electron microscopy images. architecture USED-FOR noise. gated recurrent units USED-FOR architecture. sequential data USED-FOR noise. fully unsupervised training USED-FOR network. fully unsupervised training COMPARE supervised approaches. supervised approaches COMPARE fully unsupervised training. 3D electron microscopy data sets EVALUATE-FOR supervised approaches. Material is labels and/or noise - free data sets. OtherScientificTerm are time consuming manual annotations, and imaging artifacts. Metric is empirical metrics. ","This paper proposes a fully unsupervised Noise Reconstruction and Removal Network for denoising scanning electron microscopy images at a nanoscale resolution for imaging and analysis of cellular ultrastructure. The proposed architecture is based on gated recurrent units to remove noise from sequential data. The network is trained using fully un supervised training, where labels and/or noise-free data sets are not available and time consuming manual annotations are required. Experiments are conducted on 3D electron microscopeopy data sets, and show that the proposed network outperforms other supervised approaches in terms of empirical metrics. The paper also shows that the network is robust to imaging artifacts. ","This paper proposes a fully unsupervised Noise Reconstruction and Removal Network for denoising scanning electron microscopy images at a nanoscale resolution for imaging and analysis of cellular ultrastructure. The proposed architecture is based on gated recurrent units to remove noise from sequential data. The network is trained using fully un supervised training, where labels and/or noise-free data sets are not available and time consuming manual annotations are required. Experiments are conducted on 3D electron microscopeopy data sets, and show that the proposed network outperforms other supervised approaches in terms of empirical metrics. The paper also shows that the network is robust to imaging artifacts. "
6425,SP:e6275b0b103fa90dcebcdd3d3c14c830c3402972,"Graph neural networks ( GNNs ) CONJUNCTION label propagation. label propagation CONJUNCTION Graph neural networks ( GNNs ). graph structure USED-FOR tasks. node property prediction HYPONYM-OF tasks. stacked message - passing layers USED-FOR predictive embeddings. stacked message - passing layers USED-FOR node features. neighborhood information FEATURE-OF stacked message - passing layers. stacked message - passing layers USED-FOR former. spreading label information USED-FOR unlabeled nodes. parameter - free diffusion process USED-FOR spreading label information. features CONJUNCTION labels. labels CONJUNCTION features. statistical properties PART-OF training pipeline. label trick USED-FOR training pipeline. deterministic training objective USED-FOR stochastic label trick. data - fitting term USED-FOR label leakage issues. graph structure FEATURE-OF regularization factor. Generic are latter, and two. OtherScientificTerm is GNN inputs. Material is Open Graph Benchmark ( OGB ) leaderboard. Task is label trick use cases. ","Graph neural networks (GNNs) and label propagation have been a popular topic of interest in recent years, but the latter has not been studied as much as the former due to the lack of graph structure in many tasks such as node property prediction. The former uses stacked message-passing layers with neighborhood information to learn predictive embeddings, while the latter uses the latter to learn node features and labels. This paper proposes to combine the two by spreading label information between unlabeled nodes via a parameter-free diffusion process. The paper also proposes a new training pipeline that incorporates the label trick as a deterministic training objective. The main contribution of the paper is that the training pipeline incorporates the statistical properties of the features and the labels into the training objective, and that the stochastic label trick can be used as a data-fitting term to avoid label leakage issues. The regularization factor is based on the graph structure of the training data, and the paper also introduces a regularization term that encourages the GNN inputs to have the same graph structure. Experiments are conducted on the Open Graph Benchmark (OGB) leaderboard, and label trick use cases are also discussed. ","Graph neural networks (GNNs) and label propagation have been a popular topic of interest in recent years, but the latter has not been studied as much as the former due to the lack of graph structure in many tasks such as node property prediction. The former uses stacked message-passing layers with neighborhood information to learn predictive embeddings, while the latter uses the latter to learn node features and labels. This paper proposes to combine the two by spreading label information between unlabeled nodes via a parameter-free diffusion process. The paper also proposes a new training pipeline that incorporates the label trick as a deterministic training objective. The main contribution of the paper is that the training pipeline incorporates the statistical properties of the features and the labels into the training objective, and that the stochastic label trick can be used as a data-fitting term to avoid label leakage issues. The regularization factor is based on the graph structure of the training data, and the paper also introduces a regularization term that encourages the GNN inputs to have the same graph structure. Experiments are conducted on the Open Graph Benchmark (OGB) leaderboard, and label trick use cases are also discussed. "
6450,SP:b6cbc3661f9c440687c3dd01ee35a118c87db377,"Theory of mind ( ToM ) HYPONYM-OF human intelligence. machines USED-FOR theory of mind. ToM agents USED-FOR tasks. predefined roles FEATURE-OF tasks. speaker - listener scenarios HYPONYM-OF predefined roles. strategy USED-FOR SymmToM. theory of mind USED-FOR strategy. theory of mind USED-FOR SymmToM. multi - agent deep reinforcement learning models USED-FOR mental states. modeling of theory of mind USED-FOR multi - agent scenarios. OtherScientificTerm are machine theory of mind, and grid world. Method are multiagent environment SymmToM, and ToM model. ","Theory of mind (ToM) is a core component of human intelligence, and machines can be seen as machines with a machine theory of mind. ToM agents are trained to solve tasks with predefined roles (e.g. speaker-listener scenarios). This paper proposes a new multiagent environment SymmToM, where agents are given a grid world, and are asked to solve a series of tasks that require them to perform well. The authors propose a strategy to learn a strategy based on the theory ofmind, and show that the strategy can be used to learn to solve the tasks in a multi-agent environment. The paper also shows that the proposed strategy can also be used in a single-agent setting.    The authors also show that in a grid-world setting, the proposed method can be combined with existing multiagent deep reinforcement learning models to learn the mental states of multiple agents. They show that this can be done by training a ToM model on the grid world and then fine-tune it on a single agent.  Finally, the authors show how the modeling of theory ofMind can be applied to multi-manual scenarios. ","Theory of mind (ToM) is a core component of human intelligence, and machines can be seen as machines with a machine theory of mind. ToM agents are trained to solve tasks with predefined roles (e.g. speaker-listener scenarios). This paper proposes a new multiagent environment SymmToM, where agents are given a grid world, and are asked to solve a series of tasks that require them to perform well. The authors propose a strategy to learn a strategy based on the theory ofmind, and show that the strategy can be used to learn to solve the tasks in a multi-agent environment. The paper also shows that the proposed strategy can also be used in a single-agent setting.    The authors also show that in a grid-world setting, the proposed method can be combined with existing multiagent deep reinforcement learning models to learn the mental states of multiple agents. They show that this can be done by training a ToM model on the grid world and then fine-tune it on a single agent.  Finally, the authors show how the modeling of theory ofMind can be applied to multi-manual scenarios. "
6475,SP:f8ce83805eee46c6c196e8477bf10d8d7f7e0f46,"visual data collecting and processing technology USED-FOR robots. orientations CONJUNCTION illumination. illumination CONJUNCTION orientations. smart manufacturing CONJUNCTION high - mix - low - volume production. high - mix - low - volume production CONJUNCTION smart manufacturing. Zero - shot object detection HYPONYM-OF unsupervised learning. car CONJUNCTION people. people CONJUNCTION car. bikes CONJUNCTION car. car CONJUNCTION bikes. car HYPONYM-OF outdoor scenes. people HYPONYM-OF outdoor scenes. bikes HYPONYM-OF outdoor scenes. indoor scenes FEATURE-OF zero - shot detection of daily objects. zero - shot detection USED-FOR object size level. dataset EVALUATE-FOR zero - shot detection. Method are robot vision system, vision system, zero - shot object detection, and zero - shot object detection algorithm. Task are manufacturing environment, production process, and detection of daily objects. Generic is it. OtherScientificTerm is manufacturing setup. Material is YCB Video Dataset. ","This paper proposes a robot vision system that can detect objects in a manufacturing environment. The key idea is to use visual data collecting and processing technology to train robots in an unsupervised learning setting. Zero-shot object detection is an important problem in unsuper supervised learning, but it is not well-studied in the context of smart manufacturing and high-mix-low-volume production. This paper proposes to train a vision system to detect objects that are not seen during the entire production process. This is a challenging problem because the production process is very different from the manufacturing setup, and there are many differences in orientations, illumination, etc.    The paper presents a zero-shot detection of daily objects in indoor scenes and indoor scenes, and shows that it is possible to detect daily objects even in the presence of multiple objects in the scene. The paper also shows that zero-shoot detection can be applied to any object size level, and that the detection of every object is possible even in a single scene.  The authors also show that the performance of the proposed method on the YCB Video Dataset is comparable to the state-of-the-art on the same dataset, and they show that it can also be used in the case where the object size is much smaller.  Finally, the paper shows that the proposed model is able to identify objects in an indoor scene from a single image, and it is also able to detect them in an outdoor scene from multiple images. ","This paper proposes a robot vision system that can detect objects in a manufacturing environment. The key idea is to use visual data collecting and processing technology to train robots in an unsupervised learning setting. Zero-shot object detection is an important problem in unsuper supervised learning, but it is not well-studied in the context of smart manufacturing and high-mix-low-volume production. This paper proposes to train a vision system to detect objects that are not seen during the entire production process. This is a challenging problem because the production process is very different from the manufacturing setup, and there are many differences in orientations, illumination, etc.    The paper presents a zero-shot detection of daily objects in indoor scenes and indoor scenes, and shows that it is possible to detect daily objects even in the presence of multiple objects in the scene. The paper also shows that zero-shoot detection can be applied to any object size level, and that the detection of every object is possible even in a single scene.  The authors also show that the performance of the proposed method on the YCB Video Dataset is comparable to the state-of-the-art on the same dataset, and they show that it can also be used in the case where the object size is much smaller.  Finally, the paper shows that the proposed model is able to identify objects in an indoor scene from a single image, and it is also able to detect them in an outdoor scene from multiple images. "
6500,SP:aa1dcd9217270010f16a00004facede942efea17,"generating future frames CONJUNCTION learning environment dynamics. learning environment dynamics CONJUNCTION generating future frames. Video prediction HYPONYM-OF problem. autoregressive prediction model USED-FOR image generator. autoregressive latent video models USED-FOR video prediction tool. autoregressive latent video prediction model USED-FOR high - fidelity future frames. autoregressive latent video prediction model USED-FOR high - resolution ( 256x256 ) videos. top - k sampling CONJUNCTION data augmentation. data augmentation CONJUNCTION top - k sampling. data augmentation USED-FOR video prediction quality. top - k sampling USED-FOR video prediction quality. method COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE method. method USED-FOR highresolution video prediction. complex and large - scale datasets USED-FOR highresolution video prediction. video prediction benchmarks EVALUATE-FOR state - of - the - art approaches. video prediction benchmarks EVALUATE-FOR method. Task are video prediction, and predicting high - fidelity future frames. Method are image generator model, prior models, and causal transformer model. OtherScientificTerm is latent space of the image generator. Generic is models. ","This paper tackles the problem of video prediction, i. Video prediction is a very important problem in the field of machine learning, both in terms of generating future frames and learning environment dynamics, but also in the sense of predicting high-fidelity future frames. The paper proposes a video prediction tool based on autoregressive latent video models, where the image generator model is trained to generate high-resolution (256x256) videos, and the autorespective latent video prediction model is used to predict high- fidelity future frames from the latent space of the image generation model. The authors propose to use a causal transformer model, which is a generalization of prior models, to learn a sequence of models that can be used to generate future video frames. They also propose to train a top-k sampling and data augmentation to improve the video prediction quality. The proposed method outperforms state-of-the-art approaches on several video prediction benchmarks on complex and large-scale datasets. ","This paper tackles the problem of video prediction, i. Video prediction is a very important problem in the field of machine learning, both in terms of generating future frames and learning environment dynamics, but also in the sense of predicting high-fidelity future frames. The paper proposes a video prediction tool based on autoregressive latent video models, where the image generator model is trained to generate high-resolution (256x256) videos, and the autorespective latent video prediction model is used to predict high- fidelity future frames from the latent space of the image generation model. The authors propose to use a causal transformer model, which is a generalization of prior models, to learn a sequence of models that can be used to generate future video frames. They also propose to train a top-k sampling and data augmentation to improve the video prediction quality. The proposed method outperforms state-of-the-art approaches on several video prediction benchmarks on complex and large-scale datasets. "
6525,SP:7f57896afd63bc869d2db6ddf7abbeaa71daae11,vision - specific inductive biases USED-FOR Vision Transformers ( ViTs ). image recognition EVALUATE-FOR Vision Transformers ( ViTs ). ViT architecture PART-OF generative adversarial networks ( GANs ). regularization methods USED-FOR GANs. regularization methods USED-FOR self - attention. regularization methods USED-FOR ViT discriminators. regularization techniques USED-FOR GANs. ViTs USED-FOR GANs. CIFAR-10 CONJUNCTION CelebA. CelebA CONJUNCTION CIFAR-10. approach COMPARE CNNbased GAN models. CNNbased GAN models COMPARE approach. CelebA CONJUNCTION LSUN bedroom. LSUN bedroom CONJUNCTION CelebA. ViTGAN HYPONYM-OF approach. LSUN bedroom HYPONYM-OF datasets. datasets EVALUATE-FOR CNNbased GAN models. CIFAR-10 HYPONYM-OF datasets. datasets EVALUATE-FOR approach. CelebA HYPONYM-OF datasets. Task is image generation. Method is ViT generators. OtherScientificTerm is latent and pixel mapping layers. ,"Vision Transformers (ViTs) are well-known for their vision-specific inductive biases in image recognition. However, the ViT architecture can be seen as a special case of generative adversarial networks (GANs), where the goal is to improve the image generation performance. The authors propose a new approach, called ViT-GAN, which is based on the observation that ViT discriminators can be trained with different regularization methods than standard GANs, and that the self-attention of ViT generators is biased towards the latent and pixel mapping layers of the image. They show that existing regularization techniques can be applied to regularize the discriminators of ViTs to be more robust to the discriminator of the ViTs. The proposed approach, ViTGAN, is evaluated on two datasets (CIFAR-10, CelebA, and LSUN bedroom) and compared with standard CNNbased GAN models on all three datasets.","Vision Transformers (ViTs) are well-known for their vision-specific inductive biases in image recognition. However, the ViT architecture can be seen as a special case of generative adversarial networks (GANs), where the goal is to improve the image generation performance. The authors propose a new approach, called ViT-GAN, which is based on the observation that ViT discriminators can be trained with different regularization methods than standard GANs, and that the self-attention of ViT generators is biased towards the latent and pixel mapping layers of the image. They show that existing regularization techniques can be applied to regularize the discriminators of ViTs to be more robust to the discriminator of the ViTs. The proposed approach, ViTGAN, is evaluated on two datasets (CIFAR-10, CelebA, and LSUN bedroom) and compared with standard CNNbased GAN models on all three datasets."
6550,SP:bbae3afcaea0a2e54904cb8daaed7df4fe37da6e,"sample quality EVALUATE-FOR models. variational autoencoders USED-FOR image generative modeling. entropy FEATURE-OF natural image distributions. visually imperceptible information FEATURE-OF entropy. models USED-FOR competitive likelihoods. imperceptible information PART-OF likelihood signal. good sample quality EVALUATE-FOR modeling of visually perceptible information. OtherScientificTerm are Good likelihoods, high - dimensional image data distributions, and visually perceptible bits. Task is generative modeling. ","This paper studies the problem of image generative modeling with variational autoencoders. The authors show that the sample quality of models trained with competitive likelihoods (good likelihoods) is a function of the entropy of natural image distributions with visually imperceptible information. Good likelihoods can be obtained from high-dimensional image data distributions. The paper also shows that models trained to learn competitively with these models can be trained with good sample quality. The main contribution of the paper is the observation that the modeling of visually perceptible information in the likelihood signal can lead to a good trade-off between good and bad sample quality, and that this tradeoff is more pronounced in the case of high-dimensionality of the data.    The paper is well-written, well-motivated, and well-structured. The motivation is clear. The work is well motivated, and the results are interesting. However, there are a few concerns that need to be addressed in order for the paper to be accepted as a contribution to the field of generative modelling.","This paper studies the problem of image generative modeling with variational autoencoders. The authors show that the sample quality of models trained with competitive likelihoods (good likelihoods) is a function of the entropy of natural image distributions with visually imperceptible information. Good likelihoods can be obtained from high-dimensional image data distributions. The paper also shows that models trained to learn competitively with these models can be trained with good sample quality. The main contribution of the paper is the observation that the modeling of visually perceptible information in the likelihood signal can lead to a good trade-off between good and bad sample quality, and that this tradeoff is more pronounced in the case of high-dimensionality of the data.    The paper is well-written, well-motivated, and well-structured. The motivation is clear. The work is well motivated, and the results are interesting. However, there are a few concerns that need to be addressed in order for the paper to be accepted as a contribution to the field of generative modelling."
6575,SP:bfed56018134ec66cde9a7e958df964d4cca3164,"Diffusion probabilistic models ( DPMs ) HYPONYM-OF generative models. inference USED-FOR variance. reverse process FEATURE-OF variance. optimal reverse variance CONJUNCTION optimal KL divergence. optimal KL divergence CONJUNCTION optimal reverse variance. optimal KL divergence FEATURE-OF DPM. optimal reverse variance FEATURE-OF DPM. analytic forms FEATURE-OF optimal KL divergence. analytic forms FEATURE-OF optimal reverse variance. Monte Carlo method CONJUNCTION pretrained score - based model. pretrained score - based model CONJUNCTION Monte Carlo method. Analytic - DPM HYPONYM-OF training - free inference framework. training - free inference framework USED-FOR analytic forms. Analytic - DPM USED-FOR analytic forms. Monte Carlo method USED-FOR Analytic - DPM. pretrained score - based model USED-FOR Analytic - DPM. analytic - DPM USED-FOR DPMs. analytic - DPM USED-FOR log - likelihood. log - likelihood FEATURE-OF DPMs. Task is inference of DPMs. OtherScientificTerm are score function, and lower and upper bounds. Method is score - based model. Generic is estimate. ","Diffusion probabilistic models (DPMs) are a class of generative models in which the inference of DPMs is performed in an unsupervised way, and the variance of the variance in the reverse process during inference depends on the score function. This paper studies the optimal reverse variance and optimal KL divergence of a DPM. The authors propose a training-free inference framework, called Analytic-DPM, to obtain analytic forms of these two analytic forms, which is a combination of a Monte Carlo method and a pretrained score-based model. They show that the analytic form of the DPM can be derived from the training data, and that this analytic form can be used to derive lower and upper bounds on the variance. They also show that, in practice, the analytic DPM is able to recover the log-likelihood of the optimal DPM, and can also be used as an estimate of the log likelihood of the underlying score function, which can be then used to improve the efficiency of the inference.   ","Diffusion probabilistic models (DPMs) are a class of generative models in which the inference of DPMs is performed in an unsupervised way, and the variance of the variance in the reverse process during inference depends on the score function. This paper studies the optimal reverse variance and optimal KL divergence of a DPM. The authors propose a training-free inference framework, called Analytic-DPM, to obtain analytic forms of these two analytic forms, which is a combination of a Monte Carlo method and a pretrained score-based model. They show that the analytic form of the DPM can be derived from the training data, and that this analytic form can be used to derive lower and upper bounds on the variance. They also show that, in practice, the analytic DPM is able to recover the log-likelihood of the optimal DPM, and can also be used as an estimate of the log likelihood of the underlying score function, which can be then used to improve the efficiency of the inference.   "
6600,SP:3f935ba5784c3e86db72421426bc479061af1a4b,"vision transformers ( ViTs ) COMPARE CNNs. CNNs COMPARE vision transformers ( ViTs ). transformer - based models USED-FOR medical image classification. CNNs CONJUNCTION transformers. transformers CONJUNCTION CNNs. medical image benchmark datasets CONJUNCTION tasks. tasks CONJUNCTION medical image benchmark datasets. vision transformers COMPARE CNNs. CNNs COMPARE vision transformers. CNNs COMPARE vision transformers. vision transformers COMPARE CNNs. ImageNet EVALUATE-FOR CNNs. Method is Convolutional Neural Networks ( CNNs ). Task are automated medical image diagnosis, classification, detection and segmentation tasks, medical imaging tasks, and supervised and self - supervised setting. Material is natural image domain. ","This paper investigates the performance of vision transformers (ViTs) over Convolutional Neural Networks (CNNs) in the context of automated medical image diagnosis. In particular, the authors consider classification, detection and segmentation tasks. They show that ViTs outperform CNNs and transformers on a number of medical imaging tasks. The authors also show that transformer-based models can be used for medical image classification in a supervised and self-supervised setting.   The authors conduct experiments on several medical image benchmark datasets and tasks, and show that the results are consistent across different medical imaging benchmark datasets. They also find that the performance on the natural image domain is comparable to CNNs on ImageNet. ","This paper investigates the performance of vision transformers (ViTs) over Convolutional Neural Networks (CNNs) in the context of automated medical image diagnosis. In particular, the authors consider classification, detection and segmentation tasks. They show that ViTs outperform CNNs and transformers on a number of medical imaging tasks. The authors also show that transformer-based models can be used for medical image classification in a supervised and self-supervised setting.   The authors conduct experiments on several medical image benchmark datasets and tasks, and show that the results are consistent across different medical imaging benchmark datasets. They also find that the performance on the natural image domain is comparable to CNNs on ImageNet. "
6625,SP:a64e0535f268901e38fd51e027c612ebcdbae1a4,"pretrained NLM COMPARE it. it COMPARE pretrained NLM. NLM training heuristics USED-FOR pretraining and fine - tuning stages. NLM pretraining USED-FOR Natural Language Understanding tasks. sentence representations CONJUNCTION open domain question answering abilities. open domain question answering abilities CONJUNCTION sentence representations. Method are Pretraining Neural Language Models ( NLMs ), neural architecture, pretraining example design, and self - improving representations. OtherScientificTerm is semantically related non - neighboring sentences. ","This paper studies the problem of pretraining Neural Language Models (NLM) in the context of natural language understanding (NLI) tasks. In particular, the authors consider the setting where the neural architecture of a pretrained NLM is different from that of a fully-trained NLM, and show that it can be trained in a similar way as a fully trained NLM. The authors propose two NLM training heuristics for both the pretraining and fine-tuning stages: (1) pretraining example design, where semantically related non-neighboring sentences are used to guide the training, and (2) self-improving representations, where self-improvement is used to improve the quality of the learned representations. Experiments are conducted on NLM pretraining on Natural Language Understanding tasks and show improvements in sentence representations and open domain question answering abilities. ","This paper studies the problem of pretraining Neural Language Models (NLM) in the context of natural language understanding (NLI) tasks. In particular, the authors consider the setting where the neural architecture of a pretrained NLM is different from that of a fully-trained NLM, and show that it can be trained in a similar way as a fully trained NLM. The authors propose two NLM training heuristics for both the pretraining and fine-tuning stages: (1) pretraining example design, where semantically related non-neighboring sentences are used to guide the training, and (2) self-improving representations, where self-improvement is used to improve the quality of the learned representations. Experiments are conducted on NLM pretraining on Natural Language Understanding tasks and show improvements in sentence representations and open domain question answering abilities. "
6650,SP:59066956fa2e423d5f2d2ea4f91c4ddf6afd4683,"L2O models USED-FOR optimization rules. neural networks USED-FOR L2O models. neural networks USED-FOR optimization rules. meta - training USED-FOR numerical rules. optimization rule USED-FOR L2O model. neural networks USED-FOR numerical rules. holistic symbolic representation and analysis framework USED-FOR L2O. L2O model COMPARE human - designed and tuned optimizers. human - designed and tuned optimizers COMPARE L2O model. large - scale problems EVALUATE-FOR L2O model. Task are Learning to Optimize ( L2O ), optimization procedure, and L2O research. Metric are scalability, and interpretability. OtherScientificTerm is memory overhead. Method is symbolic regression. ","This paper introduces Learning to Optimize (L2O), a method for learning to optimize a set of optimization rules. L2O models use neural networks to learn optimization rules using neural networks. The authors propose a holistic symbolic representation and analysis framework to learn the optimization rule for a given optimization procedure. They show that meta-training can be used to learn numerical rules from neural networks, and that the optimization rules learned by the neural networks can be applied to any optimization rule. They also show that the learned optimization rule can be interpreted as an optimization rule, which is useful for scalability, interpretability, and memory overhead.  The authors show that their L2o model outperforms human-designed and tuned optimizers on a number of large-scale problems, and is able to generalize well to new optimization rules that are not learned during training.    The paper is well-written, well-motivated, and well-structured. The motivation of the work is clear, and the contributions of the paper are solid. The contributions of this paper are:  1. Introducing a new way of learning a symbolic regression to learn a rule, 2. A holistic symbolic symbolic representation of the optimization process, 3. A new optimization rule is learned, and 4. A thorough analysis of the benefits and drawbacks of the proposed method, and a thorough comparison with the state-of-the-art.  This paper is a good contribution to the field of learning to optimizers, and it is an interesting contribution to improve the state of the art in the field. However, there are a few concerns that need to be addressed in the paper:  - 1. It is not clear what the contributions are in this paper.  2. There is a lack of comparison with previous work in this area.  3. There are some questions about the benefits of L2Os in terms of scalability and interpretability.  4. It would be good to see a more detailed discussion of the limitations of the previous work.  Overall, the paper is clearly written and well written. The paper has a good amount of interesting contributions. However there are some concerns about the lack of comparisons with prior work in the literature. ","This paper introduces Learning to Optimize (L2O), a method for learning to optimize a set of optimization rules. L2O models use neural networks to learn optimization rules using neural networks. The authors propose a holistic symbolic representation and analysis framework to learn the optimization rule for a given optimization procedure. They show that meta-training can be used to learn numerical rules from neural networks, and that the optimization rules learned by the neural networks can be applied to any optimization rule. They also show that the learned optimization rule can be interpreted as an optimization rule, which is useful for scalability, interpretability, and memory overhead.  The authors show that their L2o model outperforms human-designed and tuned optimizers on a number of large-scale problems, and is able to generalize well to new optimization rules that are not learned during training.    The paper is well-written, well-motivated, and well-structured. The motivation of the work is clear, and the contributions of the paper are solid. The contributions of this paper are:  1. Introducing a new way of learning a symbolic regression to learn a rule, 2. A holistic symbolic symbolic representation of the optimization process, 3. A new optimization rule is learned, and 4. A thorough analysis of the benefits and drawbacks of the proposed method, and a thorough comparison with the state-of-the-art.  This paper is a good contribution to the field of learning to optimizers, and it is an interesting contribution to improve the state of the art in the field. However, there are a few concerns that need to be addressed in the paper:  - 1. It is not clear what the contributions are in this paper.  2. There is a lack of comparison with previous work in this area.  3. There are some questions about the benefits of L2Os in terms of scalability and interpretability.  4. It would be good to see a more detailed discussion of the limitations of the previous work.  Overall, the paper is clearly written and well written. The paper has a good amount of interesting contributions. However there are some concerns about the lack of comparisons with prior work in the literature. "
6675,SP:54dfeb363beee9959aecc9e0853ff06e43bd94e4,"provable adversarial robustness FEATURE-OF deep neural networks ( DNNs ). provable adversarial robustness USED-FOR static supervised learning tasks. image classification HYPONYM-OF static supervised learning tasks. DNNs USED-FOR real - world adaptive tasks. reinforcement learning ( RL ) HYPONYM-OF real - world adaptive tasks. methods USED-FOR static setting. provable robustness FEATURE-OF RL. RL adversary USED-FOR defense strategy. procedure USED-FOR adaptive RL adversary. worst - case scenario USED-FOR certificates. Pong CONJUNCTION Freeway. Freeway CONJUNCTION Pong. Freeway CONJUNCTION Mountain Car. Mountain Car CONJUNCTION Freeway. Cartpole CONJUNCTION Pong. Pong CONJUNCTION Cartpole. environments EVALUATE-FOR method. robustness guarantees EVALUATE-FOR method. Mountain Car HYPONYM-OF environments. Cartpole HYPONYM-OF environments. Freeway HYPONYM-OF environments. Pong HYPONYM-OF environments. Generic are systems, and attacks. OtherScientificTerm are adversarial attacks, non - adaptive adversary, policy, Neyman - Pearson Lemma, adversarial perturbation, Gaussian noise, policy function, and robustness certificates. Task is randomized smoothing based defenses. Method are smoothingbased certificates, and policy smoothing. ","This paper studies the problem of provable adversarial robustness of deep neural networks (DNNs) in the setting of static supervised learning tasks (e.g., image classification). The authors consider two real-world adaptive tasks where DNNs are trained to be robust to adversarial attacks, i.e., reinforcement learning (RL) and policy optimization (policy smoothing). In RL, the goal is to train systems that are provably robust to a non-adaptive adversary. In the static setting, the authors propose two methods: (1) randomized smoothing based defenses, and (2) smoothingbased certificates. The authors show that these two methods are provable in the case of RL. In addition, they show that a defense strategy against an RL adversary can be formulated as a defense against an adaptive RL adversary, where the non-adversarial adversary is the policy.    The main contribution of the paper is the introduction of the Neyman-Pearson Lemma (Neyman et al., 2018), which states that under certain conditions, a policy can be robust against an adversarial perturbation (i.e. that the policy function is provably non-smoothed). The paper then proposes a procedure to obtain such provable robustness certificates. In order to obtain these certificates, the paper proposes a worst-case scenario in which the policy smoothing is used. The paper also shows that the robustness guarantees of the proposed method can be obtained in three environments: Cartpole, Pong, and Freeway.  The authors also provide a theoretical analysis of their method. ","This paper studies the problem of provable adversarial robustness of deep neural networks (DNNs) in the setting of static supervised learning tasks (e.g., image classification). The authors consider two real-world adaptive tasks where DNNs are trained to be robust to adversarial attacks, i.e., reinforcement learning (RL) and policy optimization (policy smoothing). In RL, the goal is to train systems that are provably robust to a non-adaptive adversary. In the static setting, the authors propose two methods: (1) randomized smoothing based defenses, and (2) smoothingbased certificates. The authors show that these two methods are provable in the case of RL. In addition, they show that a defense strategy against an RL adversary can be formulated as a defense against an adaptive RL adversary, where the non-adversarial adversary is the policy.    The main contribution of the paper is the introduction of the Neyman-Pearson Lemma (Neyman et al., 2018), which states that under certain conditions, a policy can be robust against an adversarial perturbation (i.e. that the policy function is provably non-smoothed). The paper then proposes a procedure to obtain such provable robustness certificates. In order to obtain these certificates, the paper proposes a worst-case scenario in which the policy smoothing is used. The paper also shows that the robustness guarantees of the proposed method can be obtained in three environments: Cartpole, Pong, and Freeway.  The authors also provide a theoretical analysis of their method. "
6700,SP:e0f9add5fde18eaab0eeb2b10b14928acc8ec5b8,"labeled source data CONJUNCTION unlabeled target data. unlabeled target data CONJUNCTION labeled source data. methods USED-FOR predicting the target domain accuracy. labeled source data USED-FOR methods. unlabeled target data USED-FOR methods. method USED-FOR threshold. BREEDS CONJUNCTION CIFAR. CIFAR CONJUNCTION BREEDS. ImageNet CONJUNCTION BREEDS. BREEDS CONJUNCTION ImageNet. WILDS CONJUNCTION ImageNet. ImageNet CONJUNCTION WILDS. CIFAR CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR. ATC COMPARE methods. methods COMPARE ATC. synthetic corruptions CONJUNCTION dataset reproduction. dataset reproduction CONJUNCTION synthetic corruptions. ImageNet CONJUNCTION CIFAR. CIFAR CONJUNCTION ImageNet. WILDS CONJUNCTION BREEDS. BREEDS CONJUNCTION WILDS. model architectures EVALUATE-FOR methods. datasets EVALUATE-FOR ATC. dataset reproduction HYPONYM-OF distribution shifts. WILDS HYPONYM-OF datasets. MNIST HYPONYM-OF datasets. CIFAR HYPONYM-OF datasets. BREEDS HYPONYM-OF datasets. ImageNet HYPONYM-OF datasets. ATC COMPARE prior methods. prior methods COMPARE ATC. toy distributions USED-FOR method. Task is Real - world machine learning deployments. Method is Average Thresholded Confidence ( ATC ). OtherScientificTerm are model ’s confidence, and model confidence. Metric is predicting accuracy. Generic are problem, and it. ","Real-world machine learning deployments are becoming more challenging due to distribution shifts in the source and target domain. The authors propose a new method called Average Thresholded Confidence (ATC) to improve the model’s confidence in predicting the target domain accuracy. Previous methods for the problem rely on the use of both labeled source data and unlabeled target data, while ATC focuses on the problem of only using one of the two. The proposed method, called ATC, is based on the idea that the threshold for predicting accuracy is a function of the model confidence on the source domain. ATC is evaluated on a number of datasets (WILDS, ImageNet, BREEDS, CIFAR, MNIST) and model architectures, and shows that ATC outperforms prior methods on most of the datasets. The method is also tested on toy distributions, and ATC shows better performance than prior methods when the distribution shifts are due to synthetic corruptions, dataset reproduction, etc.","Real-world machine learning deployments are becoming more challenging due to distribution shifts in the source and target domain. The authors propose a new method called Average Thresholded Confidence (ATC) to improve the model’s confidence in predicting the target domain accuracy. Previous methods for the problem rely on the use of both labeled source data and unlabeled target data, while ATC focuses on the problem of only using one of the two. The proposed method, called ATC, is based on the idea that the threshold for predicting accuracy is a function of the model confidence on the source domain. ATC is evaluated on a number of datasets (WILDS, ImageNet, BREEDS, CIFAR, MNIST) and model architectures, and shows that ATC outperforms prior methods on most of the datasets. The method is also tested on toy distributions, and ATC shows better performance than prior methods when the distribution shifts are due to synthetic corruptions, dataset reproduction, etc."
6725,SP:e748bf6ee653087cae825df32a8546f9ccebfcf1,"registration USED-FOR transformation. outliers CONJUNCTION unknown non - rigid deformations. unknown non - rigid deformations CONJUNCTION outliers. outliers FEATURE-OF robustness. partial distribution matching ( PDM ) problem USED-FOR registration problem. method USED-FOR large scale PDM problem. partial Wasserstein-1 ( PW ) discrepancy USED-FOR method. Kantorovich – Rubinstein duality USED-FOR PW discrepancy. partial Wasserstein adversarial network ( PWAN ) USED-FOR PW discrepancy. neural network USED-FOR partial Wasserstein adversarial network ( PWAN ). coherence regularizer USED-FOR non - rigid transformations. coherence regularizer USED-FOR unrealistic deformations. coherence regularizer PART-OF It. PWAN COMPARE state - of - the - art methods. state - of - the - art methods COMPARE PWAN. point set registration tasks EVALUATE-FOR PWAN. point set registration tasks EVALUATE-FOR PWAN. Generic are task, and network. OtherScientificTerm are discrete distributions, and gradient. ","This paper considers the problem of registration for a given transformation, which is an important task in the context of robustness against outliers and unknown non-rigid deformations. The registration problem is formulated as a partial distribution matching (PDM) problem, and the authors propose a method to solve the large scale PDM problem based on the partial Wasserstein-1 (PW) discrepancy. The proposed method is based on a neural network that is trained to minimize the PW discrepancy using the Kantorovich–Rubinstein duality. It also introduces a coherence regularizer to prevent unrealistic deformations, and a non-rigid transformations. The authors show that the proposed partial WASSERSTEIN adversarial network (WPAN) is able to minimize this PW discrepancy with the help of the neural network. The network is trained by minimizing the gradient of the loss on discrete distributions. Experiments on point set registration tasks show that PWAN outperforms state-of-the-art methods. ","This paper considers the problem of registration for a given transformation, which is an important task in the context of robustness against outliers and unknown non-rigid deformations. The registration problem is formulated as a partial distribution matching (PDM) problem, and the authors propose a method to solve the large scale PDM problem based on the partial Wasserstein-1 (PW) discrepancy. The proposed method is based on a neural network that is trained to minimize the PW discrepancy using the Kantorovich–Rubinstein duality. It also introduces a coherence regularizer to prevent unrealistic deformations, and a non-rigid transformations. The authors show that the proposed partial WASSERSTEIN adversarial network (WPAN) is able to minimize this PW discrepancy with the help of the neural network. The network is trained by minimizing the gradient of the loss on discrete distributions. Experiments on point set registration tasks show that PWAN outperforms state-of-the-art methods. "
6750,SP:f94f77696d100b2638fa2a6d82c8df47db3b6a36,"Hyperparameter optimization ( HPO ) PART-OF machine learning models. transfer learning USED-FOR HPO. approaches COMPARE Deep Kernel Gaussian Process surrogate. Deep Kernel Gaussian Process surrogate COMPARE approaches. Landmark Meta - features ( DKLM ) PART-OF Deep Kernel Gaussian Process surrogate. DKLM USED-FOR similarity between hyperparameter configurations. DKLM USED-FOR contextualized dataset - specific similarity representations. DKLM USED-FOR hyperparameter configurations. method COMPARE stateof - the - art baselines. stateof - the - art baselines COMPARE method. OpenML FEATURE-OF HPO meta - datasets. HPO meta - datasets EVALUATE-FOR DKLM. Generic is it. OtherScientificTerm are hyperparameter evaluations, and evaluated configurations. ","Hyperparameter optimization (HPO) is an important problem in machine learning models, and it is important for transfer learning to be considered in the context of HPO. Previous approaches have been based on the Deep Kernel Gaussian Process surrogate, which incorporates the Landmark Meta-features (DKLM) in the hyperparameter evaluations. This paper proposes to use DKLM to capture the similarity between hyperparameters configurations across different datasets, and to use contextualized dataset-specific similarity representations based on DKLM. Experiments on three HPO meta-datasets from OpenML show that the proposed method outperforms stateof-the-art baselines in terms of performance on evaluated configurations.","Hyperparameter optimization (HPO) is an important problem in machine learning models, and it is important for transfer learning to be considered in the context of HPO. Previous approaches have been based on the Deep Kernel Gaussian Process surrogate, which incorporates the Landmark Meta-features (DKLM) in the hyperparameter evaluations. This paper proposes to use DKLM to capture the similarity between hyperparameters configurations across different datasets, and to use contextualized dataset-specific similarity representations based on DKLM. Experiments on three HPO meta-datasets from OpenML show that the proposed method outperforms stateof-the-art baselines in terms of performance on evaluated configurations."
6775,SP:e3c57f3589e8ab674644d900c14b3473cd71a23f,"Generated data COMPARE real data. real data COMPARE Generated data. technology USED-FOR deep fakes. technology USED-FOR misinformation. 128 - bit fingerprint USED-FOR identifiable models. deep fake detection CONJUNCTION attribution. attribution CONJUNCTION deep fake detection. method USED-FOR deep fake detection. method USED-FOR attribution. fingerprinting mechanism USED-FOR method. Method are deep generative models, deep fake detection methods, and generative models. Generic are work, and technique. OtherScientificTerm is fingerprint. ","This paper proposes a new method for detecting fakes in deep generative models. Generated data is more likely to contain fakes than real data, and the authors propose a new technology to detect deep fakes. The proposed work is based on the observation that deep fake detection methods often fail to distinguish between real and fake data. The authors propose to use a 128-bit fingerprint to identify identifiable models. They also propose a method for both deepfake detection and attribution based on this fingerprinting mechanism. They show that their technique can detect fakes even when the number of fakes is small, and that their method can be applied to a wide range of generative methods.","This paper proposes a new method for detecting fakes in deep generative models. Generated data is more likely to contain fakes than real data, and the authors propose a new technology to detect deep fakes. The proposed work is based on the observation that deep fake detection methods often fail to distinguish between real and fake data. The authors propose to use a 128-bit fingerprint to identify identifiable models. They also propose a method for both deepfake detection and attribution based on this fingerprinting mechanism. They show that their technique can detect fakes even when the number of fakes is small, and that their method can be applied to a wide range of generative methods."
6800,SP:73bffd1a0856b80d29f7a2b2b68be57882531f07,"Post - hoc explanations USED-FOR black box models. Post - hoc explanations PART-OF classification and regression settings. model agnostic local explanations USED-FOR similarity learners. tabular and text data USED-FOR model agnostic local explanations. tabular and text data USED-FOR similarity learners. method USED-FOR feature attributions. analogies USED-FOR machine learning. analogies USED-FOR explanation. explanation PART-OF machine learning. ( latent ) factors USED-FOR model. feature attributions USED-FOR analogies. submodular FEATURE-OF analogy objective function. Generic are models, and approaches. Method are black box similarity learner, and sentence encoder. OtherScientificTerm are similarity, and complementarity. Task is healthcare utilization application. ","Post-hoc explanations for black box models are a common topic in classification and regression settings, and this paper studies the problem of model agnostic local explanations for similarity learners on tabular and text data. The authors propose a method to learn feature attributions for analogies in machine learning, which are then used to train a black box similarity learner. The main idea is to learn an analogy objective function that is submodular, i.e., the similarity between a sentence and a sentence encoder is equal to the complementarity between the two sentences. The model is then trained on a set of (latent) factors, and the proposed method is shown to be able to learn analogies that can be used as an explanation for any explanation in the context of machine learning. Experiments are conducted on a healthcare utilization application, where a model is trained on multiple datasets, and a healthcare provider is asked to provide an explanation based on a single dataset. Results show that the proposed approaches are able to achieve competitive performance.","Post-hoc explanations for black box models are a common topic in classification and regression settings, and this paper studies the problem of model agnostic local explanations for similarity learners on tabular and text data. The authors propose a method to learn feature attributions for analogies in machine learning, which are then used to train a black box similarity learner. The main idea is to learn an analogy objective function that is submodular, i.e., the similarity between a sentence and a sentence encoder is equal to the complementarity between the two sentences. The model is then trained on a set of (latent) factors, and the proposed method is shown to be able to learn analogies that can be used as an explanation for any explanation in the context of machine learning. Experiments are conducted on a healthcare utilization application, where a model is trained on multiple datasets, and a healthcare provider is asked to provide an explanation based on a single dataset. Results show that the proposed approaches are able to achieve competitive performance."
6825,SP:6a3c4ae05d582f8896840483b08c735ced2976bc,"deep neural networks ( DNN ) USED-FOR adversarial examples. empirical and theoretical defense approaches USED-FOR single ML model. robustness EVALUATE-FOR ensemble protocols. certified robustness EVALUATE-FOR ensemble ML models. ensemble models COMPARE single model. single model COMPARE ensemble models. ensemble models COMPARE single model. single model COMPARE ensemble models. certified robustness EVALUATE-FOR ensemble models. diversified gradient CONJUNCTION confidence margin. confidence margin CONJUNCTION diversified gradient. diversified gradient USED-FOR ensemble models. Ensemble - before - Smoothing strategy USED-FOR bounded model - smoothness analysis. ensemble model COMPARE single base model. single base model COMPARE ensemble model. certified robustness EVALUATE-FOR single base model. certified robustness EVALUATE-FOR ensemble model. lightweight Diversity Regularized Training ( DRT ) USED-FOR certifiably robust ensemble ML models. DRT enhanced ensembles COMPARE single and ensemble ML models. single and ensemble ML models COMPARE DRT enhanced ensembles. certified robustness EVALUATE-FOR single and ensemble ML models. ImageNet datasets EVALUATE-FOR certified L2 - robustness. certified robustness EVALUATE-FOR DRT enhanced ensembles. Method is DNNs. OtherScientificTerm are perturbations, and model - smoothness assumption. ","This paper studies the robustness of deep neural networks (DNNs) against adversarial examples. The authors propose empirical and theoretical defense approaches to certify a single ML model is robust to perturbations in the input space. They show that certified robustness for ensemble protocols can be obtained by comparing ensemble models trained with diversified gradient and confidence margin. They also provide a bounded model-smootness analysis based on the Ensemble-before-Smoothing strategy.    The authors also show that ensemble models are more robust to adversarial attacks than a single model. They further show that an ensemble model trained with DRT is more robust than the single base model. Finally, they propose a lightweight Diversity Regularized Training (DRT) for certifiably robust ensemble ML models. DRT enhanced ensembles are shown to be more robust in terms of certified L2-robustness on the ImageNet datasets, and they show that a single ensemble model can be as robust as the single model under the same model-moothness assumption. ","This paper studies the robustness of deep neural networks (DNNs) against adversarial examples. The authors propose empirical and theoretical defense approaches to certify a single ML model is robust to perturbations in the input space. They show that certified robustness for ensemble protocols can be obtained by comparing ensemble models trained with diversified gradient and confidence margin. They also provide a bounded model-smootness analysis based on the Ensemble-before-Smoothing strategy.    The authors also show that ensemble models are more robust to adversarial attacks than a single model. They further show that an ensemble model trained with DRT is more robust than the single base model. Finally, they propose a lightweight Diversity Regularized Training (DRT) for certifiably robust ensemble ML models. DRT enhanced ensembles are shown to be more robust in terms of certified L2-robustness on the ImageNet datasets, and they show that a single ensemble model can be as robust as the single model under the same model-moothness assumption. "
6850,SP:3002b29c27709780238876d8c3f81bbd6a0f8112,"message passing Graph Neural Networks ( GNNs ) USED-FOR learning with graphs. expressive power EVALUATE-FOR higherorder GNNs. strategies CONJUNCTION lower bounds. lower bounds CONJUNCTION strategies. computational cost CONJUNCTION expressive power. expressive power CONJUNCTION computational cost. model USED-FOR subgraphs. recursive pooling COMPARE higher - order GNNs. higher - order GNNs COMPARE recursive pooling. computational complexity EVALUATE-FOR higher - order GNNs. sparsity USED-FOR recursive pooling. computational complexity EVALUATE-FOR recursive pooling. near ) matching information - theoretic lower bound USED-FOR counting subgraphs. graph representations USED-FOR representations of derived ( sub-)graphs. graph representations USED-FOR near ) matching information - theoretic lower bound. lower bounds FEATURE-OF time complexity. Method are recursive pooling technique of local neighborhoods, and low - order GNNs. ","This paper studies the expressive power of message passing Graph Neural Networks (GNNs) for learning with graphs. The authors consider the recursive pooling technique of local neighborhoods, where a model is trained to find subgraphs of the original graph. They provide strategies and lower bounds on the computational cost and expressive power for higherorder GNNs. They show that the computational complexity of the proposed method is much lower than that of existing low-order graph neural networks. They also provide a (near) matching information-theoretic lower bound on the number of subgraph to be found, which is based on the sparsity of the subgraph.   ","This paper studies the expressive power of message passing Graph Neural Networks (GNNs) for learning with graphs. The authors consider the recursive pooling technique of local neighborhoods, where a model is trained to find subgraphs of the original graph. They provide strategies and lower bounds on the computational cost and expressive power for higherorder GNNs. They show that the computational complexity of the proposed method is much lower than that of existing low-order graph neural networks. They also provide a (near) matching information-theoretic lower bound on the number of subgraph to be found, which is based on the sparsity of the subgraph.   "
6875,SP:5d0cbd84336caf5f31e1f98e11f6733230e4d792,Pretrained language models ( LMs ) USED-FOR factual knowledge. external knowledge PART-OF pretrained LMs. knowledge integration ( KI ) methods USED-FOR pretrained LMs. external knowledge PART-OF knowledge integration ( KI ) methods. KI methods COMPARE vanilla LMs. vanilla LMs COMPARE KI methods. integration USED-FOR catastrophic forgetting of already learned knowledge. graph convolution operation USED-FOR KI. probe model USED-FOR knowledge - enhanced LMs. Graph Convolution Simulator ( GCS ) HYPONYM-OF probe model. GCS model USED-FOR KI process. it USED-FOR knowledge - enhanced LMs. K - Adapter CONJUNCTION ERNIE. ERNIE CONJUNCTION K - Adapter. ERNIE HYPONYM-OF knowledge - enhanced LMs. K - Adapter HYPONYM-OF knowledge - enhanced LMs. models USED-FOR factual knowledge. complex relational knowledge USED-FOR ERNIE. K - Adapter USED-FOR simple relational knowledge. K - Adapter USED-FOR time - related knowledge. Generic is methods. Method is LMs. OtherScientificTerm is relations. Material is KI corpus. ,"This paper proposes two new knowledge integration (KI) methods to integrate external knowledge into pretrained language models (LMs) for factual knowledge. The authors show that KI methods are more effective than vanilla LMs in preventing catastrophic forgetting of already learned knowledge.  The authors propose two new methods, K-Adapters and ERNIE, which integrate knowledge of relations into the training of LMs. KI is a graph convolution operation, and the authors propose to use a probe model called Graph Convolution Simulator (GCS) to investigate the performance of knowledge-enhanced LMs trained with KI. The GCS model is used in the KI process and it is shown that it can be used to train two types of knowledge enhanced LMs: K-Adapter for simple relational knowledge, and K-Env for time-related knowledge. Both models are shown to be able to transfer factual knowledge across different time steps. The KI corpus is also shown to contain more complex relational knowledge than the vanilla ones.  ","This paper proposes two new knowledge integration (KI) methods to integrate external knowledge into pretrained language models (LMs) for factual knowledge. The authors show that KI methods are more effective than vanilla LMs in preventing catastrophic forgetting of already learned knowledge.  The authors propose two new methods, K-Adapters and ERNIE, which integrate knowledge of relations into the training of LMs. KI is a graph convolution operation, and the authors propose to use a probe model called Graph Convolution Simulator (GCS) to investigate the performance of knowledge-enhanced LMs trained with KI. The GCS model is used in the KI process and it is shown that it can be used to train two types of knowledge enhanced LMs: K-Adapter for simple relational knowledge, and K-Env for time-related knowledge. Both models are shown to be able to transfer factual knowledge across different time steps. The KI corpus is also shown to contain more complex relational knowledge than the vanilla ones.  "
6900,SP:7e73948421e98307fceb69a316d8a4e7c4926cda,adaptation ( inner loop ) learning rate USED-FOR fast adaptation. adaptation ( inner loop ) learning rate USED-FOR MAML. adaptation learning rate USED-FOR meta - learning. adaptation learning rate FEATURE-OF mixed linear regression. mixed linear regression USED-FOR meta - learning. optimal adaptation learning rates USED-FOR MAML. optimal adaptation learning rates USED-FOR population risk. population risk FEATURE-OF MAML. empirical risk minimization ( ERM ) COMPARE MAML. MAML COMPARE empirical risk minimization ( ERM ). MAML USED-FOR initialization. average distance FEATURE-OF initialization. Metric is adaptation error. OtherScientificTerm is optimal adaptation learning rate. ,"This paper studies the adaptation (inner loop) learning rate for fast adaptation in MAML. The authors show that the adaptation learning rate in meta-learning with mixed linear regression is optimal for the adaptation error. They also show that optimal adaptation learning rates are optimal for minimizing the population risk of MamL. They further show that empirical risk minimization (ERM) is more effective than standard adaptation in the same number of iterations, and that the optimal adaptation loss is a function of the number of times that the adaptive learning rate is used in the inner loop. Finally, they show that in the case of initialization with an average distance of $O(1/\sqrt{n})$ to the training data, MAMM is more robust to the initialization of the initialization than ERM. ","This paper studies the adaptation (inner loop) learning rate for fast adaptation in MAML. The authors show that the adaptation learning rate in meta-learning with mixed linear regression is optimal for the adaptation error. They also show that optimal adaptation learning rates are optimal for minimizing the population risk of MamL. They further show that empirical risk minimization (ERM) is more effective than standard adaptation in the same number of iterations, and that the optimal adaptation loss is a function of the number of times that the adaptive learning rate is used in the inner loop. Finally, they show that in the case of initialization with an average distance of $O(1/\sqrt{n})$ to the training data, MAMM is more robust to the initialization of the initialization than ERM. "
6925,SP:effbc85d89b1197d9c2abcaf5ff13864135dd6e1,"source - domain data USED-FOR adaptation. Source - free domain adaptation ( SFDA ) USED-FOR model. unlabelled data USED-FOR model. labelled data USED-FOR model. methods USED-FOR SFDA. source model USED-FOR feature - space class - separation. entropy - minimization techniques USED-FOR methods. measurement shift HYPONYM-OF domain shift. it USED-FOR features. bottom - up training scheme USED-FOR FR. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. BUFR COMPARE SFDA methods. SFDA methods COMPARE BUFR. calibration CONJUNCTION data efficiency. data efficiency CONJUNCTION calibration. BUFR COMPARE source model. source model COMPARE BUFR. data efficiency EVALUATE-FOR SFDA methods. calibration EVALUATE-FOR SFDA methods. real and synthetic data EVALUATE-FOR SFDA methods. data efficiency EVALUATE-FOR BUFR. calibration EVALUATE-FOR BUFR. accuracy EVALUATE-FOR SFDA methods. real and synthetic data EVALUATE-FOR BUFR. accuracy EVALUATE-FOR BUFR. Task are classification, and model calibration. OtherScientificTerm are measurement system, source features, and approximate feature distribution. Method are feature - extractor, and Feature Restoration ( FR ). Generic is network. ","This paper studies the problem of adaptation from source-domain data to unlabelled data. The authors propose Source-free domain adaptation (SFDA) aims to train a model on unlabelling data and then adapt the model to unlabeled data in the source domain. The main contribution of this paper is to propose two methods for SFDA based on entropy-minimization techniques. The first method, Feature Restoration (BUFR), is based on the observation that the measurement system is biased in the sense that the source features of the unlabeling data are more likely to be similar to the target domain than the target data. To mitigate this issue, the authors propose a bottom-up training scheme, where the source model is trained to achieve feature-space class-separation from the target model on the labelled data, and the target network is trained on the unllabeled data. They also propose a new domain shift, called measurement shift, which is a measure of the domain shift between the source and target domains. They show that the model calibration can be improved if the target and source models share the same approximate feature distribution, and that the feature-extractor is trained so that it is able to recover the features from the source data.  The authors show that BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and data efficiency on both real and synthetic data. In addition, BUFR is shown to outperform the state-of-the-art of the art on both accuracy and calibration. ","This paper studies the problem of adaptation from source-domain data to unlabelled data. The authors propose Source-free domain adaptation (SFDA) aims to train a model on unlabelling data and then adapt the model to unlabeled data in the source domain. The main contribution of this paper is to propose two methods for SFDA based on entropy-minimization techniques. The first method, Feature Restoration (BUFR), is based on the observation that the measurement system is biased in the sense that the source features of the unlabeling data are more likely to be similar to the target domain than the target data. To mitigate this issue, the authors propose a bottom-up training scheme, where the source model is trained to achieve feature-space class-separation from the target model on the labelled data, and the target network is trained on the unllabeled data. They also propose a new domain shift, called measurement shift, which is a measure of the domain shift between the source and target domains. They show that the model calibration can be improved if the target and source models share the same approximate feature distribution, and that the feature-extractor is trained so that it is able to recover the features from the source data.  The authors show that BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and data efficiency on both real and synthetic data. In addition, BUFR is shown to outperform the state-of-the-art of the art on both accuracy and calibration. "
6950,SP:7d63034ec7e6a4f178681ff2a49feb485cd47116,"distributed learning schema USED-FOR model. Federated learning ( FL ) HYPONYM-OF distributed learning schema. adversarial training ( AT ) USED-FOR centralized learning. learning setting USED-FOR adversarial robustness. adversarial robustness FEATURE-OF non - iid users. batch - normalization statistics USED-FOR propagation approach. method USED-FOR FL remarkable robustness. Material is raw data. Method are FL, and FL techniques. OtherScientificTerm are FL users, highresource users, and low - resource users. Metric is model robustness. Generic is it. Task are FL process, and learning. ","This paper studies the problem of adversarial training (AT) in federated learning (FL) where the goal is to train a model in a distributed learning schema, i.e., Federated learning with raw data. In FL, there are multiple FL users, each of which has a different number of resources, and each of these FL users has its own dataset. The authors show that adversarial robustness in the learning setting for non-iid users suffers from the presence of highresource users, which is a common problem in centralized learning due to adversarial learning (AT). To address this problem, the authors propose a propagation approach based on batch-normalization statistics, where the high-resource users are the ones with the highest model robustness, while the low-resource (i.e. low) users are those with the lowest. The propagation approach is based on the observation that the FL process is more robust when the number of users is small, and that it is less robust when there are more than two users. The proposed method is shown to achieve FL remarkable robustness against adversarial attacks, and it is shown that the method is robust to attacks that are more powerful than the ones that are aimed at the high resource users.   The authors also show that FL techniques can be used to improve the robustness of the low resource users to attacks. The paper also shows that the adversarial attack is more powerful when low-resourced users are not the only ones in the network.  The paper is well-written and well-motivated, and the results are interesting. However, the paper suffers from a lack of clarity about the problem that the authors are trying to address, which makes it difficult to understand the impact of the proposed method and the contribution of the paper to the community. ","This paper studies the problem of adversarial training (AT) in federated learning (FL) where the goal is to train a model in a distributed learning schema, i.e., Federated learning with raw data. In FL, there are multiple FL users, each of which has a different number of resources, and each of these FL users has its own dataset. The authors show that adversarial robustness in the learning setting for non-iid users suffers from the presence of highresource users, which is a common problem in centralized learning due to adversarial learning (AT). To address this problem, the authors propose a propagation approach based on batch-normalization statistics, where the high-resource users are the ones with the highest model robustness, while the low-resource (i.e. low) users are those with the lowest. The propagation approach is based on the observation that the FL process is more robust when the number of users is small, and that it is less robust when there are more than two users. The proposed method is shown to achieve FL remarkable robustness against adversarial attacks, and it is shown that the method is robust to attacks that are more powerful than the ones that are aimed at the high resource users.   The authors also show that FL techniques can be used to improve the robustness of the low resource users to attacks. The paper also shows that the adversarial attack is more powerful when low-resourced users are not the only ones in the network.  The paper is well-written and well-motivated, and the results are interesting. However, the paper suffers from a lack of clarity about the problem that the authors are trying to address, which makes it difficult to understand the impact of the proposed method and the contribution of the paper to the community. "
6975,SP:42c7a79e58b6a9f776fa6ae928bd89c194f9303f,"utility function FEATURE-OF game. utility function USED-FOR Existing methods. transformer - like architecture USED-FOR mapping. transformer - like architecture USED-FOR symmetries. network structure inference EVALUATE-FOR methods. method COMPARE methods. methods COMPARE method. network structure inference EVALUATE-FOR method. network games EVALUATE-FOR method. synthetic and real - world data EVALUATE-FOR method. synthetic and real - world data USED-FOR network games. OtherScientificTerm are Strategic interactions, network structure, equilibrium actions, real - world scenarios, and network structure of the game. Task is economics and social sciences. ","This paper studies the problem of learning a utility function for a game with strategic interactions. Existing methods are based on the utility function of the game. However, in this paper, the goal is to learn a mapping between the game's utility function and the network structure. To achieve this, a transformer-like architecture is proposed to learn the mapping between symmetries in the game and the equilibrium actions. The proposed method is evaluated on both synthetic and real-world data for network games and shows that the proposed method outperforms existing methods in terms of network structure inference. The paper is well-written and well-motivated, and the experiments show that the method is able to learn equilibrium actions that are robust to asymmetries and can be applied to a wide range of real-life scenarios. The authors also show that their method can be used in a variety of applications in economics and social sciences.    The main contribution of this paper is that it proposes a method for learning a mapping from equilibrium actions to equilibrium actions in a game. The idea is to use a transformer to map the equilibrium action of a game to the corresponding equilibrium actions of the other players. This is a simple idea, but it seems to work well in practice. In addition, the authors show that this method can learn to learn symmetric and non-symmetric actions in the same way as existing methods.  The paper also shows that their proposed method can also be used to learn in a more general way, and that it can be combined with existing methods to improve the performance of the proposed in the paper.  Finally, the paper shows that in some cases, their method performs better than existing methods when the game structure is not symmetric.  This is an interesting contribution to the literature, and I think this is an important contribution that can be useful to the community. ","This paper studies the problem of learning a utility function for a game with strategic interactions. Existing methods are based on the utility function of the game. However, in this paper, the goal is to learn a mapping between the game's utility function and the network structure. To achieve this, a transformer-like architecture is proposed to learn the mapping between symmetries in the game and the equilibrium actions. The proposed method is evaluated on both synthetic and real-world data for network games and shows that the proposed method outperforms existing methods in terms of network structure inference. The paper is well-written and well-motivated, and the experiments show that the method is able to learn equilibrium actions that are robust to asymmetries and can be applied to a wide range of real-life scenarios. The authors also show that their method can be used in a variety of applications in economics and social sciences.    The main contribution of this paper is that it proposes a method for learning a mapping from equilibrium actions to equilibrium actions in a game. The idea is to use a transformer to map the equilibrium action of a game to the corresponding equilibrium actions of the other players. This is a simple idea, but it seems to work well in practice. In addition, the authors show that this method can learn to learn symmetric and non-symmetric actions in the same way as existing methods.  The paper also shows that their proposed method can also be used to learn in a more general way, and that it can be combined with existing methods to improve the performance of the proposed in the paper.  Finally, the paper shows that in some cases, their method performs better than existing methods when the game structure is not symmetric.  This is an interesting contribution to the literature, and I think this is an important contribution that can be useful to the community. "
7000,SP:1c7b9157cf8c06ca771da78895fc3af969b0fb85,"heterogeneous graphs USED-FOR relation prediction. ANalogy SubGraph Embedding Learning ( GraphANGEL ) HYPONYM-OF relation prediction framework. graph pattern USED-FOR logical rule. graph pattern USED-FOR explainable predictive models. inductive bias USED-FOR generalization. heterogeneous graph based recommendation CONJUNCTION knowledge graph completion. knowledge graph completion CONJUNCTION heterogeneous graph based recommendation. model COMPARE models. models COMPARE model. knowledge graph completion EVALUATE-FOR model. heterogeneous graph based recommendation EVALUATE-FOR model. knowledge graph completion EVALUATE-FOR models. heterogeneous graph based recommendation EVALUATE-FOR models. model USED-FOR explainable heat maps of attention scores. Material is knowledge graphs. OtherScientificTerm are embeddings, and subgraphs. Task is transductive setting. ","This paper proposes a novel relation prediction framework called ANalogy SubGraph Embedding Learning (GraphANGEL) for heterogeneous graphs for relation prediction. The key idea of GraphANGEL is to learn the embeddings of subgraphs of the original graph, and then use this graph pattern as a logical rule to learn explainable predictive models. This inductive bias is used to improve generalization. The proposed model outperforms existing models on heterogeneous graph based recommendation and knowledge graph completion. The model also learns explainable heat maps of attention scores for the transductive setting. ","This paper proposes a novel relation prediction framework called ANalogy SubGraph Embedding Learning (GraphANGEL) for heterogeneous graphs for relation prediction. The key idea of GraphANGEL is to learn the embeddings of subgraphs of the original graph, and then use this graph pattern as a logical rule to learn explainable predictive models. This inductive bias is used to improve generalization. The proposed model outperforms existing models on heterogeneous graph based recommendation and knowledge graph completion. The model also learns explainable heat maps of attention scores for the transductive setting. "
7025,SP:26ed25a7b42da2cf11b76a727102d8aa36d76657,"well - labeled datasets CONJUNCTION rare abnormal samples. rare abnormal samples CONJUNCTION well - labeled datasets. Few - shot learning HYPONYM-OF natural images. cross - domain tasks USED-FOR real clinics problems. contrastive learning ( CL ) USED-FOR few - shot system. label - efficient learning CONJUNCTION generalizability. generalizability CONJUNCTION label - efficient learning. contrastive learning ( CL ) CONJUNCTION latent augmentation ( LA ). latent augmentation ( LA ) CONJUNCTION contrastive learning ( CL ). latent augmentation ( LA ) USED-FOR few - shot system. LA USED-FOR semantic variations. CL COMPARE LA. LA COMPARE CL. components USED-FOR label - hungry problems. unlabeled training data USED-FOR components. LA COMPARE baselines. baselines COMPARE LA. supervised learning USED-FOR histology images. models COMPARE supervised learning. supervised learning COMPARE models. CL COMPARE supervised learning. supervised learning COMPARE CL. CL USED-FOR models. ImageNet - like images USED-FOR self - supervised learning. CL COMPARE supervised learning. supervised learning COMPARE CL. generalization EVALUATE-FOR data. generalization EVALUATE-FOR CL. generalization EVALUATE-FOR supervised learning. representation learning CONJUNCTION histological image analysis. histological image analysis CONJUNCTION representation learning. model USED-FOR representation learning. model USED-FOR histological image analysis. Task is few - shot learning in histology images. OtherScientificTerm is manual labels. Material are images, and Histology images. ","This paper studies few-shot learning in histology images, which is a setting where there is a large number of well-labeled datasets and rare abnormal samples, but there are few natural images available (fewer than 10% of the images have manual labels). Few-shot classification is an important problem in many real clinics problems, especially cross-domain tasks. This paper proposes to use contrastive learning (CL) and latent augmentation (LA) to improve the few-shooting performance of a few shot system. The key idea of CL and LA is that CL is able to capture semantic variations between images, while LA can capture the semantic variations across different semantic variations. The authors show that CL outperforms LA and LA in label-hungry problems with unlabeled training data, and that these two components can be combined to improve label-efficient learning and generalizability. Experiments are conducted on ImageNet-like images for self-supervised learning, and show that models trained with CL outperform supervised learning on histological images, and CL improves the generalization performance on this data compared to supervised learning. The paper also shows that a model trained with representation learning and histological image analysis can be used as a model for representation learning. Histology images are also used to improve generalization.   ","This paper studies few-shot learning in histology images, which is a setting where there is a large number of well-labeled datasets and rare abnormal samples, but there are few natural images available (fewer than 10% of the images have manual labels). Few-shot classification is an important problem in many real clinics problems, especially cross-domain tasks. This paper proposes to use contrastive learning (CL) and latent augmentation (LA) to improve the few-shooting performance of a few shot system. The key idea of CL and LA is that CL is able to capture semantic variations between images, while LA can capture the semantic variations across different semantic variations. The authors show that CL outperforms LA and LA in label-hungry problems with unlabeled training data, and that these two components can be combined to improve label-efficient learning and generalizability. Experiments are conducted on ImageNet-like images for self-supervised learning, and show that models trained with CL outperform supervised learning on histological images, and CL improves the generalization performance on this data compared to supervised learning. The paper also shows that a model trained with representation learning and histological image analysis can be used as a model for representation learning. Histology images are also used to improve generalization.   "
7050,SP:badbe687258cd5c282ca167b1f6fbfc6b5400dbf,"Recurrent neural networks ( RNNs ) USED-FOR irregularly - sampled time series. continuous - time hidden states USED-FOR Recurrent neural networks ( RNNs ). training FEATURE-OF gradient. memory compartment FEATURE-OF continuous - time networks. continuous - time dynamical flow PART-OF RNN. constant error propagation FEATURE-OF memory path. Mixed - MemoryRNNs COMPARE RNN - based counterparts. RNN - based counterparts COMPARE Mixed - MemoryRNNs. long - term dependencies FEATURE-OF non - uniformly sampled data. non - uniformly sampled data EVALUATE-FOR RNN - based counterparts. non - uniformly sampled data EVALUATE-FOR Mixed - MemoryRNNs. Generic are models, and it. Method are RNNs, ODE solver, and Mixed - Memory - RNNs ( mmRNNs ). OtherScientificTerm are hidden state, timecontinuous state, and time - lags. ","Recurrent neural networks (RNNs) for irregularly-sampled time series with continuous-time hidden states can be seen as a special case of RNNs, where the hidden state is a timecontinuous state, and the training of the gradient can be regarded as an ODE solver.   This paper proposes Mixed-Memory-RNN (mmRNN) which extends the memory compartment of existing continuous time-continuous RNN models to the case where the RNN is a RNN with a continuous time dynamical flow.  In particular, the authors show that the memory of an RNN can be decomposed into two parts: (1) a memory compartment that stores time-lags in the past, and (2) the memory path with constant error propagation.  The authors also show that it is possible to decompose the memory in a mixed memory RNN into two separate parts.  Experiments on non-uniform sampled data with long-term dependencies show that mixed-MemoryRNN's outperform their RNN-based counterparts on a variety of non-randomized data. ","Recurrent neural networks (RNNs) for irregularly-sampled time series with continuous-time hidden states can be seen as a special case of RNNs, where the hidden state is a timecontinuous state, and the training of the gradient can be regarded as an ODE solver.   This paper proposes Mixed-Memory-RNN (mmRNN) which extends the memory compartment of existing continuous time-continuous RNN models to the case where the RNN is a RNN with a continuous time dynamical flow.  In particular, the authors show that the memory of an RNN can be decomposed into two parts: (1) a memory compartment that stores time-lags in the past, and (2) the memory path with constant error propagation.  The authors also show that it is possible to decompose the memory in a mixed memory RNN into two separate parts.  Experiments on non-uniform sampled data with long-term dependencies show that mixed-MemoryRNN's outperform their RNN-based counterparts on a variety of non-randomized data. "
7075,SP:4efd22f9122fa5856a9f4302eb6875fa0c414912,pre - trained BERT USED-FOR Natural Language Processing ( NLP ) tasks. 1 - bit parameters CONJUNCTION bitwise operations. bitwise operations CONJUNCTION 1 - bit parameters. binarization HYPONYM-OF compression approaches. 1 - bit parameters USED-FOR binarization. computation and memory consumption EVALUATE-FOR binarization. bitwise operations USED-FOR binarization. 1 - bit weight CONJUNCTION embedding. embedding CONJUNCTION 1 - bit weight. embedding CONJUNCTION activation. activation CONJUNCTION embedding. activation HYPONYM-OF BERT. 1 - bit weight HYPONYM-OF BERT. embedding HYPONYM-OF BERT. information degradation CONJUNCTION optimization direction mismatch. optimization direction mismatch CONJUNCTION information degradation. BiBERT USED-FOR performance bottlenecks. BiBERT HYPONYM-OF fully binarized BERT. optimization direction mismatch FEATURE-OF forward and backward propagation. DirectionMatching Distillation ( DMD ) scheme USED-FOR full binarized BERT. Bi - Attention structure USED-FOR representation information statistically. Bi - Attention structure CONJUNCTION DirectionMatching Distillation ( DMD ) scheme. DirectionMatching Distillation ( DMD ) scheme CONJUNCTION Bi - Attention structure. DirectionMatching Distillation ( DMD ) scheme USED-FOR BiBERT. Bi - Attention structure USED-FOR BiBERT. BiBERT COMPARE quantized BERTs. quantized BERTs COMPARE BiBERT. BiBERT COMPARE baseline. baseline COMPARE BiBERT. baseline COMPARE quantized BERTs. quantized BERTs COMPARE baseline. ultra - low bit activations FEATURE-OF quantized BERTs. NLP benchmark EVALUATE-FOR quantized BERTs. NLP benchmark EVALUATE-FOR BiBERT. FLOPs CONJUNCTION model size. model size CONJUNCTION FLOPs. fully binarized BERT model USED-FOR real - world resource - constrained scenarios. method USED-FOR fully binarized BERT model. fully binarized BERT EVALUATE-FOR method. model size EVALUATE-FOR method,"This paper proposes BiBERT, a pre-trained BERT-like model that can be used to reduce the computational and memory footprint of BERT for natural language processing (NLP) tasks. Previous compression approaches, such as binarization, have been shown to be effective in reducing the computation and memory consumption, but they are expensive. This paper proposes a new binarized BERT model, BiBERt, that is able to achieve competitive performance on the standard NLP benchmark. The key idea is to use 1-bit parameters (1-bit weight, embedding, and bitwise operations) of the BERT to reduce its computation. The authors also propose a Bi-Attention structure to capture the representation information statistically, and a DirectionMatching Distillation (DMD) scheme to further reduce the memory and computation cost of the full binarised BERT. The experiments show that the proposed method can achieve the state-of-the-art FLOPs and model size of a fully-banned BERT, while achieving competitive performance in terms of information degradation and optimization direction mismatch in forward and backward propagation. The paper also shows that the BiBERTs outperform a baseline and a baseline with ultra-low bit activations. The proposed method is also able to address performance bottlenecks in real-world resource-constrained scenarios.   ","This paper proposes BiBERT, a pre-trained BERT-like model that can be used to reduce the computational and memory footprint of BERT for natural language processing (NLP) tasks. Previous compression approaches, such as binarization, have been shown to be effective in reducing the computation and memory consumption, but they are expensive. This paper proposes a new binarized BERT model, BiBERt, that is able to achieve competitive performance on the standard NLP benchmark. The key idea is to use 1-bit parameters (1-bit weight, embedding, and bitwise operations) of the BERT to reduce its computation. The authors also propose a Bi-Attention structure to capture the representation information statistically, and a DirectionMatching Distillation (DMD) scheme to further reduce the memory and computation cost of the full binarised BERT. The experiments show that the proposed method can achieve the state-of-the-art FLOPs and model size of a fully-banned BERT, while achieving competitive performance in terms of information degradation and optimization direction mismatch in forward and backward propagation. The paper also shows that the BiBERTs outperform a baseline and a baseline with ultra-low bit activations. The proposed method is also able to address performance bottlenecks in real-world resource-constrained scenarios.   "
7100,SP:619bd742e92bea6241852f5a9d2b7bacf13b393a,"method USED-FOR keypoint detection. method USED-FOR instance association. keypoint detection CONJUNCTION instance association. instance association CONJUNCTION keypoint detection. Transformer USED-FOR method. Transformer USED-FOR problems. association information USED-FOR keypoints grouping. self - attention PART-OF Transformer. supervising self - attention USED-FOR multi - person keypoint detection. supervising self - attention USED-FOR instance association. approach USED-FOR supervising self - attention. multi - person keypoint detection CONJUNCTION instance association. instance association CONJUNCTION multi - person keypoint detection. approach USED-FOR instance association. approach USED-FOR multi - person keypoint detection. instance masks USED-FOR self - attention. pre - defined offset vector fields CONJUNCTION embedding. embedding CONJUNCTION pre - defined offset vector fields. embedding CONJUNCTION CNN - based bottom - up models. CNN - based bottom - up models CONJUNCTION embedding. supervised attention matrix USED-FOR instance segmentation. person instance segmentation task EVALUATE-FOR method. COCO multi - person keypoint detection challenge EVALUATE-FOR method. COCO multi - person keypoint detection challenge CONJUNCTION person instance segmentation task. person instance segmentation task CONJUNCTION COCO multi - person keypoint detection challenge. OtherScientificTerm are associative information, naive attention patterns, pairwise attention scores, and self - attention behavior. Method is pixel assignment pipeline. ","This paper proposes a new method for keypoint detection and instance association using Transformer. The proposed method is based on the observation that naive attention patterns in Transformer can be problematic for these problems due to the lack of associative information. To address this problem, the authors propose to leverage the association information for keypoints grouping and propose to replace the self-attention in the Transformer with pairwise attention scores. This approach is then applied to supervising self attention for multi-person keypoint recognition, instance association, and instance segmentation tasks.    The proposed approach firstly learns the pixel assignment pipeline for each pair of keypoints in the input image. The pixel assignment is done by learning a pixel mask for each keypoint in the image. Then, the self attention is applied on the instance masks of the pair of keys. The self attention behavior is then evaluated by comparing the performance of the proposed approach to the previous approaches. The method is evaluated on the COCO multi-posteriority of multi-perspective keypoints detection and multi-people keypoint segmentation task, and is shown to outperform the previous methods. The authors also show that the supervised attention matrix is also useful for instance segmentations. The paper also shows that pre-defined offset vector fields, embedding, and CNN-based bottom-up models can be used to further improve the performance. ","This paper proposes a new method for keypoint detection and instance association using Transformer. The proposed method is based on the observation that naive attention patterns in Transformer can be problematic for these problems due to the lack of associative information. To address this problem, the authors propose to leverage the association information for keypoints grouping and propose to replace the self-attention in the Transformer with pairwise attention scores. This approach is then applied to supervising self attention for multi-person keypoint recognition, instance association, and instance segmentation tasks.    The proposed approach firstly learns the pixel assignment pipeline for each pair of keypoints in the input image. The pixel assignment is done by learning a pixel mask for each keypoint in the image. Then, the self attention is applied on the instance masks of the pair of keys. The self attention behavior is then evaluated by comparing the performance of the proposed approach to the previous approaches. The method is evaluated on the COCO multi-posteriority of multi-perspective keypoints detection and multi-people keypoint segmentation task, and is shown to outperform the previous methods. The authors also show that the supervised attention matrix is also useful for instance segmentations. The paper also shows that pre-defined offset vector fields, embedding, and CNN-based bottom-up models can be used to further improve the performance. "
7125,SP:14750819593136fc9ef4efd032ab6f94dc5f6a02,"reinforcement learning ( RL ) USED-FOR sequential decision making. Pareto efficiency FEATURE-OF MV - efficient policies. Generic are existing methods, approach, it, and methods. OtherScientificTerm are variance term, MV trade - off, expected quadratic utility function, Pareto efficient policy, computational difficulties, and gradient estimation of the variance. ","This paper considers reinforcement learning (RL) for sequential decision making, where existing methods suffer from the variance term. The authors propose a new approach, called Pareto efficient RL (Pareto-efficient RL) to address this issue. In particular, the authors show that the MV trade-off between the expected quadratic utility function and the expected variance of the policy is a Pareta efficient policy, and that existing methods do not satisfy it. The paper also shows that the Paretto efficiency of existing MV-efficient policies can be improved by considering the gradient estimation of the variance.   ","This paper considers reinforcement learning (RL) for sequential decision making, where existing methods suffer from the variance term. The authors propose a new approach, called Pareto efficient RL (Pareto-efficient RL) to address this issue. In particular, the authors show that the MV trade-off between the expected quadratic utility function and the expected variance of the policy is a Pareta efficient policy, and that existing methods do not satisfy it. The paper also shows that the Paretto efficiency of existing MV-efficient policies can be improved by considering the gradient estimation of the variance.   "
7150,SP:f675b564b3a9c8626ce7944d752fa3e0d868428e,"end - to - end learning USED-FOR communication system. autoencoder USED-FOR communication system. autoencoder USED-FOR end - to - end learning. test - time domain adaptation USED-FOR autoencoder system. fully - trained channel model CONJUNCTION autoencoder. autoencoder CONJUNCTION fully - trained channel model. error rate EVALUATE-FOR autoencoder. method USED-FOR autoencoder. feature transformations USED-FOR channel distribution. feature transformations USED-FOR decoder. feature transformations USED-FOR method. method USED-FOR MDN channel. simulated datasets CONJUNCTION real mmWave wireless channels. real mmWave wireless channels CONJUNCTION simulated datasets. error rate EVALUATE-FOR autoencoder. real mmWave wireless channels EVALUATE-FOR method. simulated datasets EVALUATE-FOR method. method USED-FOR autoencoder. error rate EVALUATE-FOR method. Generic is approach. Method are mixture density network ( MDN ), encoder and decoder neural networks, and MDN channel model. Material is unlabeled data. OtherScientificTerm are wireless link, and source distribution. ","This paper proposes a new approach for end-to-end learning of a communication system using an autoencoder. The approach is based on the idea of a mixture density network (MDN) where the encoder and decoder neural networks are trained on unlabeled data from the source domain. The authors propose a test-time domain adaptation to improve the performance of the autoencoders. The key idea is to use a wireless link between a fully-trained channel model and a fully trained autoen coder, and then use the resulting communication system as a testbed for a test domain adaptation.   The authors show that the proposed method improves the error rate of the trained method for the auteno-coder and the decoder by using feature transformations to adjust the channel distribution of the source and the target domain. They also show that this method can be applied to any MDN channel, and that the method is able to learn the MDN model in an unsupervised way.  The proposed method is evaluated on simulated datasets and real mmWave wireless channels, and shows that their method achieves a lower error rate than the state-of-the-art method. The main contribution of the paper is that the authors propose to use the test data from a source distribution that is not available to the source distribution, and to use test time domain adaptation as a way to train the test domain adaptor.","This paper proposes a new approach for end-to-end learning of a communication system using an autoencoder. The approach is based on the idea of a mixture density network (MDN) where the encoder and decoder neural networks are trained on unlabeled data from the source domain. The authors propose a test-time domain adaptation to improve the performance of the autoencoders. The key idea is to use a wireless link between a fully-trained channel model and a fully trained autoen coder, and then use the resulting communication system as a testbed for a test domain adaptation.   The authors show that the proposed method improves the error rate of the trained method for the auteno-coder and the decoder by using feature transformations to adjust the channel distribution of the source and the target domain. They also show that this method can be applied to any MDN channel, and that the method is able to learn the MDN model in an unsupervised way.  The proposed method is evaluated on simulated datasets and real mmWave wireless channels, and shows that their method achieves a lower error rate than the state-of-the-art method. The main contribution of the paper is that the authors propose to use the test data from a source distribution that is not available to the source distribution, and to use test time domain adaptation as a way to train the test domain adaptor."
7175,SP:77dc92137ea490d3e1b4b8ee1630dbe2ee0bddfa,"joint softmax focal loss HYPONYM-OF structural loss. model USED-FOR αNLI. ACC CONJUNCTION AUC. AUC CONJUNCTION ACC. AUC EVALUATE-FOR IMSL. RoBERTa - large pretrained model EVALUATE-FOR IMSL. ACC EVALUATE-FOR IMSL. Task are abductive natural language inference task ( αNLI ), and αNLI task. Method are inference network, interactive language model, and Interactive Model with Structural Loss ( IMSL ). OtherScientificTerm is reasoning abilities. ","This paper studies the abductive natural language inference task (αNLI), where the inference network is trained to infer the intent of an interactive language model. The authors propose an interactive model with structural loss (i.e., joint softmax focal loss, which is a structural loss that minimizes the difference between the output of the interactive model and the input of the model) for the αNLI task. They call it the Interactive Model with Structural Loss (IMSL). They show that IMSL outperforms the RoBERTa-large pretrained model in terms of ACC and AUC. They also show that the reasoning abilities of their model are more robust to changes in the context of the context.","This paper studies the abductive natural language inference task (αNLI), where the inference network is trained to infer the intent of an interactive language model. The authors propose an interactive model with structural loss (i.e., joint softmax focal loss, which is a structural loss that minimizes the difference between the output of the interactive model and the input of the model) for the αNLI task. They call it the Interactive Model with Structural Loss (IMSL). They show that IMSL outperforms the RoBERTa-large pretrained model in terms of ACC and AUC. They also show that the reasoning abilities of their model are more robust to changes in the context of the context."
7200,SP:17cd72df5fc19398f582d27516fd742b073f79e3,"machine learning USED-FOR safety - critical systems. assessment of uncertainy USED-FOR machine learning. deep neural networks USED-FOR overconfident predictions. certifiable OOD detector CONJUNCTION classifier. classifier CONJUNCTION certifiable OOD detector. classifier PART-OF OOD aware classifier. OOD aware classifier PART-OF method. classifier PART-OF method. certifiable OOD detector PART-OF method. prediction accuracy CONJUNCTION detection. detection CONJUNCTION prediction accuracy. detection performance EVALUATE-FOR non - manipulated OOD data. classifier USED-FOR asymptotic overconfidence problem. classifier COMPARE neural networks. neural networks COMPARE classifier. asymptotic overconfidence problem FEATURE-OF neural networks. Material is OOD data. Task is certifiably adversarially robust OOD detection. OtherScientificTerm are OOD samples, and in - distribution. ","This paper studies the problem of overconfident OOD detection in machine learning for safety-critical systems. The authors propose a method that combines a certifiable OOD detector and an OOD aware classifier, which is a classifier that is certified to be OOD-robust on OOD data. They show that deep neural networks have an asymptotic overconfidence problem for OOD samples, and that certifiably adversarially robust OOD detections are more likely to be certified if the in-distribution is also certified as OOD. They also show that the assessment of uncertainy for machine learning can be a useful tool for improving the performance of machine learning.    The method proposed in this paper combines the idea of certifiable adversarial robust Ood detection with a method for certifiable overconfidence detection. The proposed method is based on the observation that the classifier of a certified OOD classifier is more robust to OOD attacks than neural networks, and the authors show that this classifier can be used to mitigate the so-called “overconfident overconfidence” problem of neural networks. The paper also shows that the detection performance of non-manifold OOD test data can be improved by using the certified classifier.  Finally, the authors provide a theoretical analysis of the relationship between prediction accuracy and detection performance. ","This paper studies the problem of overconfident OOD detection in machine learning for safety-critical systems. The authors propose a method that combines a certifiable OOD detector and an OOD aware classifier, which is a classifier that is certified to be OOD-robust on OOD data. They show that deep neural networks have an asymptotic overconfidence problem for OOD samples, and that certifiably adversarially robust OOD detections are more likely to be certified if the in-distribution is also certified as OOD. They also show that the assessment of uncertainy for machine learning can be a useful tool for improving the performance of machine learning.    The method proposed in this paper combines the idea of certifiable adversarial robust Ood detection with a method for certifiable overconfidence detection. The proposed method is based on the observation that the classifier of a certified OOD classifier is more robust to OOD attacks than neural networks, and the authors show that this classifier can be used to mitigate the so-called “overconfident overconfidence” problem of neural networks. The paper also shows that the detection performance of non-manifold OOD test data can be improved by using the certified classifier.  Finally, the authors provide a theoretical analysis of the relationship between prediction accuracy and detection performance. "
7225,SP:9c3756f13932236aff3e8104f4fa193dcc8fde2f,"Deep Neural Networks ( DNNs ) USED-FOR transfer attacks. query - free black - box setting FEATURE-OF transfer attacks. white - box surrogate models CONJUNCTION black - box victim models. black - box victim models CONJUNCTION white - box surrogate models. datasets USED-FOR surrogate models. method USED-FOR classification information. Image Classification Eraser ( ICE ) USED-FOR classification information. Image Classification Eraser ( ICE ) HYPONYM-OF method. Cifar-10 CONJUNCTION Cifar-100. Cifar-100 CONJUNCTION Cifar-10. Cifar-100 CONJUNCTION TieredImageNet. TieredImageNet CONJUNCTION Cifar-100. ICE USED-FOR GTA problem. Cifar-10 EVALUATE-FOR ICE. TieredImageNet EVALUATE-FOR ICE. transfer attack methods USED-FOR GTA problem. transfer attack methods COMPARE ICE. ICE COMPARE transfer attack methods. Task are transfer attack, and Generalized Transferable Attack ( GTA ) problem. Generic are dataset, and them. Method is victim model. ","This paper studies transfer attacks on Deep Neural Networks (DNNs) in the query-free black-box setting. The authors propose a new transfer attack, called the Generalized Transferable Attack (GTA) problem, where the target dataset is not available to the attacker, but is available to a victim model. They show that transfer attacks can be performed in a query-less, query-based, and query-driven setting. They also show that the transfer attack can be applied to both white-box surrogate models as well as black -box victim models, and that the surrogate models can be trained on the same datasets as the victim models. They propose a method called Image Classification Eraser (ICE) to remove classification information from the target and victim models in order to prevent them from being transferred. ICE is tested on Cifar-10, CIFar-100, and TieredImageNet, and shows that ICE is able to solve the GTA problem more effectively than existing transfer attack methods. ","This paper studies transfer attacks on Deep Neural Networks (DNNs) in the query-free black-box setting. The authors propose a new transfer attack, called the Generalized Transferable Attack (GTA) problem, where the target dataset is not available to the attacker, but is available to a victim model. They show that transfer attacks can be performed in a query-less, query-based, and query-driven setting. They also show that the transfer attack can be applied to both white-box surrogate models as well as black -box victim models, and that the surrogate models can be trained on the same datasets as the victim models. They propose a method called Image Classification Eraser (ICE) to remove classification information from the target and victim models in order to prevent them from being transferred. ICE is tested on Cifar-10, CIFar-100, and TieredImageNet, and shows that ICE is able to solve the GTA problem more effectively than existing transfer attack methods. "
7250,SP:2e0447c741a3f09be1095633d870200355211260,discriminative PrLM USED-FOR contextualized representation. robustness EVALUATE-FOR PrLMs. pre - training methods USED-FOR false negative predictions. pre - training methods USED-FOR pre - training language models. false negative issue FEATURE-OF discriminative PrLMs. false negative predictions FEATURE-OF gradient updates. Generic is model. Material is GLUE and SQuAD benchmarks. ,"This paper proposes a discriminative PrLM to learn a contextualized representation that can be used to improve robustness of PrLMs. The authors show that existing pre-training methods can lead to false negative predictions in the context of language models. The paper also shows that the false negative issue is a common issue in the training process of discriminatively PrLM-based language models, and the authors propose two ways to mitigate this issue. The first is to use gradient updates with respect to the true label of the model, while the second is to train a model that is robust to false negatives. Experiments are conducted on GLUE and SQuAD benchmarks, and show that the proposed method is effective in reducing false negative errors.  ","This paper proposes a discriminative PrLM to learn a contextualized representation that can be used to improve robustness of PrLMs. The authors show that existing pre-training methods can lead to false negative predictions in the context of language models. The paper also shows that the false negative issue is a common issue in the training process of discriminatively PrLM-based language models, and the authors propose two ways to mitigate this issue. The first is to use gradient updates with respect to the true label of the model, while the second is to train a model that is robust to false negatives. Experiments are conducted on GLUE and SQuAD benchmarks, and show that the proposed method is effective in reducing false negative errors.  "
7275,SP:281bc59d639aa76d84921b3ec4ce1ee8f1ba5b51,"semi - supervised learning USED-FOR real - world settings. ORCA HYPONYM-OF end - to - end approach. uncertainty adaptive margin USED-FOR ORCA. discriminability FEATURE-OF model. discriminability EVALUATE-FOR ORCA. image classification datasets CONJUNCTION single - cell dataset. single - cell dataset CONJUNCTION image classification datasets. single - cell dataset EVALUATE-FOR ORCA. ORCA COMPARE baselines. baselines COMPARE ORCA. seen CONJUNCTION novel classes. novel classes CONJUNCTION seen. image classification datasets EVALUATE-FOR ORCA. image classification datasets EVALUATE-FOR baselines. single - cell dataset EVALUATE-FOR baselines. ORCA USED-FOR novel classes. novel classes PART-OF ImageNet dataset. seen EVALUATE-FOR ORCA. ImageNet dataset EVALUATE-FOR ORCA. Material are unlabeled test data, labeled training data, open - world semi - supervised learning setting, and labeled and unlabeled data. Generic is assumption. Task is class distribution mismatch problem. OtherScientificTerm is prior knowledge. ","This paper studies semi-supervised learning in real-world settings, where unlabeled test data is available but labeled training data is not. The authors propose an end-to-end approach called ORCA, which is based on the assumption that the class distribution mismatch problem can be alleviated. The paper proposes to use an uncertainty adaptive margin to improve the discriminability of the model.    The paper shows that ORCA improves the class-distribution mismatch problem in the open-world semi-distributed learning setting, where the class is not known prior knowledge.  The authors also show that under certain assumptions, ORCA can achieve better discriminable performance on image classification datasets and a single-cell dataset.  In addition, the authors show ORCA outperforms several baselines on the ImageNet dataset in terms of seen and novel classes, and ORCA is able to learn novel classes that are more likely to be seen in the training data. ","This paper studies semi-supervised learning in real-world settings, where unlabeled test data is available but labeled training data is not. The authors propose an end-to-end approach called ORCA, which is based on the assumption that the class distribution mismatch problem can be alleviated. The paper proposes to use an uncertainty adaptive margin to improve the discriminability of the model.    The paper shows that ORCA improves the class-distribution mismatch problem in the open-world semi-distributed learning setting, where the class is not known prior knowledge.  The authors also show that under certain assumptions, ORCA can achieve better discriminable performance on image classification datasets and a single-cell dataset.  In addition, the authors show ORCA outperforms several baselines on the ImageNet dataset in terms of seen and novel classes, and ORCA is able to learn novel classes that are more likely to be seen in the training data. "
7300,SP:6c572c4c21b01a0cf3fd9ef97fbb348ef4e405ae,"SLIM - QN HYPONYM-OF light stochastic quasi - Newton optimizer. second - order methods USED-FOR large - scale DNNs. SLIM - QN USED-FOR second - order methods. L - BFGS HYPONYM-OF stochastic training. BFGS update rule USED-FOR Hessian inverse. gradients USED-FOR BFGS update rule. BFGS update rule USED-FOR SLIM - QN. momentum CONJUNCTION adaptive damping mechanism. adaptive damping mechanism CONJUNCTION momentum. momentum USED-FOR Hessian updates. adaptive damping mechanism USED-FOR SLIM - QN. momentum USED-FOR SLIM - QN. SLIM - QN USED-FOR stable convergence. SLIM - QN USED-FOR stochastic setting. convergence EVALUATE-FOR SLIM - QN. SLIM - QN COMPARE second - order methods. second - order methods COMPARE SLIM - QN. compute and memory overhead EVALUATE-FOR second - order methods. compute and memory overhead EVALUATE-FOR SLIM - QN. SLIM - QN COMPARE SGD. SGD COMPARE SLIM - QN. large datasets EVALUATE-FOR SLIM - QN. near optimal accuracy EVALUATE-FOR SGD. wall - clock time EVALUATE-FOR SGD. near optimal accuracy EVALUATE-FOR SLIM - QN. ImageNet HYPONYM-OF large datasets. compute resources USED-FOR SGD. compute resources USED-FOR SLIM - QN. SLIM - QN USED-FOR non - convolutional architectures. Transformers HYPONYM-OF non - convolutional architectures. Metric is computational cost. OtherScientificTerm are Hessian matrix, KFAC, and convergence instability. ","This paper proposes SLIM-QN, a light stochastic quasi-Newton optimizer that is able to reduce the computational cost of second-order methods for training large-scale DNNs.    The main contribution of this paper is to extend the work of L-BFGS, which is a popular method for stabilizing the Hessian matrix of a stochtastic training (L-BFBSGS), to the case where the gradient of Hessian inverse of the gradient is non-asymptotic.  The authors show that the BFGS update rule of SLIM - QN can be viewed as a simple modification of BFGS, and that the authors propose to use momentum and an adaptive damping mechanism to mitigate the instability in Hessian updates caused by the convergence instability of KFAC.  In addition, the authors provide a theoretical analysis of the stability of the convergence of the proposed SLIM QN in the stochastastic setting, showing that the momentum is responsible for stable convergence.  Finally, they show empirically that the convergence performance of the SLIMQN outperforms SGD in terms of wall-clock time on large datasets (e.g., ImageNet) and compute and memory overhead compared to second- order methods.  They also show that SLIMqn can be applied to non-convolutional architectures such as Transformers, where the compute resources of SGD can be reduced significantly. ","This paper proposes SLIM-QN, a light stochastic quasi-Newton optimizer that is able to reduce the computational cost of second-order methods for training large-scale DNNs.    The main contribution of this paper is to extend the work of L-BFGS, which is a popular method for stabilizing the Hessian matrix of a stochtastic training (L-BFBSGS), to the case where the gradient of Hessian inverse of the gradient is non-asymptotic.  The authors show that the BFGS update rule of SLIM - QN can be viewed as a simple modification of BFGS, and that the authors propose to use momentum and an adaptive damping mechanism to mitigate the instability in Hessian updates caused by the convergence instability of KFAC.  In addition, the authors provide a theoretical analysis of the stability of the convergence of the proposed SLIM QN in the stochastastic setting, showing that the momentum is responsible for stable convergence.  Finally, they show empirically that the convergence performance of the SLIMQN outperforms SGD in terms of wall-clock time on large datasets (e.g., ImageNet) and compute and memory overhead compared to second- order methods.  They also show that SLIMqn can be applied to non-convolutional architectures such as Transformers, where the compute resources of SGD can be reduced significantly. "
7325,SP:4bffce00ebb02d2e676eec897647ac14c3344deb,"Graph Neural Networks ( GNNs ) USED-FOR graph - related tasks. GNNs USED-FOR problems. redundant components PART-OF large graphs. pre - processing step USED-FOR graph. node or edge removals USED-FOR inference. LocalitySensitive Pruning ( LSP ) USED-FOR graph pruning. systematic method USED-FOR graph pruning. Locality - Sensitive Hashing USED-FOR LocalitySensitive Pruning ( LSP ). Locality - Sensitive Hashing USED-FOR systematic method. pruning COMPARE pruning strategies. pruning strategies COMPARE pruning. locality properties USED-FOR pruning. local graph properties USED-FOR pruning. synthetic and real - world datasets EVALUATE-FOR LSP. LSP USED-FOR edges. edges PART-OF large graphs. LSP USED-FOR large graphs. Task is real - world problems. OtherScientificTerm are real - world graphs, and sparsified graph. Method is GNNs layers. ","Graph Neural Networks (GNNs) have been widely used for graph-related tasks, and for many real-world problems. However, there are many redundant components in large graphs due to the fact that GNNs are trained to solve these problems. In this paper, the authors propose a systematic method called LocalitySensitive Pruning (LSP) based on Locality-Sensitive Hashing to perform graph pruning. In particular, a pre-processing step is used to sparsify the graph, and then node or edge removals are performed during inference. The authors show that LSP is able to prune edges from large graphs, and that the pruning is more efficient than other pruning strategies based on locality properties. They also show that the LSP can be applied to both synthetic and real world datasets.    The main contribution of this paper is that the authors provide a systematic approach to pruning large graphs based on local graph properties. The main idea is to first prune a sparsified graph and then to apply LSP to the edges that are most relevant to the task at hand. The paper also shows that the number of edges pruned by LSP does not depend on the size of the graph.  The authors also provide a theoretical analysis of the effect of pruning on the final performance of the pruned edges, and show that in some cases, LSP prunes the edges more efficiently than existing pruning methods. ","Graph Neural Networks (GNNs) have been widely used for graph-related tasks, and for many real-world problems. However, there are many redundant components in large graphs due to the fact that GNNs are trained to solve these problems. In this paper, the authors propose a systematic method called LocalitySensitive Pruning (LSP) based on Locality-Sensitive Hashing to perform graph pruning. In particular, a pre-processing step is used to sparsify the graph, and then node or edge removals are performed during inference. The authors show that LSP is able to prune edges from large graphs, and that the pruning is more efficient than other pruning strategies based on locality properties. They also show that the LSP can be applied to both synthetic and real world datasets.    The main contribution of this paper is that the authors provide a systematic approach to pruning large graphs based on local graph properties. The main idea is to first prune a sparsified graph and then to apply LSP to the edges that are most relevant to the task at hand. The paper also shows that the number of edges pruned by LSP does not depend on the size of the graph.  The authors also provide a theoretical analysis of the effect of pruning on the final performance of the pruned edges, and show that in some cases, LSP prunes the edges more efficiently than existing pruning methods. "
7350,SP:c5e024f4e2079586298519ca868630efd7579eca,"Data augmentation USED-FOR contrastive self - supervised learning. identitydistinctive information FEATURE-OF R(x ). it PART-OF R(x ). VAE ’s bottleneck space FEATURE-OF G(x ). identity - disentangled adversarial augmentation ( IDAA ) USED-FOR self - supervised learning methods. benchmark datasets EVALUATE-FOR IDAA. efficiency CONJUNCTION generalization performance. generalization performance CONJUNCTION efficiency. efficiency EVALUATE-FOR IDAA. generalization performance EVALUATE-FOR IDAA. dataset USED-FOR IDAA. Generic is augmentations. Task is ineffective learning. Method are adversarial augmentation method, and contrastive learning. OtherScientificTerm are hard positives / negatives, variational auto - encoder ( VAE ) reconstruction, VAE objective, augmentation, and sample identity. ","This paper studies the problem of data augmentation in contrastive self-supervised learning. The authors propose a novel adversarial augmentation method, IDAA, which augments samples with hard positives/negative. The key idea of IDAA is to use a variational auto-encoder (VAE) reconstruction of the data and augmentations to prevent the VAE from picking up hard negatives/negative samples, which can lead to ineffective learning. To achieve this, the authors propose to disentangle the identitydistinctive information of R(x) and G(x), which is a VAE objective. They show that IDAA can be used to learn a sample identity that is disentangled from other augmentations, and that it can be incorporated into the original dataset. They also show that this augmentation can be applied to any VAE’s bottleneck space, which is the case for contrastive learning. Experiments on several benchmark datasets show that the proposed IDAA improves efficiency and generalization performance.   ","This paper studies the problem of data augmentation in contrastive self-supervised learning. The authors propose a novel adversarial augmentation method, IDAA, which augments samples with hard positives/negative. The key idea of IDAA is to use a variational auto-encoder (VAE) reconstruction of the data and augmentations to prevent the VAE from picking up hard negatives/negative samples, which can lead to ineffective learning. To achieve this, the authors propose to disentangle the identitydistinctive information of R(x) and G(x), which is a VAE objective. They show that IDAA can be used to learn a sample identity that is disentangled from other augmentations, and that it can be incorporated into the original dataset. They also show that this augmentation can be applied to any VAE’s bottleneck space, which is the case for contrastive learning. Experiments on several benchmark datasets show that the proposed IDAA improves efficiency and generalization performance.   "
7375,SP:0991bc5f213bd8ab7572e2fed309e1b57a35835b,"tests USED-FOR distribution shifts. these USED-FOR arbitrary shifts. non - sequential methods USED-FOR these. method USED-FOR harmful shifts. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. sequential tools USED-FOR risk function of interest. calibration HYPONYM-OF risk function of interest. accuracy HYPONYM-OF risk function of interest. aggregation of statistical evidence USED-FOR tracking process. constructing time - uniform confidence sequences USED-FOR aggregation of statistical evidence. simulated and real datasets EVALUATE-FOR framework. Method is machine learning models. OtherScientificTerm are data distribution, and benign shifts. Generic is model. Metric is false alarm rate. ","This paper proposes a new method to detect harmful shifts in machine learning models. The idea is to train a model on the data distribution, and then perform a series of tests to detect distribution shifts, and these can be applied to arbitrary shifts in the data. The authors argue that existing non-sequential methods do not work well for these, and propose a method to identify harmful shifts.  The authors propose two sequential tools to identify the risk function of interest: accuracy and calibration. The tracking process is based on the aggregation of statistical evidence by constructing time-uniform confidence sequences. The proposed framework is tested on both simulated and real datasets, and the results show that the proposed method is able to detect benign shifts with a lower false alarm rate. ","This paper proposes a new method to detect harmful shifts in machine learning models. The idea is to train a model on the data distribution, and then perform a series of tests to detect distribution shifts, and these can be applied to arbitrary shifts in the data. The authors argue that existing non-sequential methods do not work well for these, and propose a method to identify harmful shifts.  The authors propose two sequential tools to identify the risk function of interest: accuracy and calibration. The tracking process is based on the aggregation of statistical evidence by constructing time-uniform confidence sequences. The proposed framework is tested on both simulated and real datasets, and the results show that the proposed method is able to detect benign shifts with a lower false alarm rate. "
7400,SP:1c7b954273e3a9cda333385b15a3e8ed3bf8178a,Neural networks USED-FOR dynamics of diverse physical systems. neural implicit representations USED-FOR appearance modeling. neural implicit representations CONJUNCTION neural ordinary differential equations ( ODEs ). neural ordinary differential equations ( ODEs ) CONJUNCTION neural implicit representations. neural ordinary differential equations ( ODEs ) USED-FOR interpretable physical models. visual observations USED-FOR interpretable physical models. neural implicit representations USED-FOR processing of high - resolution videos. neural implicit representations USED-FOR synthesis of photo - realistic imagery. processing of high - resolution videos CONJUNCTION synthesis of photo - realistic imagery. synthesis of photo - realistic imagery CONJUNCTION processing of high - resolution videos. model COMPARE approaches. approaches COMPARE model. model USED-FOR physical parameters. large training datasets USED-FOR approaches. embedded neural ODE USED-FOR identification of interpretable physical parameters. known parametric form FEATURE-OF embedded neural ODE. scenes USED-FOR photo - realistic rendering. physical parameters USED-FOR scenes. physical parameters USED-FOR photo - realistic rendering. method USED-FOR physical parameters. real - world videos USED-FOR method. pendulum motion HYPONYM-OF real - world videos. real - world videos USED-FOR physical parameters. physical parameters USED-FOR reconstruction. model USED-FOR metric length of the pendulum. monocular video USED-FOR model. monocular video USED-FOR metric length of the pendulum. Generic is they. Material is high - resolution videos. Task is long - term prediction in state space. OtherScientificTerm is state space. Metric is relative error. ,"Neural networks have been shown to capture the dynamics of diverse physical systems, and they can be applied to the problem of high-resolution videos. In this paper, the authors combine neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) to learn interpretable physical models from visual observations. The authors show that neural implicit representation can be used for the processing of high resolution videos, synthesis of photo-realistic imagery, and to learn physical parameters for the synthesis of scenes from scenes. The proposed method is able to identify physical parameters from real-world videos (e.g. pendulum motion). The proposed model outperforms existing approaches that rely on large training datasets. The key contribution of the paper is the identification of an embedded neural ODE with a known parametric form that allows for identification and long-term prediction in state space. The paper also shows that the proposed method can identify the physical parameters of a pendulum in a monocular video, and that the model can be trained to predict the metric length of the pendulum from a single frame of the monocular version of the video. The method can also be used to reconstruct physical parameters in a scene from a video.   The authors also show that physical parameters can be learned for the task of reconstruction from physical parameters. The relative error of the learned physical parameters is measured in terms of the number of times that the state space is changed. ","Neural networks have been shown to capture the dynamics of diverse physical systems, and they can be applied to the problem of high-resolution videos. In this paper, the authors combine neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) to learn interpretable physical models from visual observations. The authors show that neural implicit representation can be used for the processing of high resolution videos, synthesis of photo-realistic imagery, and to learn physical parameters for the synthesis of scenes from scenes. The proposed method is able to identify physical parameters from real-world videos (e.g. pendulum motion). The proposed model outperforms existing approaches that rely on large training datasets. The key contribution of the paper is the identification of an embedded neural ODE with a known parametric form that allows for identification and long-term prediction in state space. The paper also shows that the proposed method can identify the physical parameters of a pendulum in a monocular video, and that the model can be trained to predict the metric length of the pendulum from a single frame of the monocular version of the video. The method can also be used to reconstruct physical parameters in a scene from a video.   The authors also show that physical parameters can be learned for the task of reconstruction from physical parameters. The relative error of the learned physical parameters is measured in terms of the number of times that the state space is changed. "
7425,SP:51efd1451343f4994d857daa5490e299b812bc2d,"abrupt ( discontinuous ) context changes CONJUNCTION Markovian context evolution. Markovian context evolution CONJUNCTION abrupt ( discontinuous ) context changes. Bayesian approach CONJUNCTION variational inference. variational inference CONJUNCTION Bayesian approach. Bayesian approach USED-FOR it. variational inference USED-FOR it. sticky Hierarchical Dirichlet Process ( HDP ) prior USED-FOR model learning. sticky Hierarchical Dirichlet Process ( HDP ) prior USED-FOR Markov process modeling. context distillation procedure USED-FOR spurious contexts. optimal policy USED-FOR policy learning. RL algorithms USED-FOR policy learning. state - of - the - art methods USED-FOR frameworks. Generic are case, components, and approach. OtherScientificTerm are context cardinality assumption, and drone. ","This paper considers the problem of learning a policy in a Markov process with abrupt (discontinuous) context changes and Markovian context evolution. In this case, it uses a Bayesian approach, variational inference, and a sticky Hierarchical Dirichlet Process (HDP) prior for model learning. The authors show that under the context cardinality assumption, the optimal policy for policy learning can be learned by RL algorithms. They also show that a context distillation procedure can be used to remove spurious contexts. The paper also shows that this framework can be combined with existing state-of-the-art methods to improve the performance of these frameworks.    The paper is well-written and well-motivated. The approach is well motivated. However, there are a few issues:","This paper considers the problem of learning a policy in a Markov process with abrupt (discontinuous) context changes and Markovian context evolution. In this case, it uses a Bayesian approach, variational inference, and a sticky Hierarchical Dirichlet Process (HDP) prior for model learning. The authors show that under the context cardinality assumption, the optimal policy for policy learning can be learned by RL algorithms. They also show that a context distillation procedure can be used to remove spurious contexts. The paper also shows that this framework can be combined with existing state-of-the-art methods to improve the performance of these frameworks.    The paper is well-written and well-motivated. The approach is well motivated. However, there are a few issues:"
7450,SP:ea167b126212b2092bc1190d7f8376bf7c54a888,Knowledge enriched language representation learning USED-FOR knowledge - intensive NLP tasks. monolingual knowledge graph data USED-FOR knowledge based language models. framework USED-FOR knowledge based multilingual language models ( KMLMs ). pretraining tasks USED-FOR knowledge learning. language models USED-FOR logical patterns. intraand inter - sentence structures USED-FOR pretraining tasks. intraand inter - sentence structures FEATURE-OF data. language models USED-FOR factual knowledge. factual knowledge retrieval CONJUNCTION relation classification. relation classification CONJUNCTION factual knowledge retrieval. named entity recognition CONJUNCTION factual knowledge retrieval. factual knowledge retrieval CONJUNCTION named entity recognition. pretrained KMLMs USED-FOR knowledge - intensive cross - lingual NLP tasks. relation classification CONJUNCTION task. task CONJUNCTION relation classification. task HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. logic reasoning HYPONYM-OF task. named entity recognition HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. relation classification HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. factual knowledge retrieval HYPONYM-OF knowledge - intensive cross - lingual NLP tasks. Material is Wikidata knowledge graphs. Method is pretrained language models. ,"Knowledge enriched language representation learning is an important topic in knowledge-intensive NLP tasks. This paper proposes a framework for knowledge based multilingual language models (KMLMs) trained on monolingual knowledge graph data. The authors show that the pretraining tasks for knowledge learning are dependent on the intraand inter-sentence structures of the data. They show that language models trained on Wikidata knowledge graphs are able to capture logical patterns that can be used by language models to encode factual knowledge. They also show that pretrained KMLMs can be applied to a variety of knowledge-intrinsic cross-lingual NLI tasks, including named entity recognition, factual knowledge retrieval, relation classification, and a new task called logic reasoning. ","Knowledge enriched language representation learning is an important topic in knowledge-intensive NLP tasks. This paper proposes a framework for knowledge based multilingual language models (KMLMs) trained on monolingual knowledge graph data. The authors show that the pretraining tasks for knowledge learning are dependent on the intraand inter-sentence structures of the data. They show that language models trained on Wikidata knowledge graphs are able to capture logical patterns that can be used by language models to encode factual knowledge. They also show that pretrained KMLMs can be applied to a variety of knowledge-intrinsic cross-lingual NLI tasks, including named entity recognition, factual knowledge retrieval, relation classification, and a new task called logic reasoning. "
7475,SP:6c11cf29c90f923346372ba6f11452c36e69ad6d,"agents USED-FOR altruistic behaviour. task - agnostic manner USED-FOR altruistic behaviour. multi - agent environments EVALUATE-FOR approach. unsupervised agents COMPARE them. them COMPARE unsupervised agents. Method are artificial agents, reinforcement learning agents, altruistic agent, and human agents. OtherScientificTerm are external supervision, and altruistic agent ’s behaviour. Generic is concept. ","This paper studies the problem of training artificial agents to be altruistic in the presence of external supervision. In particular, the authors consider reinforcement learning agents. They show that agents trained in a task-agnostic manner can learn to exhibit altruistic behaviour in a multi-agent environment. The authors also show that, under certain conditions, agents trained with external supervision are able to learn to behave in a similar way as humans.    The authors propose a new concept called “altruistic agent’s behaviour”, which is defined as the behavior of an agent that behaves in a way that is similar to that of an altruistic agent that is trained in an unsupervised manner. The paper shows that this concept can be used to train agents that are more like human agents. The approach is tested in a number of multi-agents environments, and the authors show that their approach is able to outperform a number (but not all) of existing unsupervisory agents, and outperform them in some cases. ","This paper studies the problem of training artificial agents to be altruistic in the presence of external supervision. In particular, the authors consider reinforcement learning agents. They show that agents trained in a task-agnostic manner can learn to exhibit altruistic behaviour in a multi-agent environment. The authors also show that, under certain conditions, agents trained with external supervision are able to learn to behave in a similar way as humans.    The authors propose a new concept called “altruistic agent’s behaviour”, which is defined as the behavior of an agent that behaves in a way that is similar to that of an altruistic agent that is trained in an unsupervised manner. The paper shows that this concept can be used to train agents that are more like human agents. The approach is tested in a number of multi-agents environments, and the authors show that their approach is able to outperform a number (but not all) of existing unsupervisory agents, and outperform them in some cases. "
7500,SP:5dbc54201ba184266c5054f0d2944bd197bc307a,"Neural Tangent Kernel USED-FOR neural networks. finite - width neural networks USED-FOR double descent. interpolation threshold FEATURE-OF double descent behaviour. optimum FEATURE-OF Hessian. loss function USED-FOR double descent. neural networks CONJUNCTION Hessian spectra. Hessian spectra CONJUNCTION neural networks. OtherScientificTerm are Double descent, and population loss. Generic is models. Method are linear and kernel regression models, influence functions, and parametric model. ","This paper studies the double descent of neural networks with Neural Tangent Kernel. Double descent is a well-studied phenomenon in linear and kernel regression models, and the authors show that for finite-width neural networks, double descent can be observed in the limit of infinite width. The authors also show that double descent is also observed for models with infinite width, and that the interpolation threshold of double descent behaviour depends on the size of the population. The main contribution of the paper is that the authors provide a new loss function for double descent, which is based on the observation that the Hessian of the optimum of a Hessian can be approximated as a function of the number of influence functions, and this can be used as a parametric model. The paper also shows that neural networks and Hessian spectra can be related to neural networks. ","This paper studies the double descent of neural networks with Neural Tangent Kernel. Double descent is a well-studied phenomenon in linear and kernel regression models, and the authors show that for finite-width neural networks, double descent can be observed in the limit of infinite width. The authors also show that double descent is also observed for models with infinite width, and that the interpolation threshold of double descent behaviour depends on the size of the population. The main contribution of the paper is that the authors provide a new loss function for double descent, which is based on the observation that the Hessian of the optimum of a Hessian can be approximated as a function of the number of influence functions, and this can be used as a parametric model. The paper also shows that neural networks and Hessian spectra can be related to neural networks. "
7525,SP:b485114712055f39a7afb951dbc3db482ff523fd,"Graph convolutional networks ( GCNs ) USED-FOR graph - structured data. over - smoothing problem FEATURE-OF deep GCNs. expressive power CONJUNCTION trainability. trainability CONJUNCTION expressive power. trainability CONJUNCTION optimization perspective. optimization perspective CONJUNCTION trainability. expressive power CONJUNCTION optimization perspective. optimization perspective CONJUNCTION expressive power. expressive power EVALUATE-FOR deep GCNs. expressivity COMPARE trainability. trainability COMPARE expressivity. Graph Neural Tangent Kernel ( GNTK ) USED-FOR optimization trajectory. Graph Neural Tangent Kernel ( GNTK ) USED-FOR wide GCNs. gradient descent USED-FOR wide GCNs. Graph Neural Tangent Kernel ( GNTK ) USED-FOR gradient descent. gradient descent USED-FOR optimization trajectory. asymptotic behaviors USED-FOR dropping trainability. dropping trainability FEATURE-OF wide and deep GCNs. exponential rate FEATURE-OF optimization process. asymptotic behaviors FEATURE-OF GNTK. large depth FEATURE-OF asymptotic behaviors. exponential rate FEATURE-OF dropping trainability. large depth FEATURE-OF GNTK. exponential rate FEATURE-OF wide and deep GCNs. theoretical framework USED-FOR residual connection - based techniques. residual connection - based techniques USED-FOR exponential decay of trainability. method COMPARE counterparts. counterparts COMPARE method. infinite - width and finite - width FEATURE-OF counterparts. Method are node representations, gradient descentbased optimizer, and DropEdge. OtherScientificTerm are expressive space, and exponential decay problem. ","Graph convolutional networks (GCNs) for graph-structured data suffer from the over-smoothing problem. This paper studies the expressive power and trainability of deep GCNs in the context of node representations. The authors show that wide GCNs can be trained with gradient descent based on the Graph Neural Tangent Kernel (GNTK) of the optimization trajectory using gradient descent, and that the expressivity is better than trainability. They also provide asymptotic behaviors of the GNTK in terms of dropping trainability at large depth and exponential rate in the optimization process.   The authors also provide a theoretical framework for residual connection-based techniques to explain the exponential decay of trainability, and propose a gradient descentbased optimizer called DropEdge. The proposed method is shown to outperform its counterparts in both infinite-width and finite-width settings. The paper also provides a theoretical analysis of the expressive space and shows that DropEdge can be used to solve an exponential decay problem. ","Graph convolutional networks (GCNs) for graph-structured data suffer from the over-smoothing problem. This paper studies the expressive power and trainability of deep GCNs in the context of node representations. The authors show that wide GCNs can be trained with gradient descent based on the Graph Neural Tangent Kernel (GNTK) of the optimization trajectory using gradient descent, and that the expressivity is better than trainability. They also provide asymptotic behaviors of the GNTK in terms of dropping trainability at large depth and exponential rate in the optimization process.   The authors also provide a theoretical framework for residual connection-based techniques to explain the exponential decay of trainability, and propose a gradient descentbased optimizer called DropEdge. The proposed method is shown to outperform its counterparts in both infinite-width and finite-width settings. The paper also provides a theoretical analysis of the expressive space and shows that DropEdge can be used to solve an exponential decay problem. "
7550,SP:25a92b3583afdc6892e59f1e769125d52c8011af,Computer vision methods USED-FOR first - order dynamics. optical flow HYPONYM-OF first - order dynamics. acceleration HYPONYM-OF higher - order changes. blood pressure CONJUNCTION arterial disease. arterial disease CONJUNCTION blood pressure. second derivative USED-FOR blood pressure. second derivative USED-FOR arterial disease. heart rate HYPONYM-OF summary statistics. videos USED-FOR cardiac measurements. waveform morphology USED-FOR clinically impactful scenarios. accuracy EVALUATE-FOR waveform morphology. loss function USED-FOR neural models. neural models USED-FOR higher - order dynamics. second - derivative inputs USED-FOR second - order dynamics. model USED-FOR left ventricle ejection time ( LVET ) intervals. second derivative PART-OF training procedure. second derivative USED-FOR model. second derivative FEATURE-OF vital sign signals. OtherScientificTerm is cardiac pulse. Task is camera - based vital sign measurement. ,"This paper presents a method for learning a second-order dynamics of a cardiac signal. The method is based on the observation that current computer vision methods are unable to capture the first-order dynamic of the signal, which is the optical flow. The authors propose a method to learn a second order dynamics from the first order dynamics. They show that this method is able to capture higher-order changes (e.g. acceleration) in the second order, which can be used to improve the accuracy of cardiac measurements from videos. ","This paper presents a method for learning a second-order dynamics of a cardiac signal. The method is based on the observation that current computer vision methods are unable to capture the first-order dynamic of the signal, which is the optical flow. The authors propose a method to learn a second order dynamics from the first order dynamics. They show that this method is able to capture higher-order changes (e.g. acceleration) in the second order, which can be used to improve the accuracy of cardiac measurements from videos. "
7575,SP:0a88d2fcbdfab3e196bf6b9c75adb1006ab87536,"simple interactions USED-FOR human language. architecture USED-FOR communication system. symbolic mapping PART-OF communication system. symbolic mapping HYPONYM-OF architecture. symbolic mapping USED-FOR language learning. referential games USED-FOR symbolic mapping. symbolic mapping USED-FOR language learning. process USED-FOR multi - agent language learning. simplicity CONJUNCTION complexity. complexity CONJUNCTION simplicity. Task are emergent communication, and vocabulary expansion. OtherScientificTerm are language, and compositional and symmetric language. Material is dialog games. ","This paper studies the problem of emergent communication in a multi-agent setting, where agents are trained to communicate through simple interactions that are similar to human language. The authors propose a new architecture, called symbolic mapping, to learn a communication system that includes a symbolic mapping between agents. The symbolic mapping is learned through referential games, and the authors show that the symbolic mapping can be used for language learning in language learning. They also show that this process can be applied to multi-agents language learning, and that the learned language is a compositional and symmetric language.  The paper also shows that the process of vocabulary expansion can be seen as an extension of dialog games, where the goal is to increase the simplicity and complexity of the communication.  ","This paper studies the problem of emergent communication in a multi-agent setting, where agents are trained to communicate through simple interactions that are similar to human language. The authors propose a new architecture, called symbolic mapping, to learn a communication system that includes a symbolic mapping between agents. The symbolic mapping is learned through referential games, and the authors show that the symbolic mapping can be used for language learning in language learning. They also show that this process can be applied to multi-agents language learning, and that the learned language is a compositional and symmetric language.  The paper also shows that the process of vocabulary expansion can be seen as an extension of dialog games, where the goal is to increase the simplicity and complexity of the communication.  "
7600,SP:89575be04cb33b41d7a0a7b62f9496c2838a1317,Robotic agents USED-FOR domestic chores. natural language directives USED-FOR Robotic agents. hierarchical modular approach USED-FOR agents. hierarchy FEATURE-OF policy. language instructions USED-FOR subgoals. navigation policy CONJUNCTION independent interaction policies. independent interaction policies CONJUNCTION navigation policy. master policy USED-FOR agent ’s navigation. interaction policy USED-FOR object masks. object masks USED-FOR manipulation actions. interaction policy USED-FOR manipulation actions. ALFRED benchmark EVALUATE-FOR hierarchical agent. OtherScientificTerm is divide - andconquer manner. Method is Compositional Reasoning. ,"Robotic agents are trained to perform domestic chores using natural language directives. The authors propose a hierarchical modular approach to train such agents, where each policy has a hierarchy of subgoals that are executed in a divide-and-conquer manner. Each subgoal can be decomposed into a set of language instructions, and the agent’s navigation is guided by a master policy. The navigation policy and the independent interaction policies are trained separately. The interaction policy is used to generate object masks that are used to guide the manipulation actions. The hierarchical agent is evaluated on the ALFRED benchmark, where it is shown to be able to achieve better performance than Compositional Reasoning. ","Robotic agents are trained to perform domestic chores using natural language directives. The authors propose a hierarchical modular approach to train such agents, where each policy has a hierarchy of subgoals that are executed in a divide-and-conquer manner. Each subgoal can be decomposed into a set of language instructions, and the agent’s navigation is guided by a master policy. The navigation policy and the independent interaction policies are trained separately. The interaction policy is used to generate object masks that are used to guide the manipulation actions. The hierarchical agent is evaluated on the ALFRED benchmark, where it is shown to be able to achieve better performance than Compositional Reasoning. "
7625,SP:e2c8efe00db7baba2368f4f6a37815809b9e235e,"relationship USED-FOR model. Nuisance - Randomized Distillation ( NURD ) USED-FOR predictive models. nuisance - label relationship FEATURE-OF distributions. nuisance - randomized distribution HYPONYM-OF distribution. representations COMPARE representations. representations COMPARE representations. NURD USED-FOR representation. distribution PART-OF nuisance - varying family. NURD USED-FOR models. models USED-FOR pneumonia. NURD USED-FOR pneumonia. non - lung patches USED-FOR NURD. tasks EVALUATE-FOR NURD. non - lung patches USED-FOR nuisance. spurious correlations USED-FOR models. tasks EVALUATE-FOR NURD. chest X - ray classification EVALUATE-FOR NURD. chest X - ray classification HYPONYM-OF tasks. Task is prediction problems. OtherScientificTerm are nuisance variable, covariates, and background. Material is natural images. ","This paper proposes Nuisance-Randomized Distillation (NURD) to improve the performance of predictive models on prediction problems where the nuisance variable is variable-dependent. The authors consider the nuisance-label relationship between two distributions (i.e., nuisance-randomized distribution and nuisance-unlabeled distribution) and show that this relationship can be used to train a model. NURD learns a representation of the distribution of the nuisance in a nuisance-varying family that is more robust to spurious correlations between the covariates of the two distributions. The paper shows that such representations are more robust than the representations learned from natural images, and that models trained with spurious correlations are able to learn models that are robust to pneumonia in the presence of non-lung patches, which is a common nuisance in real-world datasets. The results on two tasks (chest X-ray classification and pneumonia classification) demonstrate the effectiveness of NURd on both tasks.","This paper proposes Nuisance-Randomized Distillation (NURD) to improve the performance of predictive models on prediction problems where the nuisance variable is variable-dependent. The authors consider the nuisance-label relationship between two distributions (i.e., nuisance-randomized distribution and nuisance-unlabeled distribution) and show that this relationship can be used to train a model. NURD learns a representation of the distribution of the nuisance in a nuisance-varying family that is more robust to spurious correlations between the covariates of the two distributions. The paper shows that such representations are more robust than the representations learned from natural images, and that models trained with spurious correlations are able to learn models that are robust to pneumonia in the presence of non-lung patches, which is a common nuisance in real-world datasets. The results on two tasks (chest X-ray classification and pneumonia classification) demonstrate the effectiveness of NURd on both tasks."
7650,SP:c75998b76f4e0510fc719d25959a10fc07db1c40,"computer vision models USED-FOR predefined categories. natural language USED-FOR supervision. InfoNCE loss USED-FOR model. model USED-FOR text captions. images CONJUNCTION text captions. text captions CONJUNCTION images. image - text pairs USED-FOR CLIP. Optimal TransporT distillation USED-FOR zero - shot Recognition. online entropic optimal transport USED-FOR soft image - text match. online entropic optimal transport USED-FOR contrastive learning. soft image - text match USED-FOR contrastive learning. Optimal TransporT distillation USED-FOR OTTER. online entropic optimal transport USED-FOR OTTER. pretrained image and text encoders USED-FOR models. image text pairs USED-FOR models. OTTER USED-FOR models. label smoothing CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION label smoothing. InfoNCE loss CONJUNCTION label smoothing. label smoothing CONJUNCTION InfoNCE loss. InfoNCE loss COMPARE OTTER. OTTER COMPARE InfoNCE loss. OTTER COMPARE baselines. baselines COMPARE OTTER. Google Open Images CONJUNCTION multi - labeled ImageNet. multi - labeled ImageNet CONJUNCTION Google Open Images. label smoothing COMPARE OTTER. OTTER COMPARE label smoothing. knowledge distillation COMPARE OTTER. OTTER COMPARE knowledge distillation. Google Open Images EVALUATE-FOR zero - shot evaluation. zero - shot evaluation EVALUATE-FOR baselines. zero - shot evaluation EVALUATE-FOR OTTER. multi - labeled ImageNet EVALUATE-FOR OTTER. OTTER COMPARE baselines. baselines COMPARE OTTER. dataset / architecture settings EVALUATE-FOR OTTER. OtherScientificTerm are visual concepts, and supervised "" gold "" labels. ","This paper proposes OTTER, a method for zero-shot classification that uses contrastive contrastive learning and InfoNCE loss to train a model to match images and text captions. The idea is to use computer vision models to learn predefined categories, and then use natural language as supervision to guide the learning of visual concepts. The authors propose to use CLIP with image-text pairs to train CLIP, and use Optimal TransporT distillation to train the model to learn to match two sets of images and a set of texts. OTTER uses online entropic optimal transport to learn a soft image - text match. The models are trained with both pretrained image and text encoders, and the models are fine-tuned on image text pairs. Experiments show that OTTER outperforms several baselines on zero-shoot evaluation on Google Open Images and multi-labeled ImageNet, outperforming baselines that do not use supervised ""gold"" labels. The experiments also show that the models trained with OTTER are more robust to label smoothing, label distillation, and knowledge distillation. The results are also shown in different dataset/architecture settings. ","This paper proposes OTTER, a method for zero-shot classification that uses contrastive contrastive learning and InfoNCE loss to train a model to match images and text captions. The idea is to use computer vision models to learn predefined categories, and then use natural language as supervision to guide the learning of visual concepts. The authors propose to use CLIP with image-text pairs to train CLIP, and use Optimal TransporT distillation to train the model to learn to match two sets of images and a set of texts. OTTER uses online entropic optimal transport to learn a soft image - text match. The models are trained with both pretrained image and text encoders, and the models are fine-tuned on image text pairs. Experiments show that OTTER outperforms several baselines on zero-shoot evaluation on Google Open Images and multi-labeled ImageNet, outperforming baselines that do not use supervised ""gold"" labels. The experiments also show that the models trained with OTTER are more robust to label smoothing, label distillation, and knowledge distillation. The results are also shown in different dataset/architecture settings. "
7675,SP:e83cd70377542b5d187998e2e4a7ac070f453ed6,Pix2Seq USED-FOR object detection. prior knowledge USED-FOR task. prior knowledge USED-FOR approaches. language modeling task USED-FOR object detection. bounding boxes CONJUNCTION class labels. class labels CONJUNCTION bounding boxes. neural net USED-FOR image. class labels HYPONYM-OF Object descriptions. bounding boxes HYPONYM-OF Object descriptions. it COMPARE detection algorithms. detection algorithms COMPARE it. task - specific data augmentations USED-FOR approach. COCO dataset EVALUATE-FOR detection algorithms. COCO dataset EVALUATE-FOR approach. COCO dataset EVALUATE-FOR it. Pix2Seq framework USED-FOR object detection. bounding boxes CONJUNCTION class labels. class labels CONJUNCTION bounding boxes. neural net USED-FOR image. ,"This paper proposes Pix2Seq, a language modeling task for object detection. Object descriptions such as bounding boxes, class labels, and class labels are used to train a neural net to generate an image from an image, which is then used as a prior knowledge for the task. This approach is evaluated on the COCO dataset, where it outperforms existing detection algorithms, and it is shown to be more robust to task-specific data augmentations.    The paper is well-written, well-motivated, and well-structured. It is a clear contribution to the community.  The authors propose a Pix2Segment framework that can be applied to any object detection task, and show that it can be used in conjunction with the Pix2Sq framework. ","This paper proposes Pix2Seq, a language modeling task for object detection. Object descriptions such as bounding boxes, class labels, and class labels are used to train a neural net to generate an image from an image, which is then used as a prior knowledge for the task. This approach is evaluated on the COCO dataset, where it outperforms existing detection algorithms, and it is shown to be more robust to task-specific data augmentations.    The paper is well-written, well-motivated, and well-structured. It is a clear contribution to the community.  The authors propose a Pix2Segment framework that can be applied to any object detection task, and show that it can be used in conjunction with the Pix2Sq framework. "
7700,SP:abc9315f61929cc1c54dfef8ff83d7eac56ec2f2,"Deep vision models PART-OF visual reinforcement learning ( RL ). visual reinforcement learning ( RL ) USED-FOR policy networks. Deep vision models USED-FOR policy networks. hierarchical reasoning PART-OF stage - wise approach. geometric and numerical symbols CONJUNCTION operators. operators CONJUNCTION geometric and numerical symbols. approach USED-FOR policy network. approach USED-FOR interpretable symbolic policy. policy network USED-FOR interpretable symbolic policy. geometric and numerical symbols PART-OF interpretable symbolic policy. operators PART-OF interpretable symbolic policy. policy regression algorithm USED-FOR symbolic rules. RoundTourMix HYPONYM-OF policy regression algorithm. distilled symbolic policy COMPARE CNN based RL agents. CNN based RL agents COMPARE distilled symbolic policy. symbolic distillation approach USED-FOR CNN policy. policy distillation USED-FOR geometric relations. numerical state USED-FOR Detected bounding box Velocity. CNN policy network knowledge USED-FOR symbolic policy. Pong CONJUNCTION CircusCharlie. CircusCharlie CONJUNCTION Pong. Airstriker - Genesis CONJUNCTION Pong. Pong CONJUNCTION Airstriker - Genesis. CircusCharlie CONJUNCTION Seaquest. Seaquest CONJUNCTION CircusCharlie. Generic is policies. OtherScientificTerm are interpretability, input distribution shifts, and teacher - student. Method are end - to - end learning pipeline, and symbolic distillation. ","Deep vision models in visual reinforcement learning (RL) are often used to train policy networks. However, these policies often lack interpretability due to input distribution shifts. To address this issue, this paper proposes a stage-wise approach that incorporates hierarchical reasoning into the training process. The approach first trains a policy network to learn an interpretable symbolic policy that combines geometric and numerical symbols and operators. Then, a policy regression algorithm (called RoundTourMix) is used to learn the symbolic rules. Finally, a symbolic distillation approach is applied to distill the CNN policy from the learned symbolic relations into a CNN policy network knowledge.    The paper shows that the distilled symbolic policy outperforms other CNN based RL agents on Airstriker-Genesis, Pong, CircusCharlie, and Seaquest. Detected bounding box Velocity is also shown to be interpretable. The paper also proposes an end-to-end learning pipeline, where the teacher-student is trained in a self-supervised manner, and the student is trained to learn a symbolic policy based on the teacher's knowledge of the symbolic relations. The authors also show that policy distillation can be applied to learn geometric relations in a more interpretable way.","Deep vision models in visual reinforcement learning (RL) are often used to train policy networks. However, these policies often lack interpretability due to input distribution shifts. To address this issue, this paper proposes a stage-wise approach that incorporates hierarchical reasoning into the training process. The approach first trains a policy network to learn an interpretable symbolic policy that combines geometric and numerical symbols and operators. Then, a policy regression algorithm (called RoundTourMix) is used to learn the symbolic rules. Finally, a symbolic distillation approach is applied to distill the CNN policy from the learned symbolic relations into a CNN policy network knowledge.    The paper shows that the distilled symbolic policy outperforms other CNN based RL agents on Airstriker-Genesis, Pong, CircusCharlie, and Seaquest. Detected bounding box Velocity is also shown to be interpretable. The paper also proposes an end-to-end learning pipeline, where the teacher-student is trained in a self-supervised manner, and the student is trained to learn a symbolic policy based on the teacher's knowledge of the symbolic relations. The authors also show that policy distillation can be applied to learn geometric relations in a more interpretable way."
7725,SP:04e7e181aeb1244ae1c4837ad416aef93ea3ea32,"coarse - level object arrangements ( posture ) CONJUNCTION fine - grained level styling ( identity ). fine - grained level styling ( identity ) CONJUNCTION coarse - level object arrangements ( posture ). exemplar sources USED-FOR fine - grained level styling ( identity ). techniques PART-OF StyleGAN2. techniques USED-FOR PIVQGAN. generator USED-FOR pose - identity disentanglement. VQSN module USED-FOR shaping and composition information. GANInversion encoder CONJUNCTION generator. generator CONJUNCTION GANInversion encoder. self - supervision methods USED-FOR GANInversion encoder. joint - training scheme USED-FOR generator. joint - training scheme CONJUNCTION self - supervision methods. self - supervision methods CONJUNCTION joint - training scheme. joint - training scheme USED-FOR GANInversion encoder. self - supervision methods USED-FOR generator. one CONJUNCTION other. other CONJUNCTION one. other USED-FOR identity. one USED-FOR pose. one HYPONYM-OF ones. training scheme USED-FOR VQSN module. VQSN module USED-FOR pose - related representations. VQSN module CONJUNCTION training scheme. training scheme CONJUNCTION VQSN module. synthesis image quality EVALUATE-FOR model. disentangling scores EVALUATE-FOR model. synthesis image quality CONJUNCTION disentangling scores. disentangling scores CONJUNCTION synthesis image quality. latent - space reducing feature USED-FOR VQSN module. posture - identity disentangling FEATURE-OF model applications. VQSN module USED-FOR model applications. latent - space reducing feature USED-FOR model applications. PIVQGAN USED-FOR Unsupervised image - to - image translation. disentangled posture and identity control USED-FOR PIVQGAN. PIVQGAN USED-FOR segmentation - like ” masks. Task is image - to - image translation task. Material are training - set images, pose images, and referential identity images. ","This paper proposes a new image-to-image translation model, called PIVQGAN, for the task of disentangling the pose and identity of an image from a set of training images. The proposed model is based on StyleGAN2, and is able to disentangle coarse-level object arrangements (pose) and fine-grained level styling (identity) from exemplar sources.    The proposed method is built on top of techniques from StyleGAN, and the authors propose two techniques to achieve this. The first is a joint-training scheme to train a GANInversion encoder and a generator to achieve pose-identity disentanglement. The second is a VQSN module to capture shaping and composition information from training-set images, and a training scheme to improve the performance of the proposed model.  The authors also propose a latent-space reducing feature to further improve the quality of the learned pose-related representations.  Experiments are conducted on three model applications, and show that the proposed PivQGAN achieves state-of-the-art synthesis image quality and disentangled scores for all model applications. Unsupervised learning of a segmentation-like “pose-like” masks is also performed on the image, and it is shown that the learned segmentation masks can be used for segmentation in the context of a training image. The paper also shows that the model can also be used to learn a “identity” mask, which is a mask that can be applied to any training images to achieve a better segmentation performance.  Finally, the authors also show that PIVGAN can achieve good performance on the task for the case where the pose images are not available for training (e.g. in the presence of referential identity images), and that it can also perform well on the “unsupervised” setting where the training images are available. The key contribution of the paper is that the authors have proposed a novel way to learn the disentanged posture and identity control, and that they have proposed two models, one for pose and the other for identity. ","This paper proposes a new image-to-image translation model, called PIVQGAN, for the task of disentangling the pose and identity of an image from a set of training images. The proposed model is based on StyleGAN2, and is able to disentangle coarse-level object arrangements (pose) and fine-grained level styling (identity) from exemplar sources.    The proposed method is built on top of techniques from StyleGAN, and the authors propose two techniques to achieve this. The first is a joint-training scheme to train a GANInversion encoder and a generator to achieve pose-identity disentanglement. The second is a VQSN module to capture shaping and composition information from training-set images, and a training scheme to improve the performance of the proposed model.  The authors also propose a latent-space reducing feature to further improve the quality of the learned pose-related representations.  Experiments are conducted on three model applications, and show that the proposed PivQGAN achieves state-of-the-art synthesis image quality and disentangled scores for all model applications. Unsupervised learning of a segmentation-like “pose-like” masks is also performed on the image, and it is shown that the learned segmentation masks can be used for segmentation in the context of a training image. The paper also shows that the model can also be used to learn a “identity” mask, which is a mask that can be applied to any training images to achieve a better segmentation performance.  Finally, the authors also show that PIVGAN can achieve good performance on the task for the case where the pose images are not available for training (e.g. in the presence of referential identity images), and that it can also perform well on the “unsupervised” setting where the training images are available. The key contribution of the paper is that the authors have proposed a novel way to learn the disentanged posture and identity control, and that they have proposed two models, one for pose and the other for identity. "
7750,SP:e51a7f45493064972585109f203a867e9828eb15,"speech synthesis CONJUNCTION speech enhancement. speech enhancement CONJUNCTION speech synthesis. speech recognition CONJUNCTION speech synthesis. speech synthesis CONJUNCTION speech recognition. Transformers USED-FOR speech processing tasks. speech enhancement HYPONYM-OF speech processing tasks. speech recognition HYPONYM-OF speech processing tasks. speech synthesis HYPONYM-OF speech processing tasks. models USED-FOR speech related tasks. speech - MLP HYPONYM-OF multi - layer perceptron ( MLP ) architecture. speech - MLP USED-FOR multiscale local temporal dependency. keyword spotting CONJUNCTION speech enhancement. speech enhancement CONJUNCTION keyword spotting. keyword spotting EVALUATE-FOR model. speech enhancement EVALUATE-FOR model. tasks EVALUATE-FOR model. speech enhancement HYPONYM-OF tasks. keyword spotting HYPONYM-OF tasks. Google speech command V2 - 35 CONJUNCTION LibriWords. LibriWords CONJUNCTION Google speech command V2 - 35. dataset ( VoiceBank ) USED-FOR speech enhancement. benchmark datasets USED-FOR keyword spotting. benchmark datasets EVALUATE-FOR speech enhancement. benchmark datasets CONJUNCTION dataset ( VoiceBank ). dataset ( VoiceBank ) CONJUNCTION benchmark datasets. Google speech command V2 - 35 HYPONYM-OF benchmark datasets. LibriWords HYPONYM-OF benchmark datasets. speech - MLP COMPARE transformer - based solutions. transformer - based solutions COMPARE speech - MLP. Material is speech signals. OtherScientificTerm are feature channels, contextual window sizes, and resource - constrained scenarios. Generic is chunks. Metric is GFLOPS. Method is transformers. ","This paper proposes a multi-layer perceptron (MLP) architecture, called speech-MLP, for the task of multi-modal speech processing. Transformers have been widely used in speech processing tasks such as speech recognition, speech synthesis, speech enhancement, and speech enhancement. However, these models have not been applied to other speech related tasks. The paper proposes to use multi-layered MLP to capture the multiscale local temporal dependency in the speech signals. The proposed model is evaluated on two tasks, namely, keyword spotting (using the dataset (VoiceBank) of Google speech command V2-35 and LibriWords), speech enhancement (using two benchmark datasets for keyword spotting and for speech enhancement), and speech synthesis (using a dataset (SpeechNet) for speech synthesis). The paper shows that the proposed speech- MLP achieves better performance than transformer-based solutions on all three tasks.    The paper also shows that transformers can be used to split the input speech signals into chunks, and that the feature channels can be shared across different chunks. The authors also show that the GFLOPS of the proposed model are comparable to the state-of-the-art transformers. The main contribution of the paper is that the model is able to handle contextual window sizes, which is useful in resource-constrained scenarios.","This paper proposes a multi-layer perceptron (MLP) architecture, called speech-MLP, for the task of multi-modal speech processing. Transformers have been widely used in speech processing tasks such as speech recognition, speech synthesis, speech enhancement, and speech enhancement. However, these models have not been applied to other speech related tasks. The paper proposes to use multi-layered MLP to capture the multiscale local temporal dependency in the speech signals. The proposed model is evaluated on two tasks, namely, keyword spotting (using the dataset (VoiceBank) of Google speech command V2-35 and LibriWords), speech enhancement (using two benchmark datasets for keyword spotting and for speech enhancement), and speech synthesis (using a dataset (SpeechNet) for speech synthesis). The paper shows that the proposed speech- MLP achieves better performance than transformer-based solutions on all three tasks.    The paper also shows that transformers can be used to split the input speech signals into chunks, and that the feature channels can be shared across different chunks. The authors also show that the GFLOPS of the proposed model are comparable to the state-of-the-art transformers. The main contribution of the paper is that the model is able to handle contextual window sizes, which is useful in resource-constrained scenarios."
7775,SP:d708d3886f4abd4552d8ccb2096df7361c803b13,"labeled data USED-FOR massive models. data collection CONJUNCTION labeling. labeling CONJUNCTION data collection. generalization error EVALUATE-FOR transfer learning algorithm. transfer learning algorithm USED-FOR lower bound. generalization error EVALUATE-FOR lower bound. computational complexity EVALUATE-FOR transfer learning algorithm. it USED-FOR source / target data distributions. source domains USED-FOR knowledge transfer. bounds USED-FOR setting. bounds USED-FOR generalization error. real image classification CONJUNCTION action recognition data sets. action recognition data sets CONJUNCTION real image classification. lower bounds COMPARE upper - bounds. upper - bounds COMPARE lower bounds. lower bounds COMPARE transfer learning base - lines. transfer learning base - lines COMPARE lower bounds. transfer learning base - lines USED-FOR upper - bounds. source(s ) and target data sets USED-FOR weighted empirical risk minimization. weighted empirical risk minimization USED-FOR upper - bounds. weighted empirical risk minimization USED-FOR transfer learning base - lines. Task are machine learning, and binary classification problems. Method is Transfer learning. Material are labeled training data, and real world data sets. ","This paper studies the problem of knowledge transfer in machine learning, where the goal is to transfer knowledge from a source domain to a target domain. Transfer learning has been a popular topic of interest in recent years, especially in the context of massive models trained on labeled data. However, there are many challenges in this setting, such as data collection, labeling, and the computational complexity of the transfer learning algorithm. This paper proposes a new lower bound on the generalization error of a transfer learning method, which is based on the observation that there is a trade-off between the cost of data collection and labeling. The authors show that the lower bound is tight, and that it depends on the source/target data distributions. The paper also shows that the bounds are tight in the setting where the source domains are different and the target domains are the same.   The authors also show that their bounds for binary classification problems are tight. They show that for real world data sets with labeled training data, their lower bounds are tighter than existing upper-bounds based on transfer learning base-lines based on weighted empirical risk minimization on both source(s) and target data sets. They also show bounds for real image classification and action recognition data sets, showing that the bound for the generalisation error is tight. ","This paper studies the problem of knowledge transfer in machine learning, where the goal is to transfer knowledge from a source domain to a target domain. Transfer learning has been a popular topic of interest in recent years, especially in the context of massive models trained on labeled data. However, there are many challenges in this setting, such as data collection, labeling, and the computational complexity of the transfer learning algorithm. This paper proposes a new lower bound on the generalization error of a transfer learning method, which is based on the observation that there is a trade-off between the cost of data collection and labeling. The authors show that the lower bound is tight, and that it depends on the source/target data distributions. The paper also shows that the bounds are tight in the setting where the source domains are different and the target domains are the same.   The authors also show that their bounds for binary classification problems are tight. They show that for real world data sets with labeled training data, their lower bounds are tighter than existing upper-bounds based on transfer learning base-lines based on weighted empirical risk minimization on both source(s) and target data sets. They also show bounds for real image classification and action recognition data sets, showing that the bound for the generalisation error is tight. "
7800,SP:f7511ba9ccad03233b34b1bf41bbac7361d20a57,"probabilistic shape completion method USED-FOR continuous geometry of large - scale 3D scenes. Generative Cellular Automata USED-FOR multi - modal distribution. formulation USED-FOR large - scale continuous geometry. latent code PART-OF sparse voxel embedding. sparse voxel embedding USED-FOR local continuous shape. progressive generation USED-FOR generative model. training objective USED-FOR sparse voxel embedding. variational lower bound FEATURE-OF complete shape distribution. variational lower bound USED-FOR training objective. probabilistic formulation USED-FOR geometry completion. approach COMPARE deterministic models. deterministic models COMPARE approach. Material are Real - world scans of 3D scenes, and missing data. Task is shape completion. Generic is model. ","This paper proposes a probabilistic shape completion method for the continuous geometry of large-scale 3D scenes. The formulation is based on the idea of Generative Cellular Automata (GCA) to learn a multi-modal distribution over the latent code of a sparse voxel embedding, which is then used to generate a local continuous shape from the sparse embedding. A progressive generation is used to train the generative model, and the training objective is a variational lower bound for the complete shape distribution.    The paper is well-written and well-motivated. Real-world scans of 3D scene are available, and shape completion is a very important problem. This paper shows that the proposed approach outperforms existing deterministic models, and is able to handle missing data well. The paper also shows that this approach can be applied to geometry completion, and that the probablistic formulation is more robust to missing data. The model is also shown to be able to generalize well to unseen objects.","This paper proposes a probabilistic shape completion method for the continuous geometry of large-scale 3D scenes. The formulation is based on the idea of Generative Cellular Automata (GCA) to learn a multi-modal distribution over the latent code of a sparse voxel embedding, which is then used to generate a local continuous shape from the sparse embedding. A progressive generation is used to train the generative model, and the training objective is a variational lower bound for the complete shape distribution.    The paper is well-written and well-motivated. Real-world scans of 3D scene are available, and shape completion is a very important problem. This paper shows that the proposed approach outperforms existing deterministic models, and is able to handle missing data well. The paper also shows that this approach can be applied to geometry completion, and that the probablistic formulation is more robust to missing data. The model is also shown to be able to generalize well to unseen objects."
7825,SP:d22d8f074adbe8fb0f25fb8f8d96201b3159bf6b,"exploration PART-OF deep reinforcement learning. Behavioral priors USED-FOR problem. reduced generality CONJUNCTION restricted transferability. restricted transferability CONJUNCTION reduced generality. exploration USED-FOR reinforcement learning. temporal consistency USED-FOR state - independent temporal priors. probabilistic mixture of policy and temporal prior USED-FOR off - policy reinforcement learning. approach COMPARE baselines. baselines COMPARE approach. long - horizon continuous control tasks EVALUATE-FOR baselines. sparse reward settings USED-FOR baselines. sparse reward settings USED-FOR long - horizon continuous control tasks. long - horizon continuous control tasks EVALUATE-FOR approach. sparse reward settings USED-FOR approach. OtherScientificTerm are temporal priors, and behavioral priors. ","This paper studies the problem of exploration in deep reinforcement learning. Behavioral priors are used to tackle this problem. The authors propose to use temporal priors that are state-independent and state-dependent. They show that this leads to reduced generality and restricted transferability in reinforcement learning, and propose a probabilistic mixture of policy and temporal prior for off-policy reinforcement learning based on temporal consistency. Experiments on long-horizon continuous control tasks show that the proposed approach outperforms baselines in sparse reward settings. ","This paper studies the problem of exploration in deep reinforcement learning. Behavioral priors are used to tackle this problem. The authors propose to use temporal priors that are state-independent and state-dependent. They show that this leads to reduced generality and restricted transferability in reinforcement learning, and propose a probabilistic mixture of policy and temporal prior for off-policy reinforcement learning based on temporal consistency. Experiments on long-horizon continuous control tasks show that the proposed approach outperforms baselines in sparse reward settings. "
7850,SP:25e06c022ae8b3cbbb8db413d7b534a1a5c92391,stochastic optimization USED-FOR deep neural networks. Learning rate scheduling HYPONYM-OF stochastic optimizers. Adam HYPONYM-OF stochastic optimizers. pre - defined rules USED-FOR scheduling. GNS USED-FOR dynamics. directed graph USED-FOR neural network. agent USED-FOR learning rate. directed graph USED-FOR GNS. GNS USED-FOR agent. reinforcement learning USED-FOR GNS. graph message passing network USED-FOR GNS. reinforcement learning USED-FOR agent. reinforcement learning USED-FOR learning rate. graph message passing network USED-FOR dynamics. scheduler USED-FOR intermediate layer information. reward collection procedure USED-FOR training. Fashion - MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION Fashion - MNIST. CIFAR10 CONJUNCTION GLUE. GLUE CONJUNCTION CIFAR10. Fashion - MNIST CONJUNCTION GLUE. GLUE CONJUNCTION Fashion - MNIST. GLUE USED-FOR language understanding. Fashion - MNIST CONJUNCTION image classification. image classification CONJUNCTION Fashion - MNIST. CIFAR10 USED-FOR image classification. image classification CONJUNCTION GLUE. GLUE CONJUNCTION image classification. Fashion - MNIST HYPONYM-OF benchmarking datasets. CIFAR10 HYPONYM-OF benchmarking datasets. GLUE HYPONYM-OF benchmarking datasets. Fashion - MNIST EVALUATE-FOR framework. GLUE EVALUATE-FOR framework. benchmarking datasets EVALUATE-FOR framework. GNS COMPARE baselines. baselines COMPARE GNS. GNS USED-FOR CNN and Transformer models. baselines USED-FOR CNN and Transformer models. network structures USED-FOR GNS. Method is scheduling mechanism. ,"This paper proposes a new stochastic optimization for deep neural networks, called Learning rate scheduling, which is a generalization of Adam. The main idea is to use pre-defined rules to guide the scheduling, and then use GNS to learn the dynamics of the dynamics using a graph message passing network. The agent learns the learning rate using reinforcement learning using a directed graph, and the scheduling mechanism is based on a scheduler that takes as input the intermediate layer information from the scheduler. The authors also propose a reward collection procedure to speed up training. The proposed framework is evaluated on three benchmarking datasets: Fashion-MNIST, CIFAR10, and GLUE for language understanding, image classification, and image classification. The results show that GNS outperforms the baselines for CNN and Transformer models with different network structures. ","This paper proposes a new stochastic optimization for deep neural networks, called Learning rate scheduling, which is a generalization of Adam. The main idea is to use pre-defined rules to guide the scheduling, and then use GNS to learn the dynamics of the dynamics using a graph message passing network. The agent learns the learning rate using reinforcement learning using a directed graph, and the scheduling mechanism is based on a scheduler that takes as input the intermediate layer information from the scheduler. The authors also propose a reward collection procedure to speed up training. The proposed framework is evaluated on three benchmarking datasets: Fashion-MNIST, CIFAR10, and GLUE for language understanding, image classification, and image classification. The results show that GNS outperforms the baselines for CNN and Transformer models with different network structures. "
7875,SP:d73cb0471c1770607ad3e4621cfc5f170683dd8e,"high - level relational reasoning CONJUNCTION scalable machine intelligence. scalable machine intelligence CONJUNCTION high - level relational reasoning. point cloud USED-FOR high - level relational reasoning. point cloud USED-FOR scalable machine intelligence. point cloud USED-FOR deep object - centric learning. framework USED-FOR 3D point cloud. framework USED-FOR spatial mixture model. Chamfer Mixture Loss PART-OF variational training pipeline. point clouds USED-FOR spatial mixture model. scheme USED-FOR SPAIR3D. unsupervised scene decomposition EVALUATE-FOR method. Method are object - specification scheme, and unsupervised manner. OtherScientificTerm is local voxel grid cell. ","This paper proposes a new object-specification scheme, called SPAIR3D, which uses a point cloud for deep object-centric learning. The point cloud is used for both high-level relational reasoning and scalable machine intelligence. The authors propose a new framework for 3D point cloud, which is based on Chamfer Mixture Loss in the variational training pipeline. The proposed spatial mixture model is trained on point clouds, where each object is represented as a local voxel grid cell. The paper shows that the proposed scheme can be used to train a 3D 3D object model in an unsupervised manner. The method is evaluated on the task of scene decomposition, and the results show that the method achieves state-of-the-art results. ","This paper proposes a new object-specification scheme, called SPAIR3D, which uses a point cloud for deep object-centric learning. The point cloud is used for both high-level relational reasoning and scalable machine intelligence. The authors propose a new framework for 3D point cloud, which is based on Chamfer Mixture Loss in the variational training pipeline. The proposed spatial mixture model is trained on point clouds, where each object is represented as a local voxel grid cell. The paper shows that the proposed scheme can be used to train a 3D 3D object model in an unsupervised manner. The method is evaluated on the task of scene decomposition, and the results show that the method achieves state-of-the-art results. "
7900,SP:3c57e921c1bf23e482551ceb71702931a7f07439,"world knowledge USED-FOR interactive environments. large language models ( LLMs ) USED-FOR interactive environments. large language models ( LLMs ) USED-FOR world knowledge. natural language USED-FOR high - level tasks. they USED-FOR high - level tasks. LMs USED-FOR high - level tasks. LMs USED-FOR they. VirtualHome environment EVALUATE-FOR method. method COMPARE LLM baseline. LLM baseline COMPARE method. executability EVALUATE-FOR LLM baseline. VirtualHome environment EVALUATE-FOR LLM baseline. executability EVALUATE-FOR method. executability CONJUNCTION correctness. correctness CONJUNCTION executability. language models1 USED-FOR actionable knowledge. OtherScientificTerm are actionable steps, low - level plans, and admissible actions. Method is LLMs. Generic is procedure. Metric is human evaluation. ","This paper proposes to use large language models (LLMs) to leverage world knowledge from natural language for interactive environments, where they are trained with LMs to solve high-level tasks in natural language. The proposed method is evaluated on the VirtualHome environment, where it is shown to outperform the LLM baseline in terms of executability and correctness.   The key idea of the paper is to learn a set of actionable steps that can be executed in the environment, and then use these actions to guide the agent to the next state. The actionable knowledge is learned by using language models1 and 2, where the low-level plans are learned in the same way as in LLMs, but the procedure is differentiable and can be applied to a large number of environments. The paper also proposes a human evaluation to evaluate the quality of the learned actions. The evaluation shows that the agent is able to learn admissible actions that are more likely to be executed by the agent.","This paper proposes to use large language models (LLMs) to leverage world knowledge from natural language for interactive environments, where they are trained with LMs to solve high-level tasks in natural language. The proposed method is evaluated on the VirtualHome environment, where it is shown to outperform the LLM baseline in terms of executability and correctness.   The key idea of the paper is to learn a set of actionable steps that can be executed in the environment, and then use these actions to guide the agent to the next state. The actionable knowledge is learned by using language models1 and 2, where the low-level plans are learned in the same way as in LLMs, but the procedure is differentiable and can be applied to a large number of environments. The paper also proposes a human evaluation to evaluate the quality of the learned actions. The evaluation shows that the agent is able to learn admissible actions that are more likely to be executed by the agent."
7925,SP:e0159d1c9df2e657892a3a0c77549df4698d9a1a,geometrical interpretation USED-FOR Variational Autoencoder framework. VAEs USED-FOR Riemannian structure of the learned latent space. vanilla VAE COMPARE VAE models. VAE models COMPARE vanilla VAE. geometrical considerations USED-FOR vanilla VAE. benchmark datasets EVALUATE-FOR VAE models. VAE USED-FOR Riemannian manifold. Riemannian manifold USED-FOR uniform distribution. method USED-FOR deep generative models. low data regime EVALUATE-FOR method. high dimensional data CONJUNCTION low sample sizes. low sample sizes CONJUNCTION high dimensional data. high dimensional data FEATURE-OF complex neuroimaging dataset. low sample sizes FEATURE-OF complex neuroimaging dataset. complex neuroimaging dataset EVALUATE-FOR method. ,"This paper proposes a Variational Autoencoder framework based on a geometrical interpretation of the Riemannian structure of the learned latent space of VAEs. The authors show that a vanilla VAE can be viewed as a special case of a VAE that takes into account the geometric considerations of the latent space, and that VAE models trained on standard benchmark datasets can be seen as a generalization of the results obtained by VAEs on a uniform distribution over the learned space of the VAE. They also show that the uniform distribution can be expressed as a function of a Riemanian manifold. The proposed method can be applied to deep generative models in a low data regime, and is shown to be able to generalize to a complex neuroimaging dataset with high dimensional data and low sample sizes.   ","This paper proposes a Variational Autoencoder framework based on a geometrical interpretation of the Riemannian structure of the learned latent space of VAEs. The authors show that a vanilla VAE can be viewed as a special case of a VAE that takes into account the geometric considerations of the latent space, and that VAE models trained on standard benchmark datasets can be seen as a generalization of the results obtained by VAEs on a uniform distribution over the learned space of the VAE. They also show that the uniform distribution can be expressed as a function of a Riemanian manifold. The proposed method can be applied to deep generative models in a low data regime, and is shown to be able to generalize to a complex neuroimaging dataset with high dimensional data and low sample sizes.   "
7950,SP:b4b8e1727f8617894f10f20365cb68de79f0e650,"natural language processing ( NLP ) CONJUNCTION computer vision tasks. computer vision tasks CONJUNCTION natural language processing ( NLP ). computer vision tasks EVALUATE-FOR transformers. natural language processing ( NLP ) EVALUATE-FOR transformers. attention heads USED-FOR applications. redundant heads PART-OF transformers. mixture of keys PART-OF transformer architecture. redundant heads PART-OF transformer architecture. Gaussian mixture model USED-FOR mixtures of keys. Transformer - MGK USED-FOR training. transformer counterpart COMPARE Transformer - MGK. Transformer - MGK COMPARE transformer counterpart. FLOPs USED-FOR Transformer - MGK. accuracy EVALUATE-FOR Transformer - MGK. linear attentions USED-FOR Transformer - MGK. language modeling CONJUNCTION tasks. tasks CONJUNCTION language modeling. applications EVALUATE-FOR Transformer - MGK. tasks HYPONYM-OF applications. tasks EVALUATE-FOR Transformer - MGK. language modeling HYPONYM-OF applications. Wikitext-103 and Long Range Arena benchmark EVALUATE-FOR Transformer - MGKs. Transformer - MGKs COMPARE baseline transformers. baseline transformers COMPARE Transformer - MGKs. Wikitext-103 and Long Range Arena benchmark EVALUATE-FOR baseline transformers. Method are Multi - head attention, and Transformer. OtherScientificTerm are redundant embedding, and attention head. Generic is model. ","This paper proposes Multi-head attention (MGK) to replace redundant heads in transformers in two applications: natural language processing (NLP) and computer vision tasks. The authors propose to use a mixture of keys in the transformer architecture, where the mixtures of keys are modeled as a Gaussian mixture model. The proposed Transformer-MGK is shown to be more efficient than its transformer counterpart in terms of FLOPs and training time. In addition, the authors show that the proposed model is more robust to redundant embedding and that the accuracy of the Transformer - MGK is better than the original Transformer with linear attentions. Experiments are conducted on three applications: language modeling, tasks in computer vision, and the Wikitext-103 and Long Range Arena benchmark, and show the effectiveness of the proposed method. ","This paper proposes Multi-head attention (MGK) to replace redundant heads in transformers in two applications: natural language processing (NLP) and computer vision tasks. The authors propose to use a mixture of keys in the transformer architecture, where the mixtures of keys are modeled as a Gaussian mixture model. The proposed Transformer-MGK is shown to be more efficient than its transformer counterpart in terms of FLOPs and training time. In addition, the authors show that the proposed model is more robust to redundant embedding and that the accuracy of the Transformer - MGK is better than the original Transformer with linear attentions. Experiments are conducted on three applications: language modeling, tasks in computer vision, and the Wikitext-103 and Long Range Arena benchmark, and show the effectiveness of the proposed method. "
7975,SP:82731dcce233e748f63382e09b6224a513fe9689,"biological agents USED-FOR Spatial navigation. proprioception CONJUNCTION linear and angular velocity. linear and angular velocity CONJUNCTION proprioception. direct - inverse model of environment dynamics USED-FOR image and action related signals. direct - inverse model of environment dynamics USED-FOR reconstruction of the action. direct - inverse model of environment dynamics USED-FOR two – dimensional continuous environment. Resetting Path Integrator ( RPI ) HYPONYM-OF minimalistic recurrent architecture. RPI USED-FOR internal state. it USED-FOR cognitive map. internal state FEATURE-OF minimal model. architecture COMPARE LSTM networks. LSTM networks COMPARE architecture. architecture USED-FOR internal dynamics. tasks EVALUATE-FOR LSTM networks. tasks EVALUATE-FOR architecture. Generic is models. OtherScientificTerm are image signal, resetting, and integration of past movement. Method is direct - inverse models. ","Spatial navigation is an important problem for biological agents. Spatial navigation has been a focus of interest in recent years, but most of the existing models are based on LSTM. In this paper, the authors propose a minimalistic recurrent architecture called the Resetting Path Integrator (RPI), which is a direct-inverse model of environment dynamics for both image and action related signals. The authors propose to use proprioception and linear and angular velocity to model the two-dimensional continuous environment, and use a directly inverse model of the environment dynamics to learn the reconstruction of the action from the image signal. The proposed RPI learns the internal state of a minimal model by resetting, and it learns the cognitive map of the minimal model, which is the integration of past movement with the current state. The paper shows that the proposed architecture is able to capture the internal dynamics better than previous state-of-the-art methods on a number of tasks, and outperforms LSTMs.   ","Spatial navigation is an important problem for biological agents. Spatial navigation has been a focus of interest in recent years, but most of the existing models are based on LSTM. In this paper, the authors propose a minimalistic recurrent architecture called the Resetting Path Integrator (RPI), which is a direct-inverse model of environment dynamics for both image and action related signals. The authors propose to use proprioception and linear and angular velocity to model the two-dimensional continuous environment, and use a directly inverse model of the environment dynamics to learn the reconstruction of the action from the image signal. The proposed RPI learns the internal state of a minimal model by resetting, and it learns the cognitive map of the minimal model, which is the integration of past movement with the current state. The paper shows that the proposed architecture is able to capture the internal dynamics better than previous state-of-the-art methods on a number of tasks, and outperforms LSTMs.   "
8000,SP:1a27c397d1e73def5e724c5c6f25548975ba50fa,"features USED-FOR prediction. feature learning USED-FOR neural networks. practical data USED-FOR learning problems. neural networks USED-FOR problems. gradient descent USED-FOR neural networks. linear models USED-FOR data - independent features. polynomial sizes FEATURE-OF linear models. polynomial sizes FEATURE-OF data - independent features. polynomial algorithm PART-OF Statistical Query model. neural networks USED-FOR feature learning. OtherScientificTerm are class relevant patterns, background patterns, and structure of the input distribution. Material is synthetic and real data. ","This paper studies the problem of feature learning for neural networks trained with gradient descent. The authors consider learning problems on practical data with class relevant patterns and background patterns. They show that linear models with polynomial sizes are able to learn data-independent features that are robust to changes in the structure of the input distribution. They also show that the features learned during feature learning can be used to improve the prediction performance.    The main contribution of the paper is to show that neural networks can be trained to solve these problems using gradient descent, and that the feature learning performed by neural networks is robust to change in the data structure.  The authors also propose a new Statistical Query model that incorporates the polynometric algorithm. Experiments are conducted on both synthetic and real data. ","This paper studies the problem of feature learning for neural networks trained with gradient descent. The authors consider learning problems on practical data with class relevant patterns and background patterns. They show that linear models with polynomial sizes are able to learn data-independent features that are robust to changes in the structure of the input distribution. They also show that the features learned during feature learning can be used to improve the prediction performance.    The main contribution of the paper is to show that neural networks can be trained to solve these problems using gradient descent, and that the feature learning performed by neural networks is robust to change in the data structure.  The authors also propose a new Statistical Query model that incorporates the polynometric algorithm. Experiments are conducted on both synthetic and real data. "
8025,SP:8ada73ed7eade9ebdeef376485e849c42575bc5f,robustness EVALUATE-FOR machine learning models. machine learning models USED-FOR adversarial examples. robustness FEATURE-OF adversarial examples. test - time adversaries FEATURE-OF adversarial examples. data distribution CONJUNCTION attacker constraints. attacker constraints CONJUNCTION data distribution. lower bounds USED-FOR model. bounds USED-FOR arbitrary classification functions. architectures CONJUNCTION models. models CONJUNCTION architectures. neural networks HYPONYM-OF architectures. neural networks HYPONYM-OF models. methodology USED-FOR robustness. robustness EVALUATE-FOR classifier. methodology USED-FOR fixed feature extractors. robustness EVALUATE-FOR fixed feature extractors. bounds USED-FOR classifier. bounds FEATURE-OF robustness. it USED-FOR classifier. method USED-FOR collisions. closed - form expressions USED-FOR collision finding. bespoke algorithm USED-FOR arbitrary feature extractors. closed - form expressions USED-FOR linear feature extractors. convex program USED-FOR bespoke algorithm. Method is training methods. ,"This paper studies the robustness of machine learning models to adversarial examples from test-time adversaries. The authors propose a methodology to improve robustness for a classifier by using lower bounds on the model's ability to distinguish between data distribution and attacker constraints. They show that the bounds are tight for arbitrary classification functions and can be extended to arbitrary architectures and models (e.g., neural networks). They also show that their methodology improves robustness to collision attacks for fixed feature extractors, and that it can be applied to any classifier. They also propose a bespoke algorithm that uses closed-form expressions for collision finding, which is based on a convex program. The method is shown to be robust to collisions in the presence of adversarial attacks.   ","This paper studies the robustness of machine learning models to adversarial examples from test-time adversaries. The authors propose a methodology to improve robustness for a classifier by using lower bounds on the model's ability to distinguish between data distribution and attacker constraints. They show that the bounds are tight for arbitrary classification functions and can be extended to arbitrary architectures and models (e.g., neural networks). They also show that their methodology improves robustness to collision attacks for fixed feature extractors, and that it can be applied to any classifier. They also propose a bespoke algorithm that uses closed-form expressions for collision finding, which is based on a convex program. The method is shown to be robust to collisions in the presence of adversarial attacks.   "
8050,SP:874b5fa51924cbcceed490d98a0ea80f74586b32,RL USED-FOR real - world problems. Offline reinforcement learning ( RL ) USED-FOR RL. regularization or constraints USED-FOR extrapolation error. regularization or constraints USED-FOR offline RL algorithms. framework USED-FOR V -function. learning procedure PART-OF offline dataset. optimal value learning CONJUNCTION behavior cloning. behavior cloning CONJUNCTION optimal value learning. conservatism FEATURE-OF offline learning. Expectile V -Learning ( EVL ) USED-FOR generalization. implicit planning USED-FOR V -values. offline trajectories USED-FOR implicit planning. Value - based Episodic Memory ( VEM ) HYPONYM-OF offline method. D4RL benchmark EVALUATE-FOR method. sparse - reward tasks HYPONYM-OF tasks. tasks EVALUATE-FOR method. sparse - reward tasks EVALUATE-FOR method. D4RL benchmark EVALUATE-FOR VEM method. OtherScientificTerm is Q - function. ," Offline reinforcement learning (RL) is an important problem in RL to solve real-world problems. However, existing offline RL algorithms rely on regularization or constraints to reduce the extrapolation error. This paper proposes a new offline method called Value-based Episodic Memory (VEM), which is an extension of the Expectile V-Learning (EVL) framework to learn a V-function. The framework is based on the observation that the learning procedure in an offline dataset is conservative in the sense that the Q-function tends to converge to a stationary point. The paper then proposes a framework that combines optimal value learning and behavior cloning to improve the conservatism of offline learning. The proposed method is evaluated on the D4RL benchmark and is shown to outperform the state-of-the-art method on a number of tasks, including sparse-reward tasks. It is also shown that implicit planning on the V-values obtained from offline trajectories can be used to perform implicit planning in the future."," Offline reinforcement learning (RL) is an important problem in RL to solve real-world problems. However, existing offline RL algorithms rely on regularization or constraints to reduce the extrapolation error. This paper proposes a new offline method called Value-based Episodic Memory (VEM), which is an extension of the Expectile V-Learning (EVL) framework to learn a V-function. The framework is based on the observation that the learning procedure in an offline dataset is conservative in the sense that the Q-function tends to converge to a stationary point. The paper then proposes a framework that combines optimal value learning and behavior cloning to improve the conservatism of offline learning. The proposed method is evaluated on the D4RL benchmark and is shown to outperform the state-of-the-art method on a number of tasks, including sparse-reward tasks. It is also shown that implicit planning on the V-values obtained from offline trajectories can be used to perform implicit planning in the future."
8086,SP:34f08d92681504490c2f739b0d08f79f9764b2f5,"robust accuracy CONJUNCTION standard accuracy. standard accuracy CONJUNCTION robust accuracy. neural network classifiers USED-FOR adversarial perturbations. robustness FEATURE-OF neural network classifiers. adversarial training USED-FOR adversarial perturbations. adversarial training framework USED-FOR robust generalization. importance weight USED-FOR parametric function. bilevel optimization problem USED-FOR weighted adversarial training. approach COMPARE state - of - the - art baselines. state - of - the - art baselines COMPARE approach. approach COMPARE techniques. techniques COMPARE approach. clean and robust accuracy EVALUATE-FOR techniques. techniques CONJUNCTION state - of - the - art baselines. state - of - the - art baselines CONJUNCTION techniques. clean and robust accuracy EVALUATE-FOR state - of - the - art baselines. clean and robust accuracy EVALUATE-FOR approach. OtherScientificTerm are class - conditioned margin, and sample ’s multi - class margin. Method are MAML - based approaches, and robust classifier. Generic is upper - level task. ","This paper studies the robustness of neural network classifiers against adversarial perturbations in order to achieve a trade-off between robust accuracy and standard accuracy. The authors propose a new adversarial training framework for robust generalization based on the MAML-based approaches. In particular, the authors show that adversarial robustness can be achieved by minimizing the class-conditioned margin between the sample’s multi-class margin and the upper-level margin of the robust classifier. To achieve this, they propose a parametric function based on importance weight. They also propose a bilevel optimization problem for weighted adversarial learning. Experiments show that the proposed approach outperforms existing techniques and state-of-the-art baselines in terms of clean and robust accuracy on a number of datasets.","This paper studies the robustness of neural network classifiers against adversarial perturbations in order to achieve a trade-off between robust accuracy and standard accuracy. The authors propose a new adversarial training framework for robust generalization based on the MAML-based approaches. In particular, the authors show that adversarial robustness can be achieved by minimizing the class-conditioned margin between the sample’s multi-class margin and the upper-level margin of the robust classifier. To achieve this, they propose a parametric function based on importance weight. They also propose a bilevel optimization problem for weighted adversarial learning. Experiments show that the proposed approach outperforms existing techniques and state-of-the-art baselines in terms of clean and robust accuracy on a number of datasets."
8122,SP:3ad36be6b6900aabe43da043461cf178ce977082,"force CONJUNCTION velocity. velocity CONJUNCTION force. position CONJUNCTION force. force CONJUNCTION position. velocity CONJUNCTION spin. spin CONJUNCTION velocity. computational physics CONJUNCTION chemistry. chemistry CONJUNCTION computational physics. spin HYPONYM-OF covariant information. velocity HYPONYM-OF covariant information. position HYPONYM-OF covariant information. force HYPONYM-OF covariant information. vectors HYPONYM-OF covariant information. geometric and physical information USED-FOR message and update functions. steerable MLPs PART-OF model. geometric and physical information PART-OF model. MLPs USED-FOR activation functions. activation functions USED-FOR steerable feature fields. MLPs USED-FOR steerable feature fields. components PART-OF SEGNNs. non - linear message aggregation CONJUNCTION linear ( steerable ) point convolutions. linear ( steerable ) point convolutions CONJUNCTION non - linear message aggregation. invariant messages FEATURE-OF equivariant graph networks. equivariant graph networks USED-FOR steerable messages. non - linear message aggregation HYPONYM-OF components. non - linear message aggregation PART-OF SEGNNs. computational physics CONJUNCTION chemistry. chemistry CONJUNCTION computational physics. chemistry EVALUATE-FOR method. computational physics EVALUATE-FOR method. OtherScientificTerm are node and edge attributes, invariant scalars, and steerable node attributes. Method is equivariant non - linear convolutions. ","This paper proposes steerable message passing neural networks (SEGNNs) that are equivariant to node and edge attributes. The proposed model is based on steerable MLPs, which incorporate geometric and physical information to the message and update functions. The covariant information is covariant in the form of position, force, velocity, and spin.   The authors propose two activation functions for steerable feature fields based on MLPs. These activation functions can be seen as invariant scalars. The authors show that equivariance to steerable node attributes can be achieved by equivariancing the node attributes.  The paper also shows that steerable messages can be obtained by equivarient graph networks with invariant messages, which is an extension of the work on invariant non-linear convolutions [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,20].  The proposed method is evaluated on computational physics and chemistry tasks, and the results show that the proposed method outperforms the state-of-the-art methods.  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10]","This paper proposes steerable message passing neural networks (SEGNNs) that are equivariant to node and edge attributes. The proposed model is based on steerable MLPs, which incorporate geometric and physical information to the message and update functions. The covariant information is covariant in the form of position, force, velocity, and spin.   The authors propose two activation functions for steerable feature fields based on MLPs. These activation functions can be seen as invariant scalars. The authors show that equivariance to steerable node attributes can be achieved by equivariancing the node attributes.  The paper also shows that steerable messages can be obtained by equivarient graph networks with invariant messages, which is an extension of the work on invariant non-linear convolutions [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,20].  The proposed method is evaluated on computational physics and chemistry tasks, and the results show that the proposed method outperforms the state-of-the-art methods.  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10]"
8158,SP:8928aa83f7ebd4e310f4fe1d01ff0eb0c96e4d2b,"physics models CONJUNCTION gradient - based learning. gradient - based learning CONJUNCTION physics models. model explicability CONJUNCTION data efficiency. data efficiency CONJUNCTION model explicability. Differentiable physics modeling USED-FOR model explicability. gradient - based learning PART-OF Differentiable physics modeling. physics models PART-OF Differentiable physics modeling. It USED-FOR dynamics. It USED-FOR inverse problems. It USED-FOR design. inverse problems CONJUNCTION design. design CONJUNCTION inverse problems. dynamics CONJUNCTION inverse problems. inverse problems CONJUNCTION dynamics. rigid bodies CONJUNCTION deformable sheets. deformable sheets CONJUNCTION rigid bodies. rigid bodies HYPONYM-OF physics models. deformable sheets HYPONYM-OF physics models. material structures CONJUNCTION force interactions. force interactions CONJUNCTION material structures. Fine - grained models USED-FOR material structures. gradient - based learning USED-FOR Fine - grained models. Fine - grained models USED-FOR force interactions. gradient - based learning USED-FOR force interactions. individual yarn physics CONJUNCTION yarn - to - yarn interactions. yarn - to - yarn interactions CONJUNCTION individual yarn physics. differentiable fabrics model USED-FOR composite materials. cloths HYPONYM-OF composite materials. differentiable forces USED-FOR gradient - based learning. differentiable forces PART-OF empirical physics. forces USED-FOR cloths. complex physical structures CONJUNCTION heterogeneous materials. heterogeneous materials CONJUNCTION complex physical structures. data - efficiency CONJUNCTION high - fidelity. high - fidelity CONJUNCTION data - efficiency. model USED-FOR physical parameters. high - fidelity USED-FOR subtle dynamics. model USED-FOR subtle dynamics. high - fidelity EVALUATE-FOR model. data - efficiency EVALUATE-FOR model. OtherScientificTerm are complex physical phenomena, and granularity of yarns. Material is physical systems. ","Differentiable physics modeling combines physics models and gradient-based learning to achieve model explicability and data efficiency. It can model dynamics, inverse problems, and design. Fine-grained models are used to model material structures and force interactions, and physics models such as rigid bodies and deformable sheets are used. The paper proposes a differentiable fabrics model for composite materials such as cloths, where the differentiable forces in empirical physics can be used to train a model to capture subtle dynamics. The authors show that these forces can be applied to cloths to capture complex physical phenomena such as individual yarn physics and yarn-to-yoke interactions. They also show that the granularity of yarns can be controlled by the model, and that the model is able to capture physical parameters in a way that allows for data-efficiency and high-fidelity.   ","Differentiable physics modeling combines physics models and gradient-based learning to achieve model explicability and data efficiency. It can model dynamics, inverse problems, and design. Fine-grained models are used to model material structures and force interactions, and physics models such as rigid bodies and deformable sheets are used. The paper proposes a differentiable fabrics model for composite materials such as cloths, where the differentiable forces in empirical physics can be used to train a model to capture subtle dynamics. The authors show that these forces can be applied to cloths to capture complex physical phenomena such as individual yarn physics and yarn-to-yoke interactions. They also show that the granularity of yarns can be controlled by the model, and that the model is able to capture physical parameters in a way that allows for data-efficiency and high-fidelity.   "
8194,SP:2c8358c095b10981d3015b9f6c75765419a9480d,"logical composition USED-FOR framework. logical composition USED-FOR reinforcement learning. OtherScientificTerm are task - specific skill, optimal policy, Boolean expression, unknown distribution, and task distribution. Generic are algorithm, distribution, approach, and tasks. Method are transferred policy, transfer learning, and transfer learning approach. ","This paper proposes a new framework for reinforcement learning based on logical composition. In this framework, a task-specific skill is represented as a Boolean expression, and the goal is to learn an optimal policy that maximizes the expected return of the learned skill. The authors propose an algorithm for learning the optimal policy, and show that this algorithm can be seen as a generalization of a previous work (Zhang et al., 2017). The authors also show that the distribution of the optimal policies learned by the algorithm is similar to that of the distribution learned by Zhang et al. (2017).    The key idea of the proposed approach is to use an unknown distribution as a surrogate for the task distribution, and then learn a transferred policy that minimizes this distribution. This approach is called transfer learning, and it is shown that this transfer learning approach can be applied to a variety of tasks.  ","This paper proposes a new framework for reinforcement learning based on logical composition. In this framework, a task-specific skill is represented as a Boolean expression, and the goal is to learn an optimal policy that maximizes the expected return of the learned skill. The authors propose an algorithm for learning the optimal policy, and show that this algorithm can be seen as a generalization of a previous work (Zhang et al., 2017). The authors also show that the distribution of the optimal policies learned by the algorithm is similar to that of the distribution learned by Zhang et al. (2017).    The key idea of the proposed approach is to use an unknown distribution as a surrogate for the task distribution, and then learn a transferred policy that minimizes this distribution. This approach is called transfer learning, and it is shown that this transfer learning approach can be applied to a variety of tasks.  "
8230,SP:c85d71d05164d019cc32bf423e4c4fe20c169f41,"machine and deep learning solutions USED-FOR multivariate time series classification ( MTSC ). prediction accuracy EVALUATE-FOR complex models. accuracy EVALUATE-FOR solutions. ROCKET HYPONYM-OF MTSC solution. random convolutional kernels USED-FOR ROCKET. random convolutional kernels USED-FOR MTSC solution. distributed solution USED-FOR MTSC. LightWaveS HYPONYM-OF distributed solution. solution COMPARE deep learning solutions. deep learning solutions COMPARE solution. wavelet scattering transformation CONJUNCTION distributed feature selection. distributed feature selection CONJUNCTION wavelet scattering transformation. distributed feature selection USED-FOR solution. accuracy EVALUATE-FOR deep learning solutions. wavelet scattering transformation USED-FOR solution. wavelet scattering transformation USED-FOR time series. ROCKET features USED-FOR solution. accuracy EVALUATE-FOR solution. nodes CONJUNCTION channels. channels CONJUNCTION nodes. nodes USED-FOR LightWaveS. channels USED-FOR LightWaveS. it USED-FOR MTSC problem. inference speedup CONJUNCTION scalability. scalability CONJUNCTION inference speedup. accuracy CONJUNCTION inference speedup. inference speedup CONJUNCTION accuracy. training time CONJUNCTION accuracy. accuracy CONJUNCTION training time. training time EVALUATE-FOR algorithm. ROCKET USED-FOR inference. speedup EVALUATE-FOR ROCKET. speedup EVALUATE-FOR inference. edge device USED-FOR inference. datasets EVALUATE-FOR speedup. datasets EVALUATE-FOR inference. OtherScientificTerm are real - world environments, and features. Metric are prediction speed, and inference time. Task is training. ","This paper proposes a new method for multivariate time series classification (MTSC) that combines machine and deep learning solutions for the task. The authors show that the proposed method, called ROCKET, is able to achieve state-of-the-art performance in terms of prediction accuracy for complex models. The main contribution of the paper is the introduction of a novel MTSC solution based on random convolutional kernels.    The authors also show that their solution is more computationally efficient than existing deep learning approaches for the same problem, and that it can be applied to any MTSc problem.  The proposed method is based on the wavelet scattering transformation and distributed feature selection, which is a well-studied technique in real-world environments. In particular, LightWaveS is a distributed solution for the MtsC problem, where the time series is partitioned into multiple time series and each time series has a different number of nodes and channels. The proposed solution uses the same amount of nodes, channels, and features, but uses a different amount of time to process the different time series. The solution is trained on the same number of time series, and uses the ROCKET features as input, and the solution is evaluated on both training time and accuracy. The paper shows that the algorithm achieves a significant speedup in training time, accuracy, inference speedup, and accuracy with respect to the training time. In addition, the speedup of inference is further improved when the inference is performed on an edge device. ","This paper proposes a new method for multivariate time series classification (MTSC) that combines machine and deep learning solutions for the task. The authors show that the proposed method, called ROCKET, is able to achieve state-of-the-art performance in terms of prediction accuracy for complex models. The main contribution of the paper is the introduction of a novel MTSC solution based on random convolutional kernels.    The authors also show that their solution is more computationally efficient than existing deep learning approaches for the same problem, and that it can be applied to any MTSc problem.  The proposed method is based on the wavelet scattering transformation and distributed feature selection, which is a well-studied technique in real-world environments. In particular, LightWaveS is a distributed solution for the MtsC problem, where the time series is partitioned into multiple time series and each time series has a different number of nodes and channels. The proposed solution uses the same amount of nodes, channels, and features, but uses a different amount of time to process the different time series. The solution is trained on the same number of time series, and uses the ROCKET features as input, and the solution is evaluated on both training time and accuracy. The paper shows that the algorithm achieves a significant speedup in training time, accuracy, inference speedup, and accuracy with respect to the training time. In addition, the speedup of inference is further improved when the inference is performed on an edge device. "
8266,SP:db43614ca016280a79448f44a97c81c8ff5ba981,"AMOS USED-FOR text encoders. Mixture Of Signals USED-FOR auxiliary generators. Mixture Of Signals USED-FOR Adversarial learning curriculum. Adversarial learning curriculum USED-FOR text encoders. discriminator USED-FOR replaced tokens. discriminator USED-FOR encoder. encoder USED-FOR replaced tokens. auxiliary masked language models ( MLMs ) USED-FOR replaced tokens. MLMs USED-FOR training signals. mixture weights USED-FOR discriminator loss. gradient PART-OF discriminator. mixture weights USED-FOR auxiliary MLMs ’ outputs. Gumbel - Softmax USED-FOR gradient. MLMs PART-OF unified auxiliary model. AMOS COMPARE pretrained models. pretrained models COMPARE AMOS. AMOS COMPARE ELECTRA. ELECTRA COMPARE AMOS. GLUE and SQuAD benchmarks EVALUATE-FOR BERT base - sized models. ELECTRA COMPARE pretrained models. pretrained models COMPARE ELECTRA. BERT base - sized models EVALUATE-FOR pretrained models. GLUE and SQuAD benchmarks EVALUATE-FOR pretrained models. GLUE and SQuAD benchmarks EVALUATE-FOR AMOS. Method are ELECTRA - style pretraining, and MLM. Metric is pretraining efficiency. ","This paper proposes a new adversarial learning curriculum called AMOS for text encoders based on Mixture Of Signals for auxiliary generators. The idea is to train a discriminator to distinguish between replaced tokens generated by auxiliary masked language models (MLMs) and the ones generated by the original MLM. The discriminator is then used to train an encoder to generate the replaced tokens. The authors propose a unified auxiliary model consisting of two MLMs, one for each token, and the other for the training signals. The auxiliary MLMs’ outputs are conditioned on the mixture weights used in the discriminator loss, which is a combination of the gradient of the encoder’s output and the Gumbel-Softmax. AMOS is evaluated on the GLUE and SQuAD benchmarks for BERT base-sized models, and compared to ELECTRA, a previous method for ELECTRA-style pretraining, and is shown to improve pretraining efficiency. ","This paper proposes a new adversarial learning curriculum called AMOS for text encoders based on Mixture Of Signals for auxiliary generators. The idea is to train a discriminator to distinguish between replaced tokens generated by auxiliary masked language models (MLMs) and the ones generated by the original MLM. The discriminator is then used to train an encoder to generate the replaced tokens. The authors propose a unified auxiliary model consisting of two MLMs, one for each token, and the other for the training signals. The auxiliary MLMs’ outputs are conditioned on the mixture weights used in the discriminator loss, which is a combination of the gradient of the encoder’s output and the Gumbel-Softmax. AMOS is evaluated on the GLUE and SQuAD benchmarks for BERT base-sized models, and compared to ELECTRA, a previous method for ELECTRA-style pretraining, and is shown to improve pretraining efficiency. "
8302,SP:db3825633ab5d0671340390b23ab655838cc38b2,"pre - trained language models USED-FOR relational knowledge. clozestyle sentence USED-FOR pre - trained language models. clozestyle sentence USED-FOR relational knowledge. language models COMPARE knowledge graphs. knowledge graphs COMPARE language models. precision EVALUATE-FOR language models. adaptive fine - tuning USED-FOR fill - mask task. pre - trained language model USED-FOR fill - mask task. pre - trained language model USED-FOR adaptive fine - tuning. complex prompting techniques CONJUNCTION adaptive fine - tuning. adaptive fine - tuning CONJUNCTION complex prompting techniques. adaptive fine - tuning COMPARE baselines. baselines COMPARE adaptive fine - tuning. transfer learning capabilities FEATURE-OF language model. Task is relational fact extraction task. OtherScientificTerm are knowledge graph facts, and knowledge graph. Generic are model, and approach. Metric is knowledge extraction quality. ","This paper studies the relational fact extraction task, where the goal is to extract knowledge graph facts from a pre-trained language model. The authors propose to use a clozestyle sentence to extract relational knowledge from a set of pre-trainable language models. The paper shows that language models trained on clozestyles outperform existing knowledge graphs in terms of precision, and that the relational knowledge can be extracted from a small subset of the knowledge graph.  The paper also shows that adaptive fine-tuning of the fill-mask task using a pre - trained language model can improve the performance of the model in the presence of complex prompting techniques, and also improves the transfer learning capabilities of the language model when the model is trained on a smaller subset of facts. The approach is evaluated on a number of different datasets, and the results show that the proposed approach improves the knowledge extraction quality, and is more effective than baselines.   ","This paper studies the relational fact extraction task, where the goal is to extract knowledge graph facts from a pre-trained language model. The authors propose to use a clozestyle sentence to extract relational knowledge from a set of pre-trainable language models. The paper shows that language models trained on clozestyles outperform existing knowledge graphs in terms of precision, and that the relational knowledge can be extracted from a small subset of the knowledge graph.  The paper also shows that adaptive fine-tuning of the fill-mask task using a pre - trained language model can improve the performance of the model in the presence of complex prompting techniques, and also improves the transfer learning capabilities of the language model when the model is trained on a smaller subset of facts. The approach is evaluated on a number of different datasets, and the results show that the proposed approach improves the knowledge extraction quality, and is more effective than baselines.   "
8311,SP:ae25d32714b2b9f7e02cc20f4a36252e20e78e4f,"multi - relations FEATURE-OF Knowledge bases. symmetry CONJUNCTION inversion. inversion CONJUNCTION symmetry. inversion CONJUNCTION composition. composition CONJUNCTION inversion. symmetry HYPONYM-OF properties. composition HYPONYM-OF properties. inversion HYPONYM-OF properties. Euclidean embedding models USED-FOR properties. hyperbolic space USED-FOR transitivity. representation learning framework USED-FOR relation properties. geometric spaces FEATURE-OF knowledge base embeddings. out - of - taxonomy entity typing task EVALUATE-FOR aligned embeddings. knowledge graph USED-FOR entities. datasets EVALUATE-FOR approach. low dimensions CONJUNCTION small training rates. small training rates CONJUNCTION low dimensions. low dimensions EVALUATE-FOR approach. YAGO3 USED-FOR datasets. small training rates EVALUATE-FOR approach. OtherScientificTerm are Euclidean space, and tree - like properties. Method is manifold alignment. ","Knowledge bases with multi-relations are a very popular topic in machine learning. Knowledge bases are typically represented in a Euclidean space, and the authors propose a representation learning framework to learn relation properties (e.g., symmetry, inversion, composition, etc.).    The key idea is to learn these properties in a hyperbolic space, where the properties can be represented as Euclideans.   This is achieved by learning a set of properties that are invariant to transitivity in the hyper bolic space and then aligning the properties of the properties with those of the underlying geometry of the space.  The authors show that the properties learned in Euclideen embedding models are invariants to a variety of properties, including those that have tree-like properties.  They also demonstrate that the geometric spaces of the knowledge base embeddings can be mapped to geometric spaces, and that the resulting aligned embedding can be used in an out-of-taxonomy entity typing task, where entities are represented as a knowledge graph.  Finally, the approach is evaluated on two datasets (YAGO3 and YAGO2) and is shown to be effective on low dimensions and small training rates. ","Knowledge bases with multi-relations are a very popular topic in machine learning. Knowledge bases are typically represented in a Euclidean space, and the authors propose a representation learning framework to learn relation properties (e.g., symmetry, inversion, composition, etc.).    The key idea is to learn these properties in a hyperbolic space, where the properties can be represented as Euclideans.   This is achieved by learning a set of properties that are invariant to transitivity in the hyper bolic space and then aligning the properties of the properties with those of the underlying geometry of the space.  The authors show that the properties learned in Euclideen embedding models are invariants to a variety of properties, including those that have tree-like properties.  They also demonstrate that the geometric spaces of the knowledge base embeddings can be mapped to geometric spaces, and that the resulting aligned embedding can be used in an out-of-taxonomy entity typing task, where entities are represented as a knowledge graph.  Finally, the approach is evaluated on two datasets (YAGO3 and YAGO2) and is shown to be effective on low dimensions and small training rates. "
8320,SP:9ab3bc525ee4a9c96518c43e4c43082655a7674f,frequency distribution FEATURE-OF real - world knowledge graphs. approaches USED-FOR static knowledge graphs. one - shot learning framework USED-FOR link prediction. temporal knowledge graphs USED-FOR one - shot learning framework. temporal knowledge graphs USED-FOR link prediction. self - attention mechanism USED-FOR temporal interactions between entities. network USED-FOR similarity score. self - attention mechanism CONJUNCTION network. network CONJUNCTION self - attention mechanism. network USED-FOR method. self - attention mechanism USED-FOR method. algorithm COMPARE baselines. baselines COMPARE algorithm. algorithm USED-FOR sparse relations. Method is low - shot learning methods. Task is temporal settings. OtherScientificTerm is data scarcity. ,"This paper proposes a new one-shot learning framework for link prediction on temporal knowledge graphs with a frequency distribution similar to real-world knowledge graphs. Previous approaches for static knowledge graphs have been shown to be effective for low-shot performance, but not for temporal settings. The proposed method uses a self-attention mechanism to capture temporal interactions between entities and a network to compute a similarity score. The authors show that the proposed algorithm is more robust to sparse relations than baselines and can be used in situations of data scarcity.","This paper proposes a new one-shot learning framework for link prediction on temporal knowledge graphs with a frequency distribution similar to real-world knowledge graphs. Previous approaches for static knowledge graphs have been shown to be effective for low-shot performance, but not for temporal settings. The proposed method uses a self-attention mechanism to capture temporal interactions between entities and a network to compute a similarity score. The authors show that the proposed algorithm is more robust to sparse relations than baselines and can be used in situations of data scarcity."
8336,SP:91f92a40e12afd0702f07ae7f4175ecce57b7007,"solver USED-FOR task. neural module USED-FOR solver. module USED-FOR task. module USED-FOR modules. visual reasoning tasks EVALUATE-FOR model. model COMPARE attention - based baseline. attention - based baseline COMPARE model. human judges USED-FOR reasoning process. Generic is tasks. OtherScientificTerm are Lower modules, and forgetting. ","This paper proposes a neural module that learns a solver to solve a given task using only a single neural module. This module can be used to solve any task, and can be applied to a variety of tasks. Lower modules are used to reduce the forgetting. The modules are trained using a single module. The proposed model is evaluated on visual reasoning tasks, and is compared with an attention-based baseline. The model is also compared with human judges for the reasoning process. ","This paper proposes a neural module that learns a solver to solve a given task using only a single neural module. This module can be used to solve any task, and can be applied to a variety of tasks. Lower modules are used to reduce the forgetting. The modules are trained using a single module. The proposed model is evaluated on visual reasoning tasks, and is compared with an attention-based baseline. The model is also compared with human judges for the reasoning process. "
8345,SP:de33b02e7f2faec5bcae9a5516721aa1ef190572,"channel - selectivity FEATURE-OF convolutional layer. Selective Convolutional Unit ( SCU ) HYPONYM-OF architectural unit. parameter efficiency EVALUATE-FOR CNNs. architectural unit USED-FOR CNNs. Selective Convolutional Unit ( SCU ) HYPONYM-OF CNNs. bottlenecks FEATURE-OF CNNs. parameter efficiency EVALUATE-FOR architectural unit. SCU USED-FOR channel - selectivity. SCU USED-FOR training. pruning unimportant channels USED-FOR SCU. SCU - based models COMPARE baselines. baselines COMPARE SCU - based models. model compression EVALUATE-FOR baselines. postprocessing USED-FOR SCU - based models. model compression EVALUATE-FOR SCU - based models. OtherScientificTerm are identity ( e.g., residual ) connection, identity connection, pruned parameters, rewired parameters, and convolutional kernels. Method is deep convolutional neural networks ( CNN ). ","This paper proposes a new architectural unit for deep convolutional neural networks (CNN) called Selective Convolutional Unit (SCU), which is an architectural unit that aims to improve the parameter efficiency of CNNs with bottlenecks. The SCU is designed to improve channel-selectivity of the convolutionic layer, which is defined as the identity (e.g., residual) connection between the output layer and the identity connection. The authors show that SCU can be used to improve parameter efficiency by pruning unimportant channels during training. The pruned parameters are then rewired and the rewired parameters are used to train a new model. Experiments show that the SCU-based models achieve better model compression compared to the baselines that do not use postprocessing.   ","This paper proposes a new architectural unit for deep convolutional neural networks (CNN) called Selective Convolutional Unit (SCU), which is an architectural unit that aims to improve the parameter efficiency of CNNs with bottlenecks. The SCU is designed to improve channel-selectivity of the convolutionic layer, which is defined as the identity (e.g., residual) connection between the output layer and the identity connection. The authors show that SCU can be used to improve parameter efficiency by pruning unimportant channels during training. The pruned parameters are then rewired and the rewired parameters are used to train a new model. Experiments show that the SCU-based models achieve better model compression compared to the baselines that do not use postprocessing.   "
8354,SP:2d80fa4bc440061be2234b5070503d3fa056baed,positive data CONJUNCTION unlabeled data. unlabeled data CONJUNCTION positive data. positive data USED-FOR binary classifier. unlabeled data USED-FOR binary classifier. labeled positive data COMPARE unlabeled positive data. unlabeled positive data COMPARE labeled positive data. selection bias FEATURE-OF labeling process. it USED-FOR selection bias. PU learning USED-FOR Bayes optimal classifier. method USED-FOR classifier. algorithm USED-FOR scoring function. algorithm USED-FOR classifier. threshold USED-FOR classifier. method COMPARE methods. methods COMPARE method. methods USED-FOR PU learning. method USED-FOR PU learning. real - world datasets EVALUATE-FOR PU learning. real - world datasets EVALUATE-FOR method. real - world datasets EVALUATE-FOR methods. OtherScientificTerm is class posterior. ,"This paper studies the problem of learning a binary classifier from both positive data and unlabeled data. The authors show that the selection bias in the labeling process is more severe in the case of labeled positive data than the case for unlabeling positive data. They also show that it is possible to learn a Bayes optimal classifier by PU learning. They propose an algorithm to learn the scoring function that maximizes the probability that the class posterior of the positive data is the same as the one of the unlabelled positive data, and that it minimizes a selection bias. They show that this method can learn a classifier with a lower threshold than the threshold of the classifier. The proposed method is evaluated on two real-world datasets and compared with other methods for PU learning, and is shown to outperform other methods. ","This paper studies the problem of learning a binary classifier from both positive data and unlabeled data. The authors show that the selection bias in the labeling process is more severe in the case of labeled positive data than the case for unlabeling positive data. They also show that it is possible to learn a Bayes optimal classifier by PU learning. They propose an algorithm to learn the scoring function that maximizes the probability that the class posterior of the positive data is the same as the one of the unlabelled positive data, and that it minimizes a selection bias. They show that this method can learn a classifier with a lower threshold than the threshold of the classifier. The proposed method is evaluated on two real-world datasets and compared with other methods for PU learning, and is shown to outperform other methods. "
8363,SP:5f312626b0613d2e07c59214c5f00db208a98717,"auxiliary losses USED-FOR representations. approach USED-FOR statistical inefficiency. statistical inefficiency FEATURE-OF neural networks. auxiliary losses USED-FOR approach. auxiliary loss USED-FOR main loss. reinforcement learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION reinforcement learning. multi - task supervised learning CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION multi - task supervised learning. Atari games USED-FOR reinforcement learning. gridworld USED-FOR reinforcement learning. domains EVALUATE-FOR algorithm. ImageNet USED-FOR multi - task supervised learning. multi - task supervised learning HYPONYM-OF domains. reinforcement learning HYPONYM-OF domains. reinforcement learning HYPONYM-OF domains. OtherScientificTerm are auxiliary task, and adaptive weight. ","This paper proposes an approach to reduce the statistical inefficiency of neural networks using auxiliary losses to learn representations. The main loss is the auxiliary loss, and the auxiliary task is the main loss. The proposed algorithm is evaluated on three domains: multi-task supervised learning on ImageNet, reinforcement learning on a gridworld, and reinforcement learning in Atari games. The results show that the proposed algorithm can achieve better performance than existing methods. The authors also demonstrate that the adaptive weight can be used as an additional auxiliary task.","This paper proposes an approach to reduce the statistical inefficiency of neural networks using auxiliary losses to learn representations. The main loss is the auxiliary loss, and the auxiliary task is the main loss. The proposed algorithm is evaluated on three domains: multi-task supervised learning on ImageNet, reinforcement learning on a gridworld, and reinforcement learning in Atari games. The results show that the proposed algorithm can achieve better performance than existing methods. The authors also demonstrate that the adaptive weight can be used as an additional auxiliary task."
8372,SP:e270ae3eeb7ab4fa91ba37d4d68ce10f2fa0a3b5,"Adversarial examples HYPONYM-OF machine learning models. geometric framework USED-FOR high - dimensional geometry of adversarial examples. manifold reconstruction literature USED-FOR geometric framework. low - dimensional data manifolds PART-OF high - dimensional space. decision boundary USED-FOR low - dimensional data manifold. decision boundary USED-FOR Adversarial examples. nearest neighbor classifiers CONJUNCTION ball - based adversarial training. ball - based adversarial training CONJUNCTION nearest neighbor classifiers. robustness EVALUATE-FOR norms. sufficient sampling conditions USED-FOR nearest neighbor classifiers. sufficient sampling conditions USED-FOR ball - based adversarial training. OtherScientificTerm are misclassifications, codimension, adversarial examples, and manifold. Method is adversarial training. ","This paper proposes a geometric framework based on the manifold reconstruction literature to study the high-dimensional geometry of adversarial examples in machine learning models. Adversarial examples are defined as misclassifications where the codimension of the data manifold is a function of the decision boundary between a low-dimensional data manifold and a high-dimensionality space. The authors show that adversarial instances can be seen as low-dimensioned versions of the original data manifold. They also show that a decision boundary can be defined on the low-dimensions of the high dimensional space, which can be used to define a robustness to misclassification.   The authors also provide sufficient sampling conditions for nearest neighbor classifiers and ball-based adversarial training, and show that the robustness of these norms can be improved when the number of samples is large enough.  Finally, the authors provide a theoretical analysis that shows that, under certain conditions, adversarial attacks on low dimensional data manifolds are more likely to be misclassified than on high dimensional ones. They further show that this phenomenon is also observed in the case where the data is on a low dimensional manifold. ","This paper proposes a geometric framework based on the manifold reconstruction literature to study the high-dimensional geometry of adversarial examples in machine learning models. Adversarial examples are defined as misclassifications where the codimension of the data manifold is a function of the decision boundary between a low-dimensional data manifold and a high-dimensionality space. The authors show that adversarial instances can be seen as low-dimensioned versions of the original data manifold. They also show that a decision boundary can be defined on the low-dimensions of the high dimensional space, which can be used to define a robustness to misclassification.   The authors also provide sufficient sampling conditions for nearest neighbor classifiers and ball-based adversarial training, and show that the robustness of these norms can be improved when the number of samples is large enough.  Finally, the authors provide a theoretical analysis that shows that, under certain conditions, adversarial attacks on low dimensional data manifolds are more likely to be misclassified than on high dimensional ones. They further show that this phenomenon is also observed in the case where the data is on a low dimensional manifold. "
8381,SP:e07d948a79d478ecd23a0a4406d4ddd3ac5e3be3,"human cognition USED-FOR high - dimensional spaces. interpretable low - dimensional representations USED-FOR areas. representation learning algorithms USED-FOR time series data. interpretable discrete dimensionality reduction CONJUNCTION deep generative modeling. deep generative modeling CONJUNCTION interpretable discrete dimensionality reduction. deep generative modeling USED-FOR representation learning framework. interpretable discrete dimensionality reduction USED-FOR representation learning framework. framework USED-FOR discrete representations of time series. discrete representations of time series USED-FOR smooth and interpretable embeddings. non - differentiability FEATURE-OF discrete representation learning. way USED-FOR non - differentiability. self - organizing map algorithm COMPARE original. original COMPARE self - organizing map algorithm. representation space FEATURE-OF Markov model. Markov model USED-FOR probabilistic interpretation of our method. model USED-FOR natural representation of uncertainty. model USED-FOR temporal transition structure. model USED-FOR clustering. static ( Fashion-)MNIST data CONJUNCTION time series of linearly interpolated ( Fashion-)MNIST images. time series of linearly interpolated ( Fashion-)MNIST images CONJUNCTION static ( Fashion-)MNIST data. clustering CONJUNCTION interpretability. interpretability CONJUNCTION clustering. eICU data set FEATURE-OF real world medical time series application. macro states FEATURE-OF chaotic Lorenz attractor system. clustering EVALUATE-FOR model. real world medical time series application EVALUATE-FOR model. interpretability EVALUATE-FOR model. static ( Fashion-)MNIST data EVALUATE-FOR model. Material are High - dimensional time series, and real world data. OtherScientificTerm is data features. Generic are representation, method, and representations. ","High-dimensional time series are of great interest in many applications where human cognition is interested in high-dimensional spaces. However, there is a lack of interpretable low-dimensional representations for these areas. This paper proposes a new representation learning algorithms for time series data. The proposed representation learning framework is based on interpretable discrete dimensionality reduction and deep generative modeling. The framework learns discrete representations of time series to learn smooth and interpretable embeddings. The authors propose a new way to deal with non-differentiability in discrete representation learning. They propose a self-organizing map algorithm that is more interpretable than the original. They also propose a probabilistic interpretation of our method based on a Markov model in the representation space. They show that their model learns a natural representation of uncertainty that captures the temporal transition structure of the data features. They demonstrate that the proposed model can be used for clustering, interpretability, and clustering on real world data. Finally, they show that the model is able to learn a model for a real world medical time series application on the eICU data set, which is a chaotic Lorenz attractor system with macro states. ","High-dimensional time series are of great interest in many applications where human cognition is interested in high-dimensional spaces. However, there is a lack of interpretable low-dimensional representations for these areas. This paper proposes a new representation learning algorithms for time series data. The proposed representation learning framework is based on interpretable discrete dimensionality reduction and deep generative modeling. The framework learns discrete representations of time series to learn smooth and interpretable embeddings. The authors propose a new way to deal with non-differentiability in discrete representation learning. They propose a self-organizing map algorithm that is more interpretable than the original. They also propose a probabilistic interpretation of our method based on a Markov model in the representation space. They show that their model learns a natural representation of uncertainty that captures the temporal transition structure of the data features. They demonstrate that the proposed model can be used for clustering, interpretability, and clustering on real world data. Finally, they show that the model is able to learn a model for a real world medical time series application on the eICU data set, which is a chaotic Lorenz attractor system with macro states. "
8390,SP:5915ee71ea58dbdbafa31c1ad291d1e5940a0cf4,"multidimensional probability distributions USED-FOR latent space prior distributions. latent space prior distributions USED-FOR implicit generative models. linear interpolations USED-FOR latent space. random latent vectors USED-FOR decoding linear interpolations. non - linear interpolations USED-FOR distribution mismatch. latent probability distribution USED-FOR distribution mismatch. multidimensional Cauchy distribution USED-FOR prior distribution. OtherScientificTerm are latent distribution, finite mean, and latent distributions. ","This paper proposes to use multidimensional probability distributions as latent space prior distributions for implicit generative models. The authors show that linear interpolations in the latent space can be approximated by random latent vectors, and that decoding linear interpolation from a set of random latent latent vectors is equivalent to decoding a distribution mismatch between the latent distribution and the true distribution. They also show that non-linear interpolations can be used to mitigate the distribution mismatch by using the latent probability distribution of the prior distribution. The prior distribution is modeled as a multiddimensional Cauchy distribution, where the finite mean is a function of the number of latent distributions.","This paper proposes to use multidimensional probability distributions as latent space prior distributions for implicit generative models. The authors show that linear interpolations in the latent space can be approximated by random latent vectors, and that decoding linear interpolation from a set of random latent latent vectors is equivalent to decoding a distribution mismatch between the latent distribution and the true distribution. They also show that non-linear interpolations can be used to mitigate the distribution mismatch by using the latent probability distribution of the prior distribution. The prior distribution is modeled as a multiddimensional Cauchy distribution, where the finite mean is a function of the number of latent distributions."
8399,SP:19b63ca635712f1509ca6e0141303c192f2709e0,"hyperbolic space FEATURE-OF shallow networks. embeddings USED-FOR ubiquitous attention mechanisms. ubiquitous attention mechanisms USED-FOR neural networks architectures. hyperbolic geometry FEATURE-OF embeddings. hyperbolic geometry COMPARE Euclidean geometry. Euclidean geometry COMPARE hyperbolic geometry. generalization EVALUATE-FOR neural machine translation. WMT’14 FEATURE-OF neural machine translation. learning on graphs EVALUATE-FOR method. synthetic and real - world graph tasks EVALUATE-FOR learning on graphs. visual question answering ( CLEVR ) tasks EVALUATE-FOR method. neural machine translation EVALUATE-FOR method. generalization EVALUATE-FOR method. Generic are approaches, and model. Method are geometry of embedding of object representations, and neural representations. OtherScientificTerm are embedding space, and semantic distance. Material is graphs. ","This paper proposes a new geometry of embedding of object representations. The authors show that shallow networks can be embedded in hyperbolic space. They show that embeddings of ubiquitous attention mechanisms in modern neural networks architectures can be represented in this space. The paper also shows that the hyper-bolic geometry of the embeddens of modern embedding space is similar to that of Euclidean geometry.    The authors also show that existing approaches do not work well in this setting.  The proposed method is evaluated on learning on graphs on both synthetic and real-world graph tasks, and is shown to improve generalization in neural machine translation on WMT’14. The method is also tested on visual question answering (CLEVR) tasks, where it is shown that the proposed method performs better than existing methods. The main contribution of the paper is that the model is able to capture the geometry of objects in the scene, which is a key factor in the generalization performance of neural representations. In particular, the authors show how the semantic distance between two objects in a scene can be reduced to a single point in the space, which can be used to represent objects in graphs. ","This paper proposes a new geometry of embedding of object representations. The authors show that shallow networks can be embedded in hyperbolic space. They show that embeddings of ubiquitous attention mechanisms in modern neural networks architectures can be represented in this space. The paper also shows that the hyper-bolic geometry of the embeddens of modern embedding space is similar to that of Euclidean geometry.    The authors also show that existing approaches do not work well in this setting.  The proposed method is evaluated on learning on graphs on both synthetic and real-world graph tasks, and is shown to improve generalization in neural machine translation on WMT’14. The method is also tested on visual question answering (CLEVR) tasks, where it is shown that the proposed method performs better than existing methods. The main contribution of the paper is that the model is able to capture the geometry of objects in the scene, which is a key factor in the generalization performance of neural representations. In particular, the authors show how the semantic distance between two objects in a scene can be reduced to a single point in the space, which can be used to represent objects in graphs. "
8408,SP:f6049e9f80a63c9306c1cebcb6b229aa6da44ddc,"attacks USED-FOR architecture information. attacks USED-FOR deep neural networks ( DNN ). architecture information FEATURE-OF deep neural networks ( DNN ). cache side - channels FEATURE-OF DNN fingerprinting attacks. threat model USED-FOR attacks. attack USED-FOR architecture. Flush+Reload HYPONYM-OF cache side - channel technique. DeepRecon HYPONYM-OF attack. cache side - channel technique USED-FOR internal information. Flush+Reload USED-FOR internal information. internal information USED-FOR attack. VGG19 CONJUNCTION ResNet50. ResNet50 CONJUNCTION VGG19. forward propagation USED-FOR complex networks. ResNet50 HYPONYM-OF complex networks. VGG19 HYPONYM-OF complex networks. meta - model USED-FOR pretrained model. transfer learning setting FEATURE-OF pretrained model. empirical security analysis USED-FOR DNNs ’ vulnerability. cache side - channel attacks FEATURE-OF DNNs ’ vulnerability. OtherScientificTerm are black - box networks, shared framework, network architecture, and architecture attributes. Method are victim model, co - located process, deep learning ( DL ) system, framework - level defense techniques, and DNNs. Task is fingerprinting process. ","This paper studies the problem of DNN fingerprinting attacks on black-box networks. The authors propose two new attacks that exploit the architecture information of deep neural networks (DNN) to extract architecture information from the cache side-channels of the victim model. The attacks are based on the threat model of a co-attached process, where the victim and the attacker share a shared framework, and the co-location process is a deep learning (DL) system.    The authors show that the attack on the architecture of a DNN can be seen as a variant of the attack against a meta-model of a pretrained model in a transfer learning setting. The attack, called DeepRecon, is an attack that uses a cache side channel technique called Flush+Reload to extract internal information from a shared network architecture.  In the paper, the authors also show that for complex networks with forward propagation (e.g., VGG19 and ResNet50), the attack can be viewed as an extension of the existing framework-level defense techniques.  The paper also provides an empirical security analysis that shows that DNNs’ vulnerability to cache-side-channel attacks is highly correlated with the number of layers and the architecture attributes of the network.  Finally, the paper also shows that the fingerprinting process can be used to transfer the information of a pre-trained DNN to a different model. ","This paper studies the problem of DNN fingerprinting attacks on black-box networks. The authors propose two new attacks that exploit the architecture information of deep neural networks (DNN) to extract architecture information from the cache side-channels of the victim model. The attacks are based on the threat model of a co-attached process, where the victim and the attacker share a shared framework, and the co-location process is a deep learning (DL) system.    The authors show that the attack on the architecture of a DNN can be seen as a variant of the attack against a meta-model of a pretrained model in a transfer learning setting. The attack, called DeepRecon, is an attack that uses a cache side channel technique called Flush+Reload to extract internal information from a shared network architecture.  In the paper, the authors also show that for complex networks with forward propagation (e.g., VGG19 and ResNet50), the attack can be viewed as an extension of the existing framework-level defense techniques.  The paper also provides an empirical security analysis that shows that DNNs’ vulnerability to cache-side-channel attacks is highly correlated with the number of layers and the architecture attributes of the network.  Finally, the paper also shows that the fingerprinting process can be used to transfer the information of a pre-trained DNN to a different model. "
8417,SP:6a3dd89db6c24a1f98e8866ef0a4c1c2c1ec6635,"representational hierarchy USED-FOR predicting future video frames. spatiotemporal memories PART-OF representational hierarchy. hierarchical network model USED-FOR spatiotemporal memories. Hierarchical Prediction Network ( HPNet ) HYPONYM-OF hierarchical network model. feedforward, feedback and lateral recurrent circuits PART-OF mammalian hierarchical visual system. feedforward, feedback and lateral recurrent circuits USED-FOR model. recurrent connections USED-FOR spatiotemporal memories. feed - forward path CONJUNCTION feedback path. feedback path CONJUNCTION feed - forward path. feed - forward path USED-FOR spatiotemporal features. feed - forward path PART-OF model. feedback path PART-OF model. feed - forward path CONJUNCTION feedback path. feedback path CONJUNCTION feed - forward path. feedback path PART-OF recurrent gated circuit. benchmark datasets EVALUATE-FOR long range video sequence predictions. hierarchical interaction PART-OF network. predictive self - supervised learning USED-FOR representational learning. visual cortex FEATURE-OF representational learning. OtherScientificTerm are hierarchy, internal memory states, prediction errors, frame - to - frame basis, memories of global movement patterns, and early visual cortex. Generic is level. ","This paper proposes a new representational hierarchy for predicting future video frames, which is composed of spatiotemporal memories in a hierarchical hierarchy. The proposed Hierarchical Prediction Network (HPNet) is a hierarchical network model that learns to encode and store spatiotmporal memories through recurrent connections. The model is based on feedforward, feedback and lateral recurrent circuits from the mammalian hierarchical visual system, and the model consists of a feed-forward path to encode spatiotamporal features and a feedback path to predict future frames. The authors show that the hierarchy is able to learn internal memory states that are invariant to prediction errors on a frame-to-frame basis, and that this level of representation learning can be applied to long range video sequence predictions on standard benchmark datasets. The paper also shows that the model can learn representations of global movement patterns, which are similar to the memories of early visual cortex. Finally, the paper shows that predictive self-supervised learning for representational learning in the visual cortex can be learned through hierarchical interaction between layers of the network.","This paper proposes a new representational hierarchy for predicting future video frames, which is composed of spatiotemporal memories in a hierarchical hierarchy. The proposed Hierarchical Prediction Network (HPNet) is a hierarchical network model that learns to encode and store spatiotmporal memories through recurrent connections. The model is based on feedforward, feedback and lateral recurrent circuits from the mammalian hierarchical visual system, and the model consists of a feed-forward path to encode spatiotamporal features and a feedback path to predict future frames. The authors show that the hierarchy is able to learn internal memory states that are invariant to prediction errors on a frame-to-frame basis, and that this level of representation learning can be applied to long range video sequence predictions on standard benchmark datasets. The paper also shows that the model can learn representations of global movement patterns, which are similar to the memories of early visual cortex. Finally, the paper shows that predictive self-supervised learning for representational learning in the visual cortex can be learned through hierarchical interaction between layers of the network."
8426,SP:fb74e57f35666742caf651e6da33b5defcf259a8,continuous embeddings USED-FOR kmers. method USED-FOR continuous embeddings. raw RNA - seq data USED-FOR continuous embeddings. raw RNA - seq data USED-FOR kmers. DNA sequence similarity CONJUNCTION DNA sequence abundance. DNA sequence abundance CONJUNCTION DNA sequence similarity. model USED-FOR DNA sequence similarity. model USED-FOR DNA sequence abundance. DNA sequence abundance FEATURE-OF embedding latent space. latent space USED-FOR exon information. them COMPARE known gene sub - structures. known gene sub - structures COMPARE them. acute myeloid leukemia patients FEATURE-OF raw RNA - Seq data. raw RNA - Seq data USED-FOR exon information. latent space USED-FOR detection of genomic abnormalities. visualization CONJUNCTION analysis. analysis CONJUNCTION visualization. representation space USED-FOR visualization. representation space USED-FOR analysis. translocations CONJUNCTION patient - specific mutations. patient - specific mutations CONJUNCTION translocations. patient - specific mutations HYPONYM-OF detection of genomic abnormalities. translocations HYPONYM-OF detection of genomic abnormalities. Generic is vectors. OtherScientificTerm is genomic abnormalities. ,"This paper proposes a method to learn continuous embeddings for kmers from raw RNA-seq data. The idea is to use a model to learn DNA sequence similarity and DNA sequence abundance in the embedding latent space, and then use the learned vectors to predict the location of exon information in the exon embedding. The authors show that the learned embedding of exons in the latent space can be used for the detection of genomic abnormalities (e.g. translocations and patient-specific mutations) and identify genomic abnormalities in the representation space for visualization and analysis. They also show that their method is able to identify exons that are not present in the original RNA-Seq data for acute myeloid leukemia patients, and compare them to known gene sub-structure.","This paper proposes a method to learn continuous embeddings for kmers from raw RNA-seq data. The idea is to use a model to learn DNA sequence similarity and DNA sequence abundance in the embedding latent space, and then use the learned vectors to predict the location of exon information in the exon embedding. The authors show that the learned embedding of exons in the latent space can be used for the detection of genomic abnormalities (e.g. translocations and patient-specific mutations) and identify genomic abnormalities in the representation space for visualization and analysis. They also show that their method is able to identify exons that are not present in the original RNA-Seq data for acute myeloid leukemia patients, and compare them to known gene sub-structure."
8435,SP:03aca6ff6a7f0ad2d5ccbcb15ed9536e305a9880,"approach USED-FOR model compression. weight or filter space FEATURE-OF network. architecture space USED-FOR approach. 1 - D CNN encoder / decoder USED-FOR mapping. continuous embedding CONJUNCTION back. back CONJUNCTION continuous embedding. embedding USED-FOR parameter count. dataset EVALUATE-FOR architecture. gradient descent USED-FOR compression objective function. accuracy CONJUNCTION parameter count. parameter count CONJUNCTION accuracy. accuracy EVALUATE-FOR compression objective function. parameter count FEATURE-OF compression objective function. continuous space FEATURE-OF gradient descent. gradient descent USED-FOR compression phase. continuous feature USED-FOR discrete architecture. decoder USED-FOR discrete architecture. FMNIST CONJUNCTION SVHN. SVHN CONJUNCTION FMNIST. CIFAR-10/100 CONJUNCTION FMNIST. FMNIST CONJUNCTION CIFAR-10/100. CIFAR-10 EVALUATE-FOR compression. visual recognition tasks EVALUATE-FOR approach. compression EVALUATE-FOR approach. SVHN HYPONYM-OF visual recognition tasks. CIFAR-10/100 HYPONYM-OF visual recognition tasks. FMNIST HYPONYM-OF visual recognition tasks. Method are Architecture Compression, and model compression methods. OtherScientificTerm is discrete architecture space. ","This paper proposes a new approach for model compression in the architecture space, called Architecture Compression. The key idea is to learn a mapping from a 1-D CNN encoder/decoder to a network in the weight or filter space of the network. The mapping is learned by gradient descent in the continuous space, and the mapping from the continuous embedding to the back is used to compute a compression objective function that maximizes both the accuracy and the parameter count. This embedding is then used to reduce the parameter number of the compression phase. The authors show that this approach outperforms existing model compression methods on several visual recognition tasks (CIFAR-10/100, FMNIST, SVHN). The authors also show that the discrete architecture can be decomposed into a discrete feature and a discrete architecture in the decoder, and that this discrete architecture space can be used to compress a dataset.","This paper proposes a new approach for model compression in the architecture space, called Architecture Compression. The key idea is to learn a mapping from a 1-D CNN encoder/decoder to a network in the weight or filter space of the network. The mapping is learned by gradient descent in the continuous space, and the mapping from the continuous embedding to the back is used to compute a compression objective function that maximizes both the accuracy and the parameter count. This embedding is then used to reduce the parameter number of the compression phase. The authors show that this approach outperforms existing model compression methods on several visual recognition tasks (CIFAR-10/100, FMNIST, SVHN). The authors also show that the discrete architecture can be decomposed into a discrete feature and a discrete architecture in the decoder, and that this discrete architecture space can be used to compress a dataset."
8444,SP:0511b5d10a90e3fe814e2d35208b4a987894ea62,"local model - based control CONJUNCTION global value function learning. global value function learning CONJUNCTION local model - based control. global value function learning CONJUNCTION exploration. exploration CONJUNCTION global value function learning. local trajectory optimization USED-FOR value function learning. approximate value functions USED-FOR policies. approximate value functions USED-FOR planning horizon. trajectory optimization USED-FOR temporally coordinated exploration. estimating uncertainty USED-FOR value function approximation. trajectory optimization CONJUNCTION estimating uncertainty. estimating uncertainty CONJUNCTION trajectory optimization. temporally coordinated exploration CONJUNCTION estimating uncertainty. estimating uncertainty CONJUNCTION temporally coordinated exploration. humanoid locomotion CONJUNCTION dexterous in - hand manipulation. dexterous in - hand manipulation CONJUNCTION humanoid locomotion. components USED-FOR control tasks. humanoid locomotion HYPONYM-OF control tasks. dexterous in - hand manipulation HYPONYM-OF control tasks. Method are plan online and learn offline ” framework, and internal model. OtherScientificTerm are value function, and local solutions. ","This paper proposes a “plan online and learn offline” framework that combines local model-based control, global value function learning, and exploration via local trajectory optimization. The key idea is to use approximate value functions to guide the planning horizon and learn policies based on the approximate value function. The value function approximation is done by temporally coordinated exploration via trajectory optimization and estimating uncertainty for value function approximations. The paper also proposes to learn an internal model that can be used to predict the value function of the current state of the system.  The paper demonstrates the effectiveness of the proposed components on a variety of control tasks, including humanoid locomotion and dexterous in-hand manipulation.  ","This paper proposes a “plan online and learn offline” framework that combines local model-based control, global value function learning, and exploration via local trajectory optimization. The key idea is to use approximate value functions to guide the planning horizon and learn policies based on the approximate value function. The value function approximation is done by temporally coordinated exploration via trajectory optimization and estimating uncertainty for value function approximations. The paper also proposes to learn an internal model that can be used to predict the value function of the current state of the system.  The paper demonstrates the effectiveness of the proposed components on a variety of control tasks, including humanoid locomotion and dexterous in-hand manipulation.  "
8453,SP:771494fda4702cd8c7efbf225b19028f91b449b9,"parallel data USED-FOR Neural Machine Translation ( NMT ) systems. zero - shot and dual learning PART-OF approach. reinforcement learning USED-FOR duality of the machine translation task. reinforcement learning USED-FOR latter. UN corpus EVALUATE-FOR zero - shot dual system. zero - shot dual system COMPARE NMT system. NMT system COMPARE zero - shot dual system. NMT system USED-FOR zero - shot translation. English - French and English - Spanish USED-FOR zero - shot dual system. SpanishFrench EVALUATE-FOR NMT system. zero - shot dual method COMPARE LSTM - based unsupervised NMT system. LSTM - based unsupervised NMT system COMPARE zero - shot dual method. en− →fr task EVALUATE-FOR LSTM - based unsupervised NMT system. en− →fr task EVALUATE-FOR zero - shot dual method. Material are low - resource languages, monolingual data, and newstest2014. Task are unsupervised and semi - supervised methods, machine translation task, and fr− →en task. ","This paper proposes a new approach to parallelize Neural Machine Translation (NMT) systems using parallel data. The proposed approach combines zero-shot and dual learning, where the former is based on unsupervised and semi-supervised methods and the latter uses reinforcement learning to learn the duality of the machine translation task. The authors show that the zero-shoot dual system outperforms the standard NMT system on the UN corpus of English-French and English-Spanish, and outperforms an LSTM-based UMT system in the en−–fr task. They also show that their zero- shot dual system can outperform the standard UMT on SpanishFrench and SpanishFrench.    The authors also demonstrate that the proposed approach can be applied to other low-resource languages where monolingual data is not available (e.g., French-German, Spanish-English).  The paper also shows that their proposed zero-shots dual method outperforms a LSTMs-based, LMT-based and L2L-based NMT method on en− →fr task, which is a machine translation problem where the goal is to translate from one language to another in a single pass. The paper shows that this is an important problem that is often overlooked in the literature, especially in the context of unsupervision.  The results are reported in the paper in the form of newstest2014, which shows that for the en–fr–fr (French-German) task, the proposed zero–unsupervised method performs better than the standard LMT system. For the proposed method performs slightly worse than the LSTm-based one, but performs slightly better than a standard L1-based method. On the other hand, the authors report that their method performs much better on the en de–fr−f–en task, outperforming the LMT method.  Finally, the paper also reports results on the Fr−–en–fr + en−f task, where they show that they outperform their proposed method in terms of performance. ","This paper proposes a new approach to parallelize Neural Machine Translation (NMT) systems using parallel data. The proposed approach combines zero-shot and dual learning, where the former is based on unsupervised and semi-supervised methods and the latter uses reinforcement learning to learn the duality of the machine translation task. The authors show that the zero-shoot dual system outperforms the standard NMT system on the UN corpus of English-French and English-Spanish, and outperforms an LSTM-based UMT system in the en−–fr task. They also show that their zero- shot dual system can outperform the standard UMT on SpanishFrench and SpanishFrench.    The authors also demonstrate that the proposed approach can be applied to other low-resource languages where monolingual data is not available (e.g., French-German, Spanish-English).  The paper also shows that their proposed zero-shots dual method outperforms a LSTMs-based, LMT-based and L2L-based NMT method on en− →fr task, which is a machine translation problem where the goal is to translate from one language to another in a single pass. The paper shows that this is an important problem that is often overlooked in the literature, especially in the context of unsupervision.  The results are reported in the paper in the form of newstest2014, which shows that for the en–fr–fr (French-German) task, the proposed zero–unsupervised method performs better than the standard LMT system. For the proposed method performs slightly worse than the LSTm-based one, but performs slightly better than a standard L1-based method. On the other hand, the authors report that their method performs much better on the en de–fr−f–en task, outperforming the LMT method.  Finally, the paper also reports results on the Fr−–en–fr + en−f task, where they show that they outperform their proposed method in terms of performance. "
8462,SP:1558dc03f99670f9ddccdca9c223a2baf962d438,"framework USED-FOR Information - Retrieval ( IR ). IRGAN USED-FOR Information - Retrieval ( IR ). framework USED-FOR IRGAN. generator USED-FOR distribution. minimax loss function USED-FOR generator. adversarial fashion USED-FOR models. Method is Generative Adversarial Networks. Material is multiple domains. Generic are task, and model. OtherScientificTerm are conditional probability distribution, adversarial formulation, loss curves, loss functions, and co - training like setup. ","This paper proposes Generative Adversarial Networks (GANs), a framework for Information-Retrieval (IR) based on the framework of IRGAN. The idea is to train multiple models in an adversarial fashion on multiple domains, each of which is a different task, and each task is modeled as a conditional probability distribution. The generator of the distribution is trained in a minimax loss function, and the objective is to minimize the loss of the generator on the target domain. The adversarial formulation of the loss curves is similar to that of the original IRGAN, except that the loss functions are learned in a co-training like setup, where the model is trained on the source domain and target domain separately.   ","This paper proposes Generative Adversarial Networks (GANs), a framework for Information-Retrieval (IR) based on the framework of IRGAN. The idea is to train multiple models in an adversarial fashion on multiple domains, each of which is a different task, and each task is modeled as a conditional probability distribution. The generator of the distribution is trained in a minimax loss function, and the objective is to minimize the loss of the generator on the target domain. The adversarial formulation of the loss curves is similar to that of the original IRGAN, except that the loss functions are learned in a co-training like setup, where the model is trained on the source domain and target domain separately.   "
8471,SP:6a13dda852ab075a3c0fb691476d6dc57919c729,"Variational auto - encoders ( VAEs ) USED-FOR approximate inference. approximate inference USED-FOR intractable generative models. representations USED-FOR auxiliary tasks. VAEs USED-FOR latent codes. human interpretation HYPONYM-OF auxiliary tasks. classification HYPONYM-OF auxiliary tasks. variational auto - encoders CONJUNCTION sparse coding. sparse coding CONJUNCTION variational auto - encoders. sparsity FEATURE-OF latent space. latent space FEATURE-OF VAE. Spike and Slab prior distribution USED-FOR latent space. Spike and Slab prior distribution USED-FOR sparsity. evidence lower bound USED-FOR approximate posterior inference. approximate posterior inference COMPARE VAE case. VAE case COMPARE approximate posterior inference. discrete mixture recognition function USED-FOR approximate posterior inference. discrete mixture recognition function USED-FOR evidence lower bound. approach USED-FOR sparse representations. intractable non - linear probabilistic models USED-FOR sparse representations. sparse representations COMPARE VAE representations. VAE representations COMPARE sparse representations. classification accuracy CONJUNCTION robustness. robustness CONJUNCTION classification accuracy. robustness EVALUATE-FOR sparse representations. classification accuracy EVALUATE-FOR sparse representations. sparse elements USED-FOR subjectively understandable sources of variation. OtherScientificTerm are interpretability, and latent dimensions. Material is MNIST. ","This paper studies the problem of approximate inference for variational auto-encoders (VAEs) for approximate inference in intractable generative models. The authors consider the problem that VAEs learn latent codes that are highly sparsified, and that these representations can be used for various auxiliary tasks (e.g. human interpretation, classification, etc.). The authors propose a new approach to approximate inference based on the observation that the sparsity in the latent space of a VAE is related to the Spike and Slab prior distribution over the latent dimensions of the input data. They derive an evidence lower bound for approximate posterior inference using a discrete mixture recognition function, and show that this approach is able to learn sparse representations that are more interpretable than VAE representations. They also show that the sparse representations learned by the proposed approach are more robust to noise in the input, and are more likely to be subjectively understandable sources of variation. Finally, the authors show that sparse elements are also able to be used to learn subjectively intuitively intuitive sources of variations that are not present in the original latent space.    The authors demonstrate that their approach can be applied to learning sparse representations in the context of classification accuracy and robustness in the presence of noise in MNIST, and shows that the learned sparse representations are able to generalize better than the original VAE representation, and can also generalize to other types of data. ","This paper studies the problem of approximate inference for variational auto-encoders (VAEs) for approximate inference in intractable generative models. The authors consider the problem that VAEs learn latent codes that are highly sparsified, and that these representations can be used for various auxiliary tasks (e.g. human interpretation, classification, etc.). The authors propose a new approach to approximate inference based on the observation that the sparsity in the latent space of a VAE is related to the Spike and Slab prior distribution over the latent dimensions of the input data. They derive an evidence lower bound for approximate posterior inference using a discrete mixture recognition function, and show that this approach is able to learn sparse representations that are more interpretable than VAE representations. They also show that the sparse representations learned by the proposed approach are more robust to noise in the input, and are more likely to be subjectively understandable sources of variation. Finally, the authors show that sparse elements are also able to be used to learn subjectively intuitively intuitive sources of variations that are not present in the original latent space.    The authors demonstrate that their approach can be applied to learning sparse representations in the context of classification accuracy and robustness in the presence of noise in MNIST, and shows that the learned sparse representations are able to generalize better than the original VAE representation, and can also generalize to other types of data. "
8480,SP:06a22143186fa2948fbe324ccae96a62ff12064e,non - adversarial feature matching - based approach USED-FOR generative models. pretrained neural networks USED-FOR feature extraction. autoencoders CONJUNCTION ConvNet classifiers. ConvNet classifiers CONJUNCTION autoencoders. pretrained neural networks USED-FOR Generative Feature Matching Networks ( GFMN ). pretrained neural networks USED-FOR approach. ConvNet classifiers HYPONYM-OF pretrained neural networks. autoencoders HYPONYM-OF pretrained neural networks. ImageNet HYPONYM-OF challenging datasets. CIFAR10 CONJUNCTION STL10. STL10 CONJUNCTION CIFAR10. first order statistics USED-FOR approach. pretrained ImageNet classifiers USED-FOR features. challenging benchmarks EVALUATE-FOR approach. CIFAR10 HYPONYM-OF challenging benchmarks. STL10 HYPONYM-OF challenging benchmarks. ,"This paper proposes a non-adversarial feature matching-based approach for training generative models. The proposed approach, Generative Feature Matching Networks (GFMN), uses pretrained neural networks (autoencoders, ConvNet classifiers, etc.) for feature extraction. The approach is based on first order statistics, and is evaluated on challenging datasets (CIFAR10, STL10, and ImageNet). The results show that the features extracted by the pretrained ImageNet classifier are more robust to adversarial attacks. ","This paper proposes a non-adversarial feature matching-based approach for training generative models. The proposed approach, Generative Feature Matching Networks (GFMN), uses pretrained neural networks (autoencoders, ConvNet classifiers, etc.) for feature extraction. The approach is based on first order statistics, and is evaluated on challenging datasets (CIFAR10, STL10, and ImageNet). The results show that the features extracted by the pretrained ImageNet classifier are more robust to adversarial attacks. "
8489,SP:2d7cf2f07a27d6c8e304a1b47c25387ad2e4432d,Graph Neural Networks ( GNNs ) USED-FOR representation learning of graphs. representation vector USED-FOR node. neighborhood aggregation scheme USED-FOR GNNs. node and graph classification tasks EVALUATE-FOR GNN variants. GNNs USED-FOR graph representation learning. GNNs USED-FOR graph structures. theoretical framework USED-FOR graph structures. theoretical framework USED-FOR GNNs. Graph Convolutional Networks CONJUNCTION GraphSAGE. GraphSAGE CONJUNCTION Graph Convolutional Networks. Graph Convolutional Networks HYPONYM-OF GNN variants. GraphSAGE HYPONYM-OF GNN variants. architecture COMPARE WeisfeilerLehman graph isomorphism test. WeisfeilerLehman graph isomorphism test COMPARE architecture. architecture COMPARE GNNs. GNNs COMPARE architecture. graph classification benchmarks EVALUATE-FOR model. Generic is they. ,"Graph Neural Networks (GNNs) are widely used in the representation learning of graphs, but they are not well-studied. This paper proposes a neighborhood aggregation scheme for GNNs, where each node is represented as a representation vector, and the goal is to learn the representation vector for each node. The paper shows that GNN variants of GNN are able to achieve state-of-the-art performance on both node and graph classification tasks.  The paper also proposes a theoretical framework to study the graph structures learned by GNN for graph representation learning, and shows that the GNN can learn graph structures that are invariant to the WeisfeilerLehman graph isomorphism test.  Experiments are conducted on two popular GNN architectures, Graph Convolutional Networks and GraphSAGE, and show that the proposed architecture is able to learn more complex graph structures than previous GNN-based methods. The model is also shown to perform well on several graph classification benchmarks. ","Graph Neural Networks (GNNs) are widely used in the representation learning of graphs, but they are not well-studied. This paper proposes a neighborhood aggregation scheme for GNNs, where each node is represented as a representation vector, and the goal is to learn the representation vector for each node. The paper shows that GNN variants of GNN are able to achieve state-of-the-art performance on both node and graph classification tasks.  The paper also proposes a theoretical framework to study the graph structures learned by GNN for graph representation learning, and shows that the GNN can learn graph structures that are invariant to the WeisfeilerLehman graph isomorphism test.  Experiments are conducted on two popular GNN architectures, Graph Convolutional Networks and GraphSAGE, and show that the proposed architecture is able to learn more complex graph structures than previous GNN-based methods. The model is also shown to perform well on several graph classification benchmarks. "
8498,SP:51126f2dd37ce57d2614c9044ede1e43627f0829,framework USED-FOR interpretable continual learning ( ICL ). this USED-FOR ICL. ICL idea USED-FOR continual learning approaches. saliency maps USED-FOR metric. average classification accuracy EVALUATE-FOR overall continual learning performance. metric EVALUATE-FOR ICL. overall continual learning performance EVALUATE-FOR ICL. average classification accuracy EVALUATE-FOR ICL. Method is variational continual learning framework. OtherScientificTerm is catastrophic forgetting. ,"This paper proposes a new framework for interpretable continual learning (ICL) based on the variational continual learning framework. The authors show that the ICL idea can be applied to several existing continual learning approaches and show that this improves the performance of ICL. They also propose a new metric based on saliency maps to evaluate the overall continual learning performance, which shows that ICL improves the average classification accuracy and reduces the risk of catastrophic forgetting. ","This paper proposes a new framework for interpretable continual learning (ICL) based on the variational continual learning framework. The authors show that the ICL idea can be applied to several existing continual learning approaches and show that this improves the performance of ICL. They also propose a new metric based on saliency maps to evaluate the overall continual learning performance, which shows that ICL improves the average classification accuracy and reduces the risk of catastrophic forgetting. "
8507,SP:27a565b3e5442b93d208652784051e640b0c1bfe,"perturbations USED-FOR model. evaluation framework USED-FOR adversarial attacks. adversarial attacks FEATURE-OF seq2seq models. constraints USED-FOR word - based MT systems. human and automatic evaluation EVALUATE-FOR they. adversarial training USED-FOR model. meaning - preserving attacks FEATURE-OF adversarial training. adversarial robustness EVALUATE-FOR model. Material is Adversarial examples. Metric is robustness. OtherScientificTerm are semantics, and meaning preservation. Task is machine translation ( MT ). Generic is methods. ",This paper proposes a new evaluation framework for evaluating the robustness of machine translation (MT) models to adversarial attacks. Adversarial examples are generated by perturbations to the model that are meant to preserve the semantics of the input word. The authors show that seq2seq models can be robust to a variety of adversarial examples. They also show that adversarial training under meaning-preserving attacks improves adversarial robustness when the model is trained on a large number of examples.    The authors also propose a way to enforce the meaning preservation in the training process. They show that existing methods do not work well because they require human and automatic evaluation. They then propose a set of constraints for word-based MT systems to ensure that their models are robust to such attacks. ,This paper proposes a new evaluation framework for evaluating the robustness of machine translation (MT) models to adversarial attacks. Adversarial examples are generated by perturbations to the model that are meant to preserve the semantics of the input word. The authors show that seq2seq models can be robust to a variety of adversarial examples. They also show that adversarial training under meaning-preserving attacks improves adversarial robustness when the model is trained on a large number of examples.    The authors also propose a way to enforce the meaning preservation in the training process. They show that existing methods do not work well because they require human and automatic evaluation. They then propose a set of constraints for word-based MT systems to ensure that their models are robust to such attacks. 
8516,SP:54ddd8132bf9e4259d2c2d72b348d2bb5f9e227c,rewards CONJUNCTION inverse ( negative ) rewards. inverse ( negative ) rewards CONJUNCTION rewards. rewards USED-FOR policies. inverse ( negative ) rewards USED-FOR policies. policies COMPARE policies. policies COMPARE policies. policies USED-FOR mis - actions. inverse rewards USED-FOR policies. deep Q - learning CONJUNCTION double Q - learning. double Q - learning CONJUNCTION deep Q - learning. double Q - learning CONJUNCTION on - policy actor - critic. on - policy actor - critic CONJUNCTION double Q - learning. hybrid polices COMPARE algorithms. algorithms COMPARE hybrid polices. rewards FEATURE-OF hybrid polices. on - policy actor - critic USED-FOR hybrid polices. deep Q - learning USED-FOR hybrid polices. double Q - learning USED-FOR hybrid polices. polices COMPARE policies. policies COMPARE polices. Method is reinforcement learning algorithms. OtherScientificTerm is inverse policies. Material is OpenAI gym. ,"This paper studies the problem of learning policies that are robust to mis-actions in reinforcement learning algorithms. The authors show that policies trained with rewards and inverse (negative) rewards are more robust than policies trained using only inverse rewards. They also show that such policies are more sensitive to the inverse rewards than standard policies. Finally, they show that hybrid polices trained with deep Q-learning, double Q-networks, and on-policy actor-critic outperform algorithms trained with only inverse policies.   ","This paper studies the problem of learning policies that are robust to mis-actions in reinforcement learning algorithms. The authors show that policies trained with rewards and inverse (negative) rewards are more robust than policies trained using only inverse rewards. They also show that such policies are more sensitive to the inverse rewards than standard policies. Finally, they show that hybrid polices trained with deep Q-learning, double Q-networks, and on-policy actor-critic outperform algorithms trained with only inverse policies.   "
8525,SP:89a732b57934d08b937c93560f391b7758e54f8a,"object parts CONJUNCTION hierarchical structure. hierarchical structure CONJUNCTION object parts. dynamics model USED-FOR object parts. hierarchical, disentangled object representation CONJUNCTION dynamics model. dynamics model CONJUNCTION hierarchical, disentangled object representation. formulation USED-FOR hierarchical, disentangled object representation. formulation USED-FOR dynamics model. unlabeled videos USED-FOR dynamics model. structural descriptor USED-FOR low - level concepts. structural descriptor USED-FOR hierarchical structure. layered image representation USED-FOR object parts. structural descriptor USED-FOR hierarchy. PSD model USED-FOR segmenting object parts. real and synthetic datasets EVALUATE-FOR PSD model. PSD model USED-FOR tasks. PSD model USED-FOR motion distributions. motion distributions HYPONYM-OF tasks. segmenting object parts HYPONYM-OF tasks. OtherScientificTerm is system dynamics. ","This paper proposes a new formulation for learning a hierarchical, disentangled object representation and a dynamics model from unlabeled videos with object parts, hierarchical structure, and system dynamics. The hierarchical structure is learned by learning a structural descriptor for low-level concepts, and the hierarchy is learned using the structural descriptor. The object parts are represented as a layered image representation. The PSD model is evaluated on both real and synthetic datasets, and is shown to perform well on two tasks: (1) segmenting object parts and (2) learning motion distributions.","This paper proposes a new formulation for learning a hierarchical, disentangled object representation and a dynamics model from unlabeled videos with object parts, hierarchical structure, and system dynamics. The hierarchical structure is learned by learning a structural descriptor for low-level concepts, and the hierarchy is learned using the structural descriptor. The object parts are represented as a layered image representation. The PSD model is evaluated on both real and synthetic datasets, and is shown to perform well on two tasks: (1) segmenting object parts and (2) learning motion distributions."
8534,SP:bb2a655d67bed9da43f0b8ec7d888b89c217d12e,noisy training datasets USED-FOR deep neural networks. noisy ( incorrect ) class labels PART-OF Large - scale datasets. noisy datasets USED-FOR softmax neural classifier. Deep Determinantal Generative Classifier ( DDGC ) HYPONYM-OF inference method. softmax neural classifier USED-FOR decision boundary. hidden feature spaces PART-OF discriminative deep model. discriminative deep model USED-FOR generative classifier. hidden feature spaces USED-FOR generative classifier. minimum covariance determinant estimator USED-FOR generative classifier. DDGC USED-FOR adversarial perturbations. noisy labels USED-FOR DDGC. noisy labels CONJUNCTION adversarial samples. adversarial samples CONJUNCTION noisy labels. training techniques USED-FOR noisy labels. training techniques USED-FOR adversarial samples. learning models USED-FOR noisy labels. learning models USED-FOR adversarial samples. learning models USED-FOR DDGC. training techniques USED-FOR DDGC. training techniques USED-FOR learning models. test accuracy EVALUATE-FOR deep model. CIFAR10 dataset EVALUATE-FOR deep model. noisy training labels FEATURE-OF CIFAR10 dataset. noise - handling training method USED-FOR deep model. Metric is classification accuracy. OtherScientificTerm is large margin property. ,"Large-scale datasets with noisy (inaccurate) class labels are often used to train deep neural networks on noisy training datasets. The authors propose a new inference method called Deep Determinantal Generative Classifier (DDGC), which uses a softmax neural classifier trained on noisy datasets to learn the decision boundary. The generative classifier is trained on the hidden feature spaces of a discriminative deep model with a minimum covariance determinant estimator. DDGC is shown to be robust to adversarial perturbations on noisy labels and adversarial samples.  The authors show that DDGC can be trained with different training techniques to handle noisy labels or adversarial data, and that the learning models trained with these different learning models are able to deal with noisy labels. They also show that the noise-handling training method is able to improve the test accuracy of the deep model on the CIFAR10 dataset with noisy training labels.   ","Large-scale datasets with noisy (inaccurate) class labels are often used to train deep neural networks on noisy training datasets. The authors propose a new inference method called Deep Determinantal Generative Classifier (DDGC), which uses a softmax neural classifier trained on noisy datasets to learn the decision boundary. The generative classifier is trained on the hidden feature spaces of a discriminative deep model with a minimum covariance determinant estimator. DDGC is shown to be robust to adversarial perturbations on noisy labels and adversarial samples.  The authors show that DDGC can be trained with different training techniques to handle noisy labels or adversarial data, and that the learning models trained with these different learning models are able to deal with noisy labels. They also show that the noise-handling training method is able to improve the test accuracy of the deep model on the CIFAR10 dataset with noisy training labels.   "
8543,SP:0fa525cc708470b757a60117cb608bb2feaa2c50,approaches USED-FOR Reinforcement Learning ( RL ). huge state spaces CONJUNCTION sparse delayed reward feedback. sparse delayed reward feedback CONJUNCTION huge state spaces. approaches USED-FOR large - scale applications. huge state spaces FEATURE-OF large - scale applications. sparse delayed reward feedback USED-FOR large - scale applications. action selection policies USED-FOR Hierarchical Reinforcement Learning ( HRL ) methods. temporal abstraction FEATURE-OF action selection policies. skill policies USED-FOR subgoals. approaches USED-FOR subgoal discovery. subgoal discovery USED-FOR HRL. approaches USED-FOR HRL. internal reward signal USED-FOR subgoal attainment. internal reward signal USED-FOR skills. intrinsic motivation USED-FOR skills. model - free method USED-FOR subgoal discovery. incremental unsupervised learning USED-FOR model - free method. method USED-FOR subgoals. intrinsic motivation learning mechanism CONJUNCTION method. method CONJUNCTION intrinsic motivation learning mechanism. approach USED-FOR HRL. rooms environment CONJUNCTION ATARI 2600 game. ATARI 2600 game CONJUNCTION rooms environment. sparse delayed feedback FEATURE-OF RL problems. Montezuma ’s Revenge HYPONYM-OF ATARI 2600 game. RL problems EVALUATE-FOR method. ATARI 2600 game HYPONYM-OF RL problems. Montezuma ’s Revenge HYPONYM-OF RL problems. rooms environment HYPONYM-OF RL problems. OtherScientificTerm is Abstraction. Generic is model. ,"This paper proposes two approaches for Reinforcement Learning (RL) that are suitable for large-scale applications with huge state spaces and sparse delayed reward feedback. Specifically, the authors propose two action selection policies with temporal abstraction for Hierarchical Reinforcement learning (HRL) methods, where skill policies are learned to achieve subgoals. Abstraction is achieved by using an internal reward signal for each of the skills to guide the subgoal attainment. The approaches for subgoal discovery in HRL are based on two approaches: (1) a model-free method that uses incremental unsupervised learning, and (2) an intrinsic motivation learning mechanism. The proposed approach is evaluated on a number of RL problems with sparse delayed feedback (e.g. Montezuma’s Revenge in the rooms environment and the ATARI 2600 game), and shows that the proposed method is able to discover subgo goals that are more likely to be achieved by the model. The authors also show that the intrinsic motivation for the skills learned in the proposed approach improves the performance of HRL. ","This paper proposes two approaches for Reinforcement Learning (RL) that are suitable for large-scale applications with huge state spaces and sparse delayed reward feedback. Specifically, the authors propose two action selection policies with temporal abstraction for Hierarchical Reinforcement learning (HRL) methods, where skill policies are learned to achieve subgoals. Abstraction is achieved by using an internal reward signal for each of the skills to guide the subgoal attainment. The approaches for subgoal discovery in HRL are based on two approaches: (1) a model-free method that uses incremental unsupervised learning, and (2) an intrinsic motivation learning mechanism. The proposed approach is evaluated on a number of RL problems with sparse delayed feedback (e.g. Montezuma’s Revenge in the rooms environment and the ATARI 2600 game), and shows that the proposed method is able to discover subgo goals that are more likely to be achieved by the model. The authors also show that the intrinsic motivation for the skills learned in the proposed approach improves the performance of HRL. "
8552,SP:e5861538bc8bb9165cb33299bbf12dd875abf976,Representation Learning CONJUNCTION Formal Methods. Formal Methods CONJUNCTION Representation Learning. Neuro - Symbolic Methods HYPONYM-OF Formal Methods. neural framework USED-FOR Circuit Satisfiability problem. model USED-FOR SAT problem. rich embedding architecture USED-FOR problem structure. end - to - end differentiable training procedure USED-FOR Reinforcement Learning. rich embedding architecture CONJUNCTION end - to - end differentiable training procedure. end - to - end differentiable training procedure CONJUNCTION rich embedding architecture. rich embedding architecture USED-FOR framework. end - to - end differentiable training procedure USED-FOR framework. framework COMPARE NeuroSAT method. NeuroSAT method COMPARE framework. out - of - sample generalization EVALUATE-FOR framework. out - of - sample generalization EVALUATE-FOR NeuroSAT method. Method is rich neural architectures. ,"This paper proposes a neural framework for solving the Circuit Satisfiability problem in Representation Learning and Formal Methods (Neuro-Symbolic Methods). The model is trained to solve the SAT problem, and a rich embedding architecture is used to capture the problem structure. The proposed framework is based on the rich neural architectures and the end-to-end differentiable training procedure for Reinforcement Learning. The authors show that the proposed framework can achieve better out-of-sample generalization compared to the NeuroSAT method. ","This paper proposes a neural framework for solving the Circuit Satisfiability problem in Representation Learning and Formal Methods (Neuro-Symbolic Methods). The model is trained to solve the SAT problem, and a rich embedding architecture is used to capture the problem structure. The proposed framework is based on the rich neural architectures and the end-to-end differentiable training procedure for Reinforcement Learning. The authors show that the proposed framework can achieve better out-of-sample generalization compared to the NeuroSAT method. "
8561,SP:ff3e5d44619df3825632b0b1a943add081364861,"Deep neuroevolution CONJUNCTION deep reinforcement learning ( deep RL ) algorithms. deep reinforcement learning ( deep RL ) algorithms CONJUNCTION Deep neuroevolution. approaches USED-FOR policy search. deep reinforcement learning ( deep RL ) algorithms USED-FOR policy search. deep reinforcement learning ( deep RL ) algorithms HYPONYM-OF approaches. Deep neuroevolution HYPONYM-OF approaches. Deep neuroevolution USED-FOR policy search. them PART-OF approach. ad hoc evolutionary algorithm CONJUNCTION goal exploration process. goal exploration process CONJUNCTION ad hoc evolutionary algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm HYPONYM-OF sample efficient off - policy deep RL algorithm. goal exploration process CONJUNCTION Deep Deterministic Policy Gradient ( DDPG ) algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm CONJUNCTION goal exploration process. ad hoc evolutionary algorithm CONJUNCTION Deep Deterministic Policy Gradient ( DDPG ) algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm CONJUNCTION ad hoc evolutionary algorithm. Deep Deterministic Policy Gradient ( DDPG ) algorithm USED-FOR combinations. ad hoc evolutionary algorithm USED-FOR combinations. goal exploration process USED-FOR combinations. cross - entropy method ( CEM ) USED-FOR combination scheme. CEM - RL HYPONYM-OF method. sample efficiency EVALUATE-FOR CEM - RL. Generic are former, latter, and methods. OtherScientificTerm is hyper - parameter setting. Method are off - policy deep RL algorithm, and DDPG. Task is deep RL. ","This paper proposes two approaches to policy search: Deep neuroevolution and deep reinforcement learning (deep RL) algorithms. The former is based on the observation that in the hyper-parameter setting, the off-policy deep RL algorithm is more sample efficient than the one based on DDPG. The authors propose to combine them into a single approach, which they call CEM-RL. They propose an ad hoc evolutionary algorithm, a goal exploration process, and a Deep Deterministic Policy Gradient (DDPG) algorithm, which is a sample efficient version of the D.D.P.G. algorithm. They also propose a combination scheme based on cross-entropy method (CEM) to improve the sample efficiency of the proposed combination scheme. Experiments show that the proposed method outperforms the state-of-the-art method in terms of sample efficiency.","This paper proposes two approaches to policy search: Deep neuroevolution and deep reinforcement learning (deep RL) algorithms. The former is based on the observation that in the hyper-parameter setting, the off-policy deep RL algorithm is more sample efficient than the one based on DDPG. The authors propose to combine them into a single approach, which they call CEM-RL. They propose an ad hoc evolutionary algorithm, a goal exploration process, and a Deep Deterministic Policy Gradient (DDPG) algorithm, which is a sample efficient version of the D.D.P.G. algorithm. They also propose a combination scheme based on cross-entropy method (CEM) to improve the sample efficiency of the proposed combination scheme. Experiments show that the proposed method outperforms the state-of-the-art method in terms of sample efficiency."
8570,SP:78b2eb326695da0b0cc4ba39a9206d11644a5e32,forecasting EVALUATE-FOR model. multivariate time series USED-FOR predictive model. forecasting CONJUNCTION temporal and variable level importance interpretation. temporal and variable level importance interpretation CONJUNCTION forecasting. hidden state matrix CONJUNCTION update process. update process CONJUNCTION hidden state matrix. IMV - LSTM USED-FOR variableswise hidden states. hidden state matrix USED-FOR IMV - LSTM. update process USED-FOR IMV - LSTM. summarization methods USED-FOR temporal and variable importance. mixture attention mechanism CONJUNCTION summarization methods. summarization methods CONJUNCTION mixture attention mechanism. mixture attention mechanism USED-FOR temporal and variable importance. real datasets EVALUATE-FOR IMV - LSTM. IMV - LSTM COMPARE baselines. baselines COMPARE IMV - LSTM. real datasets EVALUATE-FOR baselines. end - to - end framework USED-FOR forecasting. end - to - end framework USED-FOR knowledge extraction. forecasting CONJUNCTION knowledge extraction. knowledge extraction CONJUNCTION forecasting. It USED-FOR knowledge extraction. It USED-FOR forecasting. It USED-FOR end - to - end framework. multi - variate data USED-FOR knowledge extraction. OtherScientificTerm is target and exogenous variables. Generic is it. ,"This paper proposes a new predictive model for multivariate time series. The proposed model, called IMV-LSTM, aims to improve the performance of forecasting and forecasting performance of a model that is trained on a set of target and exogenous variables. The authors propose to use a latent variable-wise LSTM to model the variableswise hidden states of the target and the exogenous variable, and then use a hidden state matrix and an update process to update the latent state of the latent variable. They also propose a mixture attention mechanism and two summarization methods to capture temporal and variable importance. The performance on real datasets is compared with several baselines, and it shows better performance. It also proposes an end-to-end framework for forecasting and knowledge extraction from multi-variate data. ","This paper proposes a new predictive model for multivariate time series. The proposed model, called IMV-LSTM, aims to improve the performance of forecasting and forecasting performance of a model that is trained on a set of target and exogenous variables. The authors propose to use a latent variable-wise LSTM to model the variableswise hidden states of the target and the exogenous variable, and then use a hidden state matrix and an update process to update the latent state of the latent variable. They also propose a mixture attention mechanism and two summarization methods to capture temporal and variable importance. The performance on real datasets is compared with several baselines, and it shows better performance. It also proposes an end-to-end framework for forecasting and knowledge extraction from multi-variate data. "
8579,SP:1c26660569b579f060f7b4a31e321c6d2356b928,"adversarial examples USED-FOR defenses. defenses USED-FOR adversarial attacks. feature smoothing HYPONYM-OF data augmentation method. feature smoothing USED-FOR neural network. virtual training data USED-FOR neural network. interpolation of features USED-FOR neural network. feature smoothing USED-FOR virtual data points. logit squeezing USED-FOR feature smoothing. adversarial and clean accuracy EVALUATE-FOR feature smoothing. weight decay CONJUNCTION mix up. mix up CONJUNCTION weight decay. logit squeezing CONJUNCTION weight decay. weight decay CONJUNCTION logit squeezing. label smoothing CONJUNCTION logit squeezing. logit squeezing CONJUNCTION label smoothing. mix up CONJUNCTION feature smoothing. feature smoothing CONJUNCTION mix up. weight decay CONJUNCTION feature smoothing. feature smoothing CONJUNCTION weight decay. feature smoothing USED-FOR unbiased estimation of the decision boundary. label smoothing CONJUNCTION weight decay. weight decay CONJUNCTION label smoothing. symmetrical assumptions CONJUNCTION label smoothing. label smoothing CONJUNCTION symmetrical assumptions. estimated variance FEATURE-OF unbiased estimation of the decision boundary. weight decay HYPONYM-OF methods. OtherScientificTerm are computational overhead, computational burden, and decision boundary. Material is MNIST and CIFAR10 datasets. Method is data augmentation methods. Generic is unified framework. ","This paper proposes a new data augmentation method called feature smoothing, which aims to reduce the computational overhead and improve the robustness of defenses against adversarial attacks. The key idea is to use the interpolation of features from virtual training data to train a neural network on the virtual data points, and then use the learned features to improve the adversarial examples that can be used as defenses. The paper shows that feature smooting can improve both adversarial and clean accuracy on MNIST and CIFAR10 datasets. The authors also show that the proposed methods, weight decay, logit squeezing, mix up, label smoothing and weight decay can all be used to improve unbiased estimation of the decision boundary with lower estimated variance.   The paper is well-written, well-motivated, and well-organized. The idea is interesting, and the experiments are well-designed. However, there are a few issues in the paper, and there is a lack of comparison with existing data augmentation methods, and it would be better if the authors could provide a unified framework to explain their findings. ","This paper proposes a new data augmentation method called feature smoothing, which aims to reduce the computational overhead and improve the robustness of defenses against adversarial attacks. The key idea is to use the interpolation of features from virtual training data to train a neural network on the virtual data points, and then use the learned features to improve the adversarial examples that can be used as defenses. The paper shows that feature smooting can improve both adversarial and clean accuracy on MNIST and CIFAR10 datasets. The authors also show that the proposed methods, weight decay, logit squeezing, mix up, label smoothing and weight decay can all be used to improve unbiased estimation of the decision boundary with lower estimated variance.   The paper is well-written, well-motivated, and well-organized. The idea is interesting, and the experiments are well-designed. However, there are a few issues in the paper, and there is a lack of comparison with existing data augmentation methods, and it would be better if the authors could provide a unified framework to explain their findings. "
8588,SP:88d652f9e411dd3a2e9ad651d9011e579653c6aa,"deep convolutional neural network ( DCNN ) HYPONYM-OF deep and locally connected nonlinear network. theoretical framework USED-FOR networks. ReLU nonlinearity FEATURE-OF networks. framework USED-FOR disentangled representations. framework USED-FOR data distribution. gradient descent rules USED-FOR data distribution. Batch Norm HYPONYM-OF regularization techniques. teacher - student setting USED-FOR framework. Gaussian inputs CONJUNCTION independence of activation. independence of activation CONJUNCTION Gaussian inputs. independence of activation HYPONYM-OF unrealistic assumptions. Gaussian inputs HYPONYM-OF unrealistic assumptions. disentangled representations PART-OF deep networks. OtherScientificTerm are projection nature, and teacher ’s computational graph. ","This paper proposes a theoretical framework for disentangling networks with ReLU nonlinearity, such as a deep convolutional neural network (DCNN) which is a deep and locally connected nonlinear network with a projection nature. The framework is based on the teacher-student setting, where the teacher’s computational graph is shared with the student, and the goal is to learn disentangled representations in deep networks. The authors show that the proposed framework is able to disentangle the data distribution under certain gradient descent rules and regularization techniques such as Batch Norm. They also show that unrealistic assumptions such as Gaussian inputs and independence of activation can be avoided. ","This paper proposes a theoretical framework for disentangling networks with ReLU nonlinearity, such as a deep convolutional neural network (DCNN) which is a deep and locally connected nonlinear network with a projection nature. The framework is based on the teacher-student setting, where the teacher’s computational graph is shared with the student, and the goal is to learn disentangled representations in deep networks. The authors show that the proposed framework is able to disentangle the data distribution under certain gradient descent rules and regularization techniques such as Batch Norm. They also show that unrealistic assumptions such as Gaussian inputs and independence of activation can be avoided. "
8597,SP:7842bbe0e2324cfd732db8745550733ccc3dfcdc,"Prefrontal cortex ( PFC ) USED-FOR behavior repertoire. connectivity CONJUNCTION human behavior formation process. human behavior formation process CONJUNCTION connectivity. Behavioral Module ( BM ) CONJUNCTION end - to - end training strategy. end - to - end training strategy CONJUNCTION Behavioral Module ( BM ). Behavioral Module ( BM ) PART-OF modular architecture of neural networks. end - to - end training strategy PART-OF modular architecture of neural networks. approach USED-FOR learning of behaviors. learning of behaviors CONJUNCTION preferences representation. preferences representation CONJUNCTION learning of behaviors. approach USED-FOR preferences representation. property USED-FOR user modeling. property USED-FOR recommendation tasks. user modeling CONJUNCTION recommendation tasks. recommendation tasks CONJUNCTION user modeling. video games playing EVALUATE-FOR method. independent learning of new behavior patterns USED-FOR network extendability. strategy USED-FOR transfer of newly learned BMs. Task are dialog agents, and personalized representations of different user states. OtherScientificTerm is BMs. ","This paper proposes a modular architecture of neural networks that includes a Behavioral Module (BM) and an end-to-end training strategy. The authors claim that this approach is able to improve the learning of behaviors, preferences representation, and the human behavior formation process in the Pre-Preferential cortex (PFC) to learn a behavior repertoire. They show that this property is useful for both user modeling and recommendation tasks. The method is evaluated on video games playing, where dialog agents are asked to learn personalized representations of different user states. They also show that the proposed strategy can be applied to transfer of newly learned BMs from one task to another. The paper also shows that the independent learning of new behavior patterns can improve network extendability. ","This paper proposes a modular architecture of neural networks that includes a Behavioral Module (BM) and an end-to-end training strategy. The authors claim that this approach is able to improve the learning of behaviors, preferences representation, and the human behavior formation process in the Pre-Preferential cortex (PFC) to learn a behavior repertoire. They show that this property is useful for both user modeling and recommendation tasks. The method is evaluated on video games playing, where dialog agents are asked to learn personalized representations of different user states. They also show that the proposed strategy can be applied to transfer of newly learned BMs from one task to another. The paper also shows that the independent learning of new behavior patterns can improve network extendability. "
8606,SP:300c391ff644b6889cd9ae27cf0d162dfcdd4451,plastic changes in synaptic connectivity USED-FOR lifelong learning. neuromodulation USED-FOR changes. learning CONJUNCTION adaptation. adaptation CONJUNCTION learning. self - modifying abilities USED-FOR biological reinforcement learning. self - modifying abilities USED-FOR learning. self - modifying abilities USED-FOR adaptation. self - modifying abilities FEATURE-OF brain. brain USED-FOR learning. brain USED-FOR adaptation. neuromodulated plasticity USED-FOR artificial neural networks. gradient descent USED-FOR artificial neural networks. differentiable formulation USED-FOR neuromodulation of plasticity. neuromodulated plasticity USED-FOR neural networks. neuromodulated plasticity USED-FOR reinforcement learning and supervised learning tasks. neural networks USED-FOR reinforcement learning and supervised learning tasks. neuromodulated plastic LSTMs COMPARE LSTMs. LSTMs COMPARE neuromodulated plastic LSTMs. task EVALUATE-FOR LSTMs. task EVALUATE-FOR neuromodulated plastic LSTMs. task EVALUATE-FOR benchmark language modeling task. benchmark language modeling task EVALUATE-FOR neuromodulated plastic LSTMs. benchmark language modeling task EVALUATE-FOR LSTMs. differentiable neuromodulation of plasticity USED-FOR neural networks. Method is differentiable Hebbian plasticity. ,"This paper studies the role of plastic changes in synaptic connectivity in lifelong learning. It shows that neuromodulation is responsible for these changes and that the self-modifying abilities of the brain in biological reinforcement learning are related to these changes. Based on this observation, the authors propose a differentiable formulation of the neurmodulation of plasticity that can be used to train artificial neural networks via gradient descent. The authors show that the neural networks trained with neuromodeated plasticity outperform LSTMs on reinforcement learning and supervised learning tasks. They also show that neural networks with differentiable Hebbian plasticity are able to learn more efficiently than neural networks that do not use the differentiable neurmodes. Finally, they show that a benchmark language modeling task on which they show the superiority of neuromoderated plastic LSTM over LSTm on this task.   ","This paper studies the role of plastic changes in synaptic connectivity in lifelong learning. It shows that neuromodulation is responsible for these changes and that the self-modifying abilities of the brain in biological reinforcement learning are related to these changes. Based on this observation, the authors propose a differentiable formulation of the neurmodulation of plasticity that can be used to train artificial neural networks via gradient descent. The authors show that the neural networks trained with neuromodeated plasticity outperform LSTMs on reinforcement learning and supervised learning tasks. They also show that neural networks with differentiable Hebbian plasticity are able to learn more efficiently than neural networks that do not use the differentiable neurmodes. Finally, they show that a benchmark language modeling task on which they show the superiority of neuromoderated plastic LSTM over LSTm on this task.   "
8615,SP:1ab5d94d31e99351433436c026799c8aa597bf73,"quantization techniques USED-FOR inference latency / memory consumption. full precision model USED-FOR non - intrusive quantization technique. quantization training process COMPARE training process. training process COMPARE quantization training process. loss function USED-FOR reduced quantization error. binary quantization USED-FOR full precision accuracy. 2 bit quantization USED-FOR full precision accuracy. 1.5 bits hybrid model COMPARE TWN LSTM model. TWN LSTM model COMPARE 1.5 bits hybrid model. WikiText-2 EVALUATE-FOR TWN LSTM model. Method are Deep Neural Networks, and binary model. Generic is techniques. Material are CIFAR dataset, and ImageNet. ","This paper proposes a non-intrusive quantization technique based on a full precision model trained with a binary quantization. The main idea is that existing quantization techniques can reduce the inference latency/memory consumption by reducing the number of parameters in Deep Neural Networks. The authors propose two techniques to achieve this. First, the authors propose a binary model that can be used to reduce the size of the quantization training process compared to the original training process. Second, they propose a new loss function to achieve a reduced quantization error.   The authors show that binary quantisation can achieve full precision accuracy with 2 bit quantization, and that the proposed techniques can be applied to the CIFAR dataset. They also show that a 1.5 bits hybrid model outperforms the original TWN LSTM model on WikiText-2 and ImageNet.","This paper proposes a non-intrusive quantization technique based on a full precision model trained with a binary quantization. The main idea is that existing quantization techniques can reduce the inference latency/memory consumption by reducing the number of parameters in Deep Neural Networks. The authors propose two techniques to achieve this. First, the authors propose a binary model that can be used to reduce the size of the quantization training process compared to the original training process. Second, they propose a new loss function to achieve a reduced quantization error.   The authors show that binary quantisation can achieve full precision accuracy with 2 bit quantization, and that the proposed techniques can be applied to the CIFAR dataset. They also show that a 1.5 bits hybrid model outperforms the original TWN LSTM model on WikiText-2 and ImageNet."
8624,SP:0876b1d9a6d664808ca1ab15865679fbf638267e,"class - irrelevant properties USED-FOR style. method USED-FOR content embedding. deep metric - learning technique USED-FOR method. deep metric - learning technique USED-FOR content embedding. content encoder PART-OF variational autoencoder ( VAE ). content encoder CONJUNCTION to - be - trained style encoder. to - be - trained style encoder CONJUNCTION content encoder. to - be - trained style encoder PART-OF variational autoencoder ( VAE ). auxiliary loss CONJUNCTION leakage filtering. leakage filtering CONJUNCTION auxiliary loss. style information USED-FOR reconstruction. style information PART-OF content representation. auxiliary loss PART-OF method. leakage filtering PART-OF method. content representation USED-FOR style representation. method USED-FOR data - set augmentation. pose CONJUNCTION expression. expression CONJUNCTION pose. expression CONJUNCTION hairstyle. hairstyle CONJUNCTION expression. lighting CONJUNCTION pose. pose CONJUNCTION lighting. decompositions USED-FOR classification. Recombinations USED-FOR creative exercise. Recombinations USED-FOR data set augmentation. approach USED-FOR content - style decomposition and recombination. specific domain knowledge USED-FOR approaches. human body pose HYPONYM-OF specific domain knowledge. leakage filtering USED-FOR STOC. objective PART-OF STOC. leakage filtering HYPONYM-OF objective. supervised training USED-FOR style and content representations. STOC USED-FOR content - style recombination. Material are visual domains, masterworks of art, and Open - Ended Content. Method are domain - independent method, and Decompositions. OtherScientificTerm are VAE reconstruction loss, content, within - class variation, betweenand within - class variation, and musical composition. Task are few - shot learning tasks, face - recognition task, and emotion - recognition task. ","This paper proposes a method for learning content embedding using a deep metric-learning technique. The method is based on a variational autoencoder (VAE) with a content encoder and a to-be-trained style encoder, where the style is modelled as a set of class-irrelevant properties, and the VAE reconstruction loss is used to capture the relationship between content and style.   The method combines an auxiliary loss, leakage filtering, and a novel method for data-set augmentation based on Recombinations for data set augmentation.  This is a domain-independent method that can be applied to few-shot learning tasks.  The paper proposes an approach for content-style decomposition and recombination. Decompositions are used for classification, and decompositions of the content and the style are used to augment the content representation of the style representation with the style information from the content.  For the face-recognition task, the paper shows that decomposing the content into within-class variation and content-based decomposition results in better performance. The paper also shows that the proposed approach STOC (Stochastic Open-Ended Content) can be used to learn content and content based on supervised training to learn both style and content representations. STOC is an extension of the original objective of STOC, which is leakage filtering.  Experiments are performed on a few standard few shot learning tasks, including lighting, lighting, pose, expression, and hairstyle, and on an emotion-reconstruction task. The results show that STOCD and STOC are able to generalize to a range of different types of content, and that the approach can generalize well to different domains. The approach also generalizes well to other approaches that rely on specific domain knowledge (e.g., human body pose, music composition).  ","This paper proposes a method for learning content embedding using a deep metric-learning technique. The method is based on a variational autoencoder (VAE) with a content encoder and a to-be-trained style encoder, where the style is modelled as a set of class-irrelevant properties, and the VAE reconstruction loss is used to capture the relationship between content and style.   The method combines an auxiliary loss, leakage filtering, and a novel method for data-set augmentation based on Recombinations for data set augmentation.  This is a domain-independent method that can be applied to few-shot learning tasks.  The paper proposes an approach for content-style decomposition and recombination. Decompositions are used for classification, and decompositions of the content and the style are used to augment the content representation of the style representation with the style information from the content.  For the face-recognition task, the paper shows that decomposing the content into within-class variation and content-based decomposition results in better performance. The paper also shows that the proposed approach STOC (Stochastic Open-Ended Content) can be used to learn content and content based on supervised training to learn both style and content representations. STOC is an extension of the original objective of STOC, which is leakage filtering.  Experiments are performed on a few standard few shot learning tasks, including lighting, lighting, pose, expression, and hairstyle, and on an emotion-reconstruction task. The results show that STOCD and STOC are able to generalize to a range of different types of content, and that the approach can generalize well to different domains. The approach also generalizes well to other approaches that rely on specific domain knowledge (e.g., human body pose, music composition).  "
8633,SP:d37e15cde7765fca87595a242f0a4511b3346d46,method USED-FOR deep reinforcement learning ( deep RL ) training. deep reinforcement learning ( deep RL ) training USED-FOR problems. state - action permissibility ( SAP ) FEATURE-OF problems. permissibility PART-OF SAP. deep RL algorithms USED-FOR state - action exploration. SAP property PART-OF deep RL algorithms. SAP property USED-FOR state - action exploration. SAP guidance USED-FOR training. ,This paper proposes a method for deep reinforcement learning (deep RL) training to tackle problems with state-action permissibility (SAP). The authors show that there is a strong correlation between the performance of deep RL algorithms with respect to the SAP property and the state of the art on a number of problems. The authors also show that the permutation of the SAP is a property that can be used to guide the training process. The paper also shows that the SAP guidance can be applied to the training of existing RL algorithms to improve the performance.,This paper proposes a method for deep reinforcement learning (deep RL) training to tackle problems with state-action permissibility (SAP). The authors show that there is a strong correlation between the performance of deep RL algorithms with respect to the SAP property and the state of the art on a number of problems. The authors also show that the permutation of the SAP is a property that can be used to guide the training process. The paper also shows that the SAP guidance can be applied to the training of existing RL algorithms to improve the performance.
8642,SP:20015d8b60e13300586b67c281858cbe28825c48,"random weights USED-FOR weight - tied multilayer vanilla autoencoders. random deep weight - tied autoencoder model USED-FOR approximate inference. deep autoencoders COMPARE shallow counterparts. shallow counterparts COMPARE deep autoencoders. layer - wise pre - training CONJUNCTION batch normalization. batch normalization CONJUNCTION layer - wise pre - training. batch normalization HYPONYM-OF techniques. layer - wise pre - training HYPONYM-OF techniques. tanh activation USED-FOR deep autoencoder. OtherScientificTerm are large dimensions, phase transition phenomena, reversibility, and Lipschitz activations. Task is training initialization practice. Method is analytical techniques. "," autoencoder models have been shown to suffer from phase transition in large dimensions. This paper proposes a random deep weight-tied multilayer vanilla autoencoders with random weights. The authors show that the phase transition phenomena is due to reversibility of the Lipschitz activations of the weights. They then propose a training initialization practice where the weights are randomly permuted. They also show that this random deep loss can be used for approximate inference.   The authors also propose a few analytical techniques to explain the phenomenon. They show that under certain assumptions on the weights, deep autoencopers are more robust than their shallow counterparts. They further propose two techniques: layer-wise pre-training and batch normalization. Finally, they show that a deep Autoencoder trained with tanh activation is more robust. "," autoencoder models have been shown to suffer from phase transition in large dimensions. This paper proposes a random deep weight-tied multilayer vanilla autoencoders with random weights. The authors show that the phase transition phenomena is due to reversibility of the Lipschitz activations of the weights. They then propose a training initialization practice where the weights are randomly permuted. They also show that this random deep loss can be used for approximate inference.   The authors also propose a few analytical techniques to explain the phenomenon. They show that under certain assumptions on the weights, deep autoencopers are more robust than their shallow counterparts. They further propose two techniques: layer-wise pre-training and batch normalization. Finally, they show that a deep Autoencoder trained with tanh activation is more robust. "
8651,SP:91764f80dbe2401ade38b35a8253ba05f0f86386,"search problem USED-FOR construction of adversarial images. model evaluations USED-FOR sporadic feedback. low frequency component PART-OF discrete cosine transform ( DCT ). iterative principle USED-FOR search strategy. iterative principle USED-FOR algorithm. method USED-FOR targeted and untargeted attacks. query efficiency EVALUATE-FOR method. median queries USED-FOR Google Cloud Vision. algorithm USED-FOR adversarial black - box attacks. PyTorch code USED-FOR it. Generic is model. Task is Model evaluations. Metric is adversarial loss. Material are ResNet-50, and adversarial ImageNet image. ","This paper considers the search problem for the construction of adversarial images, which is a search problem in which the goal is to construct an adversarial image that can be used to fool a model. Model evaluations are used as sporadic feedback, where the model evaluations are based on model evaluations and the adversarial loss is based on the loss of a low frequency component in the discrete cosine transform (DCT). The authors propose an algorithm that uses an iterative principle to find the best search strategy. The proposed method is shown to be effective for both targeted and untargeted attacks. The method is evaluated on median queries for Google Cloud Vision, and the method is also shown to improve query efficiency. The algorithm is also applied to adversarial black-box attacks, where it is trained using PyTorch code. Experiments are performed on ResNet-50 and ResNet50-50, and it is shown that the method can be applied to a single adversarial ImageNet image. ","This paper considers the search problem for the construction of adversarial images, which is a search problem in which the goal is to construct an adversarial image that can be used to fool a model. Model evaluations are used as sporadic feedback, where the model evaluations are based on model evaluations and the adversarial loss is based on the loss of a low frequency component in the discrete cosine transform (DCT). The authors propose an algorithm that uses an iterative principle to find the best search strategy. The proposed method is shown to be effective for both targeted and untargeted attacks. The method is evaluated on median queries for Google Cloud Vision, and the method is also shown to improve query efficiency. The algorithm is also applied to adversarial black-box attacks, where it is trained using PyTorch code. Experiments are performed on ResNet-50 and ResNet50-50, and it is shown that the method can be applied to a single adversarial ImageNet image. "
8660,SP:fc20ae0fbf57a1ce489c04b85c7c2f4c93dc2450,"temporal abstractions USED-FOR curse of dimensionality. Hierarchical Reinforcement Learning USED-FOR temporal abstractions. method USED-FOR temporal abstractions. options framework HYPONYM-OF hierarchical framework. heuristics USED-FOR Option discovery. method COMPARE discovering bottlenecks. discovering bottlenecks COMPARE method. method USED-FOR bottlenecks. Successor options HYPONYM-OF model. Successor representations USED-FOR Successor options. Successor representations USED-FOR model. pseudo - reward USED-FOR intra - option policies. primitive actions USED-FOR Successor representations. Incremental Successor options model USED-FOR options. grid worlds CONJUNCTION complex high dimensional environments. complex high dimensional environments CONJUNCTION grid worlds. complex high dimensional environments EVALUATE-FOR approach. Deepmind - Lab HYPONYM-OF complex high dimensional environments. grid worlds EVALUATE-FOR approach. OtherScientificTerm are task - agnostic transferable skills, bottleneck states, landmark ” sub - goals, well connected regions, and sub - goals. Task is discovering bottleneck states. ","This paper proposes a method for learning task-agnostic transferable transferable skills that can be used to tackle the curse of dimensionality in learning temporal abstractions in Hierarchical Reinforcement Learning. The authors propose a hierarchical framework, the options framework, where the goal is to discover a set of options (or “bottleneck states”) that are transferable across different tasks. Option discovery is performed using heuristics that are based on previous work on discovering bottlenecks. The proposed method, called Successor options (successor options), is a model that uses Successor representations learned from primitive actions as options, and then uses a pseudo-reward to learn intra-option policies. The options are learned using an Incremental Successor Options model, which is trained to discover options that are “stabilized” in the bottleneck states (i.e., in which the “landmark” sub-goals are found). The authors show that the proposed method outperforms existing methods for discovering bottleneck states in well connected regions, and outperforms methods that do not consider sub-goal states. The approach is evaluated on grid worlds and complex high dimensional environments (e.g. Deepmind-Lab). ","This paper proposes a method for learning task-agnostic transferable transferable skills that can be used to tackle the curse of dimensionality in learning temporal abstractions in Hierarchical Reinforcement Learning. The authors propose a hierarchical framework, the options framework, where the goal is to discover a set of options (or “bottleneck states”) that are transferable across different tasks. Option discovery is performed using heuristics that are based on previous work on discovering bottlenecks. The proposed method, called Successor options (successor options), is a model that uses Successor representations learned from primitive actions as options, and then uses a pseudo-reward to learn intra-option policies. The options are learned using an Incremental Successor Options model, which is trained to discover options that are “stabilized” in the bottleneck states (i.e., in which the “landmark” sub-goals are found). The authors show that the proposed method outperforms existing methods for discovering bottleneck states in well connected regions, and outperforms methods that do not consider sub-goal states. The approach is evaluated on grid worlds and complex high dimensional environments (e.g. Deepmind-Lab). "
8669,SP:12a172c1e2892d016b37932acfc48dcb56874a89,"probabilistic distributions USED-FOR domain division. problem USED-FOR recognition tasks. Open Set Learning ( OSL ) HYPONYM-OF recognition tasks. probabilistic way USED-FOR decision boundary. domain division algorithm USED-FOR recognition tasks. domain USED-FOR recognition tasks. bootstrapping CONJUNCTION KolmogorovSmirnov ( K - S ) Test. KolmogorovSmirnov ( K - S ) Test CONJUNCTION bootstrapping. statistical tools USED-FOR decision boundary. KolmogorovSmirnov ( K - S ) Test USED-FOR decision boundary. KolmogorovSmirnov ( K - S ) Test HYPONYM-OF statistical tools. bootstrapping HYPONYM-OF statistical tools. uncertain domain PART-OF framework. OSL and G - ZSL benchmarks EVALUATE-FOR approach. Method are classifiers, and WSVM. OtherScientificTerm is known, unknown and uncertain domains. ","This paper considers the problem of domain division, i. In this problem, recognition tasks such as Open Set Learning (OSL) are partitioned into known, unknown and uncertain domains, and the goal is to train classifiers that generalize well across all domains. The authors propose a probabilistic way to compute the decision boundary between the known and unknown domains, based on the notion of ""probabilistic distributions"". They propose a domain division algorithm that can be applied to a wide range of recognition tasks in a single domain. The proposed framework is based on WSVM, and they use a number of statistical tools such as bootstrapping and KolmogorovSmirnov (K-S) Test to compute a decision boundary. They evaluate their approach on the OSL and G-ZSL benchmarks, and show that their framework can generalize to an uncertain domain.","This paper considers the problem of domain division, i. In this problem, recognition tasks such as Open Set Learning (OSL) are partitioned into known, unknown and uncertain domains, and the goal is to train classifiers that generalize well across all domains. The authors propose a probabilistic way to compute the decision boundary between the known and unknown domains, based on the notion of ""probabilistic distributions"". They propose a domain division algorithm that can be applied to a wide range of recognition tasks in a single domain. The proposed framework is based on WSVM, and they use a number of statistical tools such as bootstrapping and KolmogorovSmirnov (K-S) Test to compute a decision boundary. They evaluate their approach on the OSL and G-ZSL benchmarks, and show that their framework can generalize to an uncertain domain."
8678,SP:28bcf7c6a4673e9ec2b4ebed09839d85188e0b2a,"neural network USED-FOR classification and regression. softmax cross - entropy CONJUNCTION mean squared error. mean squared error CONJUNCTION softmax cross - entropy. maximum margin separation CONJUNCTION simplicity ( Occam ’s Razor ). simplicity ( Occam ’s Razor ) CONJUNCTION maximum margin separation. mean squared error HYPONYM-OF solutions. softmax cross - entropy HYPONYM-OF solutions. simplicity ( Occam ’s Razor ) HYPONYM-OF inductive structures. maximum margin separation HYPONYM-OF inductive structures. polar prototype networks HYPONYM-OF networks. polar prototypes USED-FOR structure. maximal separation FEATURE-OF they. angular distances USED-FOR training. training USED-FOR regression. higher - dimensional outputs USED-FOR regression. polar interpolation USED-FOR training. large margin separation CONJUNCTION semantic class structure. semantic class structure CONJUNCTION large margin separation. semantic class structure USED-FOR polar prototype networks. large margin separation USED-FOR polar prototype networks. classification COMPARE network methods. network methods COMPARE classification. regression CONJUNCTION classification. classification CONJUNCTION regression. OtherScientificTerm are layout structures, layout, polar prototype, hypersphere, semantic priors, class prototypes, prototypes, and output dimensions. Task is minimizing angular distances. ","This paper considers the problem of training a neural network for classification and regression with different layout structures. The authors propose three solutions: softmax cross-entropy, mean squared error, and simplicity (Occam’s Razor). These networks are called polar prototype networks, and the structure is based on polar prototypes. The layout of a polar prototype is defined as a hypersphere, where the layout of each layer corresponds to a set of class prototypes. In this way, the authors show that polar prototypes can be used to learn the structure of the network, and that they have a maximal separation between the output dimension and the input dimension. The paper also shows that minimizing angular distances between two prototypes is equivalent to minimizing the distance between the outputs of the two classes.  The authors also show that training for regression with higher-dimensional outputs can be improved by training with polar interpolation. The main contribution of the paper is that the polar prototype network is able to learn a mapping from the input dimensions to the output dimensions, which is an important property for classification.  Experiments are conducted on regression and classification, and show that the classification performance of the proposed network methods is comparable to state-of-the-art network methods.  ","This paper considers the problem of training a neural network for classification and regression with different layout structures. The authors propose three solutions: softmax cross-entropy, mean squared error, and simplicity (Occam’s Razor). These networks are called polar prototype networks, and the structure is based on polar prototypes. The layout of a polar prototype is defined as a hypersphere, where the layout of each layer corresponds to a set of class prototypes. In this way, the authors show that polar prototypes can be used to learn the structure of the network, and that they have a maximal separation between the output dimension and the input dimension. The paper also shows that minimizing angular distances between two prototypes is equivalent to minimizing the distance between the outputs of the two classes.  The authors also show that training for regression with higher-dimensional outputs can be improved by training with polar interpolation. The main contribution of the paper is that the polar prototype network is able to learn a mapping from the input dimensions to the output dimensions, which is an important property for classification.  Experiments are conducted on regression and classification, and show that the classification performance of the proposed network methods is comparable to state-of-the-art network methods.  "
8687,SP:d1034342785d133cf8372b8624897963cc2ee83a,"Reinforcement learning ( RL ) agents USED-FOR features. reward function USED-FOR features. it EVALUATE-FOR idea. it USED-FOR proof - of - concept environments. proof - of - concept environments EVALUATE-FOR idea. Maximum Causal Entropy IRL USED-FOR algorithm. Generic are preferences, and robot. OtherScientificTerm are implicit preference information, and side effects. ","This paper studies the problem of learning features that are useful for RL agents to learn. In particular, the authors focus on the problem that RL agents can learn features that can be used to learn preferences. The authors propose a method to learn such features by learning a reward function that maximizes the mutual information between the reward function and the features learned by the agent and the robot. The idea is motivated by the observation that implicit preference information is not always useful to learn, and that there are often side effects of learning such features. The proposed algorithm is based on Maximum Causal Entropy IRL, and it is tested on several proof-of-concept environments.","This paper studies the problem of learning features that are useful for RL agents to learn. In particular, the authors focus on the problem that RL agents can learn features that can be used to learn preferences. The authors propose a method to learn such features by learning a reward function that maximizes the mutual information between the reward function and the features learned by the agent and the robot. The idea is motivated by the observation that implicit preference information is not always useful to learn, and that there are often side effects of learning such features. The proposed algorithm is based on Maximum Causal Entropy IRL, and it is tested on several proof-of-concept environments."
8696,SP:417a4e0acee699b3e004ad30d0ecf533a9ed987e,method USED-FOR dependency structure between latent variables. deep generative models CONJUNCTION probabilistic graphical models. probabilistic graphical models CONJUNCTION deep generative models. deep generative models PART-OF modeling and inference framework. probabilistic graphical models PART-OF modeling and inference framework. latent variable space FEATURE-OF variational autoencoder ( VAE ). flexible dependency structure FEATURE-OF Bayesian network. Bayesian network USED-FOR variational autoencoder ( VAE ). Bayesian network USED-FOR latent variable space. network parameters CONJUNCTION variational parameters. variational parameters CONJUNCTION network parameters. variational parameters CONJUNCTION latent topology. latent topology CONJUNCTION variational parameters. single objective USED-FOR latent topology. single objective USED-FOR variational parameters. single objective USED-FOR network parameters. latent variable values FEATURE-OF top - down and bottom - up reasoning. top - down and bottom - up reasoning USED-FOR Inference. sampling procedure USED-FOR Inference. MNIST CONJUNCTION Omniglot. Omniglot CONJUNCTION MNIST. Omniglot CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION Omniglot. Omniglot EVALUATE-FOR framework. CIFAR-10 EVALUATE-FOR framework. MNIST EVALUATE-FOR framework. structured variational autoencoder baselines COMPARE model. model COMPARE structured variational autoencoder baselines. Method is deep latent variable models. OtherScientificTerm is latent variable structures. ,"This paper proposes a method for learning the dependency structure between latent variables in deep latent variable models. The proposed modeling and inference framework combines the advantages of both deep generative models and probabilistic graphical models. Specifically, a variational autoencoder (VAE) with a Bayesian network with flexible dependency structure is learned in the latent variable space. Inference is performed using top-down and bottom-up reasoning over the latent variables values, and a single objective is used to learn the network parameters, variational parameters, and latent topology. In addition, a sampling procedure is proposed to improve the quality of the learned latent variable structures. Experiments on MNIST, Omniglot, and CIFAR-10 show that the proposed framework outperforms structured variational auto-encoder baselines. ","This paper proposes a method for learning the dependency structure between latent variables in deep latent variable models. The proposed modeling and inference framework combines the advantages of both deep generative models and probabilistic graphical models. Specifically, a variational autoencoder (VAE) with a Bayesian network with flexible dependency structure is learned in the latent variable space. Inference is performed using top-down and bottom-up reasoning over the latent variables values, and a single objective is used to learn the network parameters, variational parameters, and latent topology. In addition, a sampling procedure is proposed to improve the quality of the learned latent variable structures. Experiments on MNIST, Omniglot, and CIFAR-10 show that the proposed framework outperforms structured variational auto-encoder baselines. "
8705,SP:976dedab53e69610692a563382ada1dbb82c1e9d,"interconnected neurons PART-OF dynamical neural network. numerical solutions USED-FOR mathematical optimization or learning problems. computational properties FEATURE-OF It. it CONJUNCTION massively parallel computer architecture. massively parallel computer architecture CONJUNCTION it. massively parallel computer architecture USED-FOR power and throughput efficiency. local memory HYPONYM-OF local information. dynamical network USED-FOR gradients. top - down feedback CONJUNCTION contrastive learning. contrastive learning CONJUNCTION top - down feedback. dynamical network USED-FOR ` 1 - minimizing dictionary learning problem. top - down feedback USED-FOR dynamical network. contrastive learning USED-FOR dynamical network. gradients USED-FOR learning. spiking neurons USED-FOR dynamical network. OtherScientificTerm is state space. Method are computational system, and learning process. Task is dictionary learning problems. ","This paper proposes a novel dynamical neural network with interconnected neurons that can be used to learn numerical solutions to mathematical optimization or learning problems. It has a number of interesting computational properties: it has a massively parallel computer architecture that allows for power and throughput efficiency, and it can store local information (e.g. local memory) in the state space. The authors show that a dynamical network trained with spiking neurons is able to learn gradients for the `1-minimizing dictionary learning problem with top-down feedback and contrastive learning. They also show that the gradients used for learning are non-trivial to compute, and that the computational system can be decomposed into two parts: (1) a computational system that takes as input the current state of the system, and (2) a learning process that takes the current and previous state as input, and outputs the next state.   The authors also provide a theoretical analysis of dictionary learning problems, showing that the learned gradients can be efficiently computed. ","This paper proposes a novel dynamical neural network with interconnected neurons that can be used to learn numerical solutions to mathematical optimization or learning problems. It has a number of interesting computational properties: it has a massively parallel computer architecture that allows for power and throughput efficiency, and it can store local information (e.g. local memory) in the state space. The authors show that a dynamical network trained with spiking neurons is able to learn gradients for the `1-minimizing dictionary learning problem with top-down feedback and contrastive learning. They also show that the gradients used for learning are non-trivial to compute, and that the computational system can be decomposed into two parts: (1) a computational system that takes as input the current state of the system, and (2) a learning process that takes the current and previous state as input, and outputs the next state.   The authors also provide a theoretical analysis of dictionary learning problems, showing that the learned gradients can be efficiently computed. "
8714,SP:f45117a6beaeb86a70b1380b4fac3cfba37fb892,"spatial pyramid structure CONJUNCTION encoder - decoder structure. encoder - decoder structure CONJUNCTION spatial pyramid structure. semantic image segmentation CONJUNCTION lane detection. lane detection CONJUNCTION semantic image segmentation. spatial pyramid structure USED-FOR nets. encoder - decoder structure FEATURE-OF nets. nets USED-FOR lane detection. nets USED-FOR semantic image segmentation. weak visual appearance CONJUNCTION prior information. prior information CONJUNCTION weak visual appearance. multi - scale context CONJUNCTION pixel - level accuracy. pixel - level accuracy CONJUNCTION multi - scale context. network USED-FOR lane detection. encoder - decoders module USED-FOR lane detection. evaluation methods EVALUATE-FOR lane detection. Method are Convolutional neural networks ( CNNs ), and encoder - decoders nets. Task are lane detection task, and model - based lane detection. Generic is methods. ","This paper proposes a new class of Convolutional neural networks (CNNs) for the task of semantic image segmentation and lane detection. The authors show that such nets have a spatial pyramid structure and an encoder-decoder structure, and that these nets can be used for both semantic images segmentation as well as for lane detection tasks. In particular, they show that a network trained on the semantic segmentation task can be applied to the lane detection task as well.    The authors also show that, in the case of model-based lane detection, a network can be trained on a single image and then used to train a network for multiple images. They show that this network can also be trained to be able to be used in a multi-scale context and to achieve pixel-level accuracy.  Finally, the authors also propose a new evaluation methods to evaluate the performance of lane detection based on the encoder - decoders module.  The paper is well-written and well-motivated, and the proposed methods are interesting. However, there are a few issues in the paper that need to be addressed (weak visual appearance and prior information).  ","This paper proposes a new class of Convolutional neural networks (CNNs) for the task of semantic image segmentation and lane detection. The authors show that such nets have a spatial pyramid structure and an encoder-decoder structure, and that these nets can be used for both semantic images segmentation as well as for lane detection tasks. In particular, they show that a network trained on the semantic segmentation task can be applied to the lane detection task as well.    The authors also show that, in the case of model-based lane detection, a network can be trained on a single image and then used to train a network for multiple images. They show that this network can also be trained to be able to be used in a multi-scale context and to achieve pixel-level accuracy.  Finally, the authors also propose a new evaluation methods to evaluate the performance of lane detection based on the encoder - decoders module.  The paper is well-written and well-motivated, and the proposed methods are interesting. However, there are a few issues in the paper that need to be addressed (weak visual appearance and prior information).  "
8723,SP:68b0a10ca06df74612d0753cc3f3ddddde806035,policy COMPARE off - policy training data. off - policy training data COMPARE policy. supervised learning and online learning settings COMPARE batch contextual bandit learning. batch contextual bandit learning COMPARE supervised learning and online learning settings. ad platforms CONJUNCTION recommendation systems. recommendation systems CONJUNCTION ad platforms. batch learning setting USED-FOR online and interactive systems. ad platforms HYPONYM-OF online and interactive systems. recommendation systems HYPONYM-OF online and interactive systems. Policy Optimizer USED-FOR Exponential Models ( POEM ). Inverse Propensity Scoring ( IPS ) CONJUNCTION Policy Optimizer. Policy Optimizer CONJUNCTION Inverse Propensity Scoring ( IPS ). Policy Optimizer HYPONYM-OF approaches. Inverse Propensity Scoring ( IPS ) HYPONYM-OF approaches. inverse propensity weights USED-FOR approaches. Maximum Likelihood Inverse Propensity Scoring ( MLIPS ) USED-FOR batch learning. approach USED-FOR batch learning. logged bandit feedback USED-FOR Maximum Likelihood Inverse Propensity Scoring ( MLIPS ). logged bandit feedback USED-FOR batch learning. historical policy USED-FOR inverse propensity weights. logged action - context pairs USED-FOR maximum likelihood surrogate policy. MLIPS COMPARE IPS. IPS COMPARE MLIPS. nonasymptotic mean squared error EVALUATE-FOR IPS. nonasymptotic mean squared error EVALUATE-FOR MLIPS. surrogate policy COMPARE historical policy. historical policy COMPARE surrogate policy. large - scale ad placement dataset EVALUATE-FOR MLIPS. multi - label classification problems CONJUNCTION large - scale ad placement dataset. large - scale ad placement dataset CONJUNCTION multi - label classification problems. multi - label classification problems EVALUATE-FOR MLIPS. surrogate policy technique COMPARE error reduction techniques. error reduction techniques COMPARE surrogate policy technique. surrogate policy technique USED-FOR approaches. OtherScientificTerm is logged feedback. Metric is mean squared error. ,"This paper proposes a novel approach to batch contextual bandit learning based on Maximum Likelihood Inverse Propensity Scoring (MLIPS) based on logged bandit feedback. The authors show that the proposed approach outperforms existing approaches based on inverse propensity weights (Inverse Proposition Scoring and Policy Optimizer for Exponential Models (POEM) in both supervised learning and online learning settings, and outperforms the state-of-the-art results in the online and interactive systems in the batch learning setting (e.g., ad platforms and recommendation systems). The authors also show that a maximum likelihood surrogate policy is learned from logged action-contextual pairs, and that this surrogate policy performs better than a historical policy trained on the same logged feedback. In addition, they show that MLIPS achieves a nonasymptotic mean squared error that is at least as good as that of IPS, which is a lower bound of the mean square error of the log-likelihood of the historical policy. They further show that their surrogate policy technique outperforms other error reduction techniques, and is competitive with existing approaches that do not use the surrogate policy. Finally, they evaluate MLIPS on multi-label classification problems and a large-scale ad placement dataset, and show that they outperform previous work. ","This paper proposes a novel approach to batch contextual bandit learning based on Maximum Likelihood Inverse Propensity Scoring (MLIPS) based on logged bandit feedback. The authors show that the proposed approach outperforms existing approaches based on inverse propensity weights (Inverse Proposition Scoring and Policy Optimizer for Exponential Models (POEM) in both supervised learning and online learning settings, and outperforms the state-of-the-art results in the online and interactive systems in the batch learning setting (e.g., ad platforms and recommendation systems). The authors also show that a maximum likelihood surrogate policy is learned from logged action-contextual pairs, and that this surrogate policy performs better than a historical policy trained on the same logged feedback. In addition, they show that MLIPS achieves a nonasymptotic mean squared error that is at least as good as that of IPS, which is a lower bound of the mean square error of the log-likelihood of the historical policy. They further show that their surrogate policy technique outperforms other error reduction techniques, and is competitive with existing approaches that do not use the surrogate policy. Finally, they evaluate MLIPS on multi-label classification problems and a large-scale ad placement dataset, and show that they outperform previous work. "
8732,SP:8e0ed65c5dded23b34798499b2436b24422fd729,learning framework USED-FOR few - shot classification tasks. Meta - learning USED-FOR learning framework. Meta - learning USED-FOR few - shot classification tasks. meta - learner USED-FOR model optimization. parameter initialization CONJUNCTION similarity metric. similarity metric CONJUNCTION parameter initialization. model optimization CONJUNCTION parameter initialization. parameter initialization CONJUNCTION model optimization. meta - learner PART-OF meta - learning methods. individualized feature embedding USED-FOR classifying. feature embedding USED-FOR individualized feature space. kernel generator USED-FOR feature embedding. feature embedding USED-FOR query images. kernel generator USED-FOR meta - learner. meta - knowledge USED-FOR convolutional kernels. kernel generator USED-FOR convolutional kernels. training USED-FOR convolutional kernels. meta - knowledge USED-FOR kernel generator. few - shot classification data sets EVALUATE-FOR method. Omniglot CONJUNCTION miniImageNet. miniImageNet CONJUNCTION Omniglot. miniImageNet EVALUATE-FOR method. miniImageNet HYPONYM-OF few - shot classification data sets. Omniglot HYPONYM-OF few - shot classification data sets. Method is fine - tuning. ,"Meta-learning is a learning framework for few-shot classification tasks. In meta-learning methods, a meta-learner is used to guide model optimization, parameter initialization, and similarity metric. In this paper, the authors propose to use individualized feature embedding for classifying, and use a kernel generator to learn the feature embeddings for each query image, and then fine-tune the query images based on the individualized features. The authors show that the meta-knowledge of the convolutional kernels learned during training improves the performance of the proposed method on standard few-set classification data sets such as Omniglot and miniImageNet. ","Meta-learning is a learning framework for few-shot classification tasks. In meta-learning methods, a meta-learner is used to guide model optimization, parameter initialization, and similarity metric. In this paper, the authors propose to use individualized feature embedding for classifying, and use a kernel generator to learn the feature embeddings for each query image, and then fine-tune the query images based on the individualized features. The authors show that the meta-knowledge of the convolutional kernels learned during training improves the performance of the proposed method on standard few-set classification data sets such as Omniglot and miniImageNet. "
8741,SP:faa3f7ffdcfb6e3b8ec0421193dae3d9987b015c,backpropagation HYPONYM-OF gradient - based learning algorithms. gradient - based learning algorithms USED-FOR Deep artificial neural networks ( DNNs ). Q - learning CONJUNCTION policy gradients. policy gradients CONJUNCTION Q - learning. Evolution strategies ( ES ) COMPARE backprop - based algorithms. backprop - based algorithms COMPARE Evolution strategies ( ES ). policy gradients USED-FOR deep reinforcement learning ( RL ) problems. backprop - based algorithms USED-FOR deep reinforcement learning ( RL ) problems. policy gradients HYPONYM-OF backprop - based algorithms. Q - learning HYPONYM-OF backprop - based algorithms. it USED-FOR stochastic gradient descent. ES HYPONYM-OF gradient - based algorithm. finite - difference approximation of the gradient HYPONYM-OF operation. operation USED-FOR it. operation USED-FOR stochastic gradient descent. non - gradient - based evolutionary algorithms USED-FOR DNN scales. Atari CONJUNCTION humanoid locomotion. humanoid locomotion CONJUNCTION Atari. it USED-FOR hard deep RL problems. humanoid locomotion HYPONYM-OF hard deep RL problems. Atari HYPONYM-OF hard deep RL problems. Deep GA USED-FOR networks. free parameters FEATURE-OF networks. evolutionary algorithm USED-FOR neural networks. ES CONJUNCTION GA. GA CONJUNCTION ES. DNNs CONJUNCTION novelty search. novelty search CONJUNCTION DNNs. A3C CONJUNCTION ES. ES CONJUNCTION A3C. novelty search USED-FOR exploration. DQN CONJUNCTION A3C. A3C CONJUNCTION DQN. DNNs USED-FOR high - dimensional problem. reward - maximizing algorithms USED-FOR high - dimensional problem. GA HYPONYM-OF reward - maximizing algorithms. ES HYPONYM-OF reward - maximizing algorithms. DQN HYPONYM-OF reward - maximizing algorithms. A3C HYPONYM-OF reward - maximizing algorithms. A3C CONJUNCTION DQN. DQN CONJUNCTION A3C. ES CONJUNCTION A3C. A3C CONJUNCTION ES. Deep GA,"Deep artificial neural networks (DNNs) have been the focus of a lot of work in recent years, but gradient-based learning algorithms (e.g., backpropagation) have not yet been considered. Evolution strategies (ES) have recently been shown to outperform backprop-based algorithms such as Q-learning and policy gradients in a variety of deep reinforcement learning (RL) problems, and it is well-known that Evolution strategies are more efficient than backprop - based algorithms in many deep RL problems such as Atari and humanoid locomotion.    This paper studies the evolution of DNN scales using non-gradient-based evolutionary algorithms. The authors propose Deep GA, which is an evolutionary algorithm for training neural networks with free parameters. They show that it can be used to accelerate stochastic gradient descent by using an operation called the finite-difference approximation of the gradient. They also show that Deep GA is able to learn networks with networks that have free parameters, and that it performs well in a range of environments.  The paper also shows that reward-maximizing algorithms (such as A3C, DQN, ES, and GA) can be adapted to solve a high-dimensional problem using DNNs and novelty search. ","Deep artificial neural networks (DNNs) have been the focus of a lot of work in recent years, but gradient-based learning algorithms (e.g., backpropagation) have not yet been considered. Evolution strategies (ES) have recently been shown to outperform backprop-based algorithms such as Q-learning and policy gradients in a variety of deep reinforcement learning (RL) problems, and it is well-known that Evolution strategies are more efficient than backprop - based algorithms in many deep RL problems such as Atari and humanoid locomotion.    This paper studies the evolution of DNN scales using non-gradient-based evolutionary algorithms. The authors propose Deep GA, which is an evolutionary algorithm for training neural networks with free parameters. They show that it can be used to accelerate stochastic gradient descent by using an operation called the finite-difference approximation of the gradient. They also show that Deep GA is able to learn networks with networks that have free parameters, and that it performs well in a range of environments.  The paper also shows that reward-maximizing algorithms (such as A3C, DQN, ES, and GA) can be adapted to solve a high-dimensional problem using DNNs and novelty search. "
8750,SP:dfdbe3267a8160f24746884cdf5297993e424231,"rewards USED-FOR learning. episodic memory USED-FOR novelty bonus. episodic memory USED-FOR curiosity method. VizDoom CONJUNCTION DMLab. DMLab CONJUNCTION VizDoom. DMLab CONJUNCTION MuJoCo. MuJoCo CONJUNCTION DMLab. VizDoom FEATURE-OF visually rich 3D environments. visually rich 3D environments EVALUATE-FOR approach. agent COMPARE curiosity method. curiosity method COMPARE agent. agent COMPARE ICM. ICM COMPARE agent. navigational tasks EVALUATE-FOR agent. curiosity method COMPARE ICM. ICM COMPARE curiosity method. VizDoom CONJUNCTION DMLab. DMLab CONJUNCTION VizDoom. ant USED-FOR MuJoCo. curiosity module PART-OF ant. OtherScientificTerm are Rewards, sparsity, curious behaviour, real task reward, environment dynamics, and first - person - view curiosity. Method are reinforcement learning algorithms, and RL algorithms. ","This paper proposes a novel approach to learning from sparsity in reinforcement learning algorithms. Rewards in RL algorithms are often sparsified to encourage the agent to learn from the sparsity. The authors propose a curiosity method based on an episodic memory that is used as a novelty bonus for learning. The approach is tested on visually rich 3D environments such as VizDoom, DMLab, and MuJoCo, and is shown to outperform existing RL algorithms. The agent outperforms the previous curiosity method, ICM, on a number of navigational tasks.    The authors also show that the agent is able to learn to explore more efficiently than the previous agent, and that the behavior of the agent can be explained as a result of the use of a curiosity module in ant, which is a variant of an ant that is trained on the same environment dynamics as the real task reward. The paper also shows that first-person-view curiosity can be used as an additional reward for the agent.","This paper proposes a novel approach to learning from sparsity in reinforcement learning algorithms. Rewards in RL algorithms are often sparsified to encourage the agent to learn from the sparsity. The authors propose a curiosity method based on an episodic memory that is used as a novelty bonus for learning. The approach is tested on visually rich 3D environments such as VizDoom, DMLab, and MuJoCo, and is shown to outperform existing RL algorithms. The agent outperforms the previous curiosity method, ICM, on a number of navigational tasks.    The authors also show that the agent is able to learn to explore more efficiently than the previous agent, and that the behavior of the agent can be explained as a result of the use of a curiosity module in ant, which is a variant of an ant that is trained on the same environment dynamics as the real task reward. The paper also shows that first-person-view curiosity can be used as an additional reward for the agent."
8759,SP:1e58a1c5344d1b5b7c8a40210a243700bd933d65,representation USED-FOR transition models. complex uncertain domains FEATURE-OF transition models. relational rules USED-FOR representation. iterative greedy algorithm USED-FOR deictic references. Feed - forward neural networks USED-FOR transition distribution. strategy COMPARE monolithic transition model. monolithic transition model COMPARE strategy. simulated domain EVALUATE-FOR monolithic transition model. OtherScientificTerm is rule. ,"This paper proposes a new representation for transition models in complex uncertain domains based on relational rules. Feed-forward neural networks are used to model the transition distribution, and an iterative greedy algorithm is used to learn deictic references. The proposed strategy is shown to outperform a monolithic transition model in a simulated domain. ","This paper proposes a new representation for transition models in complex uncertain domains based on relational rules. Feed-forward neural networks are used to model the transition distribution, and an iterative greedy algorithm is used to learn deictic references. The proposed strategy is shown to outperform a monolithic transition model in a simulated domain. "
8768,SP:8ce00a3fedbf54a7f2c1ff414511cbb7d59b4597,"INVASE HYPONYM-OF instance - wise feature selection method. selector network CONJUNCTION predictor network. predictor network CONJUNCTION selector network. predictor network CONJUNCTION baseline network. baseline network CONJUNCTION predictor network. neural networks CONJUNCTION selector network. selector network CONJUNCTION neural networks. baseline network USED-FOR selector network. actor - critic methodology USED-FOR baseline network. actor - critic methodology USED-FOR INVASE. neural networks PART-OF INVASE. predictor network PART-OF INVASE. selector network PART-OF INVASE. baseline network PART-OF INVASE. methodology USED-FOR INVASE. INVASE COMPARE state - of - the - art benchmarks. state - of - the - art benchmarks COMPARE INVASE. synthetic and real data experiments EVALUATE-FOR INVASE. Material is big data. OtherScientificTerm is features. Task are global feature selection, and instance - wise feature selection. Generic is state - of - the - art methods. ","This paper proposes a new instance-wise feature selection method called INVASE, which is a new method for selecting the best feature from big data. The idea of INVASE is to use a combination of neural networks, a selector network, a predictor network, and a baseline network. The selector network is trained using an actor-critic methodology, and the baseline network is used to train the selector network and the predictor network.    The authors show that the proposed methodology is able to outperform existing state-of-the-art methods on both synthetic and real data experiments. They also show that INVASE outperforms other state- of-the art benchmarks in terms of global feature selection, and outperforms existing methods for instance-wide feature selection. ","This paper proposes a new instance-wise feature selection method called INVASE, which is a new method for selecting the best feature from big data. The idea of INVASE is to use a combination of neural networks, a selector network, a predictor network, and a baseline network. The selector network is trained using an actor-critic methodology, and the baseline network is used to train the selector network and the predictor network.    The authors show that the proposed methodology is able to outperform existing state-of-the-art methods on both synthetic and real data experiments. They also show that INVASE outperforms other state- of-the art benchmarks in terms of global feature selection, and outperforms existing methods for instance-wide feature selection. "
8777,SP:b91d6c33349df0bb6cb7e1c5e9433f0d4744b4da,"per - pixel annotations USED-FOR supervised models. semantic segmentation HYPONYM-OF Predicting structured outputs. convolutional neural networks HYPONYM-OF supervised models. per - pixel annotations USED-FOR Predicting structured outputs. annotations USED-FOR model finetuning. disentangled space USED-FOR discriminative feature representations of patches. label histograms USED-FOR discriminative feature representations of patches. adversarial learning scheme USED-FOR feature representations. representations USED-FOR guidance. global alignment process CONJUNCTION patch - level alignment. patch - level alignment CONJUNCTION global alignment process. global alignment process USED-FOR framework. semantic segmentation EVALUATE-FOR framework. patch - level alignment USED-FOR framework. Generic are models, and benchmark datasets. Task is annotation. Method is domain adaptation method. ","This paper proposes to use per-pixel annotations to improve the performance of supervised models such as convolutional neural networks. Predicting structured outputs (i.e., semantic segmentation) from per-pixels annotations is an important problem in the context of model finetuning. The authors propose to learn discriminative feature representations of patches in a disentangled space based on label histograms. They also propose an adversarial learning scheme to learn feature representations that can be used to guide the training of the models. The proposed framework is based on a global alignment process and patch-level alignment. The paper also proposes a domain adaptation method to learn representations that are useful for guidance. Experiments on benchmark datasets show that the proposed framework achieves state-of-the-art performance on semantic segmentations. ","This paper proposes to use per-pixel annotations to improve the performance of supervised models such as convolutional neural networks. Predicting structured outputs (i.e., semantic segmentation) from per-pixels annotations is an important problem in the context of model finetuning. The authors propose to learn discriminative feature representations of patches in a disentangled space based on label histograms. They also propose an adversarial learning scheme to learn feature representations that can be used to guide the training of the models. The proposed framework is based on a global alignment process and patch-level alignment. The paper also proposes a domain adaptation method to learn representations that are useful for guidance. Experiments on benchmark datasets show that the proposed framework achieves state-of-the-art performance on semantic segmentations. "
8786,SP:00922af13a21464cbc4cd7b34c196dd4f86c9247,"optimistic algorithms USED-FOR AMSGrad. AMSGrad CONJUNCTION Adam. Adam CONJUNCTION AMSGrad. optimistic algorithms USED-FOR Adam. predictability of gradients USED-FOR optimistic algorithms. momentum method CONJUNCTION adaptive gradient method. adaptive gradient method CONJUNCTION momentum method. algorithms USED-FOR OPTIMISTIC ONLINE LEARNING. adaptive gradient method CONJUNCTION algorithms. algorithms CONJUNCTION adaptive gradient method. algorithms USED-FOR algorithms. adaptive gradient method USED-FOR algorithms. momentum method USED-FOR algorithms. Method are optimization algorithms, and deep neural nets. OtherScientificTerm is mini - batch of stochastic gradients. Task is online learning literature. ","This paper studies the problem of online learning of optimization algorithms. In particular, the authors consider the case where a mini-batch of stochastic gradients is available for training deep neural nets. They show that optimistic algorithms such as AMSGrad and Adam can be learned from the predictability of gradients. They also show that algorithms based on the momentum method, adaptive gradient method, and algorithms inspired by these algorithms can be used for OPTIMISTIC ONLINE LEARNING. The paper is well-written and well-motivated. However, there is a lack of comparison with the online learning literature.","This paper studies the problem of online learning of optimization algorithms. In particular, the authors consider the case where a mini-batch of stochastic gradients is available for training deep neural nets. They show that optimistic algorithms such as AMSGrad and Adam can be learned from the predictability of gradients. They also show that algorithms based on the momentum method, adaptive gradient method, and algorithms inspired by these algorithms can be used for OPTIMISTIC ONLINE LEARNING. The paper is well-written and well-motivated. However, there is a lack of comparison with the online learning literature."
8795,SP:52228b48f2776d57dd422edb33b82e247f056b75,benchmarks USED-FOR image classifier robustness. classifiers USED-FOR safety - critical applications. benchmark USED-FOR corruption robustness topic. IMAGENET - C USED-FOR corruption robustness topic. IMAGENET - C HYPONYM-OF benchmark. dataset EVALUATE-FOR classifier. common perturbations FEATURE-OF classifier ’s robustness. IMAGENET - P HYPONYM-OF dataset. common corruptions CONJUNCTION perturbations. perturbations CONJUNCTION common corruptions. benchmark USED-FOR perturbations. perturbations COMPARE worst - case adversarial perturbations. worst - case adversarial perturbations COMPARE perturbations. common corruptions FEATURE-OF benchmark. AlexNet classifiers CONJUNCTION ResNet classifiers. ResNet classifiers CONJUNCTION AlexNet classifiers. relative corruption robustness EVALUATE-FOR ResNet classifiers. relative corruption robustness EVALUATE-FOR AlexNet classifiers. common perturbation robustness EVALUATE-FOR bypassed adversarial defense. Generic is networks. ,"This paper introduces a new benchmark, IMAGENET-C, to evaluate the robustness of image classifier robustness against common perturbations. The benchmark is designed to address the corruption robustness topic in the context of safety-critical applications, where classifiers are used to protect the privacy of users in the presence of adversarial attacks. The authors also introduce a new dataset called IMAGENTET-P, which is a generalization of the original dataset IMAGenet-C.  The authors show that on this new dataset, the classifier’s robustness to common adversarial perturbation is robust to a range of perturbings, including common corruptions as well as adversarial examples.  They also show that the new benchmark is more robust to perturbational examples than the worst-case adversarial robustness, and that adversarial samples are more likely to be perturbed in this new benchmark.  Finally, they show that AlexNet classifiers and ResNet classifier achieve relative corruption robustity, and bypassed adversarial defense achieves a similar level of robustness.    Overall, the paper is well-written, well-motivated, and well-designed. However, there are a few issues that need to be addressed, and the paper suffers from a lack of clarity, and it is not clear to me that the authors have made any significant contributions to the field. ","This paper introduces a new benchmark, IMAGENET-C, to evaluate the robustness of image classifier robustness against common perturbations. The benchmark is designed to address the corruption robustness topic in the context of safety-critical applications, where classifiers are used to protect the privacy of users in the presence of adversarial attacks. The authors also introduce a new dataset called IMAGENTET-P, which is a generalization of the original dataset IMAGenet-C.  The authors show that on this new dataset, the classifier’s robustness to common adversarial perturbation is robust to a range of perturbings, including common corruptions as well as adversarial examples.  They also show that the new benchmark is more robust to perturbational examples than the worst-case adversarial robustness, and that adversarial samples are more likely to be perturbed in this new benchmark.  Finally, they show that AlexNet classifiers and ResNet classifier achieve relative corruption robustity, and bypassed adversarial defense achieves a similar level of robustness.    Overall, the paper is well-written, well-motivated, and well-designed. However, there are a few issues that need to be addressed, and the paper suffers from a lack of clarity, and it is not clear to me that the authors have made any significant contributions to the field. "
8804,SP:20358ea0f769e6ea9222d8e35159d711ee1b20b2,"MAP estimation USED-FOR dropout training. model PART-OF family. models USED-FOR power mean. lower bounds FEATURE-OF stochastic subvariants. sampled dropout masks USED-FOR power mean. models PART-OF family. deterministic dropout USED-FOR MC averaging. Task is dropout. Method are conditional models, and regularisation - heavy language modelling. OtherScientificTerm are dropout objective, and deterministic subvariant ’s bound. Generic is It. ","This paper studies the problem of dropout in conditional models. The authors propose a new MAP estimation for dropout training, which they call deterministic subvariant’s bound. They show that the power mean of a family of models with sampled dropout masks can be approximated by lower bounds on stochastic subvariants of the dropout objective. They also show that deterministic dropout can be used for MC averaging.  It is interesting to see that this is a generalization of previous work on regularisation-heavy language modelling.  ","This paper studies the problem of dropout in conditional models. The authors propose a new MAP estimation for dropout training, which they call deterministic subvariant’s bound. They show that the power mean of a family of models with sampled dropout masks can be approximated by lower bounds on stochastic subvariants of the dropout objective. They also show that deterministic dropout can be used for MC averaging.  It is interesting to see that this is a generalization of previous work on regularisation-heavy language modelling.  "
8840,SP:ac1b950ad29429ae045bb5e53279014a6a0b9d2b,"redundant filters PART-OF Convolutional Neural Networks ( CNNs ). robust pruning method USED-FOR GSFP. soft pruning strategy USED-FOR GSFP. cumulative saliency strategy USED-FOR pruning. accuracy EVALUATE-FOR pruning. cumulative saliency strategy USED-FOR accuracy. pruning USED-FOR model recovery process. saliency FEATURE-OF filter. saliency FEATURE-OF filter. saliency USED-FOR pruning. pruning COMPARE local pruning. local pruning COMPARE pruning. normalization formula USED-FOR layers of filters. layers of filters PART-OF network. CNN architectures CONJUNCTION data sets. data sets CONJUNCTION CNN architectures. CNN architectures EVALUATE-FOR GSFP. data sets EVALUATE-FOR GSFP. GSFP USED-FOR global and soft pruning strategies. MNIST CONJUNCTION CIFAR10. CIFAR10 CONJUNCTION MNIST. CIFAR10 EVALUATE-FOR it. MNIST EVALUATE-FOR it. test accuracy EVALUATE-FOR it. compression ratio EVALUATE-FOR it. OtherScientificTerm are global redundancy, and excessive pruning rate. Generic is model. Task is pruning guidance. Method is pre - trained CNN model. ","This paper proposes a robust pruning method called GSFP, which aims to remove redundant filters in Convolutional Neural Networks (CNNs). The authors argue that global redundancy is the main cause of excessive pruning rate and propose a soft pruning strategy for GSFP. The authors also propose a cumulative saliency strategy to improve the accuracy of pruning in the model recovery process. The saliency of a filter depends on the number of layers in the network, and the authors show that pruning based on saliency is more effective than local pruning.  The authors further propose a normalization formula for the layers of filters in a network that can be applied to any pre-trained CNN model. They evaluate GSFP on various CNN architectures and data sets, and show that it improves the test accuracy on MNIST and CIFAR10. They also show that GSFP improves the compression ratio of the model and that it can be used as a pruning guidance.  Finally, the authors propose to use GSFP for both global and soft prune strategies. ","This paper proposes a robust pruning method called GSFP, which aims to remove redundant filters in Convolutional Neural Networks (CNNs). The authors argue that global redundancy is the main cause of excessive pruning rate and propose a soft pruning strategy for GSFP. The authors also propose a cumulative saliency strategy to improve the accuracy of pruning in the model recovery process. The saliency of a filter depends on the number of layers in the network, and the authors show that pruning based on saliency is more effective than local pruning.  The authors further propose a normalization formula for the layers of filters in a network that can be applied to any pre-trained CNN model. They evaluate GSFP on various CNN architectures and data sets, and show that it improves the test accuracy on MNIST and CIFAR10. They also show that GSFP improves the compression ratio of the model and that it can be used as a pruning guidance.  Finally, the authors propose to use GSFP for both global and soft prune strategies. "
8849,SP:621e41d4199e333ec7f9d0936d4e34c918f39c11,character - based embedder CONJUNCTION word - based classifier. word - based classifier CONJUNCTION character - based embedder. transfer learning scheme USED-FOR cross - lingual subword similarity. limited training data USED-FOR transfer learning scheme. character - based embedder USED-FOR transfer learning scheme. embedder USED-FOR vector representations. written forms USED-FOR vector representations. word vectors USED-FOR classifier. multi - task objective USED-FOR model. CACO models COMPARE cross - lingual word embedding models. cross - lingual word embedding models COMPARE CACO models. low - resource settings USED-FOR CACO models. related language pairs USED-FOR cross - lingual word embedding models. high - resource settings USED-FOR cross - lingual word embedding models. Task is Text classification. Method is joint character representation. Material is cross - lingual or monolingual resources. ,"This paper proposes a novel transfer learning scheme for cross-lingual subword similarity with limited training data based on a character-based embedder and a word-based classifier. Text classification is done by learning a joint character representation of the input word and its subword. The embedder is used to learn vector representations in written forms, and the classifier is trained on word vectors. The model is trained with a multi-task objective. The authors show that CACO models trained in low-resource settings outperform state-of-the-art cross-langual word embedding models trained with related language pairs in high-resource setting. The paper also shows that the model can be trained to transfer from one language to another without access to cross-limiting or monolingual resources. ","This paper proposes a novel transfer learning scheme for cross-lingual subword similarity with limited training data based on a character-based embedder and a word-based classifier. Text classification is done by learning a joint character representation of the input word and its subword. The embedder is used to learn vector representations in written forms, and the classifier is trained on word vectors. The model is trained with a multi-task objective. The authors show that CACO models trained in low-resource settings outperform state-of-the-art cross-langual word embedding models trained with related language pairs in high-resource setting. The paper also shows that the model can be trained to transfer from one language to another without access to cross-limiting or monolingual resources. "
8858,SP:544e421f9c747640d949f433e3091763508b7237,"marginalized average aggregation ( MAA ) module USED-FOR MAAN. latent discriminative probabilities USED-FOR MAA. latent discriminative probabilities USED-FOR MAA module. MAAN USED-FOR dense and integral action regions. MAAN USED-FOR class activation sequences. algorithm USED-FOR MAA. algorithm USED-FOR complexity. large - scale video datasets EVALUATE-FOR MAAN. MAAN USED-FOR weakly - supervised temporal action localization. large - scale video datasets EVALUATE-FOR weakly - supervised temporal action localization. OtherScientificTerm are dense and integral regions, overestimation of the most salient regions, video snippet features, averaged subset features, and O(T ). Method is marginalized average attentional network ( MAAN ). ","This paper proposes the marginalized average attentional network (MAAN), which is a generalization of marginalized average aggregation (MAAC) to the case of dense and integral regions. MAAN is based on the marginalization of the latent discriminative probabilities in the MAA module. The authors show that MAAN can be used to learn dense and Integral action regions by maximizing the overestimation of the most salient regions.  The authors also propose a new algorithm for MAA to reduce the complexity of the algorithm.   The main contribution of the paper is the proposed MAAN, which uses the marginalized average aggregation(MAAN) module in MAAC to learn the class activation sequences of the input video snippet features, and then uses the averaged subset features from the input clips to train MAAN.  Experiments are conducted on large-scale video datasets to demonstrate the effectiveness of MAAN for weakly-supervised temporal action localization on a variety of tasks. ","This paper proposes the marginalized average attentional network (MAAN), which is a generalization of marginalized average aggregation (MAAC) to the case of dense and integral regions. MAAN is based on the marginalization of the latent discriminative probabilities in the MAA module. The authors show that MAAN can be used to learn dense and Integral action regions by maximizing the overestimation of the most salient regions.  The authors also propose a new algorithm for MAA to reduce the complexity of the algorithm.   The main contribution of the paper is the proposed MAAN, which uses the marginalized average aggregation(MAAN) module in MAAC to learn the class activation sequences of the input video snippet features, and then uses the averaged subset features from the input clips to train MAAN.  Experiments are conducted on large-scale video datasets to demonstrate the effectiveness of MAAN for weakly-supervised temporal action localization on a variety of tasks. "
8867,SP:9f98c9bac99003741dd14e093b54d692c0b0e8d8,"neural models USED-FOR Natural Language Processing. structureless distributed representations USED-FOR neural models. models COMPARE representational form. representational form COMPARE models. structures PART-OF wordlevel and chunk - level representations. HRR USED-FOR models. models USED-FOR crude linguistic roles. HRR USED-FOR structured compositional representation. OtherScientificTerm are linguistic structures, and syntax. Method are language models, and Holographic Reduced Representation ( HRR ). ","This paper studies the problem of training neural models for Natural Language Processing with structureless distributed representations. The authors argue that existing language models do not take into account the linguistic structures that are present in natural language, and propose Holographic Reduced Representation (HRR) to address this problem. They show that models trained with HRR are able to learn more complex structures in wordlevel and chunk-level representations than models trained without this representational form. They also show that HRR can be used to train models to learn crude linguistic roles, and that models can be trained to learn a structured compositional representation. ","This paper studies the problem of training neural models for Natural Language Processing with structureless distributed representations. The authors argue that existing language models do not take into account the linguistic structures that are present in natural language, and propose Holographic Reduced Representation (HRR) to address this problem. They show that models trained with HRR are able to learn more complex structures in wordlevel and chunk-level representations than models trained without this representational form. They also show that HRR can be used to train models to learn crude linguistic roles, and that models can be trained to learn a structured compositional representation. "
8876,SP:5908b6acfed0e7c51e203c72eba907e6635e6c60,"Partially observable Markov decision processes ( POMDPs ) USED-FOR decision - making. perception decision CONJUNCTION planning decision. planning decision CONJUNCTION perception decision. greedy strategy USED-FOR observation selection. point - based value iteration algorithm USED-FOR near - optimal uncertainty reduction. greedy strategy USED-FOR near - optimal uncertainty reduction. sampled belief points USED-FOR near - optimal uncertainty reduction. greedy strategy PART-OF point - based value iteration algorithm. solver USED-FOR reachable subspace of belief simplex. computations USED-FOR perception. planning HYPONYM-OF computations. active perception CONJUNCTION planning. planning CONJUNCTION active perception. OtherScientificTerm are stochastic outcome, known distribution, real - world scenarios, and action space. Method are POMDP models, and selection process. Material is robotic scenarios. ","This paper considers the problem of decision-making in partially observable Markov decision processes (POMDPs) where the stochastic outcome of the decision process is unknown. The authors propose a point-based value iteration algorithm that incorporates a greedy strategy in the form of a greedy algorithm for observation selection to achieve near-optimal uncertainty reduction from sampled belief points. The solver is trained to find a reachable subspace of belief simplex that minimizes the uncertainty of the known distribution. The paper shows that this solver can be applied to several real-world scenarios, including active perception, planning, and decision making in robotic scenarios. The main contribution of the paper is that the selection process can be more efficient than existing computations for both perception decision and planning decision.    The paper is well-written and well-motivated, and the authors provide a thorough analysis of the POMDP models and the results are interesting. ","This paper considers the problem of decision-making in partially observable Markov decision processes (POMDPs) where the stochastic outcome of the decision process is unknown. The authors propose a point-based value iteration algorithm that incorporates a greedy strategy in the form of a greedy algorithm for observation selection to achieve near-optimal uncertainty reduction from sampled belief points. The solver is trained to find a reachable subspace of belief simplex that minimizes the uncertainty of the known distribution. The paper shows that this solver can be applied to several real-world scenarios, including active perception, planning, and decision making in robotic scenarios. The main contribution of the paper is that the selection process can be more efficient than existing computations for both perception decision and planning decision.    The paper is well-written and well-motivated, and the authors provide a thorough analysis of the POMDP models and the results are interesting. "
8885,SP:0adec4abec17b3aab0c6eb69d11925dc20544950,"Curriculum learning USED-FOR network. data complexity CONJUNCTION network training. network training CONJUNCTION data complexity. internal covariate shift PART-OF network forward pass. representation loss USED-FOR low weighted samples. adaptive weight CONJUNCTION representation loss. representation loss CONJUNCTION adaptive weight. adaptive weight PART-OF curriculum loss. representation loss PART-OF curriculum loss. random sampling USED-FOR curriculum learning. curriculum loss CONJUNCTION stochastic algorithms. stochastic algorithms CONJUNCTION curriculum loss. curriculum loss COMPARE SGD. SGD COMPARE curriculum loss. SGD HYPONYM-OF stochastic algorithms. Method are Deep neural networks, top layers, and learning of top layers. OtherScientificTerm are distribution changes in weight of top layers, backward pass, hard examples, noisy gradients, embedding space, fluctuation of top layers, and hard samples. Task are distribution shifting, and training. Material are Low - weighted data, and benchmark datasets. ","Deep neural networks have been shown to be sensitive to distribution changes in weight of top layers during training.  Deep neural networks can be seen as a function of the number of weights of the top layers.  Curriculum learning is used to train a network in the presence of such distribution shifts.    In this paper, the authors show that the internal covariate shift in the network forward pass is related to the data complexity of the network training, i.e., the amount of data that the network needs to store.  The authors propose a new curriculum loss that combines adaptive weight and representation loss to encourage low weighted samples to be used in the backward pass.  In the forward pass, the distribution shifts are caused by the fluctuation of the weights in the embedding space during training, which causes the hard examples to be hard to learn due to noisy gradients.  Low-weighted data is then used as hard samples, and the representation loss is used for low-weighting the hard samples.  This curriculum loss is then combined with standard stochastic algorithms such as SGD.  Experiments on benchmark datasets are conducted to show the effectiveness of the proposed curriculum loss, which is based on random sampling. ","Deep neural networks have been shown to be sensitive to distribution changes in weight of top layers during training.  Deep neural networks can be seen as a function of the number of weights of the top layers.  Curriculum learning is used to train a network in the presence of such distribution shifts.    In this paper, the authors show that the internal covariate shift in the network forward pass is related to the data complexity of the network training, i.e., the amount of data that the network needs to store.  The authors propose a new curriculum loss that combines adaptive weight and representation loss to encourage low weighted samples to be used in the backward pass.  In the forward pass, the distribution shifts are caused by the fluctuation of the weights in the embedding space during training, which causes the hard examples to be hard to learn due to noisy gradients.  Low-weighted data is then used as hard samples, and the representation loss is used for low-weighting the hard samples.  This curriculum loss is then combined with standard stochastic algorithms such as SGD.  Experiments on benchmark datasets are conducted to show the effectiveness of the proposed curriculum loss, which is based on random sampling. "
8894,SP:8b555b9f24044bc68c204169d6a37e262361d706,"heuristics USED-FOR combinatorial optimization problems. REINFORCE USED-FOR baseline. value function USED-FOR baseline. deterministic greedy rollout USED-FOR baseline. attention layers USED-FOR model. REINFORCE USED-FOR model. heuristics USED-FOR Travelling Salesman Problem ( TSP ). heuristics USED-FOR Vehicle Routing Problem ( VRP ). hyperparameters USED-FOR heuristics. Orienteering Problem ( OP ) HYPONYM-OF Vehicle Routing Problem ( VRP ). Generic are it, models, problems, and baselines. Method are Pointer Network, and Prize Collecting TSP ( PCTSP ). ","This paper studies the use of heuristics for combinatorial optimization problems. The authors propose a model that uses REINFORCE to learn a baseline for a deterministic greedy rollout, where the baseline is learned as a value function. The model is trained with attention layers, and the authors show that it can be trained to solve the Travelling Salesman Problem (TSP) and the Orienteering Problem (OP). The authors also show that heuristic heuristic can be applied to the Vehicle Routing Problem (VRP) and a variant of the Traveller Problem (OTP).   The authors further show that the model can also be trained with REinFORCE for the Pointer Network (PCTSP) problem.    Finally, the authors demonstrate that the proposed models can be used to solve a number of existing problems. They show that their heuristic outperforms baselines on the Prize Collecting TSP (PTCTSP), and they show that they can also outperform the baselines for other heuristic heuristic methods for the Travellers Problem (OTT).  The paper also shows that their model is able to outperform baselines when the number of hyperparameters is small.","This paper studies the use of heuristics for combinatorial optimization problems. The authors propose a model that uses REINFORCE to learn a baseline for a deterministic greedy rollout, where the baseline is learned as a value function. The model is trained with attention layers, and the authors show that it can be trained to solve the Travelling Salesman Problem (TSP) and the Orienteering Problem (OP). The authors also show that heuristic heuristic can be applied to the Vehicle Routing Problem (VRP) and a variant of the Traveller Problem (OTP).   The authors further show that the model can also be trained with REinFORCE for the Pointer Network (PCTSP) problem.    Finally, the authors demonstrate that the proposed models can be used to solve a number of existing problems. They show that their heuristic outperforms baselines on the Prize Collecting TSP (PTCTSP), and they show that they can also outperform the baselines for other heuristic heuristic methods for the Travellers Problem (OTT).  The paper also shows that their model is able to outperform baselines when the number of hyperparameters is small."
8903,SP:efb76bcf1dbd9a9cf6b5db74b5d4256a9f9e9e73,"time and space complexity EVALUATE-FOR neural network inference. network quantization USED-FOR neural network inference. limited computational and memory resources FEATURE-OF embedded and mobile devices. differentiable neural architecture search ( DNAS ) framework USED-FOR exponential search space. gradient - based optimization USED-FOR differentiable neural architecture search ( DNAS ) framework. neural architecture search problem USED-FOR problem. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR ResNet. ImageNet EVALUATE-FOR ResNet. quantized models COMPARE full precision models. full precision models COMPARE quantized models. model size CONJUNCTION computational cost. computational cost CONJUNCTION model size. computational cost EVALUATE-FOR full precision models. model size EVALUATE-FOR quantized models. computational cost EVALUATE-FOR quantized models. Method is quantization methods. OtherScientificTerm are design space, and bit - widths. ","This paper studies the time and space complexity of neural network inference with network quantization. The authors propose a differentiable neural architecture search (DNAS) framework for the exponential search space based on gradient-based optimization. The problem is formulated as a neural network search problem, where the goal is to find the optimal design space that maximizes the performance of the network. This is an important problem with limited computational and memory resources in embedded and mobile devices, and the authors show that existing quantization methods do not scale well when the design space is limited. The paper also shows that quantized models with larger bit-widths perform better than full precision models in terms of model size and computational cost on CIFAR-10 and ImageNet. ","This paper studies the time and space complexity of neural network inference with network quantization. The authors propose a differentiable neural architecture search (DNAS) framework for the exponential search space based on gradient-based optimization. The problem is formulated as a neural network search problem, where the goal is to find the optimal design space that maximizes the performance of the network. This is an important problem with limited computational and memory resources in embedded and mobile devices, and the authors show that existing quantization methods do not scale well when the design space is limited. The paper also shows that quantized models with larger bit-widths perform better than full precision models in terms of model size and computational cost on CIFAR-10 and ImageNet. "
8912,SP:ea4173f8265bc50296de51c4ee7ecb6b8f78bec0,"attention USED-FOR neural architectures. attention USED-FOR decoding stage. posterior attention distribution USED-FOR attention. posterior attention models COMPARE attention models. attention models COMPARE posterior attention models. morphological inflection tasks EVALUATE-FOR posterior attention models. alignment accuracy EVALUATE-FOR attention models. translation EVALUATE-FOR posterior attention models. translation CONJUNCTION morphological inflection tasks. morphological inflection tasks CONJUNCTION translation. BLEU score EVALUATE-FOR attention models. BLEU score CONJUNCTION alignment accuracy. alignment accuracy CONJUNCTION BLEU score. morphological inflection tasks EVALUATE-FOR attention models. translation EVALUATE-FOR attention models. alignment accuracy EVALUATE-FOR posterior attention models. BLEU score EVALUATE-FOR posterior attention models. Method are attention architectures, and Posterior Attention Models. Generic is architecture. ","This paper studies the effect of attention on neural architectures. The authors show that the attention used in the decoding stage of a neural architecture is related to the posterior attention distribution. They also show that posterior attention models have better BLEU score and alignment accuracy compared to other attention models on translation, morphological inflection tasks.    The authors also propose two new attention architectures, called Posterior Attention Models (PAMs), which are based on the idea that the architecture can be seen as a weighted sum of attention distributions across layers. ","This paper studies the effect of attention on neural architectures. The authors show that the attention used in the decoding stage of a neural architecture is related to the posterior attention distribution. They also show that posterior attention models have better BLEU score and alignment accuracy compared to other attention models on translation, morphological inflection tasks.    The authors also propose two new attention architectures, called Posterior Attention Models (PAMs), which are based on the idea that the architecture can be seen as a weighted sum of attention distributions across layers. "
8921,SP:987e2c14abc091d4d3ef9b48fb2046408eb1f59e,"artifacts CONJUNCTION degenerated transformations. degenerated transformations CONJUNCTION artifacts. smoothness term USED-FOR harmonic functions. harmonic functions USED-FOR consistent mappings. smoothness term PART-OF sample graph. HarmonicGAN USED-FOR bi - directional translations. similarity - consistency USED-FOR inherent selfconsistency property. histogram CONJUNCTION CNN. CNN CONJUNCTION histogram. features FEATURE-OF Distance metrics. CNN HYPONYM-OF Distance metrics. histogram HYPONYM-OF Distance metrics. CNN HYPONYM-OF features. histogram HYPONYM-OF features. HarmonicGAN COMPARE state of the art. state of the art COMPARE HarmonicGAN. CycleGAN COMPARE HarmonicGAN. HarmonicGAN COMPARE CycleGAN. interpretability EVALUATE-FOR HarmonicGAN. object transfiguration CONJUNCTION semantic labeling. semantic labeling CONJUNCTION object transfiguration. medical imaging CONJUNCTION object transfiguration. object transfiguration CONJUNCTION medical imaging. medical imaging HYPONYM-OF applications. semantic labeling HYPONYM-OF applications. object transfiguration HYPONYM-OF applications. tasks EVALUATE-FOR methods. method USED-FOR medical imaging task. Task are unpaired image - to - image translation, manifold view of the problem, and translation. Generic is it. OtherScientificTerm are pixel - to - pixel supervision, manual inputs, and mean - squared error. Metric is training - time cost. ","This paper tackles the problem of unpaired image-to-image translation, which is a manifold view of the problem where artifacts and degenerated transformations are present in the input images. The authors propose a new method called ""HarmonicGAN"" for bi-directional translations, where the goal is to learn consistent mappings between the input and the target image.    The key idea is to add a smoothness term to the harmonic functions in the sample graph to ensure that the resulting mapping is self-consistent and that it does not rely on pixel- to-pixel supervision.  The authors show that the inherent selfconsistency property of the proposed method, which they refer to as similarity - consistency, is a result of the fact that the smoothness of harmonic functions can be decomposed into two terms: (1) the mean-squared error of the resulting translation, and (2) the training-time cost.  Distance metrics are defined based on features such as histogram, CNN, etc.  Experiments are conducted on three different applications: medical imaging, object transfiguration and semantic labeling. The results show that compared to CycleGAN, the proposed HarmonicGAN achieves better interpretability and outperforms the state of the art on all three tasks. The method is also applied to a medical imaging task, where it is shown that the method can be applied to the case where the manual inputs are not available. ","This paper tackles the problem of unpaired image-to-image translation, which is a manifold view of the problem where artifacts and degenerated transformations are present in the input images. The authors propose a new method called ""HarmonicGAN"" for bi-directional translations, where the goal is to learn consistent mappings between the input and the target image.    The key idea is to add a smoothness term to the harmonic functions in the sample graph to ensure that the resulting mapping is self-consistent and that it does not rely on pixel- to-pixel supervision.  The authors show that the inherent selfconsistency property of the proposed method, which they refer to as similarity - consistency, is a result of the fact that the smoothness of harmonic functions can be decomposed into two terms: (1) the mean-squared error of the resulting translation, and (2) the training-time cost.  Distance metrics are defined based on features such as histogram, CNN, etc.  Experiments are conducted on three different applications: medical imaging, object transfiguration and semantic labeling. The results show that compared to CycleGAN, the proposed HarmonicGAN achieves better interpretability and outperforms the state of the art on all three tasks. The method is also applied to a medical imaging task, where it is shown that the method can be applied to the case where the manual inputs are not available. "
8930,SP:885a69003bad0e79cb2872a4e5c772191ad7e34f,"EVGP USED-FOR gradient components. stochastic algorithm ( h - detach ) USED-FOR LSTM optimization. stochastic algorithm ( h - detach ) USED-FOR problem. linear path ( cell state ) PART-OF LSTM computational graph. long term dependencies FEATURE-OF components. LSTM USED-FOR dependencies. seed CONJUNCTION learning rate. learning rate CONJUNCTION seed. convergence speed CONJUNCTION robustness. robustness CONJUNCTION convergence speed. robustness CONJUNCTION learning rate. learning rate CONJUNCTION robustness. robustness CONJUNCTION seed. seed CONJUNCTION robustness. benchmark datasets EVALUATE-FOR generalization. benchmark datasets EVALUATE-FOR LSTM gradient. convergence speed EVALUATE-FOR vanilla LSTM gradient based training. LSTM gradient USED-FOR generalization. Method are Recurrent neural networks, and LSTMs. Task is exploding and vanishing gradient problem ( EVGP ). OtherScientificTerm are LSTM weights, and gradients. Generic is path. ","Recurrent neural networks have been shown to suffer from the exploding and vanishing gradient problem (EVGP). This paper proposes a new stochastic algorithm (h-detach) for LSTM optimization to solve this problem. EVGP is the problem that arises when the gradient components of LSTMs have long term dependencies. The authors propose to solve the problem by introducing a linear path (cell state) in the LSTT of the LMT, which is then decomposed into two components: (1) the gradient of the current layer and (2) the gradients of the previous layer.   The authors show that this path can be decomposed as follows:  (a) The first two components are the ones that depend on the current state of the network, and (b) the second one is the one that depends on the previous state.  The main contribution of the paper is that the authors propose a new algorithm h-detached, which computes the paths of the two components in the path, and then computes a solution to the problem.  (b). The authors also show that h-Detach converges to the optimal solution for all the components of the path.  In addition, the authors show how to compute the path of the first component, and how to calculate the path for the second component.  This paper also shows that the paths can be computed in a single step.  Finally, the paper shows how to find the path that minimizes the sum of the paths in the cell state, and the paths that minimise the sum over all the layers in the network.  It is shown that the path is non-trivial to compute, and that it is a linear function of the number of layers and the size of the cell.  Experiments are conducted on several benchmark datasets to demonstrate the generalization performance of the proposed algorithm. The results show that the convergence speed and robustness of the algorithm is better than the vanilla L STM gradient based training. The generalization speed is also improved when the number or seed, robustness, and learning rate is increased.","Recurrent neural networks have been shown to suffer from the exploding and vanishing gradient problem (EVGP). This paper proposes a new stochastic algorithm (h-detach) for LSTM optimization to solve this problem. EVGP is the problem that arises when the gradient components of LSTMs have long term dependencies. The authors propose to solve the problem by introducing a linear path (cell state) in the LSTT of the LMT, which is then decomposed into two components: (1) the gradient of the current layer and (2) the gradients of the previous layer.   The authors show that this path can be decomposed as follows:  (a) The first two components are the ones that depend on the current state of the network, and (b) the second one is the one that depends on the previous state.  The main contribution of the paper is that the authors propose a new algorithm h-detached, which computes the paths of the two components in the path, and then computes a solution to the problem.  (b). The authors also show that h-Detach converges to the optimal solution for all the components of the path.  In addition, the authors show how to compute the path of the first component, and how to calculate the path for the second component.  This paper also shows that the paths can be computed in a single step.  Finally, the paper shows how to find the path that minimizes the sum of the paths in the cell state, and the paths that minimise the sum over all the layers in the network.  It is shown that the path is non-trivial to compute, and that it is a linear function of the number of layers and the size of the cell.  Experiments are conducted on several benchmark datasets to demonstrate the generalization performance of the proposed algorithm. The results show that the convergence speed and robustness of the algorithm is better than the vanilla L STM gradient based training. The generalization speed is also improved when the number or seed, robustness, and learning rate is increased."
8939,SP:9aaff3777321347d1194884af5690b0b5185eff9,posterior distribution FEATURE-OF binary weights. Bayesian deep learning perspective USED-FOR real binary weight networks. reinforcement learning scheme USED-FOR policy network. policy network USED-FOR posterior distribution. binary weights USED-FOR burn - after - reading style. binary weight instances USED-FOR recognition architecture. policy network USED-FOR binary weight instances. policy network USED-FOR recognition architecture. policy network USED-FOR neural network architecture. nested parameter structure FEATURE-OF policy network. nested parameterization USED-FOR joint posterior distribution of binary weights. ImageNet HYPONYM-OF visual recognition tasks. visual recognition tasks EVALUATE-FOR SnapQuant. ImageNet EVALUATE-FOR SnapQuant. Task is point estimation. Generic is method. ,"This paper proposes a new deep learning method for binary weight neural networks. The authors propose to learn the posterior distribution of the binary weights of a neural network using a Bayesian deep learning approach. The proposed method, called SnapQuant, is based on a reinforcement learning framework, where a policy network is trained to optimize the posterior of binary weights. The paper shows that the proposed method is able to achieve state-of-the-art performance on ImageNet classification tasks.","This paper proposes a new deep learning method for binary weight neural networks. The authors propose to learn the posterior distribution of the binary weights of a neural network using a Bayesian deep learning approach. The proposed method, called SnapQuant, is based on a reinforcement learning framework, where a policy network is trained to optimize the posterior of binary weights. The paper shows that the proposed method is able to achieve state-of-the-art performance on ImageNet classification tasks."
8948,SP:29d1f6d0661a51e56c59bbb106da56700fc22d9a,Bayesian nonparametric framework USED-FOR federated learning with neural networks. inference approach USED-FOR global network. supervision CONJUNCTION data pooling. data pooling CONJUNCTION supervision. federated learning problems EVALUATE-FOR approach. image classification datasets USED-FOR federated learning problems. OtherScientificTerm is local neural network weights. Generic is framework. ,"This paper proposes a Bayesian nonparametric framework for federated learning with neural networks. The authors propose a new inference approach for learning a global network, where the local neural network weights are shared across all clients. The proposed approach is evaluated on a number of federated classification and image classification problems, and the results show that the proposed approach outperforms the state-of-the-art baselines in terms of both supervision and data pooling.   ","This paper proposes a Bayesian nonparametric framework for federated learning with neural networks. The authors propose a new inference approach for learning a global network, where the local neural network weights are shared across all clients. The proposed approach is evaluated on a number of federated classification and image classification problems, and the results show that the proposed approach outperforms the state-of-the-art baselines in terms of both supervision and data pooling.   "
8957,SP:ab1f2bd216635d63450688866c729a501bd7e9d0,"GANs CONJUNCTION intrinsic curiosity. intrinsic curiosity CONJUNCTION GANs. GANs CONJUNCTION multi - agent RL. multi - agent RL CONJUNCTION GANs. intrinsic curiosity CONJUNCTION multi - agent RL. multi - agent RL CONJUNCTION intrinsic curiosity. differentiable games USED-FOR learning methods. approach USED-FOR learning dynamics. Opponent shaping USED-FOR learning dynamics. Opponent shaping HYPONYM-OF approach. learning dynamics FEATURE-OF games. approach USED-FOR games. theoretical guarantees FEATURE-OF algorithms. LOLA CONJUNCTION stable variant. stable variant CONJUNCTION LOLA. method USED-FOR LOLA. Stable Opponent Shaping ( SOS ) HYPONYM-OF method. LookAhead HYPONYM-OF stable variant. LookAhead USED-FOR equilibria. strict saddles PART-OF differentiable games. Method are Opponent - Learning Awareness ( LOLA ), LOLA agents, and SOS. Generic is algorithm. OtherScientificTerm are cooperation, Iterated Prisoner ’s Dilemma, ‘ arrogant ’ behaviour, and learning of opponents. ","This paper proposes a new algorithm, Opponent-Learning Awareness (LOLA), which aims to improve the generalization ability of learning methods in differentiable games. The authors consider GANs, intrinsic curiosity, multi-agent RL, and other types of games, and show that their approach ( Opponent shaping) can improve the learning dynamics of these games. They also provide theoretical guarantees for their algorithms. Finally, they propose a method called LOLA and a stable variant called Stable Opponent Shaping (SOSP) which is a variant of LOLA. LOLA agents are shown to be able to learn to avoid the ‘arrogant’ behaviour of their opponents, and SOS is shown to learn a ‘strict saddle’ that prevents the learning of opponents that are ‘archaic’ in their behavior. LookAhead is also shown to find equilibria that avoid the Iterated Prisoner’s Dilemma. ","This paper proposes a new algorithm, Opponent-Learning Awareness (LOLA), which aims to improve the generalization ability of learning methods in differentiable games. The authors consider GANs, intrinsic curiosity, multi-agent RL, and other types of games, and show that their approach ( Opponent shaping) can improve the learning dynamics of these games. They also provide theoretical guarantees for their algorithms. Finally, they propose a method called LOLA and a stable variant called Stable Opponent Shaping (SOSP) which is a variant of LOLA. LOLA agents are shown to be able to learn to avoid the ‘arrogant’ behaviour of their opponents, and SOS is shown to learn a ‘strict saddle’ that prevents the learning of opponents that are ‘archaic’ in their behavior. LookAhead is also shown to find equilibria that avoid the Iterated Prisoner’s Dilemma. "
8966,SP:bdafb5fca09a775a8c92d2826d5dc977d28091c2,"learning system USED-FOR rare events. feature space FEATURE-OF classifiers / regressors. shape feature HYPONYM-OF prior information. segmentation algorithms USED-FOR it. shape feature USED-FOR feature space. Variational Auto - Encoder(VAE ) USED-FOR segmentation result. loss function USED-FOR shape feature. ground truth masks USED-FOR VAE. VAE USED-FOR shapes. representation USED-FOR qualities of segmentation results. one - dimensional feature space FEATURE-OF representation. segmentation algorithms USED-FOR medical segmentation task. medical segmentation task EVALUATE-FOR alarm system. segmentation algorithms EVALUATE-FOR alarm system. OtherScientificTerm are low dimensional feature space, bad shapes, and loss value. Generic is system. ","This paper proposes a learning system to detect rare events in the feature space of classifiers/regressors. The key idea is to learn a low dimensional feature space, where the prior information, such as shape feature, can be used to guide the learning of the classifiers / regressors. To do so, the authors propose a Variational Auto-Encoder(VAE) that learns a segmentation result by using the shape feature as a feature space. The shape feature is learned as a loss function, and the VAE is trained using ground truth masks. The VAE learns shapes that are more likely to be in the low dimensional space. This representation is learned in a one-dimensional feature space and the authors show that the learned representation is able to capture the qualities of segmentation results. The authors also show that bad shapes can be classified as a rare event. The proposed alarm system is tested on standard segmentation algorithms on a medical segmentation task, and it outperforms the state-of-the-art segmentation methods. The paper also shows that the proposed system is robust to the presence of bad shapes.   ","This paper proposes a learning system to detect rare events in the feature space of classifiers/regressors. The key idea is to learn a low dimensional feature space, where the prior information, such as shape feature, can be used to guide the learning of the classifiers / regressors. To do so, the authors propose a Variational Auto-Encoder(VAE) that learns a segmentation result by using the shape feature as a feature space. The shape feature is learned as a loss function, and the VAE is trained using ground truth masks. The VAE learns shapes that are more likely to be in the low dimensional space. This representation is learned in a one-dimensional feature space and the authors show that the learned representation is able to capture the qualities of segmentation results. The authors also show that bad shapes can be classified as a rare event. The proposed alarm system is tested on standard segmentation algorithms on a medical segmentation task, and it outperforms the state-of-the-art segmentation methods. The paper also shows that the proposed system is robust to the presence of bad shapes.   "
8975,SP:60738395d9efe2b3fe3a00c542ebb4261e54386c,"denoising CONJUNCTION inpainting. inpainting CONJUNCTION denoising. inpainting CONJUNCTION reconstruction. reconstruction CONJUNCTION inpainting. Deep neural networks USED-FOR compressing images. Deep neural networks USED-FOR inverse problems. compressing images USED-FOR inverse problems. reconstruction HYPONYM-OF inverse problems. convolutional neural networks HYPONYM-OF Deep neural networks. denoising HYPONYM-OF inverse problems. few and noisy measurements USED-FOR reconstruction. inpainting HYPONYM-OF inverse problems. tools COMPARE imagegenerating deep neural networks. imagegenerating deep neural networks COMPARE tools. wavelets HYPONYM-OF tools. deep neural network USED-FOR natural images. deep decoder HYPONYM-OF untrained simple image model. deep decoder USED-FOR images. network weights COMPARE wavelet - based thresholding. wavelet - based thresholding COMPARE network weights. underparameterization USED-FOR deep decoder. deep decoder USED-FOR denoising. underparameterization USED-FOR overfitting. ReLU activation CONJUNCTION channelwise normalization. channelwise normalization CONJUNCTION ReLU activation. pixel - wise linear combination of channels CONJUNCTION ReLU activation. ReLU activation CONJUNCTION pixel - wise linear combination of channels. upsampling unit CONJUNCTION pixel - wise linear combination of channels. pixel - wise linear combination of channels CONJUNCTION upsampling unit. them USED-FOR signal representations. neural networks USED-FOR signal representations. it USED-FOR neural networks. neural networks USED-FOR them. theoretical analysis USED-FOR network. OtherScientificTerm are output dimension, weight parameters, convolutions, and output dimensionality. Material is large datasets. ","Deep neural networks, such as convolutional neural networks and deep decoders, are commonly used to solve inverse problems such as denoising, inpainting, and reconstruction with few and noisy measurements. Deep neural networks have been shown to be effective at compressing images and solving inverse problems with large datasets. However, these tools (e.g., wavelets) are computationally expensive compared to imagegenerating deep neural networks.   This paper proposes to use a deep neural network to generate natural images from an untrained simple image model such as a deep decoder. The authors show that the output dimension of the decoder is the same as that of the original image, and that the network weights are asymptotically similar to the wavelet-based thresholding.  The authors also show that underparameterization of the deepdecoder can lead to overfitting to the weight parameters of the image, which is a common problem in many inverse problems. The main contribution of the paper is that the authors propose a theoretical analysis that shows that the weights of the network can be learned to be similar to those of a wavelet. The paper also shows that this is the case for any upsampling unit, any pixel-wise linear combination of channels, any ReLU activation, and any channelwise normalization.  Finally, the paper shows that it is possible to train neural networks to solve these inverse problems and apply them to the signal representations learned by neural networks in order to improve their performance. The idea is to learn a set of convolutions that are similar to each other, but with a different output dimensionality.","Deep neural networks, such as convolutional neural networks and deep decoders, are commonly used to solve inverse problems such as denoising, inpainting, and reconstruction with few and noisy measurements. Deep neural networks have been shown to be effective at compressing images and solving inverse problems with large datasets. However, these tools (e.g., wavelets) are computationally expensive compared to imagegenerating deep neural networks.   This paper proposes to use a deep neural network to generate natural images from an untrained simple image model such as a deep decoder. The authors show that the output dimension of the decoder is the same as that of the original image, and that the network weights are asymptotically similar to the wavelet-based thresholding.  The authors also show that underparameterization of the deepdecoder can lead to overfitting to the weight parameters of the image, which is a common problem in many inverse problems. The main contribution of the paper is that the authors propose a theoretical analysis that shows that the weights of the network can be learned to be similar to those of a wavelet. The paper also shows that this is the case for any upsampling unit, any pixel-wise linear combination of channels, any ReLU activation, and any channelwise normalization.  Finally, the paper shows that it is possible to train neural networks to solve these inverse problems and apply them to the signal representations learned by neural networks in order to improve their performance. The idea is to learn a set of convolutions that are similar to each other, but with a different output dimensionality."
8984,SP:1c9bad3bd4d670172f65aa0304e9837ecafc6b3d,"natural language ( NL ) USED-FOR Program synthesis. SAPS HYPONYM-OF end - to - end neural network. end - to - end neural network USED-FOR multi - sentence NL specifications. pretrained word embedding CONJUNCTION bi - directional multi - layer LSTM. bi - directional multi - layer LSTM CONJUNCTION pretrained word embedding. bi - directional multi - layer LSTM USED-FOR processing of word sequences. abstract syntax trees CONJUNCTION pretrained word embedding. pretrained word embedding CONJUNCTION abstract syntax trees. pretrained word embedding USED-FOR processing of word sequences. pretrained word embedding USED-FOR architecture. bi - directional multi - layer LSTM USED-FOR architecture. abstract syntax trees USED-FOR architecture. neural components USED-FOR architecture. signal propagation schemes CONJUNCTION soft attention mechanism. soft attention mechanism CONJUNCTION signal propagation schemes. doubly - recurrent LSTM CONJUNCTION signal propagation schemes. signal propagation schemes CONJUNCTION doubly - recurrent LSTM. signal propagation schemes USED-FOR decoder. soft attention mechanism USED-FOR decoder. doubly - recurrent LSTM USED-FOR decoder. SAPS COMPARE method. method COMPARE SAPS. NL analyzer CONJUNCTION source code generator. source code generator CONJUNCTION NL analyzer. methods COMPARE it. it COMPARE methods. fixed - dimensional latent representation USED-FOR NL analyzer. post - processing USED-FOR it. fixed - dimensional latent representation USED-FOR it. Task are software development, and end - user programming. ","Program synthesis in natural language (NL) is an important problem in software development. This paper proposes SAPS, an end-to-end neural network for multi-sentence NL specifications. The architecture is based on abstract syntax trees, pretrained word embedding, and bi-directional multi-layer LSTM for processing of word sequences.  The architecture consists of three neural components: (1) a doubly-recurrent LSTMs, (2) signal propagation schemes, and (3) a soft attention mechanism for the decoder.  Experiments show that SAPS outperforms the state-of-the-art method by a large margin. The paper also shows that it can be combined with existing methods without post-processing, and that it is able to learn a fixed-dimensional latent representation for both the NL analyzer and the source code generator, which is useful for end-user programming.","Program synthesis in natural language (NL) is an important problem in software development. This paper proposes SAPS, an end-to-end neural network for multi-sentence NL specifications. The architecture is based on abstract syntax trees, pretrained word embedding, and bi-directional multi-layer LSTM for processing of word sequences.  The architecture consists of three neural components: (1) a doubly-recurrent LSTMs, (2) signal propagation schemes, and (3) a soft attention mechanism for the decoder.  Experiments show that SAPS outperforms the state-of-the-art method by a large margin. The paper also shows that it can be combined with existing methods without post-processing, and that it is able to learn a fixed-dimensional latent representation for both the NL analyzer and the source code generator, which is useful for end-user programming."
8993,SP:d2ec231bb6153a303e5110e671dea14c2721e636,"deep neural networks USED-FOR tiny input perturbations. MNIST HYPONYM-OF computer vision. deep neural networks USED-FOR MNIST. L0 robustness EVALUATE-FOR undefended networks. adversarial robustness EVALUATE-FOR MNIST. class - conditional data distributions USED-FOR robust classification model. adversarial attacks USED-FOR model. robustness EVALUATE-FOR MNIST. MNIST EVALUATE-FOR approach. robustness EVALUATE-FOR approach. Method are neural network model, L∞ defense, input binarization, and decision - based attack. OtherScientificTerm are adversarial perturbations, L2 perturbations, Lp norms, and L0, L2 and L∞ perturbations. Material are unrecognizable images, and adversarial examples. Generic is attack. ","This paper studies the problem of adversarial robustness of deep neural networks to tiny input perturbations in computer vision, such as MNIST. The authors show that a neural network model trained with L0 robustness (i.e. the L0 perturbation of the input) can be attacked by a decision-based adversarial attack. They also show that undefended networks can be robust to L0 attacks, but not to L2 attacks.    The authors propose a simple L∞ defense, which is based on input binarization. The idea is that if the input is binarized, the adversarial perturbated input will be classified as L0, L2 and L√2, and the L2 perturbings will be considered Lp norms. They show that under certain conditions, the Lp norm of L0 and L2 is equal to that of Lp-norm of the original input, and that L0 adversarial examples will be class-conditional on the input. They further show that if Lp and L0 norms are equal to Lp, the attack will not be able to classify the input correctly.  The paper also shows that a robust classification model trained on class-conditioned data distributions can be trained to be robust against adversarial attacks. The proposed approach is tested on MNIST, and is shown to improve the robustness to adversarial instances of MNIST in the presence of unrecognizable images, and to be more robust against L0 (and L2) attacks. In addition, the paper shows that the attack can be used to train a model that is robustly robust to a variety of types of attacks, including L0. ","This paper studies the problem of adversarial robustness of deep neural networks to tiny input perturbations in computer vision, such as MNIST. The authors show that a neural network model trained with L0 robustness (i.e. the L0 perturbation of the input) can be attacked by a decision-based adversarial attack. They also show that undefended networks can be robust to L0 attacks, but not to L2 attacks.    The authors propose a simple L∞ defense, which is based on input binarization. The idea is that if the input is binarized, the adversarial perturbated input will be classified as L0, L2 and L√2, and the L2 perturbings will be considered Lp norms. They show that under certain conditions, the Lp norm of L0 and L2 is equal to that of Lp-norm of the original input, and that L0 adversarial examples will be class-conditional on the input. They further show that if Lp and L0 norms are equal to Lp, the attack will not be able to classify the input correctly.  The paper also shows that a robust classification model trained on class-conditioned data distributions can be trained to be robust against adversarial attacks. The proposed approach is tested on MNIST, and is shown to improve the robustness to adversarial instances of MNIST in the presence of unrecognizable images, and to be more robust against L0 (and L2) attacks. In addition, the paper shows that the attack can be used to train a model that is robustly robust to a variety of types of attacks, including L0. "
9002,SP:91a24e7f4b952c37441feab4a7e8555014c856a4,"spectra of weight matrices PART-OF discriminator. spectra of weight matrices USED-FOR GANs. framework USED-FOR GANs. weight matrices PART-OF discriminator. slow singular value decays FEATURE-OF weight matrices. regularizers CONJUNCTION constraints. constraints CONJUNCTION regularizers. reparameterization approach USED-FOR GANs. reparameterization approach USED-FOR weight matrices. regularizers USED-FOR spectra of the weight matrices. constraints USED-FOR spectra of the weight matrices. spectrum control USED-FOR GANs. methods COMPARE method. method COMPARE methods. CIFAR-10, STL-10, and ImgaeNet datasets EVALUATE-FOR method. spectral normalization USED-FOR method. Method are Generative Adversarial Networks ( GANs ), and singular value decompositions. OtherScientificTerm is slow singular value decay. ","This paper proposes a new framework to control the spectra of weight matrices in the discriminator of Generative Adversarial Networks (GANs). The authors propose a reparameterization approach to train GANs by controlling the spectral of the weight matrix of a discriminator. The authors show that the slow singular value decays of the spectral matrices of the slow GAN discriminator are caused by two things: (1) the use of regularizers and (2) constraints on the spectrum of the light matrices. They show that these two factors can be combined to improve the performance of the GAN. They also show that spectrum control can be applied to the spectral normalization of the discriminators of GAN's to improve their performance.  The authors compare their method with existing methods on CIFAR-10, STL-10, and ImgaeNet datasets and show that their method outperforms the previous methods.   ","This paper proposes a new framework to control the spectra of weight matrices in the discriminator of Generative Adversarial Networks (GANs). The authors propose a reparameterization approach to train GANs by controlling the spectral of the weight matrix of a discriminator. The authors show that the slow singular value decays of the spectral matrices of the slow GAN discriminator are caused by two things: (1) the use of regularizers and (2) constraints on the spectrum of the light matrices. They show that these two factors can be combined to improve the performance of the GAN. They also show that spectrum control can be applied to the spectral normalization of the discriminators of GAN's to improve their performance.  The authors compare their method with existing methods on CIFAR-10, STL-10, and ImgaeNet datasets and show that their method outperforms the previous methods.   "
9011,SP:8115fd9b681198d62100c36794926fb57dc0a4f5,Acceleration USED-FOR reinforcement learning methods. Anderson acceleration technique USED-FOR value iteration. Anderson Accelerated Value Iteration ( A2VI ) HYPONYM-OF accelerated value iteration algorithm. method USED-FOR Deep Q - learning algorithm. approach USED-FOR approximation of the policy evaluation. approximate method USED-FOR policy evaluation. A2VI COMPARE policy iteration. policy iteration COMPARE A2VI. policy iteration HYPONYM-OF approximate method. toy problems CONJUNCTION Atari games. Atari games CONJUNCTION toy problems. Material is historical data. Generic is algorithm. ,This paper proposes a new accelerated value iteration algorithm called Anderson Accelerated Value Iteration (A2VI) based on the Anderson acceleration technique for value iteration in reinforcement learning methods. The proposed method can be applied to any Deep Q-learning algorithm. The authors show that the proposed approach can be used for the approximation of the policy evaluation. They show that A2VI outperforms policy iteration in a number of toy problems and several Atari games. They also show that their algorithm is more robust to historical data. ,This paper proposes a new accelerated value iteration algorithm called Anderson Accelerated Value Iteration (A2VI) based on the Anderson acceleration technique for value iteration in reinforcement learning methods. The proposed method can be applied to any Deep Q-learning algorithm. The authors show that the proposed approach can be used for the approximation of the policy evaluation. They show that A2VI outperforms policy iteration in a number of toy problems and several Atari games. They also show that their algorithm is more robust to historical data. 
9020,SP:bd79b0c0af778a36008a0c0cf2fb6393fd2789d4,method USED-FOR catastrophic forgetting problem. SupportNet USED-FOR catastrophic forgetting problem. class incremental learning scenario FEATURE-OF catastrophic forgetting problem. deep learning CONJUNCTION support vector machine ( SVM ). support vector machine ( SVM ) CONJUNCTION deep learning. support vector machine ( SVM ) PART-OF SupportNet. deep learning PART-OF SupportNet. SVM PART-OF SupportNet. consolidation regularizers USED-FOR model. SupportNet COMPARE deep learning model. deep learning model COMPARE SupportNet. SupportNet COMPARE incremental learning methods. incremental learning methods COMPARE SupportNet. tasks EVALUATE-FOR SupportNet. tasks EVALUATE-FOR method. OtherScientificTerm is catastrophic forgetting. ,This paper proposes a method called SupportNet to tackle the catastrophic forgetting problem in the class incremental learning scenario. The proposed method combines deep learning and support vector machine (SVM) in the form of SupportNet. The authors propose two consolidation regularizers to improve the performance of the proposed model. Experimental results show that SupportNet outperforms the state-of-the-art deep learning model and other incremental learning methods on a number of tasks. ,This paper proposes a method called SupportNet to tackle the catastrophic forgetting problem in the class incremental learning scenario. The proposed method combines deep learning and support vector machine (SVM) in the form of SupportNet. The authors propose two consolidation regularizers to improve the performance of the proposed model. Experimental results show that SupportNet outperforms the state-of-the-art deep learning model and other incremental learning methods on a number of tasks. 
9029,SP:d228d213f79716774043cea253305fecece659ec,"methods USED-FOR representations. methods USED-FOR unit selectivity. unit selectivity USED-FOR representations. neural networks ( NNs ) USED-FOR representations. measures PART-OF AlexNet. localist selectivity HYPONYM-OF measures. precision CONJUNCTION class - conditional mean activity selectivity CCMAS. class - conditional mean activity selectivity CCMAS CONJUNCTION precision. precision and CCMAS measures USED-FOR selectivity. fc6 CONJUNCTION conv5. conv5 CONJUNCTION fc6. units PART-OF conv5. units PART-OF fc6. RNNs COMPARE AlexNet. AlexNet COMPARE RNNs. AlexNet USED-FOR localist representations. RNNs USED-FOR localist representations. Generic is measure. Metric are top - class selectivity, and selectivity measures. Method are recurrent neural networks ( RNNs ), activation maximization ( AM ) images, fc8, and NNs. OtherScientificTerm are hidden layers, and selective units. ","This paper proposes two methods to measure the unit selectivity of representations learned by neural networks (NNs) from activation maximization (AM) images. The proposed measure, called top-class selectivity, is based on the observation that the top-ranked units in an RNN are the ones with the highest selectivity. The paper also proposes two other measures, i.e., precision and class-conditional mean activity selectivity (CCMAS), which are also used in AlexNet. The precision and CCMAS measures are used to measure selectivity in fc6 and conv5, and fc8, respectively.  The paper shows that RNNs trained with AlexNet are able to learn localist representations similar to the ones learned by RNN with fc2, fc4, and AlexNet, and that these representations are more selective than those learned by other methods. The authors also show that these selective units are more likely to be active in the same hidden layers, which is a common phenomenon in NNs. ","This paper proposes two methods to measure the unit selectivity of representations learned by neural networks (NNs) from activation maximization (AM) images. The proposed measure, called top-class selectivity, is based on the observation that the top-ranked units in an RNN are the ones with the highest selectivity. The paper also proposes two other measures, i.e., precision and class-conditional mean activity selectivity (CCMAS), which are also used in AlexNet. The precision and CCMAS measures are used to measure selectivity in fc6 and conv5, and fc8, respectively.  The paper shows that RNNs trained with AlexNet are able to learn localist representations similar to the ones learned by RNN with fc2, fc4, and AlexNet, and that these representations are more selective than those learned by other methods. The authors also show that these selective units are more likely to be active in the same hidden layers, which is a common phenomenon in NNs. "
9038,SP:b9deae0392e0160b400d76c549d382e235196f8c,"spectral methods CONJUNCTION posterior inference. posterior inference CONJUNCTION spectral methods. probabilistic graphical models USED-FOR posterior inference. graphs USED-FOR Community detection. spectral methods USED-FOR Community detection. posterior inference USED-FOR Community detection. signal - to - noise ratio FEATURE-OF statistical and computational detection thresholds. stochastic block model HYPONYM-OF random graph families. graphs USED-FOR node - wise classification problem. node - wise classification problem USED-FOR community detection. learning perspective USED-FOR it. Graph Neural Networks ( GNNs ) USED-FOR community detection problems. supervised learning setting USED-FOR community detection problems. belief propagation algorithm USED-FOR binary and multiclass stochastic block models. they COMPARE belief propagation algorithm. belief propagation algorithm COMPARE they. line graph of edge adjacencies FEATURE-OF non - backtracking operator. non - backtracking operator USED-FOR GNNs. real - world datasets EVALUATE-FOR GNNs. linear ) GNNs USED-FOR community detection problems. linear ) GNNs USED-FOR optimization landscape. Generic is approaches. Method is generative models. OtherScientificTerm are computational threshold, local minimum, and global minimum / minima. ","This paper considers the problem of community detection using graph neural networks. Community detection is an important problem in many real-world applications, where the goal is to detect the presence of a group of nodes in a node-wise classification problem on graphs, which can be solved using spectral methods and posterior inference using probabilistic graphical models. Community detectors are typically based on the stochastic block model, which is a family of random graph families where the computational threshold is a function of the signal-to-noise ratio between the statistical and computational detection thresholds.   The paper proposes two approaches to train generative models for community detection. The first is based on Graph Neural Networks (GNNs) that are trained to solve community detection problems in a supervised learning setting, where it is assumed that the community detection problem is solved on graphs.  The second approach uses a belief propagation algorithm to learn a belief for binary and multiclass Stochastic block models, and then uses it from a learning perspective to find a local minimum. The authors show that (linear) GNNs trained with a non-backtracking operator on the line graph of edge adjacencies are able to find the global minimum/minima, and that they are more robust than the standard (non-linear) belief propagation method.  Experiments are conducted on several real-life datasets, and show that the optimization landscape can be learned by (linear ) GNNS, and they outperform the state of the art. ","This paper considers the problem of community detection using graph neural networks. Community detection is an important problem in many real-world applications, where the goal is to detect the presence of a group of nodes in a node-wise classification problem on graphs, which can be solved using spectral methods and posterior inference using probabilistic graphical models. Community detectors are typically based on the stochastic block model, which is a family of random graph families where the computational threshold is a function of the signal-to-noise ratio between the statistical and computational detection thresholds.   The paper proposes two approaches to train generative models for community detection. The first is based on Graph Neural Networks (GNNs) that are trained to solve community detection problems in a supervised learning setting, where it is assumed that the community detection problem is solved on graphs.  The second approach uses a belief propagation algorithm to learn a belief for binary and multiclass Stochastic block models, and then uses it from a learning perspective to find a local minimum. The authors show that (linear) GNNs trained with a non-backtracking operator on the line graph of edge adjacencies are able to find the global minimum/minima, and that they are more robust than the standard (non-linear) belief propagation method.  Experiments are conducted on several real-life datasets, and show that the optimization landscape can be learned by (linear ) GNNS, and they outperform the state of the art. "
9047,SP:a9ed31090e55f6152fc31c7512af5d634cc7225a,"sparse weights PART-OF linear combination. provable algorithms USED-FOR dictionary learning. provable dictionary learning methods USED-FOR coefficient recovery. linear and non - linear operations PART-OF it. algorithm COMPARE state - of - the - art techniques. state - of - the - art techniques COMPARE algorithm. Task are dictionary learning problem, optimization, and recovery of the dictionary. OtherScientificTerm are dictionary, coefficients, and geometric rate. Method are linear model, NOODL, and neural architectures. ","This paper considers the dictionary learning problem, where the goal is to learn a linear combination of sparse weights in a linear model. The dictionary is constructed by computing the coefficients of the linear combination, and the goal of the optimization is to recover the coefficients. The authors propose two provable algorithms for the problem of dictionary learning. The first provable dictionary learning methods for coefficient recovery is based on previous work, and it combines linear and non-linear operations. The second provable algorithm, called NoODL, is a generalization of the previous work. The main contribution of the paper is that the recovery of the dictionary is provably provable, and that the geometric rate of the recovery is also provable. Experiments show that the proposed algorithm outperforms state-of-the-art techniques on a variety of datasets and neural architectures.   ","This paper considers the dictionary learning problem, where the goal is to learn a linear combination of sparse weights in a linear model. The dictionary is constructed by computing the coefficients of the linear combination, and the goal of the optimization is to recover the coefficients. The authors propose two provable algorithms for the problem of dictionary learning. The first provable dictionary learning methods for coefficient recovery is based on previous work, and it combines linear and non-linear operations. The second provable algorithm, called NoODL, is a generalization of the previous work. The main contribution of the paper is that the recovery of the dictionary is provably provable, and that the geometric rate of the recovery is also provable. Experiments show that the proposed algorithm outperforms state-of-the-art techniques on a variety of datasets and neural architectures.   "
9056,SP:85232b72a2643d6dc81cf952ccbb95192032b7c5,"differentiable model CONJUNCTION similarity function. similarity function CONJUNCTION differentiable model. loss function USED-FOR binary hash codes. differentiable model USED-FOR binary hash codes. loss function COMPARE prior methods. prior methods COMPARE loss function. log likelihood loss USED-FOR prior methods. log likelihood loss USED-FOR loss function. multi - indexing USED-FOR hashes. techniques USED-FOR similarity search tasks. ImageNet CONJUNCTION SIFT 1 M. SIFT 1 M CONJUNCTION ImageNet. information retrieval tasks EVALUATE-FOR SIFT 1 M. ImageNet USED-FOR information retrieval tasks. OtherScientificTerm are Hamming distance target, loss terms, and minibatch. Method is training scheme. Metric are MAP, and query cost. ","This paper proposes a new loss function for binary hash codes that combines a differentiable model and a similarity function. The loss function is similar to prior methods based on the log likelihood loss, but instead of using the Hamming distance target, the authors propose to use the multi-indexing for the hashes. The authors also propose a training scheme where the loss terms are optimized by minimizing the difference between the MAP of the query and the query of the original hash. The proposed techniques are applied to similarity search tasks on ImageNet and SIFT 1M for information retrieval tasks. The results show that the proposed training scheme outperforms prior work in terms of query cost. ","This paper proposes a new loss function for binary hash codes that combines a differentiable model and a similarity function. The loss function is similar to prior methods based on the log likelihood loss, but instead of using the Hamming distance target, the authors propose to use the multi-indexing for the hashes. The authors also propose a training scheme where the loss terms are optimized by minimizing the difference between the MAP of the query and the query of the original hash. The proposed techniques are applied to similarity search tasks on ImageNet and SIFT 1M for information retrieval tasks. The results show that the proposed training scheme outperforms prior work in terms of query cost. "
9065,SP:3bd4ccff7f48380d2db8dff2c4ca515894a7f1db,Neural architecture search ( NAS ) USED-FOR task - specific neural network topology. networks USED-FOR search. Graph HyperNetwork ( GHN ) USED-FOR search cost. graph neural network USED-FOR inference. regular hypernetworks CONJUNCTION premature early stopping. premature early stopping CONJUNCTION regular hypernetworks. GHNs USED-FOR architecture. GHNs USED-FOR network. GHNs COMPARE regular hypernetworks. regular hypernetworks COMPARE GHNs. GHNs COMPARE premature early stopping. premature early stopping COMPARE GHNs. validation accuracy EVALUATE-FOR networks. validation accuracy EVALUATE-FOR surrogate search signal. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. they COMPARE random search methods. random search methods COMPARE they. GHNs COMPARE random search methods. random search methods COMPARE GHNs. ImageNet EVALUATE-FOR random search methods. CIFAR-10 EVALUATE-FOR random search methods. networks COMPARE manual designs. manual designs COMPARE networks. GHNs USED-FOR anytime prediction setting. speed - accuracy tradeoff EVALUATE-FOR networks. Method is manual architecture designs. Generic is it. Task is NAS. ,"Neural architecture search (NAS) aims to learn a task-specific neural network topology that can be used to improve the performance of existing manual architecture designs. The authors propose a Graph HyperNetwork (GHN) to reduce the search cost by using a graph neural network for inference. They show that existing networks can be trained to perform well in search. They also show that GHNs are more efficient than regular hypernetworks and premature early stopping for learning an architecture.    The authors show that the speed-accuracy tradeoff between two networks trained with and without GHNs is a trade-off between validation accuracy and the surrogate search signal, and they show that they outperform other random search methods on CIFAR-10 and ImageNet. They further show that these networks outperform manual designs in the anytime prediction setting.  The paper is well-written, well-motivated, and well-structured. However, it is not clear to me that it is a new direction for NAS. ","Neural architecture search (NAS) aims to learn a task-specific neural network topology that can be used to improve the performance of existing manual architecture designs. The authors propose a Graph HyperNetwork (GHN) to reduce the search cost by using a graph neural network for inference. They show that existing networks can be trained to perform well in search. They also show that GHNs are more efficient than regular hypernetworks and premature early stopping for learning an architecture.    The authors show that the speed-accuracy tradeoff between two networks trained with and without GHNs is a trade-off between validation accuracy and the surrogate search signal, and they show that they outperform other random search methods on CIFAR-10 and ImageNet. They further show that these networks outperform manual designs in the anytime prediction setting.  The paper is well-written, well-motivated, and well-structured. However, it is not clear to me that it is a new direction for NAS. "
9074,SP:65ccf43cd4e033d22239069057f5200d49f33724,Imitation learning USED-FOR optimal policy. expert demonstrations USED-FOR Imitation learning. expert demonstrations USED-FOR optimal policy. expert demonstrations USED-FOR deep learning. method USED-FOR generative adversarial imitation learning. multiclass classification USED-FOR discriminator functions. method USED-FOR multiclass classification. method USED-FOR discriminator functions. method COMPARE generative adversarial imitation learning baseline. generative adversarial imitation learning baseline COMPARE method. continuous control tasks EVALUATE-FOR method. method USED-FOR policies. continuous control tasks EVALUATE-FOR generative adversarial imitation learning baseline. OtherScientificTerm is non - expert demonstrations. ,This paper proposes a new method for generative adversarial imitation learning. Imitation learning uses expert demonstrations to learn an optimal policy from expert demonstrations for deep learning. The proposed method is based on multiclass classification to learn discriminator functions that can be used to train a discriminator for non-expert demonstrations. Experiments on continuous control tasks show that the proposed method outperforms the state-of-the-art generative discriminator imitation learning baseline and is able to learn policies that are robust to non-adversarial perturbations.,This paper proposes a new method for generative adversarial imitation learning. Imitation learning uses expert demonstrations to learn an optimal policy from expert demonstrations for deep learning. The proposed method is based on multiclass classification to learn discriminator functions that can be used to train a discriminator for non-expert demonstrations. Experiments on continuous control tasks show that the proposed method outperforms the state-of-the-art generative discriminator imitation learning baseline and is able to learn policies that are robust to non-adversarial perturbations.
9083,SP:e8427949a98effbd37ce7604fa11f240e2342196,"natural science HYPONYM-OF applications. neural networks USED-FOR task. Invertible Neural Networks ( INNs ) HYPONYM-OF neural networks. neural networks USED-FOR ambiguous inverse problem. INNs USED-FOR forward process. neural networks COMPARE INNs. INNs COMPARE neural networks. latent output variables USED-FOR INNs. model USED-FOR inverse process. invertibility USED-FOR model. medicine CONJUNCTION astrophysics. astrophysics CONJUNCTION medicine. INNs USED-FOR unrecoverable parameters. INNs USED-FOR multi - modalities. INNs USED-FOR parameter correlations. parameter space FEATURE-OF multi - modalities. OtherScientificTerm are hidden system parameters, posterior parameter distribution, observed measurement, and distribution of the latent variables. Task is inverse problem. Generic is ambiguity. Method is INN. Material is artificial data. ","This paper proposes Invertible Neural Networks (INNs), a class of neural networks that can be used to solve the ambiguous inverse problem, i.e., the problem that arises when neural networks are trained to solve a particular task, but the hidden system parameters are not invertible. This is an important problem in many applications, such as natural science, medicine, astrophysics, etc. In this paper, the authors propose INNs, which are neural networks where the posterior parameter distribution over the latent variables of the forward process of the neural networks is invariant to changes in the posterior distribution of the latent output variables. They show that INNs can solve the inverse problem in a similar way as other neural networks, but INNs are different from INNs in the sense that they are able to learn a model for the inverse process with invertibility. In particular, INNs for multi-modalities (in the parameter space) can learn to recover unrecoverable parameters, and INNs that can recover parameter correlations across multiple modalities.  The main contribution of this paper is that the authors show that an INN can be trained to recover the parameters of a system from a single observation, and that this can be done even if the observed measurement is ambiguous. The ambiguity is caused by the fact that the INN is trained on artificial data, where the distribution of latent variables is not invariant.   ","This paper proposes Invertible Neural Networks (INNs), a class of neural networks that can be used to solve the ambiguous inverse problem, i.e., the problem that arises when neural networks are trained to solve a particular task, but the hidden system parameters are not invertible. This is an important problem in many applications, such as natural science, medicine, astrophysics, etc. In this paper, the authors propose INNs, which are neural networks where the posterior parameter distribution over the latent variables of the forward process of the neural networks is invariant to changes in the posterior distribution of the latent output variables. They show that INNs can solve the inverse problem in a similar way as other neural networks, but INNs are different from INNs in the sense that they are able to learn a model for the inverse process with invertibility. In particular, INNs for multi-modalities (in the parameter space) can learn to recover unrecoverable parameters, and INNs that can recover parameter correlations across multiple modalities.  The main contribution of this paper is that the authors show that an INN can be trained to recover the parameters of a system from a single observation, and that this can be done even if the observed measurement is ambiguous. The ambiguity is caused by the fact that the INN is trained on artificial data, where the distribution of latent variables is not invariant.   "
9092,SP:75c9bb53bac29bdb390f9ba5707caee4ab1f5925,"ensemble of NNs COMPARE Bayesian NNs. Bayesian NNs COMPARE ensemble of NNs. scoring rule USED-FOR ensemble of NNs. finite mixture model USED-FOR ensemble method. uniform mixing weights USED-FOR finite mixture model. adaptive, input - dependent distribution USED-FOR fixed mixing weights. NN USED-FOR adaptive, input - dependent distribution. model COMPARE approaches. approaches COMPARE model. uncertainty estimates EVALUATE-FOR model. Method are deep neural networks ( NNs ), mixture model approach, mixture density networks, and compound density networks. OtherScientificTerm are prediction uncertainty, and mixture components. Material is adversarial examples. ","This paper proposes an ensemble of deep neural networks (NNs) based on a mixture model approach. The authors show that the ensemble of NNs based on this scoring rule is more robust to prediction uncertainty than Bayesian NNs. The ensemble method relies on a finite mixture model with uniform mixing weights, which is similar to a mixture density networks. The main difference is that instead of using mixture components, the authors propose to use an adaptive, input-dependent distribution over the parameters of the NN to replace the fixed mixing weights. The proposed model is tested on adversarial examples and is shown to outperform existing approaches in terms of uncertainty estimates. ","This paper proposes an ensemble of deep neural networks (NNs) based on a mixture model approach. The authors show that the ensemble of NNs based on this scoring rule is more robust to prediction uncertainty than Bayesian NNs. The ensemble method relies on a finite mixture model with uniform mixing weights, which is similar to a mixture density networks. The main difference is that instead of using mixture components, the authors propose to use an adaptive, input-dependent distribution over the parameters of the NN to replace the fixed mixing weights. The proposed model is tested on adversarial examples and is shown to outperform existing approaches in terms of uncertainty estimates. "
9101,SP:e1e38289285c1b8fdb318e4f6d37a198a08787a2,"energy consumption CONJUNCTION communication bandwidth. communication bandwidth CONJUNCTION energy consumption. communication bandwidth CONJUNCTION storage requirements. storage requirements CONJUNCTION communication bandwidth. deep neural networks HYPONYM-OF model class. model size reduction PART-OF deep learning. pruning CONJUNCTION quantization. quantization CONJUNCTION pruning. techniques USED-FOR Shannon - style coding schemes. Shannon - style coding schemes USED-FOR empirical weight distribution. quantization HYPONYM-OF techniques. pruning HYPONYM-OF techniques. full variational distribution USED-FOR coding schemes. compression rates EVALUATE-FOR coding schemes. KullbackLeibler divergence FEATURE-OF sampled variational distribution. random sample USED-FOR network weights. constraint USED-FOR compression rate. constraint FEATURE-OF Kullback - Leibler divergence. encoding scheme COMPARE information - theoretical lower bound. information - theoretical lower bound COMPARE encoding scheme. variational family USED-FOR information - theoretical lower bound. variational family USED-FOR encoding scheme. it COMPARE approaches. approaches COMPARE it. method USED-FOR neural network compression. VGG-16 / CIFAR-10 EVALUATE-FOR approach. fixed memory budget EVALUATE-FOR approach. compression rates EVALUATE-FOR approach. compression rates EVALUATE-FOR it. OtherScientificTerm are memory footprint, deterministic weights, weight determinism, encoding distribution, and expected loss. Method is bits - back argument. ","This paper studies the problem of model size reduction in deep learning, i.e., reducing the memory footprint, energy consumption, communication bandwidth, and storage requirements of deep neural networks. The authors propose two techniques to compress Shannon-style coding schemes (pruning and quantization) that encode the empirical weight distribution of a neural network into deterministic weights. They show that the compression rates of these coding schemes can be improved by using a full variational distribution of the weights, and that the KullbackLeibler divergence between the sampled variational density of the network weights and that of the encoding distribution is lower bounded by a bits-back argument. They also show that a constraint on the compression rate is imposed on this Kullbacks-Leiblers divergence, which is a result of weight determinism.  The authors show that their encoding scheme is a variational family and that it outperforms the information-theoretical lower bound. They demonstrate that their approach can be applied to VGG-16/CIFAR-10 with a fixed memory budget and achieves better compression rates than it can be compared to existing approaches. They further show that this method can be used for neural network compression in the presence of a random sample for network weights.   ","This paper studies the problem of model size reduction in deep learning, i.e., reducing the memory footprint, energy consumption, communication bandwidth, and storage requirements of deep neural networks. The authors propose two techniques to compress Shannon-style coding schemes (pruning and quantization) that encode the empirical weight distribution of a neural network into deterministic weights. They show that the compression rates of these coding schemes can be improved by using a full variational distribution of the weights, and that the KullbackLeibler divergence between the sampled variational density of the network weights and that of the encoding distribution is lower bounded by a bits-back argument. They also show that a constraint on the compression rate is imposed on this Kullbacks-Leiblers divergence, which is a result of weight determinism.  The authors show that their encoding scheme is a variational family and that it outperforms the information-theoretical lower bound. They demonstrate that their approach can be applied to VGG-16/CIFAR-10 with a fixed memory budget and achieves better compression rates than it can be compared to existing approaches. They further show that this method can be used for neural network compression in the presence of a random sample for network weights.   "
9110,SP:ad70d8cf3a4558aab0d3b7155594464a3debd912,neural network architectures USED-FOR Neural architecture search ( NAS ). architectures USED-FOR large - scale tasks. ImageNet HYPONYM-OF large - scale tasks. GPU hours EVALUATE-FOR Differentiable NAS. continuous representation of network architecture USED-FOR Differentiable NAS. continuous representation of network architecture USED-FOR GPU hours. proxy tasks USED-FOR they. architectures USED-FOR proxy tasks. large - scale target tasks CONJUNCTION hardware platforms. hardware platforms CONJUNCTION large - scale target tasks. architectures USED-FOR large - scale target tasks. ProxylessNAS USED-FOR architectures. ProxylessNAS USED-FOR large - scale target tasks. GPU hours CONJUNCTION GPU memory. GPU memory CONJUNCTION GPU hours. computational cost EVALUATE-FOR regular training. GPU hours HYPONYM-OF computational cost. GPU memory HYPONYM-OF computational cost. CIFAR-10 CONJUNCTION ImageNet. ImageNet CONJUNCTION CIFAR-10. CIFAR-10 EVALUATE-FOR model. test error EVALUATE-FOR model. ImageNet EVALUATE-FOR model. model COMPARE MobileNetV2. MobileNetV2 COMPARE model. ImageNet EVALUATE-FOR MobileNetV2. top-1 accuracy EVALUATE-FOR MobileNetV2. GPU latency EVALUATE-FOR model. top-1 accuracy EVALUATE-FOR model. ProxylessNAS USED-FOR neural architectures. Neural architecture search ( NAS ) USED-FOR neural network architecture design. neural network architecture design USED-FOR deep learning tasks. Neural architecture search ( NAS ) USED-FOR deep learning tasks. image recognition CONJUNCTION language modeling. language modeling CONJUNCTION image recognition. image recognition HYPONYM-OF deep learning tasks. language modeling HYPONYM-OF deep learning tasks. models USED-FOR task. NAS USED-FOR large - scale task. ImageNet HYPONYM-OF large - scale task. building blocks USED-FOR proxy tasks. top - performing blocks USED-FOR large - scale target task. paradigm USED-FOR NAS algorithms. blocks USED-FOR proxy tasks. latency HYPONYM-OF hardware metrics. methods USED-FOR transferability. Proxy,"This paper proposes ProxylessNAS, a new architecture search method for neural architecture search (NAS) for large-scale neural networks. The method is based on the Differentiable NAS (DNAS) framework, which uses a continuous representation of the architecture of a neural network to search for the best architecture for a given task. The authors show that the proposed method is able to find a good architecture for large scale NAS tasks, and that it can be applied to existing NAS algorithms. The paper also shows that the method is transferable to different hardware architectures.","This paper proposes ProxylessNAS, a new architecture search method for neural architecture search (NAS) for large-scale neural networks. The method is based on the Differentiable NAS (DNAS) framework, which uses a continuous representation of the architecture of a neural network to search for the best architecture for a given task. The authors show that the proposed method is able to find a good architecture for large scale NAS tasks, and that it can be applied to existing NAS algorithms. The paper also shows that the method is transferable to different hardware architectures."
9119,SP:e5b70d43d301d1980fae02623ea711976b429c14,"Lagrangian dual FEATURE-OF problem. additive linear penalties USED-FOR Lagrangian dual. non - convex settings FEATURE-OF problem. training procedure USED-FOR non - convex, large - data settings. second - order ones FEATURE-OF linear penalties. secondorder penalties USED-FOR penalized objective. penalty coefficient USED-FOR penalized objective. method USED-FOR gradients. second - order penalties FEATURE-OF gradients. algorithm USED-FOR classifier. Metric is fairness. OtherScientificTerm are linear constraints, constrained objective, Lagrangian, deterministic saddle - point equilibrium, instability, and stochastic mini - batch settings. Method is two - player min - max games. ","This paper considers the problem of learning a Lagrangian dual of the problem under additive linear penalties. In non-convex settings, this problem can be seen as a special case of the fairness problem under linear constraints, and the authors propose a new training procedure for this problem. The main contribution of this paper is to extend the training procedure to non-consvex, large-data settings. The authors show that under the constrained objective, a deterministic saddle-point equilibrium can be found, and that under a constrained objective the Lagrangians of the two-player min-max games can also be found.   The authors also show that the problem is non-trivial in the setting of linear penalties with second-order ones. They show that a penalized objective under secondorder penalties can be derived from the penalty coefficient of the second order penalty. They also provide a method to compute the gradients of the classifier under the proposed method.  Finally, the authors provide an algorithm for training a classifier in the case of stochastic mini-batch settings. ","This paper considers the problem of learning a Lagrangian dual of the problem under additive linear penalties. In non-convex settings, this problem can be seen as a special case of the fairness problem under linear constraints, and the authors propose a new training procedure for this problem. The main contribution of this paper is to extend the training procedure to non-consvex, large-data settings. The authors show that under the constrained objective, a deterministic saddle-point equilibrium can be found, and that under a constrained objective the Lagrangians of the two-player min-max games can also be found.   The authors also show that the problem is non-trivial in the setting of linear penalties with second-order ones. They show that a penalized objective under secondorder penalties can be derived from the penalty coefficient of the second order penalty. They also provide a method to compute the gradients of the classifier under the proposed method.  Finally, the authors provide an algorithm for training a classifier in the case of stochastic mini-batch settings. "
9128,SP:e4720b8e4efdb222c45eafd47fd8a7fbf15d881d,"Sampling discrete latent variables USED-FOR highvariance gradient estimators. continuous - relaxation methods USED-FOR latter. control - variate schemes USED-FOR former. branch paths PART-OF model. control - variate schemes CONJUNCTION continuous - relaxation methods. continuous - relaxation methods CONJUNCTION control - variate schemes. control - variate schemes USED-FOR state - of - the - art methods. state - of - the - art methods USED-FOR discrete latent - variable models. it COMPARE state - of - the - art methods. state - of - the - art methods COMPARE it. models CONJUNCTION inference networks. inference networks CONJUNCTION models. importance weighted autoencoder COMPARE RWS. RWS COMPARE importance weighted autoencoder. RWS USED-FOR inference networks. RWS USED-FOR models. RWS USED-FOR deep generative models. Method are Discrete latent - variable models, and continuous latentvariable models. OtherScientificTerm is pathwise derivative. ","This paper studies the problem of sampling discrete latent variables for highvariance gradient estimators. Discrete latent-variable models are a special case of continuous latent variable models, where the pathwise derivative is a function of the number of branch paths in the model. The authors propose two control-variate schemes for the former and two continuous-relaxation methods for the latter. They compare the performance of state-of-the-art methods for discrete latent-variance models using both control-viate schemes and continuous-lossy methods. They show that RWS outperforms other models and inference networks in terms of importance weighted autoencoder, and that it outperforms state- of-the art methods for deep generative models. ","This paper studies the problem of sampling discrete latent variables for highvariance gradient estimators. Discrete latent-variable models are a special case of continuous latent variable models, where the pathwise derivative is a function of the number of branch paths in the model. The authors propose two control-variate schemes for the former and two continuous-relaxation methods for the latter. They compare the performance of state-of-the-art methods for discrete latent-variance models using both control-viate schemes and continuous-lossy methods. They show that RWS outperforms other models and inference networks in terms of importance weighted autoencoder, and that it outperforms state- of-the art methods for deep generative models. "
9137,SP:7459ae5b1d886e68930c4c9e21df508bc8ab3c9a,"human knowledge CONJUNCTION non - differentiable pipelines. non - differentiable pipelines CONJUNCTION human knowledge. non - differentiable pipelines USED-FOR scalar reward function. human knowledge USED-FOR scalar reward function. scalar reward function USED-FOR tasks. truncated randomized search USED-FOR structured prediction energy networks ( SPENs ). truncated randomized search USED-FOR reward function. structured prediction energy networks ( SPENs ) USED-FOR test - time inference. gradient - based search USED-FOR structured prediction energy networks ( SPENs ). gradient - based search USED-FOR test - time inference. truncated randomized search USED-FOR unknown local improvements. supervision USED-FOR SPENs. truncated randomized search USED-FOR supervision. truncated randomized search USED-FOR reward function. Task are structured output prediction tasks, and structured prediction. OtherScientificTerm are output space, and score landscape. Material is labeled training data. ","This paper studies the problem of learning a scalar reward function from human knowledge and non-differentiable pipelines for structured output prediction tasks. In particular, the authors consider the case where the task is structured prediction, where the goal is to learn a function in the output space that maximizes the expected return on the labeled training data. For this setting, the paper proposes to use a truncated randomized search to train structured prediction energy networks (SPENs) based on gradient-based search for test-time inference. The authors show that the scalar function for these tasks can be learned from both human knowledge as well as from non - differentiable pipelines. They also show that SPENs can be trained with supervision via truncated random search for unknown local improvements, and that the reward function can be optimized using the learned reward function via the use of truncated Randomized Search (SR).","This paper studies the problem of learning a scalar reward function from human knowledge and non-differentiable pipelines for structured output prediction tasks. In particular, the authors consider the case where the task is structured prediction, where the goal is to learn a function in the output space that maximizes the expected return on the labeled training data. For this setting, the paper proposes to use a truncated randomized search to train structured prediction energy networks (SPENs) based on gradient-based search for test-time inference. The authors show that the scalar function for these tasks can be learned from both human knowledge as well as from non - differentiable pipelines. They also show that SPENs can be trained with supervision via truncated random search for unknown local improvements, and that the reward function can be optimized using the learned reward function via the use of truncated Randomized Search (SR)."
9146,SP:638c1bc09992029b78bd83f0127594dcccb96c06,"It USED-FOR transferring policies. simulation environment FEATURE-OF transferring policies. these USED-FOR robust policies. active learning based framework USED-FOR model parameters. EffAcTS HYPONYM-OF active learning based framework. framework USED-FOR method. sample efficiency EVALUATE-FOR approach. EPOpt HYPONYM-OF method. continuous control tasks EVALUATE-FOR approach. Multi - Task Learning perspective USED-FOR Robust Policy Search. framework COMPARE Multi - Task Learning. Multi - Task Learning COMPARE framework. Task is learning policies. OtherScientificTerm are environment model parameters, and policies. Generic is approaches. ","This paper considers the problem of learning policies that are robust to perturbations in the environment model parameters. It focuses on transferring policies from a simulation environment to a real-world environment. The authors propose an active learning based framework, EffAcTS, to learn the model parameters and then use these to learn robust policies. The proposed method, EPOpt, is based on the framework of Multi-Task Learning and is shown to improve sample efficiency. The approach is evaluated on a number of continuous control tasks and is compared with existing approaches. The paper also proposes a Robust Policy Search from a Multi-task Learning perspective. ","This paper considers the problem of learning policies that are robust to perturbations in the environment model parameters. It focuses on transferring policies from a simulation environment to a real-world environment. The authors propose an active learning based framework, EffAcTS, to learn the model parameters and then use these to learn robust policies. The proposed method, EPOpt, is based on the framework of Multi-Task Learning and is shown to improve sample efficiency. The approach is evaluated on a number of continuous control tasks and is compared with existing approaches. The paper also proposes a Robust Policy Search from a Multi-task Learning perspective. "
9155,SP:491c239713a6489f0b1790ca26db54a1813c67ae,"policy evaluation CONJUNCTION control. control CONJUNCTION policy evaluation. value function USED-FOR policy evaluation. value function USED-FOR control. fixed basis CONJUNCTION fixed representation. fixed representation CONJUNCTION fixed basis. algorithms USED-FOR linear function approximation. fixed basis USED-FOR linear function approximation. fixed representation USED-FOR linear function approximation. temporal difference learning CONJUNCTION Q - learning. Q - learning CONJUNCTION temporal difference learning. extensions USED-FOR nonlinear function approximation. Q - learning HYPONYM-OF methods. temporal difference learning HYPONYM-OF methods. nonlinear gradient temporal difference learning HYPONYM-OF nonlinear function approximation. two - timescale network ( TTN ) architecture USED-FOR linear methods. algorithms USED-FOR nonlinear value estimates. algorithms USED-FOR linear setting. data - efficient least - squares methods CONJUNCTION eligibility traces. eligibility traces CONJUNCTION data - efficient least - squares methods. linear policy evaluation algorithms USED-FOR nonlinear value estimates. eligibility traces CONJUNCTION linear policy evaluation algorithms. linear policy evaluation algorithms CONJUNCTION eligibility traces. algorithms USED-FOR approach. linear policy evaluation algorithms HYPONYM-OF algorithms. data - efficient least - squares methods HYPONYM-OF algorithms. eligibility traces HYPONYM-OF algorithms. dependent features FEATURE-OF linear component. nonlinear value function approximation algorithms USED-FOR policy evaluation and control. TTNs COMPARE nonlinear value function approximation algorithms. nonlinear value function approximation algorithms COMPARE TTNs. TTNs USED-FOR policy evaluation and control. Method are reinforcement learning agents, and nonlinear representation. ","This paper considers the problem of learning a value function for policy evaluation and control in reinforcement learning agents. The authors propose two extensions to existing methods for nonlinear function approximation, namely temporal difference learning and Q-learning. The main idea is to use a two-timescale network (TTN) architecture to learn a linear function approximation on a fixed basis and a fixed representation, and then to use nonlinear gradient temporal difference (nonlinear gradient) learning to learn the nonlinear representation. The proposed approach is based on two existing algorithms: data-efficient least-squares methods (e.g., eligibility traces) and linear policy evaluation algorithms for learning nonlinear value estimates in the linear setting. The paper shows that the proposed algorithms can be used to learn nonlinear values for any linear component with dependent features, and that linear methods can be learned using the two-timescale network (which can be trained in parallel). The paper also shows that TTNs are more efficient than existing state-of-the-art linear value function approximation algorithms for both policy evaluation  and control. Finally, the paper shows how the proposed approach can be combined with existing algorithms, and shows that it outperforms existing algorithms in the case of nonlinearity. ","This paper considers the problem of learning a value function for policy evaluation and control in reinforcement learning agents. The authors propose two extensions to existing methods for nonlinear function approximation, namely temporal difference learning and Q-learning. The main idea is to use a two-timescale network (TTN) architecture to learn a linear function approximation on a fixed basis and a fixed representation, and then to use nonlinear gradient temporal difference (nonlinear gradient) learning to learn the nonlinear representation. The proposed approach is based on two existing algorithms: data-efficient least-squares methods (e.g., eligibility traces) and linear policy evaluation algorithms for learning nonlinear value estimates in the linear setting. The paper shows that the proposed algorithms can be used to learn nonlinear values for any linear component with dependent features, and that linear methods can be learned using the two-timescale network (which can be trained in parallel). The paper also shows that TTNs are more efficient than existing state-of-the-art linear value function approximation algorithms for both policy evaluation  and control. Finally, the paper shows how the proposed approach can be combined with existing algorithms, and shows that it outperforms existing algorithms in the case of nonlinearity. "
9164,SP:327d606cf3813b00a009a7785e08ef9e11f89493,"intrinsic semantic regularities PART-OF man - made environments. multi - target sub - policy CONJUNCTION Bayesian model. Bayesian model CONJUNCTION multi - target sub - policy. visual inputs USED-FOR multi - target sub - policy. semantic structures USED-FOR Bayesian model. Bayesian model PART-OF LEArning and Planning with Semantics ( LEAPS ). multi - target sub - policy PART-OF LEArning and Planning with Semantics ( LEAPS ). House3D HYPONYM-OF 3D environment. real - world objects FEATURE-OF human - designed indoor scenes. human - designed indoor scenes PART-OF 3D environment. House3D USED-FOR visual navigation tasks. LEAPS COMPARE baselines. baselines COMPARE LEAPS. semantic content USED-FOR baselines. Method are deep reinforcement learning agents, semantic model, and sub - policy. Task is AI. ","This paper proposes a method for learning a semantic regularizer for deep reinforcement learning agents. The key idea is to learn a semantic model of the environment and use it to train a multi-target sub-policy and a Bayesian model to predict the performance of a sub-sub-policy from visual inputs. The semantic structures of the environments are learned via the Bayesian network. Experiments are conducted on a 3D environment called House3D, which consists of human-designed indoor scenes with real-world objects. The paper shows that the proposed method LEAPS outperforms baselines that do not consider the semantic content of the scene. The authors also conduct experiments on visual navigation tasks on House3d.","This paper proposes a method for learning a semantic regularizer for deep reinforcement learning agents. The key idea is to learn a semantic model of the environment and use it to train a multi-target sub-policy and a Bayesian model to predict the performance of a sub-sub-policy from visual inputs. The semantic structures of the environments are learned via the Bayesian network. Experiments are conducted on a 3D environment called House3D, which consists of human-designed indoor scenes with real-world objects. The paper shows that the proposed method LEAPS outperforms baselines that do not consider the semantic content of the scene. The authors also conduct experiments on visual navigation tasks on House3d."
9173,SP:d7c26f43bc68d160095b1f50447528843d79edbd,"multi - task perception - related basic knowledge CONJUNCTION driving knowledge. driving knowledge CONJUNCTION multi - task perception - related basic knowledge. perception module CONJUNCTION driving module. driving module CONJUNCTION perception module. perception module PART-OF driving model. driving module PART-OF driving model. driving knowledge USED-FOR it. multi - task perception - related basic knowledge USED-FOR it. segmentation map CONJUNCTION depth map. depth map CONJUNCTION segmentation map. control commands USED-FOR difficult driving task. depth map USED-FOR easier drivingrelated perception problems. depth map CONJUNCTION pixel level understanding of images. pixel level understanding of images CONJUNCTION depth map. generalization CONJUNCTION accident explanation ability. accident explanation ability CONJUNCTION generalization. multitask perception knowledge USED-FOR accident explanation ability. multitask perception knowledge USED-FOR generalization. method COMPARE benchmark method. benchmark method COMPARE method. average sucess rate EVALUATE-FOR navigation tasks. average sucess rate EVALUATE-FOR benchmark method. trained weather CONJUNCTION untrained weathers. untrained weathers CONJUNCTION trained weather. method USED-FOR navigation tasks. average sucess rate EVALUATE-FOR method. Method are deep learning driving models, and driving models. OtherScientificTerm is unobserved driving environment. Material is diversity of training driving dataset. ","This paper proposes a method to improve the generalization ability of deep learning driving models in the presence of unobserved driving environment. Specifically, it uses multi-task perception-related basic knowledge (segmentation map, depth map, driving knowledge) and driving knowledge to train a driving model that combines a perception module and a driving module. The segmentation map and depth map are used for easier drivingrelated perception problems, while the driving module is used for difficult driving task with control commands. Experiments show that the proposed method improves the average sucess rate on navigation tasks, generalization and accident explanation ability using multitask perception knowledge. The method is also shown to outperform a benchmark method that does not use depth map and pixel level understanding of images. The authors also show that trained weather and untrained weathers are more diverse than trained weather, suggesting that the diversity of training driving dataset is important. ","This paper proposes a method to improve the generalization ability of deep learning driving models in the presence of unobserved driving environment. Specifically, it uses multi-task perception-related basic knowledge (segmentation map, depth map, driving knowledge) and driving knowledge to train a driving model that combines a perception module and a driving module. The segmentation map and depth map are used for easier drivingrelated perception problems, while the driving module is used for difficult driving task with control commands. Experiments show that the proposed method improves the average sucess rate on navigation tasks, generalization and accident explanation ability using multitask perception knowledge. The method is also shown to outperform a benchmark method that does not use depth map and pixel level understanding of images. The authors also show that trained weather and untrained weathers are more diverse than trained weather, suggesting that the diversity of training driving dataset is important. "
9182,SP:b6bd98cc70fab97e1245cbb63a42ef89ab7e7ed5,adversarial robustness CONJUNCTION generalization. generalization CONJUNCTION adversarial robustness. accuracy EVALUATE-FOR model. adversarial perturbations FEATURE-OF robustness. robustness EVALUATE-FOR model. robust classifiers COMPARE classifiers. classifiers COMPARE robust classifiers. robust classifiers USED-FOR feature representations. feature representations COMPARE classifiers. classifiers COMPARE feature representations. salient data characteristics CONJUNCTION human perception. human perception CONJUNCTION salient data characteristics. robust models USED-FOR features. ,"This paper studies the relationship between adversarial robustness and generalization. The authors show that robustness to adversarial perturbations is correlated with the accuracy of the model, and that robust classifiers are able to learn feature representations that are more robust than standard classifiers. They also show that features learned by robust models are more sensitive to salient data characteristics and human perception. ","This paper studies the relationship between adversarial robustness and generalization. The authors show that robustness to adversarial perturbations is correlated with the accuracy of the model, and that robust classifiers are able to learn feature representations that are more robust than standard classifiers. They also show that features learned by robust models are more sensitive to salient data characteristics and human perception. "
9191,SP:9c9275d75cd95b1b82e0cbb1421e3d3ade1ce33a,"backpropagation HYPONYM-OF reverse - mode automatic differentiation. reverse - mode automatic differentiation USED-FOR Deep neural networks. method USED-FOR gradient - based training of neural networks. Equilibrium Propagation USED-FOR gradient - based training of neural networks. local learning rules USED-FOR gradient - based training of neural networks. local learning rules USED-FOR method. iterative optimization of neural activations USED-FOR inference. iterative inference procedure USED-FOR Equilibrium propagation. feedforward network USED-FOR iterative inference procedure. feedforward network USED-FOR Initialized Equilibrium Propagation. local learning rule USED-FOR feed - forward network. initializing network USED-FOR inference. initializing network USED-FOR feedforward network. network COMPARE Equilibrium propagation. Equilibrium propagation COMPARE network. backpropagation USED-FOR deep networks. Method is Biological networks. OtherScientificTerm are gradients, neurons, and error gradient. ","Deep neural networks are trained using reverse-mode automatic differentiation (backpropagation). This paper proposes a method based on the Equilibrium Propagation to learn local learning rules for gradient-based training of neural networks. The key idea is to use the iterative optimization of neural activations during inference as an iterative inference procedure similar to that of Equilibrium propagation. The main difference is that a feedforward network is used to learn the local learning rule for the iteratively iterated iterations of the feed-forward network.    The authors show that the feedforward neural network can be trained to learn Initialized Equilibrium Proposition (i.e., a network that learns a local solution to the gradient of the gradients of all neurons in the network).   They also show that this network is able to learn a local version of the local solution that is more robust to perturbations in the weights of the network. The authors also demonstrate that the initializing network that is used for inference can be learned to be more robust than the one that was used for training the feed forward network, and that the network can learn a more robust local solution than the original one.  Finally, the authors demonstrate that for deep networks trained with backpropagating, the network trained with the proposed network outperforms the one trained with Equ equilibrium propagation in terms of the error gradient.","Deep neural networks are trained using reverse-mode automatic differentiation (backpropagation). This paper proposes a method based on the Equilibrium Propagation to learn local learning rules for gradient-based training of neural networks. The key idea is to use the iterative optimization of neural activations during inference as an iterative inference procedure similar to that of Equilibrium propagation. The main difference is that a feedforward network is used to learn the local learning rule for the iteratively iterated iterations of the feed-forward network.    The authors show that the feedforward neural network can be trained to learn Initialized Equilibrium Proposition (i.e., a network that learns a local solution to the gradient of the gradients of all neurons in the network).   They also show that this network is able to learn a local version of the local solution that is more robust to perturbations in the weights of the network. The authors also demonstrate that the initializing network that is used for inference can be learned to be more robust than the one that was used for training the feed forward network, and that the network can learn a more robust local solution than the original one.  Finally, the authors demonstrate that for deep networks trained with backpropagating, the network trained with the proposed network outperforms the one trained with Equ equilibrium propagation in terms of the error gradient."
9200,SP:ac9ea91eb465517de495477cf67bc94d5ed1b0cb,gradient - free operations CONJUNCTION signSGD. signSGD CONJUNCTION gradient - free operations. gradient - free operations PART-OF ZO - signSGD. signSGD PART-OF ZO - signSGD. latter COMPARE SGD - type algorithms. SGD - type algorithms COMPARE latter. convergence speed EVALUATE-FOR SGD - type algorithms. convergence speed EVALUATE-FOR latter. sign information of gradient estimates USED-FOR latter. ZO - signSGD COMPARE signSGD. signSGD COMPARE ZO - signSGD. gradient estimators USED-FOR ZO - signSGD. gradient estimators USED-FOR convergence. ZO - signSGD CONJUNCTION black - box adversarial attacks. black - box adversarial attacks CONJUNCTION ZO - signSGD. ZO - signSGD USED-FOR robust deep learning. black - box adversarial attacks USED-FOR robust deep learning. ZO - signSGD USED-FOR generation of adversarial examples. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. image classification datasets EVALUATE-FOR ZO - signSGD. CIFAR-10 HYPONYM-OF image classification datasets. black - box neural networks USED-FOR generation of adversarial examples. Metric is convergence rate. OtherScientificTerm is optimization variables. ,"This paper proposes ZO-signSGD, which combines gradient-free operations with signSGD to improve the convergence speed of SGD-type algorithms. The latter uses the sign information of gradient estimates to guide the convergence rate. The authors show that the convergence is faster than the previous state-of-the-art result (ZO-SGD) when the optimization variables are sign-free. They also show that using gradient estimators that are more sensitive to the sign of the gradient is also beneficial to the convergence. The paper also shows that ZO - sign SGD is more robust to black-box adversarial attacks for the generation of adversarial examples in black- box neural networks. Experiments on two image classification datasets (MNIST and CIFAR-10) show that on MNIST, the authors demonstrate that the gradient estimator used for the convergence improves the performance.   ","This paper proposes ZO-signSGD, which combines gradient-free operations with signSGD to improve the convergence speed of SGD-type algorithms. The latter uses the sign information of gradient estimates to guide the convergence rate. The authors show that the convergence is faster than the previous state-of-the-art result (ZO-SGD) when the optimization variables are sign-free. They also show that using gradient estimators that are more sensitive to the sign of the gradient is also beneficial to the convergence. The paper also shows that ZO - sign SGD is more robust to black-box adversarial attacks for the generation of adversarial examples in black- box neural networks. Experiments on two image classification datasets (MNIST and CIFAR-10) show that on MNIST, the authors demonstrate that the gradient estimator used for the convergence improves the performance.   "
9209,SP:5f79b11777f6ef1d70c85418bfc2e4616dd7d960,"Deep learning USED-FOR artificial intelligence applications. energy - limited edge device USED-FOR complex neural network model. optimization method USED-FOR convolutional neural networks. multiply - accumulate ( MAC ) operations PART-OF convolutional filter. checkpoint USED-FOR MAC process. fine - tuning process USED-FOR accuracy drop. CIFAR-10 example model CONJUNCTION Network in Network. Network in Network CONJUNCTION CIFAR-10 example model. MAC operations EVALUATE-FOR method. CIFAR-10 and CIFAR-100 datasets EVALUATE-FOR Network in Network. accuracy drop EVALUATE-FOR method. method COMPARE method. method COMPARE method. CIFAR-100 dataset EVALUATE-FOR method. CIFAR-10 dataset EVALUATE-FOR method. CIFAR-10 dataset EVALUATE-FOR method. Method is convolutional operations. OtherScientificTerm are activation or pooling layers, filter, and checkpoints. ","This paper proposes a new optimization method for training convolutional neural networks.   Deep learning is an important problem in many artificial intelligence applications, especially in the context of energy-limited edge device, where a complex neural network model is required to be trained on a small amount of data.  This paper proposes to add multiply-accommodate (MAC) operations to the convolution filter, which is a simple optimization method that can be applied to any convolution filters.  The idea is to add a checkpoint to each layer of the filter during the optimization of the MAC process, so that the number of operations in the filter does not increase too much during the fine-tuning process.  In order to do so, the authors propose a simple modification to the original convolution operations, where the activation or pooling layers are replaced by a filter that has a fixed number of layers.  Experiments on the CIFAR-10 example model and the Network in Network are conducted on both the standard and the more complex CIFar-10 and CIFARS-100 datasets.  Results show that the proposed method is able to achieve a small accuracy drop in terms of MAC operations compared to the previous method, and that the accuracy drop is even larger when the filter is trained with multiple checkpoints. ","This paper proposes a new optimization method for training convolutional neural networks.   Deep learning is an important problem in many artificial intelligence applications, especially in the context of energy-limited edge device, where a complex neural network model is required to be trained on a small amount of data.  This paper proposes to add multiply-accommodate (MAC) operations to the convolution filter, which is a simple optimization method that can be applied to any convolution filters.  The idea is to add a checkpoint to each layer of the filter during the optimization of the MAC process, so that the number of operations in the filter does not increase too much during the fine-tuning process.  In order to do so, the authors propose a simple modification to the original convolution operations, where the activation or pooling layers are replaced by a filter that has a fixed number of layers.  Experiments on the CIFAR-10 example model and the Network in Network are conducted on both the standard and the more complex CIFar-10 and CIFARS-100 datasets.  Results show that the proposed method is able to achieve a small accuracy drop in terms of MAC operations compared to the previous method, and that the accuracy drop is even larger when the filter is trained with multiple checkpoints. "
9218,SP:7801e9c854ad7d960c0d24fda15597af6994c23f,"adversarial examples USED-FOR neural network models. unique data properties USED-FOR learning principles. temporal dependency USED-FOR adversarial examples. temporal dependency USED-FOR discriminate power. temporal dependency FEATURE-OF audio data. automatic speech recognition ( ASR ) tasks CONJUNCTION audio adversarial attacks. audio adversarial attacks CONJUNCTION automatic speech recognition ( ASR ) tasks. temporal dependency USED-FOR discriminative power. image adversarial defense USED-FOR input transformation. robustness EVALUATE-FOR ASR systems. domain - specific data properties USED-FOR adversarial examples. OtherScientificTerm are adversarial inputs, audio adversarial examples, and adaptive attacks. ","This paper studies the problem of generating adversarial examples for neural network models that are robust to adversarial inputs. The authors propose two learning principles based on unique data properties. First, they show that the discriminative power depends on the temporal dependency of the audio data. Second, they propose an image adversarial defense for input transformation that is based on the observation that temporal dependency on the input transformation can be used to improve the discriminate power. They demonstrate the effectiveness of their approach on both automatic speech recognition (ASR) tasks and audio adversarial attacks. They also show that adversarial robustness of ASR systems can be improved by incorporating domain-specific data properties to generate more robust examples. Finally, the authors show that audio robustness can be enhanced by generating more robust and adaptive attacks.  ","This paper studies the problem of generating adversarial examples for neural network models that are robust to adversarial inputs. The authors propose two learning principles based on unique data properties. First, they show that the discriminative power depends on the temporal dependency of the audio data. Second, they propose an image adversarial defense for input transformation that is based on the observation that temporal dependency on the input transformation can be used to improve the discriminate power. They demonstrate the effectiveness of their approach on both automatic speech recognition (ASR) tasks and audio adversarial attacks. They also show that adversarial robustness of ASR systems can be improved by incorporating domain-specific data properties to generate more robust examples. Finally, the authors show that audio robustness can be enhanced by generating more robust and adaptive attacks.  "
9227,SP:51830b811a8e39b4f0a5b7609df719e026fac6a1,"They USED-FOR representations. images USED-FOR approaches. core inductive biases USED-FOR approaches. generator USED-FOR GAN. composition USED-FOR images. real - world images USED-FOR generative model. multi - object image datasets EVALUATE-FOR approach. generative model USED-FOR images. reference distribution FEATURE-OF images. Method are Deep generative models, and object representations. OtherScientificTerm is representational level. ","Deep generative models are an important problem in many applications. They are able to learn representations that are highly interpretable and interpretable. However, existing approaches rely on images that are generated from core inductive biases. This paper proposes to learn a generative model from real-world images. The key idea is that the generator of a GAN should be able to generate images from a composition that is interpretable at a representational level. The approach is evaluated on multi-object image datasets, and the results show that the generated images are interpretable in terms of their representation. The authors also show that images generated from the generative process are invariant to changes in the reference distribution, and that the resulting images are similar to the original images. ","Deep generative models are an important problem in many applications. They are able to learn representations that are highly interpretable and interpretable. However, existing approaches rely on images that are generated from core inductive biases. This paper proposes to learn a generative model from real-world images. The key idea is that the generator of a GAN should be able to generate images from a composition that is interpretable at a representational level. The approach is evaluated on multi-object image datasets, and the results show that the generated images are interpretable in terms of their representation. The authors also show that images generated from the generative process are invariant to changes in the reference distribution, and that the resulting images are similar to the original images. "
9236,SP:fb59990b8da0e95d8202383478a456667de60449,"Learning disentangled representations USED-FOR computer vision tasks. visual data USED-FOR Learning disentangled representations. referencebased disentangling HYPONYM-OF learning setting. deep generative model USED-FOR weak supervisory signal. reference - based variational autoencoders HYPONYM-OF deep generative model. reference set USED-FOR weak supervisory signal. adversarial learning USED-FOR objective function. adversarial learning USED-FOR variational inference framework. variational inference framework USED-FOR training. model USED-FOR disentangled representations. feature learning CONJUNCTION conditional image generation. conditional image generation CONJUNCTION feature learning. conditional image generation CONJUNCTION attribute transfer. attribute transfer CONJUNCTION conditional image generation. tasks EVALUATE-FOR model. minimal supervision USED-FOR model. attribute transfer HYPONYM-OF tasks. feature learning HYPONYM-OF tasks. conditional image generation HYPONYM-OF tasks. minimal supervision USED-FOR disentangled representations. OtherScientificTerm are high - level generative factors, target factors, supervision, and factors of interest. Method is Supervised approaches. Generic is representation. ","Learning disentangled representations for computer vision tasks on visual data is an important problem. In this paper, the authors propose a novel learning setting called referencebased disentangling, where the high-level generative factors are not available, but the target factors are. Supervised approaches to disentangle these factors have been shown to perform well in the literature.  This paper proposes a deep generative model, called reference-based variational autoencoders, to learn a weak supervisory signal from a reference set. During training, a variational inference framework based on adversarial learning is used to learn the objective function. The authors show that the proposed model is able to learn disentanged representations with minimal supervision on three tasks: feature learning, conditional image generation, and attribute transfer. They also show that this representation can be learned without any additional supervision. ","Learning disentangled representations for computer vision tasks on visual data is an important problem. In this paper, the authors propose a novel learning setting called referencebased disentangling, where the high-level generative factors are not available, but the target factors are. Supervised approaches to disentangle these factors have been shown to perform well in the literature.  This paper proposes a deep generative model, called reference-based variational autoencoders, to learn a weak supervisory signal from a reference set. During training, a variational inference framework based on adversarial learning is used to learn the objective function. The authors show that the proposed model is able to learn disentanged representations with minimal supervision on three tasks: feature learning, conditional image generation, and attribute transfer. They also show that this representation can be learned without any additional supervision. "
9245,SP:dbc1983d9b9d72aa14f8e8515d793d2bbde26c9c,"Deep neural network models USED-FOR rapid online adaptation. method USED-FOR continual online learning. deep neural network models USED-FOR method. mixture of models USED-FOR non - stationary task distributions. expectation maximization algorithm USED-FOR mixture of models. stochastic gradient descent USED-FOR model parameters. expectation maximization algorithm USED-FOR non - stationary task distributions. Chinese restaurant process USED-FOR expectation maximization algorithm. stochastic gradient descent USED-FOR online learning procedure. models COMPARE models. models COMPARE models. meta - learning USED-FOR model. SGD USED-FOR online adaptation. meta - learning USED-FOR online learning ( MOLe ) approach. motor failures CONJUNCTION unexpected disturbances. unexpected disturbances CONJUNCTION motor failures. varying terrains CONJUNCTION motor failures. motor failures CONJUNCTION varying terrains. MOLe COMPARE prior methods. prior methods COMPARE MOLe. MOLe USED-FOR continuous adaptation. continuous adaptation USED-FOR non - stationary task distributions. predictive model USED-FOR control. meta - learning USED-FOR model - based reinforcement learning. online learning ( MOLe ) approach USED-FOR model - based reinforcement learning. unexpected disturbances HYPONYM-OF non - stationary task distributions. varying terrains HYPONYM-OF non - stationary task distributions. motor failures HYPONYM-OF non - stationary task distributions. Method are predictive models, and large function approximators. OtherScientificTerm is real - world phenomena. ","This paper proposes a method for continual online learning based on deep neural network models for rapid online adaptation. Specifically, a mixture of models is used to approximate non-stationary task distributions using an expectation maximization algorithm based on the Chinese restaurant process. The online learning procedure is based on stochastic gradient descent to update the model parameters. The authors show that the proposed online learning (MOLe) approach uses meta-learning to adapt the model to a new task using SGD. They also show that their MOLe outperforms prior methods for continuous adaptation to non-stochastic task distributions (e.g. varying terrains, motor failures, and unexpected disturbances). They also demonstrate that the predictive model can be used for control, which is a useful tool for real-world phenomena. ","This paper proposes a method for continual online learning based on deep neural network models for rapid online adaptation. Specifically, a mixture of models is used to approximate non-stationary task distributions using an expectation maximization algorithm based on the Chinese restaurant process. The online learning procedure is based on stochastic gradient descent to update the model parameters. The authors show that the proposed online learning (MOLe) approach uses meta-learning to adapt the model to a new task using SGD. They also show that their MOLe outperforms prior methods for continuous adaptation to non-stochastic task distributions (e.g. varying terrains, motor failures, and unexpected disturbances). They also demonstrate that the predictive model can be used for control, which is a useful tool for real-world phenomena. "
9254,SP:5665e5f006f84927beb0440e145f476e02538077,"distributed prioritized experience replay USED-FOR RNN - based RL agents. representational drift CONJUNCTION recurrent state staleness. recurrent state staleness CONJUNCTION representational drift. parameter lag USED-FOR representational drift. single network architecture CONJUNCTION hyperparameters. hyperparameters CONJUNCTION single network architecture. single network architecture USED-FOR agent. Recurrent Replay Distributed DQN HYPONYM-OF agent. It HYPONYM-OF agent. human - level performance EVALUATE-FOR It. human - level performance EVALUATE-FOR agent. Atari games EVALUATE-FOR It. Method are distributed training of RL agents, and training strategy. Material are Atari-57, and DMLab-30. ","This paper studies the problem of distributed prioritized experience replay for RNN-based RL agents. The authors propose a new training strategy, called Recurrent Replay Distributed DQN, to address the issue of representational drift and recurrent state staleness in distributed training of RL agents due to parameter lag. The agent is trained with a single network architecture and hyperparameters. It is shown to achieve human-level performance on two Atari games, Atari-57 and DMLab-30. ","This paper studies the problem of distributed prioritized experience replay for RNN-based RL agents. The authors propose a new training strategy, called Recurrent Replay Distributed DQN, to address the issue of representational drift and recurrent state staleness in distributed training of RL agents due to parameter lag. The agent is trained with a single network architecture and hyperparameters. It is shown to achieve human-level performance on two Atari games, Atari-57 and DMLab-30. "
9263,SP:47ace37f31a46d5ee85c283e62ddb71a12f2c5c4,sequential generative models USED-FOR coordinated multi - agent trajectory behavior. offensive basketball gameplay HYPONYM-OF coordinated multi - agent trajectory behavior. hierarchical models USED-FOR long - term coordination. hierarchical models USED-FOR settings. intermediate variables USED-FOR hierarchical models. intermediate variables USED-FOR high - level behavioral semantics. hierarchical framework USED-FOR sequential generative models. programmatically produced weak labels USED-FOR spatiotemporal regime. programmatically produced weak labels USED-FOR approach. framework USED-FOR complex interactions between basketball players. framework USED-FOR realistic multi - agent trajectories of basketball gameplay. quantitative and qualitative evaluations EVALUATE-FOR approach. OtherScientificTerm is synthetic settings. ,"This paper proposes a hierarchical generative models for coordinated multi-agent trajectory behavior (e.g., offensive basketball gameplay). The authors propose a hierarchical framework for learning sequential generative model for long-term coordination between agents. They show that hierarchical models for these settings can be trained with intermediate variables that capture high-level behavioral semantics. The proposed approach is based on programmatically produced weak labels for the spatiotemporal regime. The authors show that the framework can capture complex interactions between basketball players and demonstrate that the proposed framework is able to capture realistic multimodal trajectories of basketball gameplay with quantitative and qualitative evaluations on synthetic settings. ","This paper proposes a hierarchical generative models for coordinated multi-agent trajectory behavior (e.g., offensive basketball gameplay). The authors propose a hierarchical framework for learning sequential generative model for long-term coordination between agents. They show that hierarchical models for these settings can be trained with intermediate variables that capture high-level behavioral semantics. The proposed approach is based on programmatically produced weak labels for the spatiotemporal regime. The authors show that the framework can capture complex interactions between basketball players and demonstrate that the proposed framework is able to capture realistic multimodal trajectories of basketball gameplay with quantitative and qualitative evaluations on synthetic settings. "
9272,SP:1a90cdf028068528b0559e7d44bf26dda20310bd,vision model USED-FOR interacting agents. method USED-FOR temporal information. ambiguous visual information USED-FOR dynamics model. dynamics model USED-FOR method. method COMPARE baselines. baselines COMPARE method. one CONJUNCTION one. one CONJUNCTION one. one EVALUATE-FOR method. one EVALUATE-FOR method. soccer game engine USED-FOR one. real basketball trajectories USED-FOR one. one HYPONYM-OF sports datasets. one HYPONYM-OF sports datasets. sports datasets EVALUATE-FOR method. sports datasets EVALUATE-FOR baselines. ,"This paper proposes a new vision model for interacting agents. The proposed method learns a dynamics model from ambiguous visual information and uses this method to capture temporal information. The method is evaluated on two sports datasets, one using a soccer game engine and one using real basketball trajectories, and shows that the proposed method outperforms the baselines on both sports datasets.","This paper proposes a new vision model for interacting agents. The proposed method learns a dynamics model from ambiguous visual information and uses this method to capture temporal information. The method is evaluated on two sports datasets, one using a soccer game engine and one using real basketball trajectories, and shows that the proposed method outperforms the baselines on both sports datasets."
9281,SP:8392f04b7265f665ba6d44d297bca245d44b4708,"Deep neural networks USED-FOR approximating complicated functions. gradient descent methods USED-FOR Deep neural networks. neural network USED-FOR functionality. method USED-FOR end - to - end training. base neural network USED-FOR end - to - end training. method USED-FOR base neural network. differentiable neural network USED-FOR black - box functionality. differentiable estimator CONJUNCTION external blackbox non - differentiable counterpart. external blackbox non - differentiable counterpart CONJUNCTION differentiable estimator. neural network USED-FOR input to blackbox functionality. Estimate and Replace ” paradigm USED-FOR neural network. black - box function USED-FOR integrated model. integrated model COMPARE fully differentiable model. fully differentiable model COMPARE integrated model. black - box function USED-FOR inference. black - box function USED-FOR fully differentiable model. integrated model COMPARE RL - based methods. RL - based methods COMPARE integrated model. Task are training, and end - to - end optimization process. Generic is task. OtherScientificTerm are black - box functions, blackbox functions, black - box function interface, and intermediate labels. Method is base network. ","Deep neural networks are well known for approximating complicated functions using gradient descent methods. Deep neural networks can be trained end-to-end, but training can be expensive due to the large number of black-box functions. This paper proposes a novel method to perform a base neural network with a differentiable neural network that can be used to learn a specific functionality from the input to the output of the neural network. The proposed method can be applied to any end- to-end training where the input function is not available in the input space.    The authors propose a method to train a base network that is differentiable for a specific task, and then replace the input with a sequence of blackbox functions that are present in the blackbox function interface. The idea is that a neural network trained with the “Estimate and Replace” paradigm is able to learn an input to blackbox functionality from a single neural network to an input with multiple functions. The differentiable estimator is trained with an external blackbox non-differentiable counterpart. The authors show that the integrated model trained with a black-board function is more efficient than a fully differentiable model that uses a black -box function for inference. The integrated model is also able to be trained in a way that avoids the need for an end-of-end optimization process, which is common in RL-based methods.  The main contribution of the paper is that the authors propose to replace the base network with an input function that is present in a specific function in a particular function interface, and that the differentiable network is trained to be able to adapt to this input to a specific input function. The paper also proposes a way to handle the intermediate labels. ","Deep neural networks are well known for approximating complicated functions using gradient descent methods. Deep neural networks can be trained end-to-end, but training can be expensive due to the large number of black-box functions. This paper proposes a novel method to perform a base neural network with a differentiable neural network that can be used to learn a specific functionality from the input to the output of the neural network. The proposed method can be applied to any end- to-end training where the input function is not available in the input space.    The authors propose a method to train a base network that is differentiable for a specific task, and then replace the input with a sequence of blackbox functions that are present in the blackbox function interface. The idea is that a neural network trained with the “Estimate and Replace” paradigm is able to learn an input to blackbox functionality from a single neural network to an input with multiple functions. The differentiable estimator is trained with an external blackbox non-differentiable counterpart. The authors show that the integrated model trained with a black-board function is more efficient than a fully differentiable model that uses a black -box function for inference. The integrated model is also able to be trained in a way that avoids the need for an end-of-end optimization process, which is common in RL-based methods.  The main contribution of the paper is that the authors propose to replace the base network with an input function that is present in a specific function in a particular function interface, and that the differentiable network is trained to be able to adapt to this input to a specific input function. The paper also proposes a way to handle the intermediate labels. "
9290,SP:13fb86de763a0b34ac6fa34ea9dfbd1c476ce43e,learning USED-FOR task. data - driven inductive bias USED-FOR learning. gradient - based meta - learning CONJUNCTION hierarchical Bayes. hierarchical Bayes CONJUNCTION gradient - based meta - learning. function approximator USED-FOR mixture of hierarchical Bayesian models. neural network HYPONYM-OF function approximator. stochastic expectation maximization procedure USED-FOR parameter initializations. parameter initializations USED-FOR gradient descent. stochastic expectation maximization procedure USED-FOR gradient descent. stochastic expectation maximization procedure USED-FOR latent assignment of tasks. initializations USED-FOR latent assignment of tasks. approach USED-FOR diversity of training tasks. inductive biases PART-OF hyperparameters. miniImageNet benchmark EVALUATE-FOR 1 - shot classification. miniImageNet benchmark EVALUATE-FOR generalization. method USED-FOR task distribution. non - parametric variant USED-FOR task distribution. method USED-FOR non - parametric variant. few - shot regression tasks EVALUATE-FOR non - parametric variant. OtherScientificTerm is transfer. ,"This paper proposes a data-driven inductive bias for learning for a new task. The authors combine gradient-based meta-learning and hierarchical Bayes, where a function approximator (i.e., a neural network) is used to approximate a mixture of hierarchical Bayesian models, and a stochastic expectation maximization procedure is used for parameter initializations for gradient descent. These initializations are used for the latent assignment of tasks, and the authors show that the proposed approach is able to improve the diversity of training tasks by incorporating inductive biases into the hyperparameters. Experiments on the miniImageNet benchmark for 1-shot classification show improved generalization on the transfer from one task to another. A non-parametric variant of the proposed method is also applied to the task distribution for a few-shot regression tasks. ","This paper proposes a data-driven inductive bias for learning for a new task. The authors combine gradient-based meta-learning and hierarchical Bayes, where a function approximator (i.e., a neural network) is used to approximate a mixture of hierarchical Bayesian models, and a stochastic expectation maximization procedure is used for parameter initializations for gradient descent. These initializations are used for the latent assignment of tasks, and the authors show that the proposed approach is able to improve the diversity of training tasks by incorporating inductive biases into the hyperparameters. Experiments on the miniImageNet benchmark for 1-shot classification show improved generalization on the transfer from one task to another. A non-parametric variant of the proposed method is also applied to the task distribution for a few-shot regression tasks. "
9299,SP:a410144dbe19713a06c63da87d9fb58b999a7492,Auxiliary learning USED-FOR principal task. domain knowledge USED-FOR manually - defined auxiliary tasks. auxiliary tasks USED-FOR auxiliary tasks. Meta Auxiliary Learning ( MAXL ) USED-FOR image classification. hierarchical sub - class image classification HYPONYM-OF auxiliary task. meta learner USED-FOR sub - class target labels. meta learner USED-FOR multi - task evaluator. MAXL COMPARE baseline auxiliary learning methods. baseline auxiliary learning methods COMPARE MAXL. CIFAR datasets EVALUATE-FOR MAXL. MAXL COMPARE method. method COMPARE MAXL. CIFAR datasets EVALUATE-FOR baseline auxiliary learning methods. human - defined sub - class hierarchies USED-FOR method. MAXL USED-FOR automated generalisation. OtherScientificTerm is human knowledge. ,"This paper proposes Meta Auxiliary Learning (MAXL) for image classification. Auxiliary learning is used to augment the principal task with a set of auxiliary tasks that are based on domain knowledge. The auxiliary task is hierarchical sub-class image classification, where the auxiliary tasks are manually-defined. A meta learner is trained to learn the sub-classes of the auxiliary task, and a multi-task evaluator is trained using the learned meta learners. The proposed MAXL outperforms several baseline auxiliary learning methods on CIFAR datasets, and is shown to generalize better than a simple method that does not rely on human-defined auxiliary tasks. The authors also show that the proposed method can be applied to automated generalisation in the presence of human knowledge.   ","This paper proposes Meta Auxiliary Learning (MAXL) for image classification. Auxiliary learning is used to augment the principal task with a set of auxiliary tasks that are based on domain knowledge. The auxiliary task is hierarchical sub-class image classification, where the auxiliary tasks are manually-defined. A meta learner is trained to learn the sub-classes of the auxiliary task, and a multi-task evaluator is trained using the learned meta learners. The proposed MAXL outperforms several baseline auxiliary learning methods on CIFAR datasets, and is shown to generalize better than a simple method that does not rely on human-defined auxiliary tasks. The authors also show that the proposed method can be applied to automated generalisation in the presence of human knowledge.   "
9308,SP:76248e1c914c60ce69de244fe7ec62488d01e161,neural network based representation USED-FOR open set recognition problem. datasets EVALUATE-FOR approaches. Generic is representation. ,This paper proposes a neural network based representation for the open set recognition problem. The key idea of the paper is to learn a representation of the set of open sets. The paper shows that the proposed approaches outperform existing approaches on two datasets.    The paper is well-written and well-motivated. ,This paper proposes a neural network based representation for the open set recognition problem. The key idea of the paper is to learn a representation of the set of open sets. The paper shows that the proposed approaches outperform existing approaches on two datasets.    The paper is well-written and well-motivated. 
9317,SP:d4ee856bbf2dfb6390e5247086fec2e52dcb6858,"ResNet-34 CONJUNCTION ResNet-50. ResNet-50 CONJUNCTION ResNet-34. ResNet-50 CONJUNCTION ResNet-152. ResNet-152 CONJUNCTION ResNet-50. Inception - v3 CONJUNCTION densenet161. densenet161 CONJUNCTION Inception - v3. densenet161 CONJUNCTION VGG-16bn networks. VGG-16bn networks CONJUNCTION densenet161. ResNet-152 CONJUNCTION Inception - v3. Inception - v3 CONJUNCTION ResNet-152. accuracy EVALUATE-FOR full - precision baseline networks. finetuning USED-FOR full - precision baseline networks. ImageNet classification benchmark EVALUATE-FOR VGG-16bn networks. ResNet-18 CONJUNCTION ResNet-34. ResNet-34 CONJUNCTION ResNet-18. accuracy EVALUATE-FOR full - precision baseline networks. stochastic gradient descent USED-FOR training error. pretrained fp32 precision baseline networks CONJUNCTION fine - tuning. fine - tuning CONJUNCTION pretrained fp32 precision baseline networks. pretrained fp32 precision baseline networks USED-FOR solution distance. matched learning rate annealing USED-FOR combat noise. techniques USED-FOR low - precision networks. techniques CONJUNCTION activation function range calibration. activation function range calibration CONJUNCTION techniques. activation function range calibration USED-FOR low - precision networks. Task is embedded deep network inference. Metric are energy and area efficiency, and energy. Method are pretrained models, and fp32 precision baseline networks. Generic are 4 - bit models, baseline networks, noise, and they. OtherScientificTerm are cosine similarity, gradient noise, quantization, and maximum variance of the gradient estimates. ","This paper studies the problem of energy and area efficiency in embedded deep network inference. The authors consider the case of 4-bit models where the cosine similarity between the input and the output of the network is a function of the energy and the number of bits used to compute the solution. In this setting, the authors show that the accuracy of full-precision baseline networks trained with finetuning (ResNet-34, ResNet-50, Resnet-152, ResNets-152), Inception-v3, densenet161, and VGG-16bn networks on the ImageNet classification benchmark is comparable to that of pretrained models trained with fp32 precision baseline networks. They also show that training error is bounded by stochastic gradient descent, and that the solution distance between pretrained fp2 precision baseline and fine-tuned networks is bounded.    The authors also propose two techniques to improve the performance of the baseline networks: (1) matching learning rate annealing to combat combat noise, (2) gradient noise quantization, and (3) quantization to reduce the maximum variance of the gradient estimates. They show that these techniques can be used to calibrate the accuracy and energy of the low-probability baseline networks, and they are shown to be effective in practice.","This paper studies the problem of energy and area efficiency in embedded deep network inference. The authors consider the case of 4-bit models where the cosine similarity between the input and the output of the network is a function of the energy and the number of bits used to compute the solution. In this setting, the authors show that the accuracy of full-precision baseline networks trained with finetuning (ResNet-34, ResNet-50, Resnet-152, ResNets-152), Inception-v3, densenet161, and VGG-16bn networks on the ImageNet classification benchmark is comparable to that of pretrained models trained with fp32 precision baseline networks. They also show that training error is bounded by stochastic gradient descent, and that the solution distance between pretrained fp2 precision baseline and fine-tuned networks is bounded.    The authors also propose two techniques to improve the performance of the baseline networks: (1) matching learning rate annealing to combat combat noise, (2) gradient noise quantization, and (3) quantization to reduce the maximum variance of the gradient estimates. They show that these techniques can be used to calibrate the accuracy and energy of the low-probability baseline networks, and they are shown to be effective in practice."
9326,SP:6bfdc37b346e6ddfa049e0414647f4beda8ede3f,approach USED-FOR surface properties. model USED-FOR post - bounce trajectories. bouncing restitution CONJUNCTION effective collision normals. effective collision normals CONJUNCTION bouncing restitution. model USED-FOR physical properties. sensor inputs USED-FOR post - bounce trajectories. physical properties USED-FOR bouncing restitution. physical properties USED-FOR effective collision normals. sensor inputs USED-FOR model. Physics Inference Module ( PIM ) CONJUNCTION Visual Inference Module ( VIM ). Visual Inference Module ( VIM ) CONJUNCTION Physics Inference Module ( PIM ). modules PART-OF model. Bounce PART-OF model. Visual Inference Module ( VIM ) HYPONYM-OF modules. Physics Inference Module ( PIM ) HYPONYM-OF modules. Visual Inference Module ( VIM ) PART-OF model. PIM USED-FOR physical interactions. PIM USED-FOR prediction task. VIM USED-FOR physical parameters. physical parameters CONJUNCTION observed pre - collision 3D trajectories. observed pre - collision 3D trajectories CONJUNCTION physical parameters. physical interactions USED-FOR prediction task. physical parameters USED-FOR PIM. observed pre - collision 3D trajectories USED-FOR PIM. dataset EVALUATE-FOR model. dataset EVALUATE-FOR baselines. predicting post - bounce trajectories CONJUNCTION physical properties. physical properties CONJUNCTION predicting post - bounce trajectories. model COMPARE baselines. baselines COMPARE model. model USED-FOR predicting post - bounce trajectories. model USED-FOR physical properties. trajectory fitting USED-FOR post - bounce trajectories. Newtonian physics USED-FOR trajectory fitting. trajectory fitting HYPONYM-OF baselines. Material is Bounce Dataset. OtherScientificTerm is physics simulations. ,"This paper proposes a new approach to learn surface properties for bouncing trajectories. The authors propose a model that predicts post-bounce trajectories based on sensor inputs and physical properties such as bouncing restitution and effective collision normals. The model is composed of two modules: Physics Inference Module (PIM) and Visual Inference module (VIM). PIM learns physical interactions between physical parameters and observed pre-collision 3D trajectories for the prediction task, and VIM learns to infer physical parameters from the physical parameters of PIM and observed trajectories of the bouncing trajectory. The proposed model is evaluated on a dataset called the Bounce Dataset and compared with several baselines including trajectory fitting based on Newtonian physics. The results show that the proposed model outperforms the baselines in terms of predicting post-batches trajectories and the physical properties.   ","This paper proposes a new approach to learn surface properties for bouncing trajectories. The authors propose a model that predicts post-bounce trajectories based on sensor inputs and physical properties such as bouncing restitution and effective collision normals. The model is composed of two modules: Physics Inference Module (PIM) and Visual Inference module (VIM). PIM learns physical interactions between physical parameters and observed pre-collision 3D trajectories for the prediction task, and VIM learns to infer physical parameters from the physical parameters of PIM and observed trajectories of the bouncing trajectory. The proposed model is evaluated on a dataset called the Bounce Dataset and compared with several baselines including trajectory fitting based on Newtonian physics. The results show that the proposed model outperforms the baselines in terms of predicting post-batches trajectories and the physical properties.   "
9335,SP:010bd055310c363d3cb0fbe0e11546de58220e15,"neural networks USED-FOR adversarial images. gradients USED-FOR adversarial vulnerability. ` 1 - norm FEATURE-OF gradients. OtherScientificTerm are targeted but imperceptible image perturbations, image size, and network ’s weight distribution. Method is network architectures. Generic is nets. ","This paper studies the vulnerability of neural networks to adversarial images. The authors show that the adversarial vulnerability is related to the gradients of the `1-norm of the network’s weight distribution. They also show that adversarial examples are imperceptible in the presence of targeted but imperceptibly image perturbations. The paper also shows that the image size, the number of layers, and the network architectures are the most important factors that affect the vulnerability.  ","This paper studies the vulnerability of neural networks to adversarial images. The authors show that the adversarial vulnerability is related to the gradients of the `1-norm of the network’s weight distribution. They also show that adversarial examples are imperceptible in the presence of targeted but imperceptibly image perturbations. The paper also shows that the image size, the number of layers, and the network architectures are the most important factors that affect the vulnerability.  "
9351,SP:5fa3ae057e55be6b71cc94a7dbfe31e54e1c536f,"agent modeling USED-FOR mind model. probing USED-FOR agent modeling. pure curiosity - driven reinforcement learning USED-FOR probing policy. imitation learning USED-FOR approximated agent model. pure curiosity - driven reinforcement learning HYPONYM-OF learning processes. learning processes PART-OF framework. pure curiosity - driven reinforcement learning PART-OF framework. imitation learning PART-OF framework. imitation learning HYPONYM-OF learning processes. tasks EVALUATE-FOR approach. collaboration CONJUNCTION competition. competition CONJUNCTION collaboration. passive observation CONJUNCTION random probing. random probing CONJUNCTION passive observation. agent model COMPARE ones. ones COMPARE agent model. distilling optimal planning CONJUNCTION collaboration. collaboration CONJUNCTION distilling optimal planning. random probing CONJUNCTION curiositydriven approaches. curiositydriven approaches CONJUNCTION random probing. distilling optimal planning CONJUNCTION policy net. policy net CONJUNCTION distilling optimal planning. curiositydriven approaches USED-FOR ones. random probing USED-FOR agent model. approach USED-FOR agent model. competition HYPONYM-OF applications. distilling optimal planning HYPONYM-OF applications. passive observation USED-FOR agent model. passive observation USED-FOR ones. random probing USED-FOR ones. collaboration HYPONYM-OF applications. Method are interactive agent modeling scheme, and probing agent. ","This paper proposes a new interactive agent modeling scheme that uses probing to learn a mind model for agent modeling. The proposed framework is composed of two learning processes: (1) pure curiosity-driven reinforcement learning that learns a probing policy, and (2) imitation learning to learn an approximated agent model. The approach is evaluated on a variety of tasks, including distilling optimal planning, collaboration, and competition. The agent model trained with the proposed approach outperforms existing ones based on passive observation, random probing, and curiositydriven approaches. ","This paper proposes a new interactive agent modeling scheme that uses probing to learn a mind model for agent modeling. The proposed framework is composed of two learning processes: (1) pure curiosity-driven reinforcement learning that learns a probing policy, and (2) imitation learning to learn an approximated agent model. The approach is evaluated on a variety of tasks, including distilling optimal planning, collaboration, and competition. The agent model trained with the proposed approach outperforms existing ones based on passive observation, random probing, and curiositydriven approaches. "
9367,SP:3af184a5529d6ec2a0862efd1af80ef5b50d2952,"modification USED-FOR Artificial Neural Networks ( ANNs ). Artificial Neural Networks ( ANNs ) USED-FOR ANNs. firing modes FEATURE-OF biological neuron. peripheral factors FEATURE-OF biological neuron. neuromodulators HYPONYM-OF peripheral factors. modification USED-FOR ANN nodes. ANN nodes USED-FOR activation sensitivities. Convolutional Neural Networks CONJUNCTION Long Short - Term Memory networks. Long Short - Term Memory networks CONJUNCTION Convolutional Neural Networks. modification COMPARE ANN nodes. ANN nodes COMPARE modification. ANN nodes USED-FOR Convolutional Neural Networks. ANN nodes USED-FOR Long Short - Term Memory networks. OtherScientificTerm are biological neurons, Biological neurons, biological neuromodulators, modulators, and slope of the activation function. ","This paper proposes a modification to Artificial Neural Networks (ANNs) that is designed to improve the performance of ANNs. The main idea is to modify the firing modes of a biological neuron by modifying the peripheral factors (i.e., neuromodulators) that control the firing of the biological neurons. Biological neurons are modulated by a set of modulators, and the authors show that the modification to ANN nodes can improve the activation sensitivities of biological neurons, and that the slope of the activation function is similar to that of the modulators. The authors also show that this modification is more effective than existing ANN nodes for Convolutional Neural Networks and Long Short-Term Memory networks. ","This paper proposes a modification to Artificial Neural Networks (ANNs) that is designed to improve the performance of ANNs. The main idea is to modify the firing modes of a biological neuron by modifying the peripheral factors (i.e., neuromodulators) that control the firing of the biological neurons. Biological neurons are modulated by a set of modulators, and the authors show that the modification to ANN nodes can improve the activation sensitivities of biological neurons, and that the slope of the activation function is similar to that of the modulators. The authors also show that this modification is more effective than existing ANN nodes for Convolutional Neural Networks and Long Short-Term Memory networks. "
9383,SP:287a577834fd2820a939a1113b39146a22727491,voice CONJUNCTION pitch. pitch CONJUNCTION voice. neural analysis and synthesis ( NANSY ) framework USED-FOR voice. neural analysis and synthesis ( NANSY ) framework USED-FOR pitch. information bottleneck USED-FOR analysis features. analysis features USED-FOR controllable synthesis. information perturbation USED-FOR training strategy. formant CONJUNCTION pitch. pitch CONJUNCTION formant. pitch CONJUNCTION frequency response. frequency response CONJUNCTION pitch. reconstruction quality CONJUNCTION controllability. controllability CONJUNCTION reconstruction quality. controllability EVALUATE-FOR it. reconstruction quality EVALUATE-FOR it. wav2vec feature CONJUNCTION pitch feature. pitch feature CONJUNCTION wav2vec feature. Yingram USED-FOR self - supervised training. pitch feature CONJUNCTION Yingram. Yingram CONJUNCTION pitch feature. analysis features USED-FOR NANSY. pitch feature HYPONYM-OF analysis features. wav2vec feature HYPONYM-OF analysis features. Yingram HYPONYM-OF analysis features. selfsupervised training USED-FOR NANSY. NANSY USED-FOR multilingual setting. multilingual dataset USED-FOR NANSY. multilingual dataset USED-FOR it. zero - shot voice conversion CONJUNCTION pitch shift. pitch shift CONJUNCTION zero - shot voice conversion. pitch shift CONJUNCTION time - scale modification. time - scale modification CONJUNCTION pitch shift. NANSY USED-FOR zero - shot voice conversion. NANSY USED-FOR applications. NANSY USED-FOR pitch shift. NANSY USED-FOR time - scale modification. time - scale modification HYPONYM-OF applications. zero - shot voice conversion HYPONYM-OF applications. pitch shift HYPONYM-OF applications. Method is synthesis networks. OtherScientificTerm is bottleneck structures. Material is speech data. ,"This paper proposes a neural analysis and synthesis (NANSY) framework for voice, pitch, and frequency. The key idea of NANSY is to use the information bottleneck to learn the analysis features for controllable synthesis. The training strategy is based on information perturbation, where the synthesis networks are trained on a pre-trained multilingual dataset, and the bottleneck structures are learned on the speech data. The paper shows that it can improve the reconstruction quality and controllability of voice and pitch. The analysis features used in NANSy are the wav2vec feature, pitch feature, and Yingram, which is used for self-supervised training. NansY is also applied to the multilingual setting, and it is trained on multilingual datasets. NANsY is applied to three applications: zero-shot voice conversion, pitch shift, and time-scale modification.","This paper proposes a neural analysis and synthesis (NANSY) framework for voice, pitch, and frequency. The key idea of NANSY is to use the information bottleneck to learn the analysis features for controllable synthesis. The training strategy is based on information perturbation, where the synthesis networks are trained on a pre-trained multilingual dataset, and the bottleneck structures are learned on the speech data. The paper shows that it can improve the reconstruction quality and controllability of voice and pitch. The analysis features used in NANSy are the wav2vec feature, pitch feature, and Yingram, which is used for self-supervised training. NansY is also applied to the multilingual setting, and it is trained on multilingual datasets. NANsY is applied to three applications: zero-shot voice conversion, pitch shift, and time-scale modification."
9408,SP:90f35ad1ec0c38b0817f5678ee2a5c4f0e08fb38,"gradient - based ) bilevel programming framework USED-FOR hyperparameter optimization. overfitting FEATURE-OF validation set. expectation bound USED-FOR cross - validation algorithm. gradient - based algorithms COMPARE cross - validation. cross - validation COMPARE gradient - based algorithms. regularization terms USED-FOR overfitting problem. regularization terms USED-FOR gradient - based algorithms. overfitting problem PART-OF gradient - based algorithms. outer and inner levels FEATURE-OF regularization terms. feature learning CONJUNCTION data reweighting. data reweighting CONJUNCTION feature learning. data reweighting USED-FOR noisy labels. OtherScientificTerm are optimization properties, and uniform stability. Task is generalization. Method is bilevel programming. ","This paper studies hyperparameter optimization in the (gradient-based) bilevel programming framework. The authors show that gradient-based algorithms suffer from the same overfitting problem as cross-validation, and that the overfitting of the validation set can be explained by the optimization properties of the hyperparameters. They provide an expectation bound for the cross-validation algorithm, and show that regularization terms on the outer and inner levels of the gradient -based algorithms can help alleviate the problem. They also show that feature learning and data reweighting for noisy labels can be used to mitigate the effect of noisy labels.    The authors also provide a theoretical analysis of the generalization performance of gradient based algorithms. They show that uniform stability is achieved when the number of parameters is small, but not when the parameters are large. They further show that this is due to the fact that the regularization term on the inner and outer layers of the algorithm has uniform stability, which is not the case for blevel programming. ","This paper studies hyperparameter optimization in the (gradient-based) bilevel programming framework. The authors show that gradient-based algorithms suffer from the same overfitting problem as cross-validation, and that the overfitting of the validation set can be explained by the optimization properties of the hyperparameters. They provide an expectation bound for the cross-validation algorithm, and show that regularization terms on the outer and inner levels of the gradient -based algorithms can help alleviate the problem. They also show that feature learning and data reweighting for noisy labels can be used to mitigate the effect of noisy labels.    The authors also provide a theoretical analysis of the generalization performance of gradient based algorithms. They show that uniform stability is achieved when the number of parameters is small, but not when the parameters are large. They further show that this is due to the fact that the regularization term on the inner and outer layers of the algorithm has uniform stability, which is not the case for blevel programming. "
9433,SP:42f52aec3a776d87daa5fd72b8e6325d12c88d63,"knowledge distillation approach USED-FOR transfer of dark knowledge. student models USED-FOR methods. algorithm USED-FOR student - friendly representations. algorithm USED-FOR student branches. knowledge distillation methods USED-FOR student models. accuracy CONJUNCTION convergence speed. convergence speed CONJUNCTION accuracy. approach USED-FOR teacher models. technique USED-FOR student models. technique USED-FOR knowledge distillation methods. convergence speed EVALUATE-FOR student models. knowledge distillation techniques EVALUATE-FOR algorithm. teacher and student models USED-FOR knowledge distillation techniques. accuracy EVALUATE-FOR algorithm. Task are knowledge transfer, and knowledge distillation procedure. Method are teacher model, and teacher networks. ","This paper proposes a new knowledge distillation approach for the transfer of dark knowledge, i.e. knowledge transfer between two different models. The authors propose an algorithm to learn student-friendly representations by distilling the knowledge of a teacher model into the student branches of the same teacher model. They show that this approach can be used to train teacher models to transfer knowledge from one branch to the other. They also show that the proposed algorithm can be applied to existing knowledge distillations methods to train student models to achieve better accuracy and convergence speed. Finally, the authors show that their technique can be combined with other existing student distillation methods to improve the performance of student models. They evaluate their algorithm on a variety of knowledge distilling techniques based on both teacher and student models and show that it can improve the accuracy of the student models in terms of the distance between the teacher networks.   ","This paper proposes a new knowledge distillation approach for the transfer of dark knowledge, i.e. knowledge transfer between two different models. The authors propose an algorithm to learn student-friendly representations by distilling the knowledge of a teacher model into the student branches of the same teacher model. They show that this approach can be used to train teacher models to transfer knowledge from one branch to the other. They also show that the proposed algorithm can be applied to existing knowledge distillations methods to train student models to achieve better accuracy and convergence speed. Finally, the authors show that their technique can be combined with other existing student distillation methods to improve the performance of student models. They evaluate their algorithm on a variety of knowledge distilling techniques based on both teacher and student models and show that it can improve the accuracy of the student models in terms of the distance between the teacher networks.   "
9458,SP:e15a1c21229233fd97dc1dfa0a4ef48b69dc9f95,"Generalization PART-OF machine learning. invariant features USED-FOR algorithms. invariance USED-FOR OOD generalization. generalization USED-FOR out - of - distribution. expansion function USED-FOR OOD generalization. model selection module PART-OF OOD learning algorithm. model selection criterion FEATURE-OF theory. model selection criterion COMPARE baselines. baselines COMPARE model selection criterion. benchmark OOD datasets EVALUATE-FOR model selection criterion. benchmark OOD datasets EVALUATE-FOR baselines. Task are extracting invariant features, OOD, and OOD problem. OtherScientificTerm is OOD generalization error bounds. ","This paper studies the problem of extracting invariant features for out-of-distribution (OOD) generalization in machine learning. The authors show that invariance is necessary for OOD generalization, and propose two algorithms based on this idea. The main contribution of the paper is to propose an expansion function that is invariant to OOD and can be used to improve the generalization performance of OOD learning algorithms. The paper also introduces a model selection module that is used to select the OLD learning algorithm. The theory of the model selection criterion in the theory shows that the OOD problem can be formulated as a model-selection problem, and the authors provide a theoretical analysis of this problem. Experiments on several benchmark OOD datasets show that the proposed model selection criteria outperforms the baselines. Finally, the paper also shows that their model selection algorithm outperforms baselines in terms of generalization error bounds.  ","This paper studies the problem of extracting invariant features for out-of-distribution (OOD) generalization in machine learning. The authors show that invariance is necessary for OOD generalization, and propose two algorithms based on this idea. The main contribution of the paper is to propose an expansion function that is invariant to OOD and can be used to improve the generalization performance of OOD learning algorithms. The paper also introduces a model selection module that is used to select the OLD learning algorithm. The theory of the model selection criterion in the theory shows that the OOD problem can be formulated as a model-selection problem, and the authors provide a theoretical analysis of this problem. Experiments on several benchmark OOD datasets show that the proposed model selection criteria outperforms the baselines. Finally, the paper also shows that their model selection algorithm outperforms baselines in terms of generalization error bounds.  "
9483,SP:37b04b9068d39bcf0a581eb8181d13cf1a8926bf,"stationary distribution USED-FOR meta - learning. Dynamic Gaussian Mixture Model USED-FOR meta - parameters. Dynamic Gaussian Mixture Model USED-FOR VC - BML. Chinese Restaurant Process USED-FOR number of component distributions. Dynamic mixtures USED-FOR meta - parameter level. Dynamic mixtures USED-FOR negative knowledge transfer problem. Dynamic mixtures USED-FOR diverse and dissimilar tasks. structured variational inference USED-FOR avoiding forgetting knowledge. posterior approximation method USED-FOR avoiding forgetting knowledge. point estimation method COMPARE posterior approximation method. posterior approximation method COMPARE point estimation method. structured variational inference HYPONYM-OF posterior approximation method. point estimation method USED-FOR posteriors of model parameters. VC - BML USED-FOR catastrophic forgetting. tasks EVALUATE-FOR VC - BML. non - stationary distributions FEATURE-OF tasks. Task is online setting. OtherScientificTerm are non - stationary distribution, and parameter space. ","This paper studies the problem of meta-learning in the online setting, where the meta-parameters are drawn from a stationary distribution. The authors propose VC-BML, which uses a Dynamic Gaussian Mixture Model to model the number of component distributions in a meta-learned model. The Chinese Restaurant Process is used to estimate the number and number of components in a non-stationary distribution. Dynamic mixtures are used to model a multi-task learning problem, which can be seen as a negative knowledge transfer problem, where each task in a task family has a different number of parameters. Dynamic mixture models can be used for diverse and dissimilar tasks.    The authors show that VC-BML is able to avoid catastrophic forgetting by using a point estimation method similar to the posterior approximation method in structured variational inference for avoiding forgetting knowledge in the parameter space. They also show that the proposed VC-MBL can be applied to a variety of tasks with non-stochastic distributions.  The paper is well-written and well-motivated. ","This paper studies the problem of meta-learning in the online setting, where the meta-parameters are drawn from a stationary distribution. The authors propose VC-BML, which uses a Dynamic Gaussian Mixture Model to model the number of component distributions in a meta-learned model. The Chinese Restaurant Process is used to estimate the number and number of components in a non-stationary distribution. Dynamic mixtures are used to model a multi-task learning problem, which can be seen as a negative knowledge transfer problem, where each task in a task family has a different number of parameters. Dynamic mixture models can be used for diverse and dissimilar tasks.    The authors show that VC-BML is able to avoid catastrophic forgetting by using a point estimation method similar to the posterior approximation method in structured variational inference for avoiding forgetting knowledge in the parameter space. They also show that the proposed VC-MBL can be applied to a variety of tasks with non-stochastic distributions.  The paper is well-written and well-motivated. "
9508,SP:776d5b02b8d3a8bbcc1f52706f3887c384cb149e,"boundary conditions FEATURE-OF ordinary differential equations. it USED-FOR BVPs. Gauss – Markov prior CONJUNCTION it. it CONJUNCTION Gauss – Markov prior. linear time FEATURE-OF posterior distribution. mesh refinement CONJUNCTION hyperparameter adaptation. hyperparameter adaptation CONJUNCTION mesh refinement. uncertainty quantification CONJUNCTION mesh refinement. mesh refinement CONJUNCTION uncertainty quantification. model USED-FOR uncertainty quantification. model USED-FOR mesh refinement. model USED-FOR hyperparameter adaptation. probabilistic BVP solver COMPARE non - probabilistic algorithms. non - probabilistic algorithms COMPARE probabilistic BVP solver. algorithms USED-FOR ODE boundary value problems. first - order problems USED-FOR higher - order problems. manifold learning USED-FOR BVPs. numerical simulation CONJUNCTION probabilistic inference. probabilistic inference CONJUNCTION numerical simulation. lineartime complexity CONJUNCTION adaptive step - size selection. adaptive step - size selection CONJUNCTION lineartime complexity. adaptive step - size selection CONJUNCTION polynomial convergence rates. polynomial convergence rates CONJUNCTION adaptive step - size selection. adaptive step - size selection FEATURE-OF probabilistic solvers. lineartime complexity FEATURE-OF probabilistic solvers. probabilistic solvers USED-FOR initial value problems. Generic are algorithm, and scheme. Method are non - probabilistic methods, statistical modelling tool - chain, and Probabilistic numerical algorithms. Task are Boundary value problems, first - order boundary value problem, machine learning, and Neural Information Processing Systems. OtherScientificTerm are computational pipelines, leftand right - hand side boundary conditions, vector field, ODE knowledge, infectious disease, integration domain, and structured output uncertainty. ","Boundary value problems are a fundamental problem in machine learning. Boundary conditions of ordinary differential equations can be expressed as a function of a set of computational pipelines, and the goal is to find the optimal solution to a first-order boundary value problem.    This paper proposes a new algorithm, called probabilistic BVP solver, which is a generalization of existing non-probabilistic methods.  The key idea is to use a Gauss–Markov prior on the vector field and use it to solve BVPs using manifold learning.  This is a well-motivated idea, as it is well known that it is possible to learn the posterior distribution in linear time, and that it can be used to learn a model for uncertainty quantification, mesh refinement, hyperparameter adaptation, etc.  In this paper, the authors propose a statistical modelling tool-chain, where the leftand right-hand side boundary conditions are defined as the vector fields of a vector field.  They show that the probabilism of the proposed algorithm is equivalent to solving a first order problem, and they show that it converges to the solution of the first order BVP in a similar way to the non-Probababilistic algorithms. They also show that their probabilist BVP solution is more efficient than non-observational algorithms for solving ODE boundary value problems.  Finally, they demonstrate that their algorithms are more computationally efficient for ODEs that are more general than for higher-order problems, and show that they can be applied to any machine learning problem. The paper also shows that their algorithm is more tractable than existing algorithms for first order problems, which are computationally expensive to compute.  Experiments are conducted in numerical simulation, probabilistically inference, and in the integration domain, where numerical simulation is used for numerical simulation.  Probabilistic numerical algorithms are shown to be able to solve the lineartime complexity, adaptive step-size selection, and polynomial convergence rates. The authors also provide a theoretical analysis of their scheme, which shows that probabilists are able to find a solution with a polynomially optimal solution, and can also solve the initial value problems with a smaller number of samples. ","Boundary value problems are a fundamental problem in machine learning. Boundary conditions of ordinary differential equations can be expressed as a function of a set of computational pipelines, and the goal is to find the optimal solution to a first-order boundary value problem.    This paper proposes a new algorithm, called probabilistic BVP solver, which is a generalization of existing non-probabilistic methods.  The key idea is to use a Gauss–Markov prior on the vector field and use it to solve BVPs using manifold learning.  This is a well-motivated idea, as it is well known that it is possible to learn the posterior distribution in linear time, and that it can be used to learn a model for uncertainty quantification, mesh refinement, hyperparameter adaptation, etc.  In this paper, the authors propose a statistical modelling tool-chain, where the leftand right-hand side boundary conditions are defined as the vector fields of a vector field.  They show that the probabilism of the proposed algorithm is equivalent to solving a first order problem, and they show that it converges to the solution of the first order BVP in a similar way to the non-Probababilistic algorithms. They also show that their probabilist BVP solution is more efficient than non-observational algorithms for solving ODE boundary value problems.  Finally, they demonstrate that their algorithms are more computationally efficient for ODEs that are more general than for higher-order problems, and show that they can be applied to any machine learning problem. The paper also shows that their algorithm is more tractable than existing algorithms for first order problems, which are computationally expensive to compute.  Experiments are conducted in numerical simulation, probabilistically inference, and in the integration domain, where numerical simulation is used for numerical simulation.  Probabilistic numerical algorithms are shown to be able to solve the lineartime complexity, adaptive step-size selection, and polynomial convergence rates. The authors also provide a theoretical analysis of their scheme, which shows that probabilists are able to find a solution with a polynomially optimal solution, and can also solve the initial value problems with a smaller number of samples. "
9533,SP:86aac0c6b75fdc12f84bba342934865616f866d4,"partially observable system FEATURE-OF near optimal policy. episodic reinforcement learning USED-FOR reward - mixingMarkov decision process ( MDP ). reward models USED-FOR reward function. near optimal policy USED-FOR reward - mixing MDPs. algorithmic and analysis techniques USED-FOR problem. polynomial - time algorithm USED-FOR -optimal policy. observation space COMPARE latent state space. latent state space COMPARE observation space. Task is reinforcement learning. Method are reward model, and switching reward - models. OtherScientificTerm are dynamics, time - horizon, and partially observed environments. Generic are approaches, assumptions, and algorithm. ","This paper studies the problem of reward mixing in reinforcement learning. The authors consider reward-mixingMarkov decision process (MDP) in an episodic reinforcement learning setting, where the reward model is not available but the dynamics of the environment is, and the goal is to learn a near optimal policy in a partially observable system. In this setting, the authors consider switching reward models for the reward function can be difficult because the observation space is not the same as the latent state space, and switching reward-models is expensive due to the time-horizon. To address this problem, they propose a polynomial-time algorithm for learning a -optimal policy in the partially observed environments. They show that under certain assumptions, the proposed algorithm converges to the optimal policy. They also provide algorithmic and analysis techniques to solve the problem. ","This paper studies the problem of reward mixing in reinforcement learning. The authors consider reward-mixingMarkov decision process (MDP) in an episodic reinforcement learning setting, where the reward model is not available but the dynamics of the environment is, and the goal is to learn a near optimal policy in a partially observable system. In this setting, the authors consider switching reward models for the reward function can be difficult because the observation space is not the same as the latent state space, and switching reward-models is expensive due to the time-horizon. To address this problem, they propose a polynomial-time algorithm for learning a -optimal policy in the partially observed environments. They show that under certain assumptions, the proposed algorithm converges to the optimal policy. They also provide algorithmic and analysis techniques to solve the problem. "
9558,SP:1a3c70ae9cf2a806d603f4b9e7ca6e10b720a956,"methods USED-FOR conditional average treatment effect estimation. two - step procedure USED-FOR multi - cause treatment effect. Single - cause Perturbation ( SCP ) USED-FOR multi - cause treatment effect. Single - cause Perturbation ( SCP ) HYPONYM-OF two - step procedure. covariate adjustment USED-FOR estimator. covariate adjustment USED-FOR augmented dataset. It USED-FOR estimator. covariate adjustment USED-FOR It. synthetic and semi - synthetic experiments EVALUATE-FOR SCP. Generic are applications, problem, and procedure. Task are multi - cause treatment effect problems, multi - cause problem, and causal inference. OtherScientificTerm are confounding bias, cause combination, and single - cause interventions. Material is observational dataset. ","This paper proposes two new methods for conditional average treatment effect estimation, which is an important problem in many applications. The authors focus on multi-cause treatment effect problems, where there is a confounding bias between the treatment effect of a single treatment and the treatment of a multi-causal combination of treatments.   The authors propose a two-step procedure called Single-causality perturbation (SCP) to address this problem. First, the authors propose to augment the observational dataset by augmenting it with a cause combination of the treatments. Then, they propose a causal inference procedure to identify the cause of the confounding bias.  Second, the proposed procedure is based on the idea of causal inference. It uses a covariate adjustment to improve the estimator of the augmented dataset.  They evaluate the performance of the proposed SCP on both synthetic and semi-synthetic experiments.","This paper proposes two new methods for conditional average treatment effect estimation, which is an important problem in many applications. The authors focus on multi-cause treatment effect problems, where there is a confounding bias between the treatment effect of a single treatment and the treatment of a multi-causal combination of treatments.   The authors propose a two-step procedure called Single-causality perturbation (SCP) to address this problem. First, the authors propose to augment the observational dataset by augmenting it with a cause combination of the treatments. Then, they propose a causal inference procedure to identify the cause of the confounding bias.  Second, the proposed procedure is based on the idea of causal inference. It uses a covariate adjustment to improve the estimator of the augmented dataset.  They evaluate the performance of the proposed SCP on both synthetic and semi-synthetic experiments."
9583,SP:247bc6675cce89d51558537daf63dadb0c4307f8,"multiwavelet - based neural operator learning scheme USED-FOR operator ’s kernel. fine - grained wavelets USED-FOR multiwavelet - based neural operator learning scheme. multiwavelet polynomial bases USED-FOR projection of the kernel. Burgers ’ equation CONJUNCTION Darcy Flow. Darcy Flow CONJUNCTION Burgers ’ equation. Darcy Flow CONJUNCTION Navier - Stokes equation. Navier - Stokes equation CONJUNCTION Darcy Flow. Korteweg - de Vries ( KdV ) equation CONJUNCTION Burgers ’ equation. Burgers ’ equation CONJUNCTION Korteweg - de Vries ( KdV ) equation. neural operator approaches COMPARE model. model COMPARE neural operator approaches. state - of - the - art EVALUATE-FOR model. accuracy EVALUATE-FOR neural operator approaches. accuracy EVALUATE-FOR model. relative L2 error EVALUATE-FOR Burgers ’ ( KdV ) equation. method USED-FOR time - varying equations. relative L2 error EVALUATE-FOR method. mappings between function spaces USED-FOR method. lower - resolution data USED-FOR method. OtherScientificTerm are inverse operator map, and complex dependencies. Method are inverse multiwavelet filters, projected kernel, and resolution - independent scheme. ","This paper proposes a multi-wavelet-based neural operator learning scheme based on fine-grained wavelets to learn an operator’s kernel from a large amount of data. The projection of the kernel is based on multiwavelet polynomial bases, and the inverse operator map is learned by learning inverse multiwavelets filters. The proposed method is tested on the Korteweg-de Vries (KdV) equation, Burgers’ equation, Darcy Flow, and Navier-Stokes equation, and shows better accuracy than state-of-the-art neural operator approaches. The method is also applied to time-varying equations, and is shown to have a lower relative L2 error than the previous state of the art.    The method uses mappings between function spaces to learn the projection of a projected kernel, which is a resolution-independent scheme. The authors also show that the proposed method can be applied to lower-resolution data, and that the method is robust to complex dependencies. ","This paper proposes a multi-wavelet-based neural operator learning scheme based on fine-grained wavelets to learn an operator’s kernel from a large amount of data. The projection of the kernel is based on multiwavelet polynomial bases, and the inverse operator map is learned by learning inverse multiwavelets filters. The proposed method is tested on the Korteweg-de Vries (KdV) equation, Burgers’ equation, Darcy Flow, and Navier-Stokes equation, and shows better accuracy than state-of-the-art neural operator approaches. The method is also applied to time-varying equations, and is shown to have a lower relative L2 error than the previous state of the art.    The method uses mappings between function spaces to learn the projection of a projected kernel, which is a resolution-independent scheme. The authors also show that the proposed method can be applied to lower-resolution data, and that the method is robust to complex dependencies. "
9608,SP:1153785e6a016cfee2644952a772aa08927299b6,"Binary neural networks ( BNNs ) USED-FOR full - precision weights. 1 - bit with sign function USED-FOR Binary neural networks ( BNNs ). gradient FEATURE-OF sign function. approximate gradient USED-FOR optimization difficulty. sine functions USED-FOR BNNs. frequency domain approximation ( FDA ) HYPONYM-OF BNNs. Fourier frequency domain FEATURE-OF gradient of sign function. frequency domain approximation ( FDA ) HYPONYM-OF sine functions. low - frequency information FEATURE-OF sign function. noise adaptation module USED-FOR approximation error. benchmark datasets CONJUNCTION neural architectures. neural architectures CONJUNCTION benchmark datasets. method USED-FOR binary network. Method is back - propagation. Generic are approximations, and approach. OtherScientificTerm are factual gradient, and high - frequency coefficients. ","Binary neural networks (BNNs) with 1-bit with sign function can be used to compute full-precision weights, but the gradient of sign function in the Fourier frequency domain is non-convex and cannot be computed efficiently. Binary neural networks with sign functions are a special case of BNNs with sine functions, e.g., frequency domain approximation (FDA). The authors propose two approximations to this problem, one based on back-propagation, the other based on a noise adaptation module. The authors show that the approximate gradient of the sign function is the best way to reduce the optimization difficulty, and that this approximation error depends on the low-frequency information in the gradient. They also show that this method can be applied to any binary network with a sign function. They show that their method is more robust to noise in the factual gradient, and more robust when high-frequency coefficients are used. They evaluate their approach on several benchmark datasets and neural architectures. ","Binary neural networks (BNNs) with 1-bit with sign function can be used to compute full-precision weights, but the gradient of sign function in the Fourier frequency domain is non-convex and cannot be computed efficiently. Binary neural networks with sign functions are a special case of BNNs with sine functions, e.g., frequency domain approximation (FDA). The authors propose two approximations to this problem, one based on back-propagation, the other based on a noise adaptation module. The authors show that the approximate gradient of the sign function is the best way to reduce the optimization difficulty, and that this approximation error depends on the low-frequency information in the gradient. They also show that this method can be applied to any binary network with a sign function. They show that their method is more robust to noise in the factual gradient, and more robust when high-frequency coefficients are used. They evaluate their approach on several benchmark datasets and neural architectures. "
9633,SP:33b95ea8da4d30b8e8f9d3fe3acca023d4b8d831,cortical areas USED-FOR tasks. Recurrent neural networks ( RNNs ) USED-FOR cortical areas. Recurrent neural networks ( RNNs ) USED-FOR neuroscience - based tasks. cortical area USED-FOR tasks. multi - area RNNs USED-FOR multi - area computation. neuroscience - inspired architecture constraints FEATURE-OF multi - area RNNs. Dale ’s Law USED-FOR networks. full observability FEATURE-OF RNNs. full observability USED-FOR output - relevant information. modular computation USED-FOR minimal sufficient representations of task information. cortex USED-FOR minimal sufficient representations of task information. modular computation USED-FOR cortex. constrained multi - area RNNs USED-FOR computations. distributed computation PART-OF neural systems. OtherScientificTerm is coordination of multiple brain areas. Generic is computation. ,"This paper studies the role of the coordination of multiple brain areas in neuroscience-based tasks. Recurrent neural networks (RNNs) have been shown to be able to coordinate different cortical areas for different tasks, and the authors show that multi-area RNNs with neuroscience-inspired architecture constraints are able to perform well at multi- area computation. The authors also show that the networks satisfy Dale’s Law, which states that networks with full observability of the output-relevant information will perform well on a variety of tasks.  The authors further show that a modular computation in the cortex is able to capture minimal sufficient representations of task information, and that this modular computation can be used to constrain the number of computations in a single RNN.   The paper concludes with a discussion of the benefits of distributed computation in neural systems, and shows that, in the case of constrained multi-region RNN, these computations can be performed efficiently. ","This paper studies the role of the coordination of multiple brain areas in neuroscience-based tasks. Recurrent neural networks (RNNs) have been shown to be able to coordinate different cortical areas for different tasks, and the authors show that multi-area RNNs with neuroscience-inspired architecture constraints are able to perform well at multi- area computation. The authors also show that the networks satisfy Dale’s Law, which states that networks with full observability of the output-relevant information will perform well on a variety of tasks.  The authors further show that a modular computation in the cortex is able to capture minimal sufficient representations of task information, and that this modular computation can be used to constrain the number of computations in a single RNN.   The paper concludes with a discussion of the benefits of distributed computation in neural systems, and shows that, in the case of constrained multi-region RNN, these computations can be performed efficiently. "
9658,SP:db3ced65d67e3373fb3936ec50f41c8ef010bbbe,Saliency maps USED-FOR convolutional neural networks ( CNNs ). convolutional neural networks ( CNNs ) USED-FOR image classification. saliency map USED-FOR image of interest. maps USED-FOR classification. confidence EVALUATE-FOR classifier. structured attention graphs ( SAGs ) USED-FOR attention maps. compact and representative SAG USED-FOR visualization. approach USED-FOR compact and representative SAG. diverse sampling USED-FOR approach. diverse sampling USED-FOR compact and representative SAG. diverse sampling USED-FOR visualization. SAGs COMPARE saliency maps. saliency maps COMPARE SAGs. SAGs USED-FOR comparative counterfactual questions. saliency maps USED-FOR comparative counterfactual questions. user study USED-FOR comparative counterfactual questions. user study EVALUATE-FOR SAGs. image classifications FEATURE-OF comparative counterfactual questions. SAGs COMPARE saliency map baselines. saliency map baselines COMPARE SAGs. user accuracy EVALUATE-FOR SAGs. user accuracy EVALUATE-FOR saliency map baselines. Method is beam search algorithm. OtherScientificTerm is image regions. ,"Saliency maps for convolutional neural networks (CNNs) for image classification are an important and important problem.   In this paper, the authors propose a novel beam search algorithm, where the saliency map for an image of interest is used to search for a set of image regions in the image. The maps are used for classification, and the confidence of the classifier is used as a metric to measure the quality of the maps. The attention maps are constructed using structured attention graphs (SAGs).   The authors propose an approach to learn a compact and representative SAG for visualization based on diverse sampling.  They show that SAGs outperform saliency maps on the comparative counterfactual questions on the user study for the task of comparing the performance of different image classifications.  The paper also shows that the proposed approach outperforms the state-of-the-art on user accuracy on the question of whether saliencymap baselines. ","Saliency maps for convolutional neural networks (CNNs) for image classification are an important and important problem.   In this paper, the authors propose a novel beam search algorithm, where the saliency map for an image of interest is used to search for a set of image regions in the image. The maps are used for classification, and the confidence of the classifier is used as a metric to measure the quality of the maps. The attention maps are constructed using structured attention graphs (SAGs).   The authors propose an approach to learn a compact and representative SAG for visualization based on diverse sampling.  They show that SAGs outperform saliency maps on the comparative counterfactual questions on the user study for the task of comparing the performance of different image classifications.  The paper also shows that the proposed approach outperforms the state-of-the-art on user accuracy on the question of whether saliencymap baselines. "
9683,SP:f2b385bfd9ada0e26aa8829214b424f58582d9f7,loss functions CONJUNCTION regularizers. regularizers CONJUNCTION loss functions. loss functions USED-FOR image classification tasks. image classification tasks EVALUATE-FOR regularizers. test accuracy EVALUATE-FOR loss functions. test accuracy EVALUATE-FOR regularizers. loss functions USED-FOR representations. representations USED-FOR downstream tasks. loss functions USED-FOR downstream tasks. transferability EVALUATE-FOR hidden representations of convolutional neural networks. ImageNet USED-FOR hidden representations of convolutional neural networks. fixed feature extractors USED-FOR downstream tasks. networks USED-FOR tasks. objectives COMPARE vanilla softmax cross - entropy. vanilla softmax cross - entropy COMPARE objectives. ImageNet accuracy EVALUATE-FOR vanilla softmax cross - entropy. ImageNet accuracy EVALUATE-FOR objectives. centered kernel alignment USED-FOR hidden representations of networks. objectives CONJUNCTION hyperparameter combinations. hyperparameter combinations CONJUNCTION objectives. hyperparameter combinations USED-FOR class separation. objectives USED-FOR class separation. features USED-FOR downstream tasks. accuracy EVALUATE-FOR task. class separation FEATURE-OF Representations. task EVALUATE-FOR Representations. accuracy EVALUATE-FOR Representations. learning invariant features CONJUNCTION features. features CONJUNCTION learning invariant features. features USED-FOR transfer tasks. learning invariant features USED-FOR task. OtherScientificTerm is loss. Generic is network. ,"This paper studies the transferability of the hidden representations of convolutional neural networks trained on ImageNet with different loss functions and regularizers on different image classification tasks. The authors show that the representations learned with different types of loss functions can achieve similar test accuracy on different downstream tasks. They also show that networks trained with the same objective on different tasks can learn representations that are transferable across different tasks.    The paper also shows that networks with fixed feature extractors can learn to transfer the representations of different tasks using the same loss functions. Representations learned on the same task with different objectives are more transferable than vanilla softmax cross-entropy on the ImageNet accuracy. The main contribution of the paper is that the authors propose to use centered kernel alignment as a regularizer to improve the performance of hidden representations learned by networks. The paper shows that the proposed objectives and hyperparameter combinations are able to achieve better class separation across different objectives and different hyperparameters, and that learning invariant features and features learned on a single task can be used to learn features for multiple transfer tasks.","This paper studies the transferability of the hidden representations of convolutional neural networks trained on ImageNet with different loss functions and regularizers on different image classification tasks. The authors show that the representations learned with different types of loss functions can achieve similar test accuracy on different downstream tasks. They also show that networks trained with the same objective on different tasks can learn representations that are transferable across different tasks.    The paper also shows that networks with fixed feature extractors can learn to transfer the representations of different tasks using the same loss functions. Representations learned on the same task with different objectives are more transferable than vanilla softmax cross-entropy on the ImageNet accuracy. The main contribution of the paper is that the authors propose to use centered kernel alignment as a regularizer to improve the performance of hidden representations learned by networks. The paper shows that the proposed objectives and hyperparameter combinations are able to achieve better class separation across different objectives and different hyperparameters, and that learning invariant features and features learned on a single task can be used to learn features for multiple transfer tasks."
9708,SP:b66b5e24f68563e2e200eda660f0dbaff53efeff,"spatial sampling CONJUNCTION temporal frequency of sampling. temporal frequency of sampling CONJUNCTION spatial sampling. neural network training strategy USED-FOR deep generative models of latent dynamics. selective backpropagation through time ( SBTT ) HYPONYM-OF neural network training strategy. SBTT USED-FOR sequential autoencoders. electrophysiological and calcium imaging data USED-FOR neural population dynamics. SBTT USED-FOR inference of neuronal population dynamics. electrophysiology USED-FOR inference of neuronal population dynamics. SBTT USED-FOR electrophysiology. interface bandwidths FEATURE-OF inference of neuronal population dynamics. SBTT USED-FOR high - frequency temporal structure. high - frequency temporal structure FEATURE-OF neural population activity. SBTT USED-FOR neural population activity. SBTT USED-FOR two - photon calcium imaging. limited, highbandwidth sampling USED-FOR pretrain dynamics models. SBTT USED-FOR models. models USED-FOR sparsely - sampled data. OtherScientificTerm are neural interfaces, brain circuits, bandwidth limits, latent low - dimensional population dynamics, latent dynamics, neuronal population dynamics, and implanted neuroelectronic interfaces. Task is Neural Information Processing Systems. ","This paper proposes selective backpropagation through time (SBTT) as a neural network training strategy for deep generative models of latent dynamics. SBTT is an extension of sequential autoencoders, and is designed for neural interfaces where spatial sampling and the temporal frequency of sampling is limited. The authors show that SBTT improves the inference of neuronal population dynamics from electrophysiological and calcium imaging data to infer neural population dynamics in the context of neural interfaces.    The main contribution of this paper is that the authors demonstrate that the neural population activity of a population of neurons in the brain is highly dependent on the interface bandwidths of the brain circuits, and SBTT can be used to learn a latent low-dimensional population dynamics that is more robust to bandwidth limits.  The authors also show that the latent dynamics of a neural population can be learned using SBTT, and that the learned latent dynamics can capture the high-frequency temporal structure of the neural activity.  In addition, the authors show the effectiveness of SBTT on two-photon calcium imaging, where SBTT shows that neural populations of neurons with high frequency temporal structure are more likely to share the same interface bandwidth than neural populations with low frequency.  Finally, they show that models trained with SBTT are able to learn sparsely-sampled data with limited, highbandwidth sampling to pretrain dynamics models.  This is an interesting contribution to the field of Neural Information Processing Systems (NIS).   This paper is well-written and well-motivated, and the results are interesting. However, there is a lack of comparison with previous work, and there is not a clear connection between SBTT and neuroscience and neuroscience.  I have a few comments:","This paper proposes selective backpropagation through time (SBTT) as a neural network training strategy for deep generative models of latent dynamics. SBTT is an extension of sequential autoencoders, and is designed for neural interfaces where spatial sampling and the temporal frequency of sampling is limited. The authors show that SBTT improves the inference of neuronal population dynamics from electrophysiological and calcium imaging data to infer neural population dynamics in the context of neural interfaces.    The main contribution of this paper is that the authors demonstrate that the neural population activity of a population of neurons in the brain is highly dependent on the interface bandwidths of the brain circuits, and SBTT can be used to learn a latent low-dimensional population dynamics that is more robust to bandwidth limits.  The authors also show that the latent dynamics of a neural population can be learned using SBTT, and that the learned latent dynamics can capture the high-frequency temporal structure of the neural activity.  In addition, the authors show the effectiveness of SBTT on two-photon calcium imaging, where SBTT shows that neural populations of neurons with high frequency temporal structure are more likely to share the same interface bandwidth than neural populations with low frequency.  Finally, they show that models trained with SBTT are able to learn sparsely-sampled data with limited, highbandwidth sampling to pretrain dynamics models.  This is an interesting contribution to the field of Neural Information Processing Systems (NIS).   This paper is well-written and well-motivated, and the results are interesting. However, there is a lack of comparison with previous work, and there is not a clear connection between SBTT and neuroscience and neuroscience.  I have a few comments:"
9733,SP:3513a83806e71006b86d60b779d8bd6bb87c3546,"Sequence - to - sequence learning USED-FOR sequence prediction tasks. neural networks USED-FOR Sequence - to - sequence learning. approach USED-FOR local distribution. neural network USED-FOR approach. neural network USED-FOR local distribution. hierarchical approach USED-FOR sequence - to - sequence learning. quasi - synchronous grammars USED-FOR hierarchical approach. style transfer CONJUNCTION small - scale machine translation. small - scale machine translation CONJUNCTION style transfer. compositional generalization ( SCAN ) CONJUNCTION style transfer. style transfer CONJUNCTION compositional generalization ( SCAN ). it COMPARE baselines. baselines COMPARE it. latent neural grammar USED-FOR domains. latent neural grammar USED-FOR diagnostic language navigation task. diagnostic language navigation task EVALUATE-FOR compositional generalization ( SCAN ). diagnostic language navigation task EVALUATE-FOR small - scale machine translation. diagnostic language navigation task HYPONYM-OF domains. style transfer HYPONYM-OF diagnostic language navigation task. compositional generalization ( SCAN ) HYPONYM-OF domains. small - scale machine translation HYPONYM-OF domains. style transfer HYPONYM-OF domains. Generic is models. Task is compositional generalization. Method are neural parameterization of the grammar, and manual feature engineering. OtherScientificTerm is combinatorial space of derivation rules. ","Sequence-to-sequence learning for sequence prediction tasks with neural networks is an important problem. The current state-of-the-art models are based on a hierarchical approach to sequence-to sequence learning using neural networks. This approach learns a local distribution through a neural network, which is based on quasi-synchronous grammars. The authors propose a new approach, called compositional generalization (SCAN), which learns a neural parameterization of the grammar, which can be applied to any combinatorial space of derivation rules. They show that it outperforms the baselines in three different domains, including a diagnostic language navigation task with a latent neural grammar, a compositional learning task with style transfer, and a small-scale machine translation task. They also show that their approach can be used for manual feature engineering.","Sequence-to-sequence learning for sequence prediction tasks with neural networks is an important problem. The current state-of-the-art models are based on a hierarchical approach to sequence-to sequence learning using neural networks. This approach learns a local distribution through a neural network, which is based on quasi-synchronous grammars. The authors propose a new approach, called compositional generalization (SCAN), which learns a neural parameterization of the grammar, which can be applied to any combinatorial space of derivation rules. They show that it outperforms the baselines in three different domains, including a diagnostic language navigation task with a latent neural grammar, a compositional learning task with style transfer, and a small-scale machine translation task. They also show that their approach can be used for manual feature engineering."
9769,SP:d06fc251f2a9287f7a2236a188349628d8f39d9a,Feature Selection CONJUNCTION Functional Data Analysis. Functional Data Analysis CONJUNCTION Feature Selection. algorithm USED-FOR function - on - scalar feature selection. algorithm USED-FOR Group Elastic Net. Group Elastic Net USED-FOR function - on - scalar feature selection. scalar predictors USED-FOR functional response. algorithm USED-FOR Group Elastic Net. sparsity structure FEATURE-OF Augmented Lagrangian. algorithm USED-FOR ultrahigh dimensional settings. algorithm USED-FOR sparsity structure. ultrahigh dimensional settings FEATURE-OF Group Elastic Net. algorithm USED-FOR function - on - scalar regression framework. Functional Principal Components USED-FOR algorithm. approach COMPARE competitors. competitors COMPARE approach. simulations EVALUATE-FOR approach. Genome Wide Association Study USED-FOR application. Task is analysis of large and complex data sets. OtherScientificTerm is computational burden. ,"This paper proposes a new algorithm for function-on-scalar feature selection in Feature Selection and Functional Data Analysis. The authors propose an algorithm for Group Elastic Net, which is a general algorithm for the analysis of large and complex data sets. The algorithm is based on the Functional Principal Components of the functional response of scalar predictors. The paper shows that the proposed algorithm is able to recover the sparsity structure of the Augmented Lagrangian of the Group Elastic net in ultrahigh dimensional settings. The proposed algorithm can also be applied to the function-onscalar regression framework, where the computational burden is much lower. Experiments on simulations show that this approach outperforms competitors and is applicable to the application of the Genome Wide Association Study. ","This paper proposes a new algorithm for function-on-scalar feature selection in Feature Selection and Functional Data Analysis. The authors propose an algorithm for Group Elastic Net, which is a general algorithm for the analysis of large and complex data sets. The algorithm is based on the Functional Principal Components of the functional response of scalar predictors. The paper shows that the proposed algorithm is able to recover the sparsity structure of the Augmented Lagrangian of the Group Elastic net in ultrahigh dimensional settings. The proposed algorithm can also be applied to the function-onscalar regression framework, where the computational burden is much lower. Experiments on simulations show that this approach outperforms competitors and is applicable to the application of the Genome Wide Association Study. "
9805,SP:e0b53f76f3a6b756fedd09926f9cf034f89f4a5a,functional principal component analysis ( FPCA ) USED-FOR model estimation. real data analyses EVALUATE-FOR framework. Material is Structured point process data. Generic is matrix. OtherScientificTerm is log - Gaussian Cox processes. ,"This paper considers the problem of model estimation using functional principal component analysis (FPCA) for Structured point process data. The authors propose a new matrix, called FPCA, which is a generalization of the log-Gaussian Cox processes. The proposed framework is validated on several real data analyses. ","This paper considers the problem of model estimation using functional principal component analysis (FPCA) for Structured point process data. The authors propose a new matrix, called FPCA, which is a generalization of the log-Gaussian Cox processes. The proposed framework is validated on several real data analyses. "
9841,SP:3aa213076f3e9f9838ac654517df2fe1fca33499,"online multi - task learning approach USED-FOR adaptive nonlinear control. adversarial disturbance CONJUNCTION unknown environmentdependent nonlinear dynamics. unknown environmentdependent nonlinear dynamics CONJUNCTION adversarial disturbance. unknown environmentdependent nonlinear dynamics FEATURE-OF nonlinear system. adversarial disturbance FEATURE-OF nonlinear system. shared representation USED-FOR environmentdependent dynamics. approach USED-FOR robot control. unified framework USED-FOR control - theoretic and learning - theoretic guarantees. non - asymptotic endto - end convergence guarantee USED-FOR multi - task nonlinear control. OMAC CONJUNCTION deep representation learning. deep representation learning CONJUNCTION OMAC. OMAC COMPARE adaptive control approaches. adaptive control approaches COMPARE OMAC. Method are Online Meta - Adaptive Control ( OMAC ), online representation learning, and control theory. Task is robotic system. ","This paper proposes an online multi-task learning approach for adaptive nonlinear control, called Online Meta-Adaptive Control (OMAC). The key idea is to learn a nonlinear system with adversarial disturbance and unknown environmentdependent nonlinear dynamics, and then use a shared representation to model the environmentdependent dynamics. The approach is applied to robot control, where the goal is to control a robotic system in an online setting. The authors provide a unified framework for both control-theoretic and learning-thruthic guarantees. They also provide a non-asymptotic endto-end convergence guarantee for multi-tasks non-linear control. They show that OMAC can be combined with deep representation learning, and show that the performance of OMAC is comparable to other adaptive control approaches.   ","This paper proposes an online multi-task learning approach for adaptive nonlinear control, called Online Meta-Adaptive Control (OMAC). The key idea is to learn a nonlinear system with adversarial disturbance and unknown environmentdependent nonlinear dynamics, and then use a shared representation to model the environmentdependent dynamics. The approach is applied to robot control, where the goal is to control a robotic system in an online setting. The authors provide a unified framework for both control-theoretic and learning-thruthic guarantees. They also provide a non-asymptotic endto-end convergence guarantee for multi-tasks non-linear control. They show that OMAC can be combined with deep representation learning, and show that the performance of OMAC is comparable to other adaptive control approaches.   "
9877,SP:cb274c93a169b199ea09120ca02105a3f16b31c5,"bound propagation based certified robust training methods USED-FOR neural networks. certifiable robustness guarantees FEATURE-OF neural networks. interval bound propagation ( IBP ) CONJUNCTION CROWN - IBP. CROWN - IBP CONJUNCTION interval bound propagation ( IBP ). interval bound propagation ( IBP ) HYPONYM-OF SOTA ) methods. CROWN - IBP PART-OF SOTA ) methods. weight initialization method USED-FOR IBP training. regularization USED-FOR ReLU activation states. regularization USED-FOR certified bounds. BN USED-FOR ReLU activation states. Batch Normalization ( BN ) USED-FOR model. regularization USED-FOR certified training. verified error CONJUNCTION verified error. verified error CONJUNCTION verified error. verified error FEATURE-OF TinyImageNet. network architecture USED-FOR SOTA. Metric is per - batch training complexity. Method are neural network training, and Fast - Certified - Robust - Training. Generic are they, and methods. OtherScientificTerm are exploded bounds, long warmup schedules, and training schedules. Task is wamrup. Material is CIFAR-10. ","This paper studies bound propagation based certified robust training methods for training neural networks with certifiable robustness guarantees. The authors propose two (SOTA) methods: interval bound propagation (IBP) and CROWN-IBP, and show that they can be fast-certified robust (in terms of per-batch training complexity) and robust to exploding bounds. They also show that a weight initialization method can be used for IBP training.   The authors also propose Fast-Certified-Robust-Training (FCTR), which uses Batch Normalization (BN) to regularize the model during training. They show that this regularization can be applied to the ReLU activation states of the BN to improve the certified bounds.  They also demonstrate that the proposed regularization is effective for certified training, and that FCTR achieves wamrup on CIFAR-10 and TinyImageNet with verified error and verified error w.r.t. the network architecture.  Finally, the authors show that their methods are robust to long warmup schedules, long training schedules, and overfitting. ","This paper studies bound propagation based certified robust training methods for training neural networks with certifiable robustness guarantees. The authors propose two (SOTA) methods: interval bound propagation (IBP) and CROWN-IBP, and show that they can be fast-certified robust (in terms of per-batch training complexity) and robust to exploding bounds. They also show that a weight initialization method can be used for IBP training.   The authors also propose Fast-Certified-Robust-Training (FCTR), which uses Batch Normalization (BN) to regularize the model during training. They show that this regularization can be applied to the ReLU activation states of the BN to improve the certified bounds.  They also demonstrate that the proposed regularization is effective for certified training, and that FCTR achieves wamrup on CIFAR-10 and TinyImageNet with verified error and verified error w.r.t. the network architecture.  Finally, the authors show that their methods are robust to long warmup schedules, long training schedules, and overfitting. "
9913,SP:18ffeb199a670fb2b1f4417b8653479001944dab,"change point detection method USED-FOR adversaries. Huber ε - contamination framework USED-FOR adversarial attacks. phase transition phenomenon FEATURE-OF change point detection. minimax lower bound USED-FOR computationally - feasible method. Task are Change point detection, and univariate mean change point detection problem. Method are theoretically - justified methods, and robust change point detection methods. OtherScientificTerm are model violations, heavy - tailed noise distribution, isolate outliers, systematic contamination, spurious change points, contamination distributions, detection boundary, contamination proportion ε, contamination proportion, and logarithmic factors. Metric is minimax - rate optimal localisation error rate. ","This paper studies the problem of change point detection, i. The authors consider the setting where the model violations are caused by a heavy-tailed noise distribution and the goal is to identify the source of the change point. They show that theoretically-probabilistic methods can be shown to be robust to the presence of systematic contamination. They also show that a change-point detection method is computationally-feasible under certain conditions. ","This paper studies the problem of change point detection, i. The authors consider the setting where the model violations are caused by a heavy-tailed noise distribution and the goal is to identify the source of the change point. They show that theoretically-probabilistic methods can be shown to be robust to the presence of systematic contamination. They also show that a change-point detection method is computationally-feasible under certain conditions. "
9949,SP:d03617b5fc446768809cf015c9234b0c9386a690,"differentiable model CONJUNCTION neural network. neural network CONJUNCTION differentiable model. batch Gradient Descent ( GD ) USED-FOR empirical loss. batch Gradient Descent ( GD ) USED-FOR learning. paradigms USED-FOR learning problems. GD USED-FOR learning. SGD USED-FOR learning. SGD CONJUNCTION GD. GD CONJUNCTION SGD. precision ρ FEATURE-OF gradient calculations. statistical queries ( SQ ) USED-FOR learning. SGD USED-FOR sample - based learning algorithm. learning power EVALUATE-FOR PAC learning. SGD USED-FOR SQ learning. fine enough precision COMPARE minibatch size. minibatch size COMPARE fine enough precision. GD USED-FOR sample - based learning algorithm. fine enough precision USED-FOR GD. GD USED-FOR PAC learning. SGD USED-FOR PAC learning. SGD CONJUNCTION GD. GD CONJUNCTION SGD. SGD COMPARE SQ learning. SQ learning COMPARE SGD. OtherScientificTerm are population loss, bρ, ρ, and mini - batch size. ","This paper studies the problem of learning a differentiable model and a neural network in the presence of a population loss. The authors show that batch Gradient Descent (GD) can be used to approximate the empirical loss, and that SGD and GD can improve the learning power of learning with statistical queries (SQ) in PAC learning. They also show that GD and SGD improve the sample-based learning algorithm with fine enough precision compared to GD for PAC learning, and SQ learning with SGD for SQ learning. Finally, they show that the precision ρ of the gradient calculations is the same as the precision of the population loss, which means that GD can learn with bρ, SGD cannot learn with ρ.    The paper also shows that GD does not improve the performance of SQ learning when the mini-batch size is smaller than the population size, but SGD does. ","This paper studies the problem of learning a differentiable model and a neural network in the presence of a population loss. The authors show that batch Gradient Descent (GD) can be used to approximate the empirical loss, and that SGD and GD can improve the learning power of learning with statistical queries (SQ) in PAC learning. They also show that GD and SGD improve the sample-based learning algorithm with fine enough precision compared to GD for PAC learning, and SQ learning with SGD for SQ learning. Finally, they show that the precision ρ of the gradient calculations is the same as the precision of the population loss, which means that GD can learn with bρ, SGD cannot learn with ρ.    The paper also shows that GD does not improve the performance of SQ learning when the mini-batch size is smaller than the population size, but SGD does. "
9985,SP:1de2864fe2f53e25596a9bd2c61e2048e79296f6,"machine learning CONJUNCTION inverse problems. inverse problems CONJUNCTION machine learning. model probability distribution USED-FOR discrete data. discrete data USED-FOR inverse problems. discrete data USED-FOR machine learning. Wasserstein distance FEATURE-OF model distribution. uniform probability distribution USED-FOR Wasserstein distance. convergence FEATURE-OF Lloyd - type algorithm. ambient space FEATURE-OF point cloud. point cloud USED-FOR algorithm. Poliak - Łojasiewicz inequality USED-FOR Wasserstein distance cost. Task is minimization problem. Method are Lloyd ’s algorithm, and gradient descent. OtherScientificTerm are Voronoi cells, Power cells, spurious critical points, error term, and discrete distribution. Metric is Wasserstein error. Generic are problem, and bounds. ","This paper studies the minimization problem of the Wasserstein distance between the model probability distribution for discrete data in machine learning and inverse problems. The authors consider a variant of Lloyd’s algorithm, where the Voronoi cells are replaced by Power cells, and the goal is to minimise the model distribution of the data.    In this paper, the authors consider the case where the model is a uniform probability distribution and the data is discrete.  They show that if the model has a uniform distribution over the data, then they can prove that the minimisation problem can be solved exactly.  In particular, they prove that for any uniform distribution $p(x) = p(y|x)$, if $y$ is a point cloud in the ambient space, then the problem is minimised exactly. This is a special case of the problem studied in [1].   The authors also prove that if $x$ is discrete, then there are no spurious critical points.  [1] [2] [3] [4] [5] [6] [7] [8] [9]   [10] [11]  [12] [13] [14] ","This paper studies the minimization problem of the Wasserstein distance between the model probability distribution for discrete data in machine learning and inverse problems. The authors consider a variant of Lloyd’s algorithm, where the Voronoi cells are replaced by Power cells, and the goal is to minimise the model distribution of the data.    In this paper, the authors consider the case where the model is a uniform probability distribution and the data is discrete.  They show that if the model has a uniform distribution over the data, then they can prove that the minimisation problem can be solved exactly.  In particular, they prove that for any uniform distribution $p(x) = p(y|x)$, if $y$ is a point cloud in the ambient space, then the problem is minimised exactly. This is a special case of the problem studied in [1].   The authors also prove that if $x$ is discrete, then there are no spurious critical points.  [1] [2] [3] [4] [5] [6] [7] [8] [9]   [10] [11]  [12] [13] [14] "
10021,SP:c3d364aeee55230a436c3ce4e8dc8310ee73959e,"Convolution HYPONYM-OF feature transform. Convolution HYPONYM-OF neural networks. feature transform PART-OF neural networks. convolution layers CONJUNCTION self - attention blocks. self - attention blocks CONJUNCTION convolution layers. convolution layers PART-OF Transformer networks. dynamic transforms USED-FOR video understanding. correspondence relations USED-FOR representation. motion information HYPONYM-OF correspondence relations. self - attention HYPONYM-OF dynamic transforms. relational kernels CONJUNCTION relational contexts. relational contexts CONJUNCTION relational kernels. rich structures of spatio - temporal relations USED-FOR relational feature transform. relational kernels USED-FOR rich structures of spatio - temporal relations. relational self - attention ( RSA ) HYPONYM-OF relational feature transform. Diving48 CONJUNCTION FineGym. FineGym CONJUNCTION Diving48. Something - Something - V1&V2 CONJUNCTION Diving48. Diving48 CONJUNCTION Something - Something - V1&V2. RSA network COMPARE convolution and self - attention counterparts. convolution and self - attention counterparts COMPARE RSA network. motion - centric benchmarks USED-FOR video action recognition. Something - Something - V1&V2 HYPONYM-OF video action recognition. Diving48 HYPONYM-OF video action recognition. motion - centric benchmarks EVALUATE-FOR RSA network. FineGym HYPONYM-OF motion - centric benchmarks. Something - Something - V1&V2 HYPONYM-OF motion - centric benchmarks. Diving48 HYPONYM-OF motion - centric benchmarks. Method are deep learning, stationary convolution kernels, and dynamic feature transforms. ","This paper proposes a novel relational self-attention (RSA) feature transform for video understanding. In contrast to traditional convolutional kernels, RSA uses relational kernels to capture the rich structures of spatio-temporal relations in the input video. The authors show that RSA can be used to improve the performance of CNNs and transformers on motion-centric benchmarks.  ","This paper proposes a novel relational self-attention (RSA) feature transform for video understanding. In contrast to traditional convolutional kernels, RSA uses relational kernels to capture the rich structures of spatio-temporal relations in the input video. The authors show that RSA can be used to improve the performance of CNNs and transformers on motion-centric benchmarks.  "
10057,SP:2c2530069d5cab485629090243da464d107feadd,"mean field theory FEATURE-OF multilayer neural networks. mean field limit USED-FOR learning dynamics. infinite - width limit FEATURE-OF random fluctuation. large - width expansion USED-FOR random fluctuation. formulation USED-FOR stochastic dependency. fluctuation FEATURE-OF multilayer networks. system of dynamical equations USED-FOR limiting fluctuation distribution. second - order mean field limit HYPONYM-OF system of dynamical equations. stochasticity CONJUNCTION nonlinear time evolution. nonlinear time evolution CONJUNCTION stochasticity. nonlinear time evolution FEATURE-OF limiting fluctuation. cross - layer dependency CONJUNCTION nonlinear time evolution. nonlinear time evolution CONJUNCTION cross - layer dependency. cross - layer dependency FEATURE-OF stochasticity. large - width networks USED-FOR fluctuation. vanishing fluctuation FEATURE-OF output function. squared loss FEATURE-OF empirical risk minimization setting. empirical risk minimization setting FEATURE-OF shallow networks. loss function FEATURE-OF multilayer networks. squared loss FEATURE-OF shallow networks. OtherScientificTerm are infinite - width scaling, network depth, complex interaction, limit theorem, large - width regime, training trajectory, and global optimum. Material is multilayer case. Method are neuronal embedding framework, and gradient descent mean field training. Generic are it, and network. ","This paper studies the mean field theory of multilayer neural networks. The authors prove a mean field limit on the learning dynamics of the multilayers in the infinite-width limit of the neural network, which is a result of the neuronal embedding framework. In particular, the authors prove that the random fluctuation of the network depth is a function of the number of layers and of the amount of network depth. They show that in the large-width regime, the random fluctuations of the training trajectory converge to a global optimum. They also prove that this limit theorem holds in the case of infinite-wide networks.    The authors also prove a limit theorem for the random-flutter case, which shows that there exists an infinite- width limit on random fluctuations in the multilevel setting, and that it holds for any network with a large width. They then show that this infinite-widething limit depends on the number and depth of the layers and on the complexity of the complex interaction among the layers.  Finally, they show that the limit theorem also holds for multillayer networks with fluctuation in the limit.  The main contribution of this paper is the formulation of the stochastic dependency of the fluctuation to the number, and the analysis of the gradient descent mean field training. They prove a system of dynamical equations for the limiting fluctuation distribution, which they call the second-order means field limit. This formulation allows them to prove that any large-wider-than-the-training-plane network has a second order mean field.  In addition, they prove that for any loss function that is close to the limit, there exists a limit on how much the loss function changes in the training process.  They also show that for a certain loss function, the limit is tight. ","This paper studies the mean field theory of multilayer neural networks. The authors prove a mean field limit on the learning dynamics of the multilayers in the infinite-width limit of the neural network, which is a result of the neuronal embedding framework. In particular, the authors prove that the random fluctuation of the network depth is a function of the number of layers and of the amount of network depth. They show that in the large-width regime, the random fluctuations of the training trajectory converge to a global optimum. They also prove that this limit theorem holds in the case of infinite-wide networks.    The authors also prove a limit theorem for the random-flutter case, which shows that there exists an infinite- width limit on random fluctuations in the multilevel setting, and that it holds for any network with a large width. They then show that this infinite-widething limit depends on the number and depth of the layers and on the complexity of the complex interaction among the layers.  Finally, they show that the limit theorem also holds for multillayer networks with fluctuation in the limit.  The main contribution of this paper is the formulation of the stochastic dependency of the fluctuation to the number, and the analysis of the gradient descent mean field training. They prove a system of dynamical equations for the limiting fluctuation distribution, which they call the second-order means field limit. This formulation allows them to prove that any large-wider-than-the-training-plane network has a second order mean field.  In addition, they prove that for any loss function that is close to the limit, there exists a limit on how much the loss function changes in the training process.  They also show that for a certain loss function, the limit is tight. "
10093,SP:a3d927854d9d7fd39b8d05a79666810d585d5062,inductive biases USED-FOR predictive extrapolation. Hamiltonian / Lagrangian form USED-FOR structure. inductive biases USED-FOR Forecasting of time - series data. dissipative brackets PART-OF metriplectic dynamical systems. metriplectic dynamical systems USED-FOR parameterization of dissipative brackets. process USED-FOR generalized Casimirs. generalized Casimirs USED-FOR entropy. dynamics COMPARE penalty - based approaches. penalty - based approaches COMPARE dynamics. time - series data USED-FOR dynamical system. data - driven modeling USED-FOR physical systems. data - driven modeling CONJUNCTION machine learning ( ML ) tasks. machine learning ( ML ) tasks CONJUNCTION data - driven modeling. learnable dynamics FEATURE-OF dynamical system. physics - based structure USED-FOR architectures. minimal bias FEATURE-OF black - box model form. approaches USED-FOR structure preserving models of reversible dynamics. structure preserving models of reversible dynamics USED-FOR inductive bias. approaches USED-FOR inductive bias. algebraic structure of Hamiltonian / Lagrangian dynamics USED-FOR flow map. energy FEATURE-OF flow map. symplectic structure FEATURE-OF flow map. approaches USED-FOR reversible systems. entropy HYPONYM-OF generalized Casimirs. framework USED-FOR Physical systems. thermodynamic consistency FEATURE-OF mimetic properties. first and second laws of thermodynamics HYPONYM-OF mimetic properties. fluctuation dissipation theorem ( FDT ) USED-FOR closed stochastic systems. model USED-FOR metriplectic systems. algebraic structure FEATURE-OF system. first principles modeling USED-FOR system. system USED-FOR multiscale problems. time history USED-FOR multiscale problems. training strategy USED-FOR NODEs. metriplectic system USED-FOR time - series data. training strategy USED-FOR algebraic objects. internal entropy CONJUNCTION temperature. temperature CONJUNCTION internal entropy. non - observable states FEATURE-OF dissipative systems. internal entropy HYPONYM-OF non - observable states. temperature HYPONYM-OF non - observable states. null - spaces USED-FOR reversible and irreversible components of the dynamics. dissipative chaotic systems USED-FOR science and engineering problems. latent dimension FEATURE-OF,"Forecasting of time-series data based on inductive biases is an important problem in predictive extrapolation. Forecasting of physical systems using data-driven modeling and machine learning (ML) tasks is challenging due to the lack of learnable dynamics of the dynamical system. This paper proposes a new parameterization of dissipative brackets in metriplectic dynamical systems, which is based on the metrization of the Hamiltonian/Lagrangian form of the structure of the system. The authors show that this process is equivalent to generalized Casimirs, i.e., entropy. The dynamics of this dynamics is more stable than penalty-based approaches, and the authors propose a framework for modeling reversible systems. They show that existing approaches to learn inductive bias for reversible systems can be used to learn a structure preserving models of reversible dynamics, and that the physics-based structure of these architectures allows to learn architectures with minimal bias in a black-box model form.  The authors propose to use the fluctuation dissipation theorem (FTD) to model closed stochastic systems, and show that the flow map of a system with the algebraic structure of Hamiltonian / Lagrangian dynamics has a flow map with respect to the energy, and a symplectic structure. Physical systems with this framework can be seen as a generalization of the framework for dissipative chaotic systems. The proposed training strategy for NODEs is similar to the training strategy used to model algebraic objects in the literature, but for a more general system with a different latent dimension. In particular, the authors use the same training strategy to model the system as a system that has non-observicially observable states and non-uniform non-localities, and to learn the system's dynamics. This system is then used to solve multiscale problems with time history, and is shown to be able to learn to predict a system's internal entropy, temperature, and other non-inverse properties. The system is also shown to exhibit thermodynamic consistency in terms of mimetic properties such as the first and second laws of thermodynamics. The paper also shows that the system is able to model non-irreversible and irreversible components of the dynamics in null-spaces, and it is shown that the learned model can be applied to a variety of science and engineering problems where the system can be viewed as an algebraic chaotic system. ","Forecasting of time-series data based on inductive biases is an important problem in predictive extrapolation. Forecasting of physical systems using data-driven modeling and machine learning (ML) tasks is challenging due to the lack of learnable dynamics of the dynamical system. This paper proposes a new parameterization of dissipative brackets in metriplectic dynamical systems, which is based on the metrization of the Hamiltonian/Lagrangian form of the structure of the system. The authors show that this process is equivalent to generalized Casimirs, i.e., entropy. The dynamics of this dynamics is more stable than penalty-based approaches, and the authors propose a framework for modeling reversible systems. They show that existing approaches to learn inductive bias for reversible systems can be used to learn a structure preserving models of reversible dynamics, and that the physics-based structure of these architectures allows to learn architectures with minimal bias in a black-box model form.  The authors propose to use the fluctuation dissipation theorem (FTD) to model closed stochastic systems, and show that the flow map of a system with the algebraic structure of Hamiltonian / Lagrangian dynamics has a flow map with respect to the energy, and a symplectic structure. Physical systems with this framework can be seen as a generalization of the framework for dissipative chaotic systems. The proposed training strategy for NODEs is similar to the training strategy used to model algebraic objects in the literature, but for a more general system with a different latent dimension. In particular, the authors use the same training strategy to model the system as a system that has non-observicially observable states and non-uniform non-localities, and to learn the system's dynamics. This system is then used to solve multiscale problems with time history, and is shown to be able to learn to predict a system's internal entropy, temperature, and other non-inverse properties. The system is also shown to exhibit thermodynamic consistency in terms of mimetic properties such as the first and second laws of thermodynamics. The paper also shows that the system is able to model non-irreversible and irreversible components of the dynamics in null-spaces, and it is shown that the learned model can be applied to a variety of science and engineering problems where the system can be viewed as an algebraic chaotic system. "
10129,SP:32e8e83e06b1e9a4dad761334d5947c91bfd1853,"Fairness CONJUNCTION robustness. robustness CONJUNCTION Fairness. robustness PART-OF Trustworthy AI. Fairness PART-OF Trustworthy AI. Fairness CONJUNCTION robustness. robustness CONJUNCTION Fairness. sample selection - based algorithm USED-FOR fair and robust training. combinatorial optimization problem USED-FOR unbiased selection of samples. greedy algorithm USED-FOR optimization problem. algorithm COMPARE state - of - the - art technique. state - of - the - art technique COMPARE algorithm. fairness CONJUNCTION robustness. robustness CONJUNCTION fairness. robustness EVALUATE-FOR state - of - the - art technique. fairness EVALUATE-FOR state - of - the - art technique. robustness EVALUATE-FOR algorithm. fairness EVALUATE-FOR algorithm. synthetic and benchmark real datasets EVALUATE-FOR algorithm. synthetic and benchmark real datasets EVALUATE-FOR state - of - the - art technique. fair and robust training baselines COMPARE algorithm. algorithm COMPARE fair and robust training baselines. sampling step USED-FOR batch selection. sampling step USED-FOR algorithm. clean data USED-FOR algorithm. Method are unbiased model, and training algorithm. OtherScientificTerm is data corruption. ","This paper proposes a sample selection-based algorithm for fair and robust training in Trustworthy AI. Fairness and robustness are two of the most important issues in the training of an unbiased model. In this paper, the authors propose an unbiased selection of samples is formulated as a combinatorial optimization problem, and the optimization problem is solved by a greedy algorithm. The authors show that their algorithm outperforms the state-of-the-art technique in terms of fairness as well as robustness on synthetic and benchmark real datasets. They also show that the algorithm is more robust to data corruption than the state of the art fairness and robust robust training baselines.  The authors also propose a new sampling step for batch selection, where the algorithm only uses clean data for the training algorithm. ","This paper proposes a sample selection-based algorithm for fair and robust training in Trustworthy AI. Fairness and robustness are two of the most important issues in the training of an unbiased model. In this paper, the authors propose an unbiased selection of samples is formulated as a combinatorial optimization problem, and the optimization problem is solved by a greedy algorithm. The authors show that their algorithm outperforms the state-of-the-art technique in terms of fairness as well as robustness on synthetic and benchmark real datasets. They also show that the algorithm is more robust to data corruption than the state of the art fairness and robust robust training baselines.  The authors also propose a new sampling step for batch selection, where the algorithm only uses clean data for the training algorithm. "
10165,SP:991127729bf067fe27fdd7ed360aab39e4df5921,"Neural network models USED-FOR hidden data biases. function space FEATURE-OF inductive biases. inductive biases USED-FOR models. periodic activation functions USED-FOR Bayesian neural networks. triangular wave CONJUNCTION periodic ReLU activation functions. periodic ReLU activation functions CONJUNCTION triangular wave. deep neural networks USED-FOR out - of - domain detection. periodic activation functions USED-FOR deep neural networks. in - domain data EVALUATE-FOR periodic activation functions. Generic is them. OtherScientificTerm are network weights, translation - invariant, stationary Gaussian process priors, sinusoidal ( Fourier ) activations, and perturbed inputs. ","Neural network models are known to have hidden data biases. However, it is not known whether these models are invariant to inductive biases in the function space. This paper proposes to use periodic activation functions for Bayesian neural networks and shows that they are. The authors also show that the network weights are translation-invariant, stationary Gaussian process priors, and that sinusoidal (Fourier) activations are not invariant. The paper also shows that the triangular wave and periodic ReLU activation functions can be used to train deep neural networks for out-of-domain detection on in-domain data with perturbed inputs.","Neural network models are known to have hidden data biases. However, it is not known whether these models are invariant to inductive biases in the function space. This paper proposes to use periodic activation functions for Bayesian neural networks and shows that they are. The authors also show that the network weights are translation-invariant, stationary Gaussian process priors, and that sinusoidal (Fourier) activations are not invariant. The paper also shows that the triangular wave and periodic ReLU activation functions can be used to train deep neural networks for out-of-domain detection on in-domain data with perturbed inputs."
10201,SP:d61a2aecfea4612c473b4e6fd41f3dc2fcbb04a1,user interaction CONJUNCTION complex dynamic systems. complex dynamic systems CONJUNCTION user interaction. complex dynamic systems FEATURE-OF programs. user interaction FEATURE-OF programs. mouse based games HYPONYM-OF complex dynamic systems. autonomous methods USED-FOR feedback. unit tests USED-FOR interactive programs. feedback USED-FOR interactive programs. classifying Markov Decision Processes ( MDPs ) USED-FOR feedback. dynamics and reward model USED-FOR MDP. agent USED-FOR differential trajectories. agent CONJUNCTION autoregressive model. autoregressive model CONJUNCTION agent. differential trajectories PART-OF MDP. agent USED-FOR cooperative objective. autoregressive model USED-FOR cooperative objective. method USED-FOR automatic feedback system. automatic feedback system USED-FOR interactive code assignments. anonymized student submissions FEATURE-OF dataset. Task is coding education. Method is classifier. Material is hand - coded bug labels. ,"This paper proposes a method to train an automatic feedback system for interactive programming. The idea is to train a classifier to identify bugs in a program, and then train an autoregressive agent to learn to solve the problem. The authors show that the proposed method is able to learn a good classifier that can be applied to a wide range of interactive programming tasks. The paper also shows that the learned classifier can be used as a tool to improve the quality of the feedback provided by an agent.","This paper proposes a method to train an automatic feedback system for interactive programming. The idea is to train a classifier to identify bugs in a program, and then train an autoregressive agent to learn to solve the problem. The authors show that the proposed method is able to learn a good classifier that can be applied to a wide range of interactive programming tasks. The paper also shows that the learned classifier can be used as a tool to improve the quality of the feedback provided by an agent."
10237,SP:daf99ad91613d6e11b13315ccbd1bbe25094ae4b,"superpixels CONJUNCTION attentions. attentions CONJUNCTION superpixels. attentions CONJUNCTION saliency maps. saliency maps CONJUNCTION attentions. superpixels USED-FOR low - level input features. high - level latent object features USED-FOR approach. disentangled representation USED-FOR high - level latent object features. identifiable latent representation USED-FOR independent factors of variation. mimic tree USED-FOR DRL action values. identifiable latent representation USED-FOR Represent And Mimic ( RAMi ) framework. fidelity EVALUATE-FOR mimic tree. Minimum Description Length ( MDL ) objective EVALUATE-FOR mimic tree. Information Bottleneck ( IB ) principle USED-FOR Minimum Description Length ( MDL ) objective. mimic tree COMPARE baseline models. baseline models COMPARE mimic tree. decision rules CONJUNCTION causal impacts. causal impacts CONJUNCTION decision rules. latent traversals CONJUNCTION decision rules. decision rules CONJUNCTION latent traversals. causal impacts CONJUNCTION human evaluation results. human evaluation results CONJUNCTION causal impacts. latent traversals FEATURE-OF mimic tree. decision rules PART-OF mimic tree. Task is Interpreting Deep Reinforcement Learning ( DRL ) models. OtherScientificTerm are transparency regulations, latent features, IB - optimal mimic tree, and nodes. Method is DRL model. ","This paper tackles the problem of Interpreting Deep Reinforcement Learning (DRL) models in the presence of transparency regulations. The approach is based on high-level latent object features extracted from superpixels and low-level input features (superpixels, attentions, saliency maps). The authors propose a Represent And Mimic (RAMi) framework, which uses an identifiable latent representation to capture independent factors of variation in the latent features, and a disentangled representation for the high level latent object feature. A mimic tree of DRL action values is proposed, which is trained to maximize the fidelity of the mimic tree under the Minimum Description Length (MDL) objective based on the Information Bottleneck (IB) principle. The authors show that the IB-optimal mimic tree can achieve the same fidelity as a DRL model trained on the same number of nodes. The mimic tree is shown to be more robust to decision rules, decision rules and causal impacts, as well as human evaluation results.","This paper tackles the problem of Interpreting Deep Reinforcement Learning (DRL) models in the presence of transparency regulations. The approach is based on high-level latent object features extracted from superpixels and low-level input features (superpixels, attentions, saliency maps). The authors propose a Represent And Mimic (RAMi) framework, which uses an identifiable latent representation to capture independent factors of variation in the latent features, and a disentangled representation for the high level latent object feature. A mimic tree of DRL action values is proposed, which is trained to maximize the fidelity of the mimic tree under the Minimum Description Length (MDL) objective based on the Information Bottleneck (IB) principle. The authors show that the IB-optimal mimic tree can achieve the same fidelity as a DRL model trained on the same number of nodes. The mimic tree is shown to be more robust to decision rules, decision rules and causal impacts, as well as human evaluation results."
10273,SP:84560de78af979354fff83d1370d8675c1e9191f,"weather forecasts CONJUNCTION political prognostications. political prognostications CONJUNCTION weather forecasts. political prognostications CONJUNCTION financial projections. financial projections CONJUNCTION political prognostications. Bayesian framework USED-FOR structure of dynamic predictions. GLIM HYPONYM-OF Bayesian framework. Gaussian latent information martingale HYPONYM-OF Bayesian framework. historical data USED-FOR latent process of information flow. martingale structure CONJUNCTION volatility. volatility CONJUNCTION martingale structure. approach USED-FOR probability paths. volatility HYPONYM-OF probability paths. martingale structure HYPONYM-OF probability paths. GLIM COMPARE baseline methods. baseline methods COMPARE GLIM. metrics USED-FOR estimated posterior probability path distributions. estimated posterior probability path distributions EVALUATE-FOR GLIM. estimated posterior probability path distributions EVALUATE-FOR baseline methods. metrics EVALUATE-FOR baseline methods. metrics EVALUATE-FOR GLIM. Task are probability estimates of future binary outcomes, and time series analysis. Generic are first, second, former, and trajectories. OtherScientificTerm is dynamic structure of predictions. ","This paper proposes a Bayesian framework for learning the structure of dynamic predictions based on Gaussian latent information martingale (GLIM).    The authors consider the problem of learning probability estimates of future binary outcomes, which is an important problem in time series analysis.  They consider weather forecasts, political prognostications, and financial projections, where the latent process of information flow is modeled using historical data.  The proposed approach is able to learn probability paths that can be interpreted as a combination of two types of probability paths: the Martingale structure and the volatility.  In the first case, the probability of the first path depends on the history of the data, while in the second, the former depends on whether the trajectory of the information flow has changed significantly in the past few years.  Experiments show that GLIM outperforms baseline methods on several metrics for estimating the estimated posterior probability path distributions, and that the dynamic structure of predictions is more robust to changes in trajectories.  ","This paper proposes a Bayesian framework for learning the structure of dynamic predictions based on Gaussian latent information martingale (GLIM).    The authors consider the problem of learning probability estimates of future binary outcomes, which is an important problem in time series analysis.  They consider weather forecasts, political prognostications, and financial projections, where the latent process of information flow is modeled using historical data.  The proposed approach is able to learn probability paths that can be interpreted as a combination of two types of probability paths: the Martingale structure and the volatility.  In the first case, the probability of the first path depends on the history of the data, while in the second, the former depends on whether the trajectory of the information flow has changed significantly in the past few years.  Experiments show that GLIM outperforms baseline methods on several metrics for estimating the estimated posterior probability path distributions, and that the dynamic structure of predictions is more robust to changes in trajectories.  "
10309,SP:0c4bfb44e0a353256692d5e5ae96f65c1a14363d,"fixed confidence FEATURE-OF active pure exploration. generic stochastic bandit environments USED-FOR active pure exploration. instance - specific lower bounds FEATURE-OF expected sample complexity. instance - specific lower bounds USED-FOR problem. proportions USED-FOR optimization problem. tractability FEATURE-OF optimization problem. algorithm USED-FOR pure exploration problems. Frank - Wolfe algorithm USED-FOR lower - bound optimization problem. Frank - Wolfe algorithm USED-FOR it. FWS USED-FOR pure exploration tasks. arm identification HYPONYM-OF pure exploration tasks. FWS COMPARE state - of - art algorithms. state - of - art algorithms COMPARE FWS. OtherScientificTerm are sampling budget, structural properties of the environment, and lower bounds. Method are Oracle algorithm, and learning algorithms. Metric is sample complexity. ","This paper studies the problem of active pure exploration with fixed confidence in generic stochastic bandit environments, where the sampling budget is fixed and the structural properties of the environment are unknown. The authors provide instance-specific lower bounds on the expected sample complexity of this problem, which are based on the Oracle algorithm. They show that under certain proportions, the optimization problem has tractability. They then propose a new algorithm, called FWS, for solving pure exploration problems, and extend it to the Frank-Wolfe algorithm for solving the lower-bound optimization problem. FWS is shown to outperform existing state-of-the-art algorithms on two pure exploration tasks, namely, arm identification and finding the best arm, and achieves sample complexity asymptotically tight as the number of learning algorithms increases. ","This paper studies the problem of active pure exploration with fixed confidence in generic stochastic bandit environments, where the sampling budget is fixed and the structural properties of the environment are unknown. The authors provide instance-specific lower bounds on the expected sample complexity of this problem, which are based on the Oracle algorithm. They show that under certain proportions, the optimization problem has tractability. They then propose a new algorithm, called FWS, for solving pure exploration problems, and extend it to the Frank-Wolfe algorithm for solving the lower-bound optimization problem. FWS is shown to outperform existing state-of-the-art algorithms on two pure exploration tasks, namely, arm identification and finding the best arm, and achieves sample complexity asymptotically tight as the number of learning algorithms increases. "
10345,SP:0947a0f08fba53d3c8af9b78dd64e6e10fc73e32,"sequences CONJUNCTION trees. trees CONJUNCTION sequences. trees CONJUNCTION graphs. graphs CONJUNCTION trees. graphs HYPONYM-OF optimizing combinatorial spaces. sequences HYPONYM-OF optimizing combinatorial spaces. trees HYPONYM-OF optimizing combinatorial spaces. black - box function evaluations USED-FOR combinatorial spaces. Bayesian optimization ( BO ) USED-FOR problems. framework USED-FOR problems. BO approach USED-FOR combinatorial spaces. deep generative models ( DGMs ) USED-FOR latent representation of structures. discrete structure USED-FOR function evaluation. latent space USED-FOR surrogate model. DGM USED-FOR surrogate model. LADDER HYPONYM-OF approach. latent space representation USED-FOR surrogate modeling. structural information PART-OF decoded structures. structural information PART-OF structure - coupled kernel. real - world benchmarks EVALUATE-FOR LADDER. LADDER COMPARE BO. BO COMPARE LADDER. LADDER COMPARE state - of - the - art methods. state - of - the - art methods COMPARE LADDER. LADDER COMPARE latent space method. latent space method COMPARE LADDER. real - world benchmarks EVALUATE-FOR BO. BO COMPARE latent space method. latent space method COMPARE BO. real - world benchmarks EVALUATE-FOR state - of - the - art methods. Task is drug design. OtherScientificTerm are physical lab experiments, continuous spaces, continuous space, inductive bias, and black - box function. ","This paper considers the problem of optimizing combinatorial spaces (e.g., sequences, trees, graphs, etc.) with black-box function evaluations. The authors extend Bayesian optimization (BO) to these problems and propose a new framework, called LADDER, which extends the BO approach to the case where the function evaluation is based on a discrete structure rather than on continuous spaces.   The authors propose to use deep generative models (DGMs) to learn a latent representation of structures, which is then used to train a surrogate model in the latent space. The surrogate model is trained using a DGM. The latent space representation is used for surrogate modeling, and the structural information in the decoded structures is incorporated into a structure-coupled kernel.  Experiments on real-world benchmarks on drug design are conducted to show the effectiveness of the proposed approach, LADder, compared to BO and other state-of-the-art methods.  The paper also shows that the inductive bias is not present in the case of continuous space, and that the surrogate model can be learned from the learned latent space without the use of a black box function.","This paper considers the problem of optimizing combinatorial spaces (e.g., sequences, trees, graphs, etc.) with black-box function evaluations. The authors extend Bayesian optimization (BO) to these problems and propose a new framework, called LADDER, which extends the BO approach to the case where the function evaluation is based on a discrete structure rather than on continuous spaces.   The authors propose to use deep generative models (DGMs) to learn a latent representation of structures, which is then used to train a surrogate model in the latent space. The surrogate model is trained using a DGM. The latent space representation is used for surrogate modeling, and the structural information in the decoded structures is incorporated into a structure-coupled kernel.  Experiments on real-world benchmarks on drug design are conducted to show the effectiveness of the proposed approach, LADder, compared to BO and other state-of-the-art methods.  The paper also shows that the inductive bias is not present in the case of continuous space, and that the surrogate model can be learned from the learned latent space without the use of a black box function."
10381,SP:37adabdc6615c5199a481553c8ccc06d57363614,"representation of state - action value functions USED-FOR regret minimization. constant regret FEATURE-OF MDP. linear reward function FEATURE-OF MDP. low - rank MDPs CONJUNCTION zero inherent Bellman error. zero inherent Bellman error CONJUNCTION low - rank MDPs. condition USED-FOR problems. LSVI - UCB CONJUNCTION ELEANOR. ELEANOR CONJUNCTION LSVI - UCB. constant regret bound USED-FOR optimistic algorithms. LSVI - UCB HYPONYM-OF optimistic algorithms. ELEANOR HYPONYM-OF optimistic algorithms. algorithm USED-FOR representation selection. representations CONJUNCTION them. them CONJUNCTION representations. constant regret EVALUATE-FOR it. representations USED-FOR it. OtherScientificTerm are linear structure, universally spanning optimal features ( UNISOFT ), Bellman closure assumption, and UNISOFT condition. Generic is representation. ","This paper considers the problem of regret minimization over the representation of state-action value functions in an MDP with linear structure. The authors consider the setting of universally spanning optimal features (UNISOFT) where the MDP has a linear reward function and the Bellman closure assumption is satisfied. They prove a constant regret bound for any MDP that satisfies the UNISOFT condition. This condition holds for both low-rank MDPs with zero inherent Bellman error and for problems with linear reward functions. They also provide two optimistic algorithms (LSVI-UCB and ELEANOR) that achieve the constant regret under this condition. Finally, the authors propose an algorithm for representation selection, and show that it achieves the same constant regret with respect to the representations (or at least one of them) that satisfy the UN ISOFT condition, and that it does not require any additional information about the representation. ","This paper considers the problem of regret minimization over the representation of state-action value functions in an MDP with linear structure. The authors consider the setting of universally spanning optimal features (UNISOFT) where the MDP has a linear reward function and the Bellman closure assumption is satisfied. They prove a constant regret bound for any MDP that satisfies the UNISOFT condition. This condition holds for both low-rank MDPs with zero inherent Bellman error and for problems with linear reward functions. They also provide two optimistic algorithms (LSVI-UCB and ELEANOR) that achieve the constant regret under this condition. Finally, the authors propose an algorithm for representation selection, and show that it achieves the same constant regret with respect to the representations (or at least one of them) that satisfy the UN ISOFT condition, and that it does not require any additional information about the representation. "
10417,SP:92566b664ab2f6ee9b73f29327aeef85d14ecf60,"energy conservation FEATURE-OF dynamics. Lagrangian or Hamiltonian dynamics PART-OF neural network architecture. differential equations USED-FOR approaches. legged robots CONJUNCTION robotic manipulators. robotic manipulators CONJUNCTION legged robots. contacts CONJUNCTION collisions. collisions CONJUNCTION contacts. contacts PART-OF physical systems. collisions PART-OF physical systems. robotic manipulators HYPONYM-OF physical systems. legged robots HYPONYM-OF physical systems. differentiable contact model USED-FOR contact mechanics. frictionless / frictional CONJUNCTION elastic / inelastic. elastic / inelastic CONJUNCTION frictionless / frictional. frictionless / frictional HYPONYM-OF differentiable contact model. elastic / inelastic HYPONYM-OF differentiable contact model. elastic / inelastic HYPONYM-OF contact mechanics. frictionless / frictional HYPONYM-OF contact mechanics. model USED-FOR inequality constraints. contact model USED-FOR Lagrangian and Hamiltonian neural networks. simultaneous learning of contact and system properties USED-FOR contact model. coefficients of restitution FEATURE-OF 2D and 3D physical systems. 2D and 3D physical systems EVALUATE-FOR framework. differentiable physics simulator USED-FOR downstream gradient - based optimization tasks. dynamics USED-FOR differentiable physics simulator. dynamics USED-FOR downstream gradient - based optimization tasks. planning and control HYPONYM-OF downstream gradient - based optimization tasks. OtherScientificTerm are inductive bias, and joint angles. ","This paper proposes a differentiable contact model for learning contact mechanics, frictionless/frictional, elastic/elastic, and elastic/inelastic contact models for physical systems with contacts and collisions. The authors propose a neural network architecture that incorporates Lagrangian or Hamiltonian dynamics into the dynamics to ensure energy conservation. Previous approaches are based on differential equations, which are computationally expensive to compute. This paper proposes to learn the inductive bias by learning the joint angles between two points in the system. The proposed contact model allows for simultaneous learning of contact and system properties, and the authors show that the proposed model can be used to enforce inequality constraints. The framework is evaluated on 2D and 3D physical systems that have coefficients of restitution, and on legged robots and robotic manipulators. The results show that this differentiable physics simulator is able to learn dynamics for downstream gradient-based optimization tasks such as planning and control. The paper also shows that the contact model can also be used in the training of Lagrangians and Hamiltonian neural networks.","This paper proposes a differentiable contact model for learning contact mechanics, frictionless/frictional, elastic/elastic, and elastic/inelastic contact models for physical systems with contacts and collisions. The authors propose a neural network architecture that incorporates Lagrangian or Hamiltonian dynamics into the dynamics to ensure energy conservation. Previous approaches are based on differential equations, which are computationally expensive to compute. This paper proposes to learn the inductive bias by learning the joint angles between two points in the system. The proposed contact model allows for simultaneous learning of contact and system properties, and the authors show that the proposed model can be used to enforce inequality constraints. The framework is evaluated on 2D and 3D physical systems that have coefficients of restitution, and on legged robots and robotic manipulators. The results show that this differentiable physics simulator is able to learn dynamics for downstream gradient-based optimization tasks such as planning and control. The paper also shows that the contact model can also be used in the training of Lagrangians and Hamiltonian neural networks."
10453,SP:82d59a3609dfd458f90f23d4e477c8b497e9dc18,"Lipschitz constant USED-FOR parameter trajectory. 1st layer bias FEATURE-OF NNs. bounded complexity EVALUATE-FOR NNs. Task is Benevolent Training Hypothesis ( BTH ). Metric is complexity. Method are deep neural network ( NN ), and stochastic training procedure. OtherScientificTerm are training dynamics, BTH, NN ’s Lipschitz constant, input space, Dropout, and trainingand datadependent generalization bound. ",This paper studies the Benevolent training hypothesis (BTH) for deep neural networks. The authors show that the Lipschitz constant of a deep neural network (DNN) is bounded in the presence of a 1st-layer bias. They also show that this is the case for any stochastic gradient descent algorithm.    The authors also provide a generalization bound for DNNs. ,This paper studies the Benevolent training hypothesis (BTH) for deep neural networks. The authors show that the Lipschitz constant of a deep neural network (DNN) is bounded in the presence of a 1st-layer bias. They also show that this is the case for any stochastic gradient descent algorithm.    The authors also provide a generalization bound for DNNs. 
10489,SP:9b329c915fa8d4045c167c9df37a49ee314d190e,"operation USED-FOR distribution. Forster transform HYPONYM-OF operation. disjoint mixture of few distributions USED-FOR distribution. polynomial - time algorithm USED-FOR distribution - independent PAC learning of halfspaces. distribution - independent PAC learning of halfspaces PART-OF Massart noise model. polynomial sample complexity FEATURE-OF polynomial - time algorithm. algorithms USED-FOR learning problem. sample complexity EVALUATE-FOR algorithms. OtherScientificTerm are anticoncentration properties, and bit complexity. ","This paper studies the anticoncentration properties of the Forster transform, an operation that can be used to learn a distribution over a disjoint mixture of few distributions. The authors propose a polynomial-time algorithm for the distribution-independent PAC learning of halfspaces in the Massart noise model, which has polynomials sample complexity. They also provide algorithms for the learning problem that achieve sample complexity of polynoms in the order of the bit complexity. ","This paper studies the anticoncentration properties of the Forster transform, an operation that can be used to learn a distribution over a disjoint mixture of few distributions. The authors propose a polynomial-time algorithm for the distribution-independent PAC learning of halfspaces in the Massart noise model, which has polynomials sample complexity. They also provide algorithms for the learning problem that achieve sample complexity of polynoms in the order of the bit complexity. "
10525,SP:e5229305af00067ae2dbabd903e585964aec8928,"models USED-FOR graph - based learning tasks. Graph neural networks USED-FOR graph - based learning tasks. Graph neural networks HYPONYM-OF models. adversarial attacks USED-FOR graph - level classification. biochemistry and social network analysis HYPONYM-OF real - life applications. Bayesian optimisation - based attack method USED-FOR graph classification models. graph properties CONJUNCTION constraints. constraints CONJUNCTION graph properties. constraints CONJUNCTION modes of attack. modes of attack CONJUNCTION constraints. graph properties FEATURE-OF graph classification tasks. graph classification tasks EVALUATE-FOR method. adversarial robustness EVALUATE-FOR graph classification models. Task is node - level classification tasks. OtherScientificTerm are unrealistic setups, perturbation, and adversarial samples. ","Graph neural networks are one of the most popular models for graph-based learning tasks. However, adversarial attacks against graph-level classification are not well-studied in real-life applications (e.g. biochemistry and social network analysis). This paper proposes a Bayesian optimisation-based attack method to attack graph classification models. The authors show that the adversarial robustness of node-level classifiers can be improved by perturbing the underlying graph properties, constraints, and modes of attack. The proposed method is evaluated on several graph classification tasks and shows that the proposed method can improve the robustness against adversarial perturbation. ","Graph neural networks are one of the most popular models for graph-based learning tasks. However, adversarial attacks against graph-level classification are not well-studied in real-life applications (e.g. biochemistry and social network analysis). This paper proposes a Bayesian optimisation-based attack method to attack graph classification models. The authors show that the adversarial robustness of node-level classifiers can be improved by perturbing the underlying graph properties, constraints, and modes of attack. The proposed method is evaluated on several graph classification tasks and shows that the proposed method can improve the robustness against adversarial perturbation. "
10561,SP:4999e5664383066fdacd14be6242c7b83f85f3dd,"distribution shifts FEATURE-OF Machine learning models. adaptation USED-FOR label distribution shift. adaptation USED-FOR online setting. online learning USED-FOR online label shift adaptation. Follow The Leader ( FTL ) CONJUNCTION Online Gradient Descent ( OGD ). Online Gradient Descent ( OGD ) CONJUNCTION Follow The Leader ( FTL ). Online Gradient Descent ( OGD ) USED-FOR adaptation algorithms. online learning techniques USED-FOR adaptation algorithms. Online Gradient Descent ( OGD ) HYPONYM-OF online learning techniques. Follow The Leader ( FTL ) HYPONYM-OF online learning techniques. OGD USED-FOR label shift scenarios. OtherScientificTerm are test - time label distribution, regret bounds, and simulated and real world label distribution shifts. Generic is model. Task is estimation of the expected test loss. ","This paper studies the problem of online label shift adaptation in the online setting. Machine learning models are sensitive to distribution shifts in the test-time label distribution, and adaptation to label distribution shift is an important problem in online learning. The authors propose two adaptation algorithms based on existing online learning techniques, namely Follow The Leader (FTL) and Online Gradient Descent (OGD), to adapt the model to the label distribution of the test distribution. They provide regret bounds for both simulated and real world label distribution shifts. The regret bounds are based on the estimation of the expected test loss. They also show that OGD can be used to adapt to different label shift scenarios. ","This paper studies the problem of online label shift adaptation in the online setting. Machine learning models are sensitive to distribution shifts in the test-time label distribution, and adaptation to label distribution shift is an important problem in online learning. The authors propose two adaptation algorithms based on existing online learning techniques, namely Follow The Leader (FTL) and Online Gradient Descent (OGD), to adapt the model to the label distribution of the test distribution. They provide regret bounds for both simulated and real world label distribution shifts. The regret bounds are based on the estimation of the expected test loss. They also show that OGD can be used to adapt to different label shift scenarios. "
10597,SP:806515ae07fb1c9d02773592005d53d4158ef102,distribution FEATURE-OF detection and localization of gradual changes. time - ordered observations USED-FOR distribution. time - ordered observations USED-FOR detection and localization of gradual changes. discontinuity jump in distribution USED-FOR abrupt setting. method USED-FOR detecting and localizing gradual changes. features FEATURE-OF distribution. prior knowledge FEATURE-OF distribution. prior knowledge FEATURE-OF features. detection CONJUNCTION localization. localization CONJUNCTION detection. method USED-FOR detection. method USED-FOR localization. Method is data generating model. ,"This paper proposes a method for detecting and localizing gradual changes in the distribution of time-ordered observations for the detection and localization of gradual changes from time-order observations. The authors consider the abrupt setting where there is a discontinuity jump in distribution, but the data generating model has not been trained. The proposed method is able to learn the features of the distribution with prior knowledge. The method is shown to be effective for both detection and for localization. ","This paper proposes a method for detecting and localizing gradual changes in the distribution of time-ordered observations for the detection and localization of gradual changes from time-order observations. The authors consider the abrupt setting where there is a discontinuity jump in distribution, but the data generating model has not been trained. The proposed method is able to learn the features of the distribution with prior knowledge. The method is shown to be effective for both detection and for localization. "
10633,SP:7a3c8a7b17ecab19361d36e1d3d73fa35b71214c,"brain USED-FOR blind source separation ( BSS ) problems. linear BSS problems PART-OF signal processing. Independent Component Analysis ( ICA ) USED-FOR linear BSS problems. neural architecture CONJUNCTION synaptic learning rules. synaptic learning rules CONJUNCTION neural architecture. objective function USED-FOR biologically plausible NN. objective function USED-FOR ICA. neural architecture PART-OF biologically plausible NN. synaptic learning rules PART-OF biologically plausible NN. synaptic plasticity USED-FOR algorithm. extracellular calcium CONJUNCTION local field potential. local field potential CONJUNCTION extracellular calcium. local field potential CONJUNCTION nitric oxide. nitric oxide CONJUNCTION local field potential. neuromodulators CONJUNCTION extracellular calcium. extracellular calcium CONJUNCTION neuromodulators. OtherScientificTerm are biological circuit, biophysical variables, and synapse. Method are ICA neural network ( NN ), and NN. Task is synaptic weight update. ","This paper studies the blind source separation (BSS) problems in the brain. The authors propose to use Independent Component Analysis (ICA) to solve the linear BSS problems in signal processing. They show that a biologically plausible NN with neural architecture and synaptic learning rules is biologically plausible under the objective function of ICA. They also show that the ICA neural network (NN) is able to learn the parameters of a biological circuit. The algorithm is based on the observation that synaptic plasticity plays a key role in the success of the algorithm, and that biophysical variables (e.g. neuromodulators, extracellular calcium, local field potential, and nitric oxide) are important for the success. The paper also shows that the performance of the NN is highly correlated with the synaptic weight update. ","This paper studies the blind source separation (BSS) problems in the brain. The authors propose to use Independent Component Analysis (ICA) to solve the linear BSS problems in signal processing. They show that a biologically plausible NN with neural architecture and synaptic learning rules is biologically plausible under the objective function of ICA. They also show that the ICA neural network (NN) is able to learn the parameters of a biological circuit. The algorithm is based on the observation that synaptic plasticity plays a key role in the success of the algorithm, and that biophysical variables (e.g. neuromodulators, extracellular calcium, local field potential, and nitric oxide) are important for the success. The paper also shows that the performance of the NN is highly correlated with the synaptic weight update. "
10669,SP:22f8b517a3df65144412938f5891c463d7bae0ab,"neural activity USED-FOR task - related behavior. Recurrent Neural Networks ( RNNs ) USED-FOR neural activity. Recurrent Neural Networks ( RNNs ) USED-FOR task - related behavior. neuroscience CONJUNCTION machine learning. machine learning CONJUNCTION neuroscience. space of solutions FEATURE-OF task. RNNs COMPARE neural data. neural data COMPARE RNNs. space of solutions USED-FOR tasks. two - neuron network USED-FOR task. discrete dynamical regimes USED-FOR diversity. Delayed discrimination CONJUNCTION Interval discrimination. Interval discrimination CONJUNCTION Delayed discrimination. Interval discrimination CONJUNCTION Time reproduction. Time reproduction CONJUNCTION Interval discrimination. Delayed discrimination HYPONYM-OF neuroscience - inspired tasks. Time reproduction HYPONYM-OF neuroscience - inspired tasks. Interval discrimination HYPONYM-OF neuroscience - inspired tasks. neural activity FEATURE-OF networks. extrapolation patterns USED-FOR dynamical objects. tool USED-FOR reduced dynamics of networks. compact directed graph USED-FOR tool. compact directed graph USED-FOR reduced dynamics of networks. Machine learning CONJUNCTION Neuroscience. Neuroscience CONJUNCTION Machine learning. Method is machine learning algorithms. OtherScientificTerm are underspecification, hidden structure, and neural features. Generic is representation. ","This paper investigates the neural activity of Recurrent Neural Networks (RNNs) in order to understand the task-related behavior of RNNs. The authors show that RNN's activity is similar to neural data, and that the space of solutions for a given task can be represented by a two-neuron network. They also show that the diversity of the solution space in discrete dynamical regimes can be modeled as a function of the number of neurons in a RNN.    The authors also propose a tool to study the reduced dynamics of networks based on a compact directed graph, which is a tool that can be applied to a wide range of machine learning algorithms. They show that a number of neuroscience-inspired tasks (Delayed discrimination, Interval discrimination, Time Reproduction, etc.) and machine learning (e.g., neuroscience, machine learning, and neuroscience) can be understood as a result of this representation. They further show that extrapolation patterns for dynamical objects are similar to those of the hidden structure of a neural network, which suggests that the neural features of a network can be used as a proxy for the underlying dynamics of the environment.","This paper investigates the neural activity of Recurrent Neural Networks (RNNs) in order to understand the task-related behavior of RNNs. The authors show that RNN's activity is similar to neural data, and that the space of solutions for a given task can be represented by a two-neuron network. They also show that the diversity of the solution space in discrete dynamical regimes can be modeled as a function of the number of neurons in a RNN.    The authors also propose a tool to study the reduced dynamics of networks based on a compact directed graph, which is a tool that can be applied to a wide range of machine learning algorithms. They show that a number of neuroscience-inspired tasks (Delayed discrimination, Interval discrimination, Time Reproduction, etc.) and machine learning (e.g., neuroscience, machine learning, and neuroscience) can be understood as a result of this representation. They further show that extrapolation patterns for dynamical objects are similar to those of the hidden structure of a neural network, which suggests that the neural features of a network can be used as a proxy for the underlying dynamics of the environment."
10705,SP:9b08a0f547ead3b59077a43b1052c6d46a0730f6,"Modeling distributions of covariates PART-OF unsupervised learning. density estimation PART-OF unsupervised learning. density estimation HYPONYM-OF Modeling distributions of covariates. arbitrary conditional density estimation USED-FOR conditional distribution. covariates FEATURE-OF conditional distribution. arbitrary conditional density estimation HYPONYM-OF problem. prior knowledge USED-FOR inference. unobserved features CONJUNCTION observed features xo. observed features xo CONJUNCTION unobserved features. ACE USED-FOR complexity. learning one - dimensional conditionals USED-FOR problem. energy function USED-FOR densities. approach COMPARE prior methods. prior methods COMPARE approach. arbitrary conditional likelihood estimation CONJUNCTION data imputation. data imputation CONJUNCTION arbitrary conditional likelihood estimation. ACE USED-FOR arbitrary conditional likelihood estimation. ACE USED-FOR data imputation. state - of - the - art USED-FOR arbitrary conditional likelihood estimation. state - of - the - art EVALUATE-FOR ACE. benchmarks EVALUATE-FOR state - of - the - art. benchmarks EVALUATE-FOR data imputation. benchmarks EVALUATE-FOR ACE. OtherScientificTerm are distributions of covariates, joint distribution, and one - dimensional conditionals. Generic is method. Method is Arbitrary Conditioning with Energy ( ACE ). ","This paper considers the problem of density estimation in unsupervised learning, i.e., modeling distributions of covariates in the context of density estimations. The authors propose a new problem, called arbitrary conditional density estimation, which is an extension of the previous problem of learning one-dimensional conditionals for the conditional distribution over the covariates. The proposed method, called Arbitrary Conditioning with Energy (ACE), is based on the observation that the density of the joint distribution can be approximated by the energy function of the densities, and that prior knowledge can be used for inference. The paper shows that ACE is able to reduce the complexity of the problem to a single-dimensional case by learning one of the one-dimensionality of the density. The approach is compared to prior methods and is shown to outperform the state-of-the-art in terms of performance in both arbitrary conditional likelihood estimation and data imputation on standard benchmarks.   ","This paper considers the problem of density estimation in unsupervised learning, i.e., modeling distributions of covariates in the context of density estimations. The authors propose a new problem, called arbitrary conditional density estimation, which is an extension of the previous problem of learning one-dimensional conditionals for the conditional distribution over the covariates. The proposed method, called Arbitrary Conditioning with Energy (ACE), is based on the observation that the density of the joint distribution can be approximated by the energy function of the densities, and that prior knowledge can be used for inference. The paper shows that ACE is able to reduce the complexity of the problem to a single-dimensional case by learning one of the one-dimensionality of the density. The approach is compared to prior methods and is shown to outperform the state-of-the-art in terms of performance in both arbitrary conditional likelihood estimation and data imputation on standard benchmarks.   "
10741,SP:f2b14f5854e6aa6922795d1d2051b7402486cef6,"MSE or L1 loss function USED-FOR low - level vision. single image super - resolution ( SISR ) HYPONYM-OF low - level vision. texture and edge areas COMPARE smooth areas. smooth areas COMPARE texture and edge areas. smooth areas PART-OF photographic images. adaptive weighted loss USED-FOR deep networks. adaptive weighted loss USED-FOR SISR. adaptive weighted loss USED-FOR situations. SISR USED-FOR deep networks. textured and edge pixels HYPONYM-OF situations. variance estimation USED-FOR SISR solutions. sparsity prior USED-FOR regularizing SISR solutions. uncertainty estimation USED-FOR regularizing SISR solutions. visual quality EVALUATE-FOR SISR. uncertainty - driven loss COMPARE MSE or L1 loss. MSE or L1 loss COMPARE uncertainty - driven loss. uncertainty - driven loss COMPARE loss functions. loss functions COMPARE uncertainty - driven loss. SISR networks EVALUATE-FOR uncertainty - driven loss. computation EVALUATE-FOR loss functions. OtherScientificTerm are visual information, pixel - by - pixel basis, high - resolution image ( mean ), and uncertainty ( variance ). Task is spatial adaptation. Method is network architectures. ","This paper proposes a new loss function for low-level vision, called single image super-resolution (SISR), which is a variant of the MSE or L1 loss function. SISR uses adaptive weighted loss to train deep networks in two situations: (1) when the visual information is sparse on a pixel-by-pixel basis, and (2) when high-resolution image (mean) and low-resolution images (variance) are not available. The paper shows that adaptive weights are effective for both situations, and that the adaptive weights improve the visual quality of the trained deep networks.  The paper also shows that texture and edge areas are more important than smooth areas in the original photographic images.    The main contribution of the paper is the use of a sparsity prior for regularizing the loss function, which is based on the assumption that the high-resolution image (measured by the mean) and the low-res (measurement of the variance) are close to each other. The variance estimation is used to regularize the regularization of the loss for the SisR solutions based on uncertainty estimation.  Experiments show that the uncertainty-driven loss outperforms the standard MSE/L1 loss and other loss functions in terms of computation and performance. The authors also show that adaptive weighting can be used to improve the performance of trained SISRs in two other situations (textured and edge pixels).   In addition, the paper shows how the spatial adaptation can be performed in the case of different network architectures. ","This paper proposes a new loss function for low-level vision, called single image super-resolution (SISR), which is a variant of the MSE or L1 loss function. SISR uses adaptive weighted loss to train deep networks in two situations: (1) when the visual information is sparse on a pixel-by-pixel basis, and (2) when high-resolution image (mean) and low-resolution images (variance) are not available. The paper shows that adaptive weights are effective for both situations, and that the adaptive weights improve the visual quality of the trained deep networks.  The paper also shows that texture and edge areas are more important than smooth areas in the original photographic images.    The main contribution of the paper is the use of a sparsity prior for regularizing the loss function, which is based on the assumption that the high-resolution image (measured by the mean) and the low-res (measurement of the variance) are close to each other. The variance estimation is used to regularize the regularization of the loss for the SisR solutions based on uncertainty estimation.  Experiments show that the uncertainty-driven loss outperforms the standard MSE/L1 loss and other loss functions in terms of computation and performance. The authors also show that adaptive weighting can be used to improve the performance of trained SISRs in two other situations (textured and edge pixels).   In addition, the paper shows how the spatial adaptation can be performed in the case of different network architectures. "
10777,SP:9997583f40fa648adf57bb4fc34228f357be0cf1,"PAC - Bayesian generalization bounds USED-FOR adversarial robustness. PACBayesian framework USED-FOR averaged risk. perturbations FEATURE-OF averaged risk. majority votes FEATURE-OF perturbations. robust model USED-FOR attacks. adversarial attacks HYPONYM-OF attacks. Generic is model. Method are worst - case analysis, theoretically founded analysis, and PAC - Bayesian framework. Task is learning phase. ","This paper provides PAC-Bayesian generalization bounds for adversarial robustness based on the PACBayesian framework. The authors provide a worst-case analysis of the generalization error of the model, which is based on a theoretically founded analysis. The paper also provides a PAC Bayesian framework for estimating the averaged risk under perturbations with majority votes. The main contribution of the paper is that the authors show that the robust model can be robust to several types of attacks, including adversarial attacks, and that a robust model is robust to such attacks in the learning phase.    The paper is well-written and well-motivated, and the authors have provided a theoretical analysis of their results. ","This paper provides PAC-Bayesian generalization bounds for adversarial robustness based on the PACBayesian framework. The authors provide a worst-case analysis of the generalization error of the model, which is based on a theoretically founded analysis. The paper also provides a PAC Bayesian framework for estimating the averaged risk under perturbations with majority votes. The main contribution of the paper is that the authors show that the robust model can be robust to several types of attacks, including adversarial attacks, and that a robust model is robust to such attacks in the learning phase.    The paper is well-written and well-motivated, and the authors have provided a theoretical analysis of their results. "
10813,SP:90b72e8dc41584e38f25dff9fb2853f5b11dc8fa,Logical reasoning USED-FOR querying mechanism. large and incomplete databases USED-FOR querying mechanism. Knowledge Graphs ( KGs ) USED-FOR Logical reasoning. spatial geometries USED-FOR query representations. spatial geometries USED-FOR approaches. boxes HYPONYM-OF spatial geometries. transformation tricks USED-FOR unions. Probabilistic Entity Representation Model ( PERM ) USED-FOR entities. semantic position CONJUNCTION smooth decision boundary. smooth decision boundary CONJUNCTION semantic position. Multivariate Gaussian density USED-FOR semantic position. Multivariate Gaussian density USED-FOR smooth decision boundary. mean and covariance parameters USED-FOR semantic position. Multivariate Gaussian density USED-FOR Probabilistic Entity Representation Model ( PERM ). mean and covariance parameters FEATURE-OF Multivariate Gaussian density. Multivariate Gaussian density USED-FOR entities. intersection CONJUNCTION union. union CONJUNCTION intersection. projection CONJUNCTION intersection. intersection CONJUNCTION projection. projection HYPONYM-OF closed logical operations. union HYPONYM-OF closed logical operations. intersection HYPONYM-OF closed logical operations. end - to - end objective function USED-FOR union. end - to - end objective function USED-FOR closed logical operations. PERM COMPARE state - of - the - art methods. state - of - the - art methods COMPARE PERM. logical query reasoning problem EVALUATE-FOR PERM. logical query reasoning problem EVALUATE-FOR state - of - the - art methods. evaluation metrics EVALUATE-FOR public benchmark KG datasets. public benchmark KG datasets EVALUATE-FOR state - of - the - art methods. evaluation metrics EVALUATE-FOR state - of - the - art methods. public benchmark KG datasets EVALUATE-FOR PERM. evaluation metrics EVALUATE-FOR PERM. work COMPARE methods. methods COMPARE work. F1 EVALUATE-FOR methods. F1 EVALUATE-FOR work. COVID-19 drugrepurposing case study EVALUATE-FOR PERM ’s competence. low - dimensional visualization of the Gaussian representations USED-FOR query answering process. Task is logical operations of projection and intersection. OtherScientific,"Logical reasoning in Knowledge Graphs (KGs) is an important problem for querying mechanism in large and incomplete databases. Logical reasoning is often performed by considering the logical operations of projection and intersection, but previous approaches rely on spatial geometries (e.g. boxes) to represent query representations. In this paper, the authors propose a Probabilistic Entity Representation Model (PERM) that represents entities as a Multivariate Gaussian density with mean and covariance parameters for semantic position and a smooth decision boundary. The proposed PERM uses transformation tricks to transform unions into closed logical operations (projection, intersection, and union). The authors also propose an end-to-end objective function for each union, which can be used to learn closed logical operators. The authors evaluate PERM on three public benchmark KG datasets and show that PERM outperforms state-of-the-art methods in terms of F1 and other evaluation metrics on the public benchmark datasets. PERM’s competence is also demonstrated on a COVID-19 drugrepurposing case study, where the query answering process is based on a low-dimensional visualization of the Gaussian representations. The results show that the proposed work outperforms previous methods on F1, and is competitive on the logical query reasoning problem.","Logical reasoning in Knowledge Graphs (KGs) is an important problem for querying mechanism in large and incomplete databases. Logical reasoning is often performed by considering the logical operations of projection and intersection, but previous approaches rely on spatial geometries (e.g. boxes) to represent query representations. In this paper, the authors propose a Probabilistic Entity Representation Model (PERM) that represents entities as a Multivariate Gaussian density with mean and covariance parameters for semantic position and a smooth decision boundary. The proposed PERM uses transformation tricks to transform unions into closed logical operations (projection, intersection, and union). The authors also propose an end-to-end objective function for each union, which can be used to learn closed logical operators. The authors evaluate PERM on three public benchmark KG datasets and show that PERM outperforms state-of-the-art methods in terms of F1 and other evaluation metrics on the public benchmark datasets. PERM’s competence is also demonstrated on a COVID-19 drugrepurposing case study, where the query answering process is based on a low-dimensional visualization of the Gaussian representations. The results show that the proposed work outperforms previous methods on F1, and is competitive on the logical query reasoning problem."
10849,SP:b6184c9732dbb7eba7c20cae8869d975c428efe4,"memory scaling CONJUNCTION gradient degradation issues. gradient degradation issues CONJUNCTION memory scaling. Gradient - based hyperparameter optimization USED-FOR few - shot meta - learning. algorithm USED-FOR memory scaling issues. forward - mode differentiation USED-FOR memory scaling issues. noise reduction properties EVALUATE-FOR algorithm. theoretical guarantees FEATURE-OF algorithm. noise reduction properties FEATURE-OF theoretical guarantees. greedy gradientbased alternatives COMPARE black - box methods. black - box methods COMPARE greedy gradientbased alternatives. hyperparameter search ranges FEATURE-OF CIFAR-10. Generic is tasks. OtherScientificTerm are hyperparameters, and greediness. Method is unrolled optimization. ","Gradient-based hyperparameter optimization is a popular technique for few-shot meta-learning. This paper proposes a new algorithm that addresses memory scaling and gradient degradation issues by forward-mode differentiation. The authors provide theoretical guarantees on the noise reduction properties of the proposed algorithm. They show that greedy gradientbased alternatives outperform existing black-box methods on a number of tasks. They also show that the hyperparameters of the algorithm are not greedy, and that the greediness is not due to unrolled optimization, but rather to the fact that the algorithm is able to deal with memory scaling issues. Finally, the authors provide experiments on CIFAR-10 with a variety of tasks, and show that their algorithm outperforms existing algorithms in terms of noise reduction.","Gradient-based hyperparameter optimization is a popular technique for few-shot meta-learning. This paper proposes a new algorithm that addresses memory scaling and gradient degradation issues by forward-mode differentiation. The authors provide theoretical guarantees on the noise reduction properties of the proposed algorithm. They show that greedy gradientbased alternatives outperform existing black-box methods on a number of tasks. They also show that the hyperparameters of the algorithm are not greedy, and that the greediness is not due to unrolled optimization, but rather to the fact that the algorithm is able to deal with memory scaling issues. Finally, the authors provide experiments on CIFAR-10 with a variety of tasks, and show that their algorithm outperforms existing algorithms in terms of noise reduction."
10885,SP:9c3a326e5ee4e862923d3bf9415f32a077db8534,"systems PART-OF Human reasoning. Neural sequence models USED-FOR structured tasks. neural sequence model USED-FOR candidate generations. symbolic reasoning module USED-FOR logical consistency. neural System 1 CONJUNCTION logical System 2. logical System 2 CONJUNCTION neural System 1. neural inference USED-FOR neural System 1. neural inference USED-FOR approach. story generation CONJUNCTION grounded instruction - following. grounded instruction - following CONJUNCTION story generation. accuracy EVALUATE-FOR neurally - based generations. coherence CONJUNCTION accuracy. accuracy CONJUNCTION coherence. coherence EVALUATE-FOR neurally - based generations. approach USED-FOR neurally - based generations. coherence EVALUATE-FOR approach. accuracy EVALUATE-FOR approach. Method are System 1, System 1 - like sequence models, and System 2 - inspired logical reasoning. Generic is they. ","This paper proposes a neural sequence models for structured tasks. Neural sequence models can be seen as systems in the context of Human reasoning, and the authors propose to learn candidate generations from a single neural sequence model. The idea is to learn a neural System 1 and a logical System 2, where System 1-like sequence models are used to generate a sequence of candidate generations, and a symbolic reasoning module is used to enforce logical consistency. The approach is based on neural inference, where neural inference is applied to the neural system 1 and neural System 2. Experiments on story generation and grounded instruction-following show that the proposed approach improves coherence and accuracy of neurally-based generations. The authors also show that System 2-inspired logical reasoning can be used to improve the coherence of neural systems, and that they are able to generalize to unseen tasks.","This paper proposes a neural sequence models for structured tasks. Neural sequence models can be seen as systems in the context of Human reasoning, and the authors propose to learn candidate generations from a single neural sequence model. The idea is to learn a neural System 1 and a logical System 2, where System 1-like sequence models are used to generate a sequence of candidate generations, and a symbolic reasoning module is used to enforce logical consistency. The approach is based on neural inference, where neural inference is applied to the neural system 1 and neural System 2. Experiments on story generation and grounded instruction-following show that the proposed approach improves coherence and accuracy of neurally-based generations. The authors also show that System 2-inspired logical reasoning can be used to improve the coherence of neural systems, and that they are able to generalize to unseen tasks."
10921,SP:d77d046095e4c8336c0c76ac48cb046923230753,"off - policy evaluation ( OPE ) USED-FOR continuous treatment settings. personalized dose - finding HYPONYM-OF continuous treatment settings. decision rule USED-FOR historical data. discrete treatment settings FEATURE-OF OPE. estimation method USED-FOR OPE. estimation method USED-FOR continuous treatments. deep jump learning USED-FOR estimation method. deep learning CONJUNCTION multiscale change point detection. multiscale change point detection CONJUNCTION deep learning. OPE methods USED-FOR continuous treatments. OPE methods USED-FOR discrete treatments. OtherScientificTerm are treatment decision rule, and treatment space. Generic is method. Method is deep discretization. Task is Warfarin Dosing. ","This paper studies the problem of off-policy evaluation (OPE) in continuous treatment settings, such as personalized dose-finding, where the treatment decision rule is not available for historical data. The authors propose a new estimation method for OPE in discrete treatment settings based on deep jump learning. The proposed method is based on the observation that deep learning and multiscale change point detection can be combined with deep discretization to improve the performance of OPE methods for continuous treatments. Experiments on Warfarin Dosing show that the proposed method outperforms existing methods. ","This paper studies the problem of off-policy evaluation (OPE) in continuous treatment settings, such as personalized dose-finding, where the treatment decision rule is not available for historical data. The authors propose a new estimation method for OPE in discrete treatment settings based on deep jump learning. The proposed method is based on the observation that deep learning and multiscale change point detection can be combined with deep discretization to improve the performance of OPE methods for continuous treatments. Experiments on Warfarin Dosing show that the proposed method outperforms existing methods. "
10957,SP:4d085e57286fdd36143108a002d16914222c239a,natural sciences CONJUNCTION engineering applications. engineering applications CONJUNCTION natural sciences. modeling framework USED-FOR inference in time - series data. Switching dynamical systems USED-FOR modeling framework. inference in time - series data USED-FOR engineering applications. inference in time - series data USED-FOR natural sciences. biology CONJUNCTION discrete - event systems. discrete - event systems CONJUNCTION biology. subordinated diffusion process FEATURE-OF Markov jump process. continuous time FEATURE-OF areas. Markov jump process USED-FOR model. biology HYPONYM-OF areas. discrete - event systems HYPONYM-OF areas. evolution equations USED-FOR prior and posterior marginal densities. Gaussian process approximation CONJUNCTION posterior inference. posterior inference CONJUNCTION Gaussian process approximation. posterior inference USED-FOR Markov jump processes. Gaussian process approximation USED-FOR diffusion level. posterior inference PART-OF continuous - time variational inference algorithm. Gaussian process approximation PART-OF continuous - time variational inference algorithm. path - wise Kullback - Leibler divergence USED-FOR Bayesian latent state estimates. variational expectation maximization USED-FOR point estimates of unknown system parameters. real - world examples EVALUATE-FOR algorithm. Material is time - series data. OtherScientificTerm is real axis. ,"This paper proposes a new modeling framework for inference in time-series data from Switching dynamical systems. The model is based on a Markov jump process with a subordinated diffusion process, which is used to model the prior and posterior marginal densities of the evolution equations. This is an important problem in both natural sciences and engineering applications where inference is important in both the natural sciences (biology and discrete-event systems) and in engineering applications (machine learning).  The authors propose a continuous-time variational inference algorithm that combines Gaussian process approximation for the diffusion level and posterior inference for Markov jumping processes. The authors use a path-wise Kullback-Leibler divergence for Bayesian latent state estimates and variational expectation maximization for point estimates of unknown system parameters.  The algorithm is evaluated on several real-world examples and is shown to perform well.   ","This paper proposes a new modeling framework for inference in time-series data from Switching dynamical systems. The model is based on a Markov jump process with a subordinated diffusion process, which is used to model the prior and posterior marginal densities of the evolution equations. This is an important problem in both natural sciences and engineering applications where inference is important in both the natural sciences (biology and discrete-event systems) and in engineering applications (machine learning).  The authors propose a continuous-time variational inference algorithm that combines Gaussian process approximation for the diffusion level and posterior inference for Markov jumping processes. The authors use a path-wise Kullback-Leibler divergence for Bayesian latent state estimates and variational expectation maximization for point estimates of unknown system parameters.  The algorithm is evaluated on several real-world examples and is shown to perform well.   "
10993,SP:d1f396e691f9d331adfb7b694a99c50e8004331f,"A HYPONYM-OF linear mapping. compressed sensing CONJUNCTION phase retrieval. phase retrieval CONJUNCTION compressed sensing. model USED-FOR signal processing problems. nonlinear processing function USED-FOR model. phase retrieval HYPONYM-OF signal processing problems. compressed sensing HYPONYM-OF signal processing problems. spectrum of sensing matrices HYPONYM-OF sensing matrices. expectation propagation algorithm ( EP ) HYPONYM-OF recovery methods. spikiness FEATURE-OF spectrum. measure USED-FOR EP. EP USED-FOR recovery. spikiness of the spectrum USED-FOR EP recovery. Task are nonlinear inverse problem, phase - retrieval problems, and 1 - bit compressed sensing problems. Method are componentwise nonlinear transformation, and optimal sensing systems. OtherScientificTerm are f, spikier spectrums, and sub - Gaussian and orthogonal matrices. Generic is framework. ","This paper considers the nonlinear inverse problem, which is a componentwise nonlinear transformation of a linear mapping (e.g., A) to a nonlinear mapping f. The authors propose a new model for two signal processing problems: compressed sensing and phase retrieval. The model is based on a previously proposed nonlinear processing function, and the authors show that f can be decomposed into a spectrum of sensing matrices (i.e., the spectrum of the mapping from f to f). The authors also show that for phase-retrieval problems, the spikier spectrums of f are more likely to be the optimal sensing systems.  The authors then propose two recovery methods based on the expectation propagation algorithm (EP) and show that EP can be recovered from the EP based on spikiness of the spectrum. They also provide a new measure for EP that can be used for recovery.  Finally, the authors provide some experiments on 1-bit compressed sensing problems, showing that the EP can recover the optimal solution in the case of sub-Gaussian and orthogonal matrices.   ","This paper considers the nonlinear inverse problem, which is a componentwise nonlinear transformation of a linear mapping (e.g., A) to a nonlinear mapping f. The authors propose a new model for two signal processing problems: compressed sensing and phase retrieval. The model is based on a previously proposed nonlinear processing function, and the authors show that f can be decomposed into a spectrum of sensing matrices (i.e., the spectrum of the mapping from f to f). The authors also show that for phase-retrieval problems, the spikier spectrums of f are more likely to be the optimal sensing systems.  The authors then propose two recovery methods based on the expectation propagation algorithm (EP) and show that EP can be recovered from the EP based on spikiness of the spectrum. They also provide a new measure for EP that can be used for recovery.  Finally, the authors provide some experiments on 1-bit compressed sensing problems, showing that the EP can recover the optimal solution in the case of sub-Gaussian and orthogonal matrices.   "
11029,SP:ee66604d4da9fd04826e90ccbb94f0499eba4c63,auxiliary semantic information USED-FOR Generalized Zero - Shot Learning ( GZSL ). category attributes HYPONYM-OF auxiliary semantic information. cross - domain transferability CONJUNCTION category discriminability. category discriminability CONJUNCTION cross - domain transferability. category discriminability EVALUATE-FOR visual representations. cross - domain transferability EVALUATE-FOR visual representations. prototypes USED-FOR prototypical visual patterns. attribute prototypes USED-FOR DPPN. DPPN USED-FOR attribute - related local regions. attribute prototypes USED-FOR attribute - region correspondence. DPPN USED-FOR attribute - region correspondence. attribute prototypes USED-FOR DPPN. DPPN USED-FOR visual representations. semantic - visual alignment CONJUNCTION representation transferability. representation transferability CONJUNCTION semantic - visual alignment. attribute localization ability FEATURE-OF visual representations. DPPN USED-FOR visual representations. progressive attribute localization CONJUNCTION DPPN. DPPN CONJUNCTION progressive attribute localization. category prototypes USED-FOR DPPN. unifed framework USED-FOR visual representations. DPPN USED-FOR visual representations. unifed framework USED-FOR attribute and category prototypes. DPPN USED-FOR domain shift problem. DPPN USED-FOR GZSL. domain shift problem FEATURE-OF GZSL. Generic is approach. Method is Dual Progressive Prototype Network ( DPPN ). ,"This paper proposes a new approach for zero-shot learning based on auxiliary semantic information (i.e., category attributes) for Generalized Zero-Shot Learning (GZSL). The proposed approach, called Dual Progressive Prototype Network (DPPN), aims to improve cross-domain transferability and category discriminability of visual representations. To achieve this, DPPN uses attribute prototypes as prototypical visual patterns and uses these prototypes to learn the attribute-related local regions. The attribute-region correspondence between the attribute prototypes and the corresponding category prototypes is learned by DPPP. Experiments on semantic-visual alignment and representation transferability show that the proposed approach achieves state-of-the-art performance. The paper also shows that the attribute localization ability of the visual representations learned by the proposed visual representations can be improved by combining progressive attribute localization and DPPn. Finally, the paper proposes an unifed framework for learning the attribute and category prototypes for visual representations, which can be used to improve the performance of the learned visual representations in GZSL. ","This paper proposes a new approach for zero-shot learning based on auxiliary semantic information (i.e., category attributes) for Generalized Zero-Shot Learning (GZSL). The proposed approach, called Dual Progressive Prototype Network (DPPN), aims to improve cross-domain transferability and category discriminability of visual representations. To achieve this, DPPN uses attribute prototypes as prototypical visual patterns and uses these prototypes to learn the attribute-related local regions. The attribute-region correspondence between the attribute prototypes and the corresponding category prototypes is learned by DPPP. Experiments on semantic-visual alignment and representation transferability show that the proposed approach achieves state-of-the-art performance. The paper also shows that the attribute localization ability of the visual representations learned by the proposed visual representations can be improved by combining progressive attribute localization and DPPn. Finally, the paper proposes an unifed framework for learning the attribute and category prototypes for visual representations, which can be used to improve the performance of the learned visual representations in GZSL. "
11065,SP:61eb6297568c3f6869fbb03eaf6a21260de5466c,"Defocus blur HYPONYM-OF blur effects. blur effects PART-OF images. end - to - end deep learning approach USED-FOR removing defocus blur. all - in - focus image USED-FOR consequent vision tasks. end - to - end deep learning approach USED-FOR all - in - focus image. accuracy EVALUATE-FOR models. linear parametric form FEATURE-OF spatially variant defocus blur kernels. fixed - point iteration USED-FOR GKM - based deblurring. fixed - point iteration USED-FOR deep neural network. GKMNet HYPONYM-OF deep neural network. scale - recurrent attention module USED-FOR mixing coefficients. GKM USED-FOR defocus deblurring. mixing coefficients PART-OF GKM. lightweight scale - recurrent architecture CONJUNCTION scale - recurrent attention module. scale - recurrent attention module CONJUNCTION lightweight scale - recurrent architecture. mixing coefficients USED-FOR defocus deblurring. scale - recurrent attention module USED-FOR GKMNet. lightweight scale - recurrent architecture USED-FOR GKMNet. model complexity CONJUNCTION computational efficiency. computational efficiency CONJUNCTION model complexity. GKMNet COMPARE defocus deblurring methods. defocus deblurring methods COMPARE GKMNet. computational efficiency EVALUATE-FOR GKMNet. model complexity EVALUATE-FOR GKMNet. OtherScientificTerm are spatially variant amount, and defocus blur. ","Defocus blur is one of the most common blur effects in images, and this paper proposes an end-to-end deep learning approach for removing defocus blur from an all-in-focus image for consequent vision tasks. The paper shows that models trained with GKM have a linear parametric form of the spatially variant blur kernels, and that the accuracy of the models is highly correlated with the amount of blur in the input image. The authors propose a deep neural network called GKKMNet, which uses a fixed-point iteration to perform the standard GkM-based deblurring. They also propose a lightweight scale-recurrent architecture and a scale-reward attention module to learn the mixing coefficients of the GKMs, which is used to further reduce the model complexity and improve the computational efficiency. Experiments show that the proposed GKMCNet outperforms the state-of-the-art in terms of model complexity, computational efficiency, and accuracy.   ","Defocus blur is one of the most common blur effects in images, and this paper proposes an end-to-end deep learning approach for removing defocus blur from an all-in-focus image for consequent vision tasks. The paper shows that models trained with GKM have a linear parametric form of the spatially variant blur kernels, and that the accuracy of the models is highly correlated with the amount of blur in the input image. The authors propose a deep neural network called GKKMNet, which uses a fixed-point iteration to perform the standard GkM-based deblurring. They also propose a lightweight scale-recurrent architecture and a scale-reward attention module to learn the mixing coefficients of the GKMs, which is used to further reduce the model complexity and improve the computational efficiency. Experiments show that the proposed GKMCNet outperforms the state-of-the-art in terms of model complexity, computational efficiency, and accuracy.   "
11101,SP:18bf447c90935c373e5ec4cdfbbf8f2a273d2edb,"models USED-FOR SSVRL. visual content PART-OF videos. RGB frames CONJUNCTION motion vectors. motion vectors CONJUNCTION RGB frames. motion vectors USED-FOR low - resolution optical flows. compressed videos USED-FOR motion vectors. supervision signals FEATURE-OF motion vectors. multi - instance InfoNCE loss USED-FOR cross guidance contrastive learning algorithm. downstream tasks EVALUATE-FOR MVCGC. MVCGC COMPARE competitors. competitors COMPARE MVCGC. Generic are methods, and method. OtherScientificTerm is mutual information. Metric is storage and computation efficiency. ","This paper proposes a new contrastive contrastive learning method for low-resolution video. The proposed method is based on the idea of mutual information between RGB frames and motion vectors, which can be used to train models for SSVRL. Specifically, the authors propose to use motion vectors from compressed videos with low-resolution optical flows, where the visual content of the videos is not available, but only the RGB frames are available. The authors propose a cross-guided learning algorithm based on a multi-instance InfoNCE loss to learn the motion vectors with respect to supervision signals. Experiments on several downstream tasks show that MVCGC outperforms competitors in terms of both storage and computation efficiency. ","This paper proposes a new contrastive contrastive learning method for low-resolution video. The proposed method is based on the idea of mutual information between RGB frames and motion vectors, which can be used to train models for SSVRL. Specifically, the authors propose to use motion vectors from compressed videos with low-resolution optical flows, where the visual content of the videos is not available, but only the RGB frames are available. The authors propose a cross-guided learning algorithm based on a multi-instance InfoNCE loss to learn the motion vectors with respect to supervision signals. Experiments on several downstream tasks show that MVCGC outperforms competitors in terms of both storage and computation efficiency. "
11137,SP:8c7b1d976d9758cd534c565ec31a23f97892e503,"Bayesian treatment USED-FOR overconfidence. Bayesian treatment USED-FOR ReLU nets. overconfidence FEATURE-OF ReLU nets. features FEATURE-OF BNN. ReLU features USED-FOR Bayesian linear models. it USED-FOR BNNs. model COMPARE BNNs. BNNs COMPARE model. infinite ReLU features FEATURE-OF finite ReLU BNNs. GP USED-FOR finite ReLU BNNs. model USED-FOR GP posterior. it USED-FOR ReLU BNN. Method are ReLU Bayesian neural networks ( BNNs ), and Gaussian process ( GP ). OtherScientificTerm are infinite - width limit, and asymptotic overconfidence. ","This paper studies the overconfidence of ReLU Bayesian neural networks (BNNs) in the infinite-width limit. The authors propose a Bayesian treatment for overconfidence in ReLU nets, which is based on the Gaussian process (GP). The authors show that under certain assumptions on the features of the BNN, Bayesian linear models with ReLU features can be approximated by ReLU linear models. They also show that the infinite ReLU BNNs with infinite GP are equivalent to finite ReLU neural networks with infinite width. Finally, they show that it is possible to approximate the GP posterior of the model with the same number of parameters as the Bnns, and that it can be used to approximate any BNN with finite width.  ","This paper studies the overconfidence of ReLU Bayesian neural networks (BNNs) in the infinite-width limit. The authors propose a Bayesian treatment for overconfidence in ReLU nets, which is based on the Gaussian process (GP). The authors show that under certain assumptions on the features of the BNN, Bayesian linear models with ReLU features can be approximated by ReLU linear models. They also show that the infinite ReLU BNNs with infinite GP are equivalent to finite ReLU neural networks with infinite width. Finally, they show that it is possible to approximate the GP posterior of the model with the same number of parameters as the Bnns, and that it can be used to approximate any BNN with finite width.  "
11173,SP:e77276f61626e896f6a985296f1d832129242cdf,"tools USED-FOR finite - sample confidence bounds. LUCB CONJUNCTION Successive Elimination. Successive Elimination CONJUNCTION LUCB. tools USED-FOR estimation of potentially complex nuisance functions. finite - sample confidence bounds USED-FOR estimation of potentially complex nuisance functions. finite - sample confidence bounds USED-FOR asymptotic variance. Successive Elimination USED-FOR best - arm - identification algorithms. LUCB USED-FOR best - arm - identification algorithms. bounds USED-FOR best - arm - identification algorithms. sample complexity EVALUATE-FOR upper bounds. upper bounds EVALUATE-FOR method. sample complexity EVALUATE-FOR method. OtherScientificTerm are data collection mechanism, and arm. Method is bestarm - identification bandit framework. Material is artificially generated data. ","This paper proposes two tools to improve the finite-sample confidence bounds for the estimation of potentially complex nuisance functions. The main contribution of the paper is to extend the bestarm-identification bandit framework to the case where the data collection mechanism is non-convex. The authors show that the asymptotic variance of the best-arm-id identification algorithms based on LUCB and Successive Elimination can be approximated by finite-sampling confidence bounds. They also show that these bounds can be used to derive the upper bounds for best-armed-invalidation algorithms. Finally, they show that their method achieves a sample complexity of $O(\sqrt{n\log n})$ for artificially generated data. ","This paper proposes two tools to improve the finite-sample confidence bounds for the estimation of potentially complex nuisance functions. The main contribution of the paper is to extend the bestarm-identification bandit framework to the case where the data collection mechanism is non-convex. The authors show that the asymptotic variance of the best-arm-id identification algorithms based on LUCB and Successive Elimination can be approximated by finite-sampling confidence bounds. They also show that these bounds can be used to derive the upper bounds for best-armed-invalidation algorithms. Finally, they show that their method achieves a sample complexity of $O(\sqrt{n\log n})$ for artificially generated data. "
11209,SP:471361588bfc6c6033631509d1e43e77fd9721ce,"scalability EVALUATE-FOR distributed learning. communication FEATURE-OF gradient. algorithm USED-FOR biased compression. variance FEATURE-OF stochastic gradient. moving average USED-FOR history gradients. moving average USED-FOR variance. compression error USED-FOR ErrorCompensatedX. asymptotic convergence rate EVALUATE-FOR ErrorCompensatedX. unified theoretical analysis framework USED-FOR variance reduced algorithms. Metric are Communication cost, communication cost, and convergence speed. Method are stochastic gradient descent, training without compression, and error compensation. ","This paper considers the problem of distributed learning with the goal of improving the scalability of the communication in distributed learning. Communication cost is a major factor in the communication cost. The authors propose a new algorithm called ErrorCompensatedX to address the issue of biased compression. The algorithm is based on the observation that the variance of stochastic gradient descent in training without compression can be reduced by training with compression without error compensation. To achieve this, the authors propose to use a moving average of the history gradients to reduce the variance in the gradient. The variance reduced algorithms are analyzed under a unified theoretical analysis framework, and the authors show that the convergence speed is O(1/\sqrt{n}) for training with and without compression, and O(O(n^{-1/2}) for learning with compression error. They also provide an asymptotic convergence rate that matches the best known one for error compensation, which is the one for the algorithm that uses the moving average.   ","This paper considers the problem of distributed learning with the goal of improving the scalability of the communication in distributed learning. Communication cost is a major factor in the communication cost. The authors propose a new algorithm called ErrorCompensatedX to address the issue of biased compression. The algorithm is based on the observation that the variance of stochastic gradient descent in training without compression can be reduced by training with compression without error compensation. To achieve this, the authors propose to use a moving average of the history gradients to reduce the variance in the gradient. The variance reduced algorithms are analyzed under a unified theoretical analysis framework, and the authors show that the convergence speed is O(1/\sqrt{n}) for training with and without compression, and O(O(n^{-1/2}) for learning with compression error. They also provide an asymptotic convergence rate that matches the best known one for error compensation, which is the one for the algorithm that uses the moving average.   "
11245,SP:3b7ff0dc668cac2191d95fcc4dc6e0335dec3206,"graph USED-FOR model. local explainability CONJUNCTION global explainability. global explainability CONJUNCTION local explainability. performant paradigm USED-FOR multi - grained explainability. pre - training and fine - tuning idea USED-FOR explainer. pre - training and fine - tuning idea USED-FOR multi - grained explanations. explainer USED-FOR multi - grained explanations. synthetic and real - world datasets EVALUATE-FOR explainer. AUC EVALUATE-FOR baselines. explainer COMPARE baselines. baselines COMPARE explainer. explaining graph classification EVALUATE-FOR baselines. AUC EVALUATE-FOR explaining graph classification. synthetic and real - world datasets EVALUATE-FOR baselines. explaining graph classification EVALUATE-FOR explainer. AUC EVALUATE-FOR explainer. Method are graph neural network ( GNN ), explainers, pre - training phase, and fine - tuning phase. Metric is explainability. Generic is approaches. OtherScientificTerm are class - wise patterns, local context, and class - wise characteristics. ","This paper proposes a new approach to explain the performance of graph neural network (GNN) models. The authors argue that existing GNN explainers tend to focus on local explainability (i.e., explainability of the model on a given graph) and ignore class-wise patterns. They argue that these approaches are not performant because they do not consider the local context and do not take into account the global context. They propose a performant paradigm for multi-grained explainability, i.e. the model is trained on a single graph, and then a pre-training phase is followed by a fine-tuning phase. They show that their explainer outperforms baselines in terms of AUC for explaining graph classification on both synthetic and real-world datasets. They also show that the proposed explainer is able to provide multi-granularity in explaining the performance, which is a result of the pre-train and fine-tune idea for the explainer.   ","This paper proposes a new approach to explain the performance of graph neural network (GNN) models. The authors argue that existing GNN explainers tend to focus on local explainability (i.e., explainability of the model on a given graph) and ignore class-wise patterns. They argue that these approaches are not performant because they do not consider the local context and do not take into account the global context. They propose a performant paradigm for multi-grained explainability, i.e. the model is trained on a single graph, and then a pre-training phase is followed by a fine-tuning phase. They show that their explainer outperforms baselines in terms of AUC for explaining graph classification on both synthetic and real-world datasets. They also show that the proposed explainer is able to provide multi-granularity in explaining the performance, which is a result of the pre-train and fine-tune idea for the explainer.   "
11281,SP:9b5a62d3a2b27bc60da28980e9fb0ecdff1215c0,"subgraph USED-FOR methods. method USED-FOR counterfactual explanations. GNNs USED-FOR counterfactual explanations. GNNs USED-FOR common decision logic. common decision boundaries USED-FOR GNN. GNN USED-FOR they. common decision boundaries USED-FOR they. Method is Graph Neural Networks ( GNNs ). OtherScientificTerm are noise, human intuition, explanations, and edges. ","This paper proposes a method to generate counterfactual explanations for Graph Neural Networks (GNNs). The methods are based on a subgraph, where the noise is added to each node in the subgraph. The authors show that GNNs are able to learn a common decision logic, and that they can be used to generate explanations that are consistent with human intuition. They also show that the explanations can be generated by using common decision boundaries in the GNN. ","This paper proposes a method to generate counterfactual explanations for Graph Neural Networks (GNNs). The methods are based on a subgraph, where the noise is added to each node in the subgraph. The authors show that GNNs are able to learn a common decision logic, and that they can be used to generate explanations that are consistent with human intuition. They also show that the explanations can be generated by using common decision boundaries in the GNN. "
11317,SP:4edb870786c9cea2c6075359cb4e79b02a8e2f5f,"information bottleneck CONJUNCTION adversarial feedback. adversarial feedback CONJUNCTION information bottleneck. information bottleneck USED-FOR VoiceMixer. adversarial feedback USED-FOR VoiceMixer. self - supervised representation learning USED-FOR information bottleneck. self - supervision USED-FOR model. adversarial feedback USED-FOR discriminator. voice style FEATURE-OF generalization. content and style discriminator PART-OF discriminator. generalization EVALUATE-FOR model. self - supervision USED-FOR content and style discriminator. transfer EVALUATE-FOR model. content information USED-FOR audio quality. audio quality EVALUATE-FOR model. Task is voice conversion. Material is converted voice. OtherScientificTerm are converted speech containing source speech style, and source speech content. ","This paper tackles the problem of voice conversion, where the goal is to convert the converted voice to a speech that is similar to the original voice, but the converted speech containing source speech style is different from the original speech. The authors propose VoiceMixer, which uses an information bottleneck and adversarial feedback to improve the generalization ability of voice style. The idea is to use self-supervised representation learning to tackle the information bottleneck, which is achieved by training a discriminator that is trained with adversarial training. The discriminator is composed of a content and style discriminator, and the discriminator can be trained with self-submission. Experiments show that the proposed model achieves better generalization and transfer performance when the source speech content is not available. The model also shows that the content information helps improve the audio quality. ","This paper tackles the problem of voice conversion, where the goal is to convert the converted voice to a speech that is similar to the original voice, but the converted speech containing source speech style is different from the original speech. The authors propose VoiceMixer, which uses an information bottleneck and adversarial feedback to improve the generalization ability of voice style. The idea is to use self-supervised representation learning to tackle the information bottleneck, which is achieved by training a discriminator that is trained with adversarial training. The discriminator is composed of a content and style discriminator, and the discriminator can be trained with self-submission. Experiments show that the proposed model achieves better generalization and transfer performance when the source speech content is not available. The model also shows that the content information helps improve the audio quality. "
11353,SP:9fbb0c6beb3f8f88972f13dcf0e1fe7db03233c7,"Siamese voxel - to - BEV tracker USED-FOR tracking. sparse 3D point clouds FEATURE-OF tracking. Siamese shape - aware feature learning network CONJUNCTION voxel - to - BEV target localization network. voxel - to - BEV target localization network CONJUNCTION Siamese shape - aware feature learning network. Siamese shape - aware feature learning network PART-OF it. voxel - to - BEV target localization network PART-OF it. Siamese shape - aware feature learning network USED-FOR discriminative features. Siamese shape - aware feature learning network USED-FOR 3D shape information. dense 3D shape USED-FOR shape information. 2D center CONJUNCTION z - axis center. z - axis center CONJUNCTION 2D center. 2D center CONJUNCTION z - axis center. z - axis center CONJUNCTION 2D center. voxelized point cloud USED-FOR dense BEV feature map. max pooling USED-FOR dense BEV feature map. max pooling USED-FOR voxelized point cloud. KITTI and nuScenes datasets EVALUATE-FOR method. method COMPARE state - of - the - art methods. state - of - the - art methods COMPARE method. Task is 3D object tracking in point clouds. Material is point clouds. OtherScientificTerm are dynamic environments, sparse point clouds, and template ’s feature. Method are template feature embedding, and voxel - toBEV target localization network. ","This paper proposes a Siamese voxel-to-BEV tracker for 3D object tracking in point clouds. The tracking is performed on sparse 3D point clouds, which is challenging in dynamic environments due to the sparse point clouds and lack of training data. The proposed method is based on a template feature embedding, and it is composed of two components:  1. A Siamesa shape-aware feature learning network that learns the discriminative features between a 2D center and a z-axis center, and 2. A voxels-toBEV target localization network that uses a voxeled point cloud with max pooling to learn a dense BEV feature map.  2. The Siamesed Shape-aware Feature Learning Network (Siamese Shape-Aware) is used to extract 3D shape information from a dense 3Dshape.  Experiments on KITTI and nuScenes datasets show that the proposed method outperforms the state-of-the-art methods. ","This paper proposes a Siamese voxel-to-BEV tracker for 3D object tracking in point clouds. The tracking is performed on sparse 3D point clouds, which is challenging in dynamic environments due to the sparse point clouds and lack of training data. The proposed method is based on a template feature embedding, and it is composed of two components:  1. A Siamesa shape-aware feature learning network that learns the discriminative features between a 2D center and a z-axis center, and 2. A voxels-toBEV target localization network that uses a voxeled point cloud with max pooling to learn a dense BEV feature map.  2. The Siamesed Shape-aware Feature Learning Network (Siamese Shape-Aware) is used to extract 3D shape information from a dense 3Dshape.  Experiments on KITTI and nuScenes datasets show that the proposed method outperforms the state-of-the-art methods. "
11389,SP:8b788c78680a54c453a04f4551436763ee57585e,Positional encoding USED-FOR attention - based deep model architectures. Transformer HYPONYM-OF attention - based deep model architectures. learnable Fourier features USED-FOR positional encoding method. multi - layer perceptron USED-FOR trainable encoding. learnable Fourier feature mapping USED-FOR trainable encoding. representation USED-FOR spatial multi - dimensional position. L2 distances CONJUNCTION positional relationships. positional relationships CONJUNCTION L2 distances. image FEATURE-OF pixel positions. image HYPONYM-OF spatial multi - dimensional position. pixel positions HYPONYM-OF spatial multi - dimensional position. learnable Fourier feature representation USED-FOR multi - dimensional positional encoding. learnable Fourier feature representation COMPARE methods. methods COMPARE learnable Fourier feature representation. faster convergence EVALUATE-FOR methods. accuracy EVALUATE-FOR learnable Fourier feature representation. accuracy EVALUATE-FOR methods. Method is Attentional mechanisms. ,"This paper proposes a positional encoding method based on learnable Fourier features for attention-based deep model architectures such as the Transformer. Attentional mechanisms are used to encode spatial multi-dimensional position (e.g. pixel positions in an image) and positional relationships between two points in the image. A trainable encoding is learned using a multi-layer perceptron, which uses a learnable feature mapping. The representation is then used to represent a spatial multi dimensional position, i.e. the L2 distances between the two points and their positional relationships. The paper shows that the learnability of the multi-dimensions of the representation is beneficial for multi-dimension positional encoding, and that the proposed positional encoding can be trained with a single multi-layered perceptron. Experiments are conducted to show that learning the multi dimensional positional encoding with learnable fourier feature representation leads to better accuracy and faster convergence than existing methods. ","This paper proposes a positional encoding method based on learnable Fourier features for attention-based deep model architectures such as the Transformer. Attentional mechanisms are used to encode spatial multi-dimensional position (e.g. pixel positions in an image) and positional relationships between two points in the image. A trainable encoding is learned using a multi-layer perceptron, which uses a learnable feature mapping. The representation is then used to represent a spatial multi dimensional position, i.e. the L2 distances between the two points and their positional relationships. The paper shows that the learnability of the multi-dimensions of the representation is beneficial for multi-dimension positional encoding, and that the proposed positional encoding can be trained with a single multi-layered perceptron. Experiments are conducted to show that learning the multi dimensional positional encoding with learnable fourier feature representation leads to better accuracy and faster convergence than existing methods. "
11425,SP:d2ac1b6381315bce4449f09bd519f33a2a42d714,"latent variables CONJUNCTION selection bias. selection bias CONJUNCTION latent variables. causal MAG FEATURE-OF system. observational data USED-FOR system. observational data USED-FOR causal MAG. Constraint - based methods USED-FOR problem. latter USED-FOR CI tests. computational complexity EVALUATE-FOR former. lower bound USED-FOR constraint - based method. lower bound USED-FOR CI tests. CI tests USED-FOR constraint - based method. upper bound CONJUNCTION approach. approach CONJUNCTION upper bound. approach COMPARE state of the art. state of the art COMPARE approach. synthetic and real - world structures EVALUATE-FOR state of the art. synthetic and real - world structures EVALUATE-FOR approach. Generic are methods, and technique. OtherScientificTerm are large graphs, completeness guarantees, structure, and conditional independence ( CI ) tests. Method is recursive constraint - based method. ","This paper considers the problem of conditional independence (CI) tests for large graphs, where there are latent variables and a selection bias between the system's causal MAG and observational data. Constraint-based methods have been proposed to solve this problem, but these methods are computationally expensive. This paper proposes a new technique, called a recursive constraint-based method, which is able to achieve completeness guarantees in the case of large graphs where the structure of the system is not known. The authors show that the resulting technique is computationally efficient.    The authors also propose a new lower bound for the computational complexity of the proposed constraint - based method based on existing CI tests, and show that this lower bound matches the upper bound of the existing lower bound. The latter is used to compute the CI tests and the former to compute a lower bound on the number of samples required to complete the test. The paper also shows that the proposed upper bound and approach outperform the state of the art on both synthetic and real-world structures. ","This paper considers the problem of conditional independence (CI) tests for large graphs, where there are latent variables and a selection bias between the system's causal MAG and observational data. Constraint-based methods have been proposed to solve this problem, but these methods are computationally expensive. This paper proposes a new technique, called a recursive constraint-based method, which is able to achieve completeness guarantees in the case of large graphs where the structure of the system is not known. The authors show that the resulting technique is computationally efficient.    The authors also propose a new lower bound for the computational complexity of the proposed constraint - based method based on existing CI tests, and show that this lower bound matches the upper bound of the existing lower bound. The latter is used to compute the CI tests and the former to compute a lower bound on the number of samples required to complete the test. The paper also shows that the proposed upper bound and approach outperform the state of the art on both synthetic and real-world structures. "
11461,SP:49a4912ce457f5f5ec62c44fa10444af8075fabf,information parallelism USED-FOR online decision making problems. stochastic multi - arm bandit CONJUNCTION linear contextual bandit. linear contextual bandit CONJUNCTION stochastic multi - arm bandit. batch Thompson Sampling framework USED-FOR canonical online decision making problems. linear contextual bandit HYPONYM-OF canonical online decision making problems. stochastic multi - arm bandit HYPONYM-OF canonical online decision making problems. asymptotic ) regret bound EVALUATE-FOR batch Thompson Sampling policy. batch policy USED-FOR exploration - exploitation trade - off. batch policy USED-FOR exponential reduction. dynamic batch allocation COMPARE natural baselines. natural baselines COMPARE dynamic batch allocation. static batch allocations HYPONYM-OF natural baselines. ,"This paper studies online decision making problems with information parallelism. The authors propose the batch Thompson Sampling framework to solve two canonical online decision-making problems, the stochastic multi-arm bandit and the linear contextual bandit. They provide a (asymptotic) regret bound for the case of a dynamic batch allocation, and show that the batch policy is able to achieve an exponential reduction in the exploration-exploitation trade-off when the number of samples is large enough. They also show that dynamic batch allocations outperform natural baselines (e.g., static batch allocations). ","This paper studies online decision making problems with information parallelism. The authors propose the batch Thompson Sampling framework to solve two canonical online decision-making problems, the stochastic multi-arm bandit and the linear contextual bandit. They provide a (asymptotic) regret bound for the case of a dynamic batch allocation, and show that the batch policy is able to achieve an exponential reduction in the exploration-exploitation trade-off when the number of samples is large enough. They also show that dynamic batch allocations outperform natural baselines (e.g., static batch allocations). "
11497,SP:653a519e3c799c25e0d0b4240322642040b121a3,"multiple source DA CONJUNCTION domain generalization ( DG ) settings. domain generalization ( DG ) settings CONJUNCTION multiple source DA. upper - bounds USED-FOR domain - invariant representations. upper - bounds USED-FOR target general loss. Task is Domain adaptation ( DA ). Method is domain - invariant representation. Generic are representations, them, and theory. "," Domain adaptation (DA) aims to learn a domain-invariant representation that is invariant to changes in the target domain. The paper considers the multiple source DA and domain generalization (DG) settings, and provides upper-bounds on the target general loss for learning domain- invariant representations. The authors show that these representations are invariant in the sense that they do not change when the source domain changes. They also show that learning these representations is equivalent to learning them from scratch. The theory is well-motivated."," Domain adaptation (DA) aims to learn a domain-invariant representation that is invariant to changes in the target domain. The paper considers the multiple source DA and domain generalization (DG) settings, and provides upper-bounds on the target general loss for learning domain- invariant representations. The authors show that these representations are invariant in the sense that they do not change when the source domain changes. They also show that learning these representations is equivalent to learning them from scratch. The theory is well-motivated."
11533,SP:2a7bee950cd07494d59dfee60ac2e86cc0e481b1,"lightweight architectures USED-FOR SR methods. neural architecture search CONJUNCTION knowledge distillation. knowledge distillation CONJUNCTION neural architecture search. memory and computation resources USED-FOR model compression techniques. knowledge distillation HYPONYM-OF model compression techniques. neural architecture search HYPONYM-OF model compression techniques. network pruning HYPONYM-OF model compression technique. it USED-FOR SR networks. filter pruning USED-FOR residual blocks. L2 regularization USED-FOR sparsity. L2 regularization USED-FOR scale parameters. L2 regularization USED-FOR aligned structured sparsity learning ( ASSL ). weight normalization layer PART-OF aligned structured sparsity learning ( ASSL ). sparsity structure alignment penalty term USED-FOR norm of soft mask gram matrix. layers FEATURE-OF pruned filter locations. sparsity structure alignment penalty term USED-FOR pruned filter locations. aligned structured sparsity learning strategy USED-FOR image SR network. model size CONJUNCTION computation. computation CONJUNCTION model size. ASSLN HYPONYM-OF image SR network. ASSLN COMPARE methods. methods COMPARE ASSLN. OtherScientificTerm are moderate model size, and network parameters. Generic is state - of - the - art methods. Method is lightweight SR networks. ","This paper proposes an aligned structured sparsity learning (ASSL) method to improve the performance of SR methods with lightweight architectures. The authors argue that existing model compression techniques such as neural architecture search and knowledge distillation require large memory and computation resources and moderate model size. The paper proposes a new model compression technique called network pruning, and shows that it can be applied to SR networks with moderate size. Specifically, filter pruning is used to prune residual blocks and scale parameters, and L2 regularization is applied to encourage sparsity.   The paper also proposes an alignment of the weight normalization layer in the proposed method, which is used as a sparsity structure alignment penalty term to enforce the norm of soft mask gram matrix between the pruned filter locations at different layers.  The authors show that the aligned structured learning strategy is able to reduce the size of an image SR network (i.e., ASSLN, a modified version of the original ASSL network) by a large margin, and show that it outperforms state-of-the-art methods in terms of model size and computation. They also show that, when the model size, computation, and number of network parameters are small, the proposed AssLN outperforms existing methods. ","This paper proposes an aligned structured sparsity learning (ASSL) method to improve the performance of SR methods with lightweight architectures. The authors argue that existing model compression techniques such as neural architecture search and knowledge distillation require large memory and computation resources and moderate model size. The paper proposes a new model compression technique called network pruning, and shows that it can be applied to SR networks with moderate size. Specifically, filter pruning is used to prune residual blocks and scale parameters, and L2 regularization is applied to encourage sparsity.   The paper also proposes an alignment of the weight normalization layer in the proposed method, which is used as a sparsity structure alignment penalty term to enforce the norm of soft mask gram matrix between the pruned filter locations at different layers.  The authors show that the aligned structured learning strategy is able to reduce the size of an image SR network (i.e., ASSLN, a modified version of the original ASSL network) by a large margin, and show that it outperforms state-of-the-art methods in terms of model size and computation. They also show that, when the model size, computation, and number of network parameters are small, the proposed AssLN outperforms existing methods. "
11569,SP:e9830bb9e7d3ddc3bd1c2994590fdb5d8f3668be,"exploration USED-FOR complex coordination problems. EMC HYPONYM-OF Episodic Multi - agent reinforcement learning. reward backpropagation USED-FOR centralized training. individual utility functions USED-FOR local execution. individual utility functions HYPONYM-OF induced "" individual Q - values. episodic memory USED-FOR policy training. episodic memory USED-FOR explored informative experience. intrinsic rewards USED-FOR coordinated exploration. intrinsic reward USED-FOR coordinated exploration. method COMPARE MARL baselines. MARL baselines COMPARE method. StarCraft II micromanagement benchmark FEATURE-OF tasks. didactic examples USED-FOR method. tasks EVALUATE-FOR MARL baselines. tasks EVALUATE-FOR method. StarCraft II micromanagement benchmark EVALUATE-FOR MARL baselines. Method is factorized MARL algorithms. OtherScientificTerm are embeddings of local actionobservation histories, and individual Q - value function. ","This paper proposes Episodic Multi-agent reinforcement learning (EMC), which is an extension of EMC to factorized MARL algorithms. In EMC, agents are encouraged to learn embeddings of local actionobservation histories and use exploration to solve complex coordination problems. The authors propose to use reward backpropagation for centralized training, and use individual utility functions (i.e., ""induced"" individual Q-values) to guide the local execution. They also introduce an episodic memory for policy training to store the explored informative experience. The intrinsic rewards for coordinated exploration are modeled as a weighted sum of the individual reward for each agent. The proposed method is evaluated on a number of tasks from the StarCraft II micromanagement benchmark, and compared with several MARL baselines. The method is shown to outperform the baselines on most of the tasks, and the authors also show that the proposed method performs well on a few didactic examples. ","This paper proposes Episodic Multi-agent reinforcement learning (EMC), which is an extension of EMC to factorized MARL algorithms. In EMC, agents are encouraged to learn embeddings of local actionobservation histories and use exploration to solve complex coordination problems. The authors propose to use reward backpropagation for centralized training, and use individual utility functions (i.e., ""induced"" individual Q-values) to guide the local execution. They also introduce an episodic memory for policy training to store the explored informative experience. The intrinsic rewards for coordinated exploration are modeled as a weighted sum of the individual reward for each agent. The proposed method is evaluated on a number of tasks from the StarCraft II micromanagement benchmark, and compared with several MARL baselines. The method is shown to outperform the baselines on most of the tasks, and the authors also show that the proposed method performs well on a few didactic examples. "
11605,SP:c7e33d479575c88e22282ee6fd4f978bcd3c06ed,"Gaussian covariates USED-FOR linear regression model. Statistical Query ( SQ ) lower bound USED-FOR problem. upper bounds USED-FOR task. SQ lower bound COMPARE algorithms. algorithms COMPARE SQ lower bound. Task is list - decodable linear regression. OtherScientificTerm are noise distribution, hypothesis vectors, and regression vector. ","This paper studies the problem of list-decodable linear regression with Gaussian covariates, where the noise distribution of the data is known. The authors provide a Statistical Query (SQ) lower bound for this problem, which is based on the observation that the linear regression model can be decomposed into two parts: (1) the hypothesis vectors, and (2) the regression vector. They also provide upper bounds for this task. The SQ lower bound is shown to be tighter than existing algorithms. ","This paper studies the problem of list-decodable linear regression with Gaussian covariates, where the noise distribution of the data is known. The authors provide a Statistical Query (SQ) lower bound for this problem, which is based on the observation that the linear regression model can be decomposed into two parts: (1) the hypothesis vectors, and (2) the regression vector. They also provide upper bounds for this task. The SQ lower bound is shown to be tighter than existing algorithms. "
11641,SP:7b258252a9063514348f5fa8d9c85afd85748747,"expert domain knowledge USED-FOR ML models. patient health status CONJUNCTION disease progression. disease progression CONJUNCTION patient health status. pharmacology USED-FOR domain knowledge. systems of Ordinary Differential Equations ( ODEs ) USED-FOR Pharmacological models. expert - designed ODEs CONJUNCTION machine - learned Neural ODEs. machine - learned Neural ODEs CONJUNCTION expert - designed ODEs. expert and latent variables USED-FOR observable quantities. synthetic data EVALUATE-FOR LHM. Task is Modeling a system ’s temporal behaviour. Method are Machine Learning ( ML ) approaches, and latent hybridisation model ( LHM ). OtherScientificTerm is small sample regime. Generic are application, models, variables, and system. ","This paper considers the problem of Modeling a system’s temporal behaviour, which is an important problem for many Machine Learning (ML) approaches. In this application, ML models are trained with expert domain knowledge (e.g. patient health status and disease progression) from pharmacology. Pharmacological models are typically based on systems of Ordinary Differential Equations (ODEs). This paper proposes a latent hybridisation model (LHM) that combines expert-designed ODEs with machine-learned Neural ODE. The LHM is trained in a small sample regime and is able to learn observable quantities from both expert and latent variables. Experiments on synthetic data show that the LHM achieves state-of-the-art performance in this application. The paper also shows that the models are able to generalize to unseen variables in the system.","This paper considers the problem of Modeling a system’s temporal behaviour, which is an important problem for many Machine Learning (ML) approaches. In this application, ML models are trained with expert domain knowledge (e.g. patient health status and disease progression) from pharmacology. Pharmacological models are typically based on systems of Ordinary Differential Equations (ODEs). This paper proposes a latent hybridisation model (LHM) that combines expert-designed ODEs with machine-learned Neural ODE. The LHM is trained in a small sample regime and is able to learn observable quantities from both expert and latent variables. Experiments on synthetic data show that the LHM achieves state-of-the-art performance in this application. The paper also shows that the models are able to generalize to unseen variables in the system."
11677,SP:3ea9e86e5755ef84d28e3163c60531ace5d62e3a,Representation learning USED-FOR meta - learning. Representation learning USED-FOR rapid learning of new tasks. meta - learning USED-FOR rapid learning of new tasks. works USED-FOR task - specific representations. MAML USED-FOR task - specific representations. MAML HYPONYM-OF works. fine - tuning - based objective HYPONYM-OF per - task adaptation. per - task adaptation USED-FOR representation. representation USED-FOR task - specific representations. theoretical framework USED-FOR MAML - like algorithm. risk bounds FEATURE-OF predictors. shared structure USED-FOR method. finetuning USED-FOR risk bounds. finetuning USED-FOR predictors. gradient descent USED-FOR finetuning. gradient descent USED-FOR predictors. logistic regression and neural network settings EVALUATE-FOR bounds. OtherScientificTerm is frozen representation ” objective. Generic is algorithm. Method is few - shot learning. ,"Representation learning for meta-learning is an important problem in rapid learning of new tasks. Previous works, such as MAML, have been shown to learn task-specific representations, but the “frozen representation” objective has not been studied. This paper proposes a new per-task adaptation, which is a fine-tuning-based objective, to learn a representation with a shared representation across different tasks. The paper also proposes a theoretical framework for a MAMML-like algorithm. The proposed method is based on a shared structure, and the authors show that finetuning with gradient descent improves the risk bounds of the predictors trained with finetuned gradient descent. The bounds are tested on logistic regression and neural network settings, and show that the proposed algorithm performs well in few-shot learning. ","Representation learning for meta-learning is an important problem in rapid learning of new tasks. Previous works, such as MAML, have been shown to learn task-specific representations, but the “frozen representation” objective has not been studied. This paper proposes a new per-task adaptation, which is a fine-tuning-based objective, to learn a representation with a shared representation across different tasks. The paper also proposes a theoretical framework for a MAMML-like algorithm. The proposed method is based on a shared structure, and the authors show that finetuning with gradient descent improves the risk bounds of the predictors trained with finetuned gradient descent. The bounds are tested on logistic regression and neural network settings, and show that the proposed algorithm performs well in few-shot learning. "
11713,SP:8ba5a2ac80f7c53f81ad008e96c033ecad14ac0d,"paired images CONJUNCTION texts. texts CONJUNCTION paired images. lexicalist approach USED-FOR compositional and grounded meaning representation of language. grounded data USED-FOR compositional and grounded meaning representation of language. paired images HYPONYM-OF grounded data. texts HYPONYM-OF grounded data. neural network embedding USED-FOR shiny objects. symbolic form FEATURE-OF neuro - symbolic semantic program. lexical meanings USED-FOR neuro - symbolic program. syntax USED-FOR lexical meanings. joint parsing CONJUNCTION expected execution algorithm. expected execution algorithm CONJUNCTION joint parsing. exponentiallygrowing compositional space FEATURE-OF learning. expected execution algorithm USED-FOR learning. joint parsing USED-FOR learning. visual reasoning CONJUNCTION language - driven navigation. language - driven navigation CONJUNCTION visual reasoning. language - driven navigation EVALUATE-FOR G2L2. visual reasoning EVALUATE-FOR G2L2. domains EVALUATE-FOR G2L2. language - driven navigation HYPONYM-OF domains. visual reasoning HYPONYM-OF domains. G2L2 USED-FOR compositions of words. OtherScientificTerm are syntactic type, syntactic type of adjective, and local marginalization. Method is meaning programs. Metric is training time. ","This paper proposes a lexicalist approach to learn a compositional and grounded meaning representation of language from grounded data (e.g., paired images, texts, etc.). The authors propose a neuro-symbolic semantic program in symbolic form, where each syntactic type is represented as a program, and a neural network embedding is used to map the program to a set of shiny objects. These meaning programs can then be used to train a classifier to predict the lexical meanings of the program given the syntax. The learning is based on joint parsing and an expected execution algorithm, where learning is performed in an exponentiallygrowing compositional space. The authors evaluate G2L2 on two domains: visual reasoning and language-driven navigation, and show that G2l2 can learn compositions of words that are compositional (i.e., that are more likely to be the same syntactically type of adjective) and that are grounded in the symbolic form. They also show that the training time can be reduced to a single step, and that local marginalization can be used. ","This paper proposes a lexicalist approach to learn a compositional and grounded meaning representation of language from grounded data (e.g., paired images, texts, etc.). The authors propose a neuro-symbolic semantic program in symbolic form, where each syntactic type is represented as a program, and a neural network embedding is used to map the program to a set of shiny objects. These meaning programs can then be used to train a classifier to predict the lexical meanings of the program given the syntax. The learning is based on joint parsing and an expected execution algorithm, where learning is performed in an exponentiallygrowing compositional space. The authors evaluate G2L2 on two domains: visual reasoning and language-driven navigation, and show that G2l2 can learn compositions of words that are compositional (i.e., that are more likely to be the same syntactically type of adjective) and that are grounded in the symbolic form. They also show that the training time can be reduced to a single step, and that local marginalization can be used. "
11749,SP:16c458651815813efdcbe8ba1205bbddbe3e4e68,stochastic Newton algorithm USED-FOR homogeneous distributed stochastic convex optimization. stochastic gradients CONJUNCTION stochastic Hessian - vector products. stochastic Hessian - vector products CONJUNCTION stochastic gradients. stochastic gradients FEATURE-OF population objective. method COMPARE methods. methods COMPARE method. convergence guarantees FEATURE-OF quasi - self - concordant objectives. method USED-FOR communication rounds. communication rounds COMPARE methods. methods COMPARE communication rounds. logistic regression HYPONYM-OF quasi - self - concordant objectives. OtherScientificTerm is stochastic computations. ,"This paper proposes a stochastic Newton algorithm for homogeneous distributed stoching convex optimization. The authors show that the population objective of the population is a convex function of the stochedastic gradients of the target function and of the Hessian-vector products. They also provide convergence guarantees for quasi-self-concordant objectives (e.g., logistic regression). Finally, the authors demonstrate that the proposed method can reduce the number of communication rounds compared to existing methods. ","This paper proposes a stochastic Newton algorithm for homogeneous distributed stoching convex optimization. The authors show that the population objective of the population is a convex function of the stochedastic gradients of the target function and of the Hessian-vector products. They also provide convergence guarantees for quasi-self-concordant objectives (e.g., logistic regression). Finally, the authors demonstrate that the proposed method can reduce the number of communication rounds compared to existing methods. "
11785,SP:d7e479d59f82d4c55372a68ca7b4516f2871f346,"Chamfer Distance ( CD ) CONJUNCTION Earth Mover ’s Distance ( EMD ). Earth Mover ’s Distance ( EMD ) CONJUNCTION Chamfer Distance ( CD ). Chamfer Distance ( CD ) HYPONYM-OF metrics. Earth Mover ’s Distance ( EMD ) HYPONYM-OF metrics. global distribution USED-FOR EMD. them USED-FOR consistent evaluation. Density - aware Chamfer Distance ( DCD ) HYPONYM-OF similarity measure. it USED-FOR disparity of density distributions. it COMPARE EMD. EMD COMPARE it. it COMPARE CD. CD COMPARE it. it USED-FOR detailed structures. CD USED-FOR It. DCD USED-FOR point cloud completion task. CD CONJUNCTION EMD. EMD CONJUNCTION CD. DCD USED-FOR local geometric details. DCD USED-FOR training loss. metrics EVALUATE-FOR model. CD loss USED-FOR model. CD CONJUNCTION EMD. EMD CONJUNCTION CD. DCD USED-FOR it. OtherScientificTerm are mismatched local density, fidelity of detailed structures, unbounded value range, outliers, and bounded value range. Method is point discriminator module. Task are guided downsampling step, and point cloud similarity evaluation. ","This paper proposes a new similarity measure called Density-aware Chamfer Distance (DDCD) that is based on Chamfer distance (CD) and Earth Mover’s Distance (EMD). The authors claim that the proposed DCD is more robust to mismatched local density and better captures the fidelity of detailed structures. The authors also propose a point discriminator module that can be applied to any point cloud completion task.    The paper also proposes a guided downsampling step to improve the performance of the model.  The authors show that it is more sensitive to disparity of density distributions than CD and EMD. They also show that DCD can be used as a training loss for a model trained with CD loss, and that it outperforms CD, EMD, and DCD on point cloud similarity evaluation.  They also propose to use DCD for the pointcloud completion task, and show that the model can be trained with different metrics. ","This paper proposes a new similarity measure called Density-aware Chamfer Distance (DDCD) that is based on Chamfer distance (CD) and Earth Mover’s Distance (EMD). The authors claim that the proposed DCD is more robust to mismatched local density and better captures the fidelity of detailed structures. The authors also propose a point discriminator module that can be applied to any point cloud completion task.    The paper also proposes a guided downsampling step to improve the performance of the model.  The authors show that it is more sensitive to disparity of density distributions than CD and EMD. They also show that DCD can be used as a training loss for a model trained with CD loss, and that it outperforms CD, EMD, and DCD on point cloud similarity evaluation.  They also propose to use DCD for the pointcloud completion task, and show that the model can be trained with different metrics. "
11821,SP:e4b302009520770814ff2c096020b779a9fc38fe,Knowledge distillation USED-FOR small student network. small student network USED-FOR teacher model. ensemble of networks HYPONYM-OF teacher model. knowledge distillation USED-FOR student generalization. dataset USED-FOR distillation. Generic is it. OtherScientificTerm is predictive distributions. Task is optimization. ,"Knowledge distillation is a popular technique to train a small student network to improve the performance of a teacher model (e.g., an ensemble of networks). However, it can be problematic when the predictive distributions of the student and the teacher are very different. This paper studies the problem of knowledge distillation to improve student generalization. The authors propose a new dataset for distillation, and show that the optimization can be done in an unsupervised way. ","Knowledge distillation is a popular technique to train a small student network to improve the performance of a teacher model (e.g., an ensemble of networks). However, it can be problematic when the predictive distributions of the student and the teacher are very different. This paper studies the problem of knowledge distillation to improve student generalization. The authors propose a new dataset for distillation, and show that the optimization can be done in an unsupervised way. "
11857,SP:895c7e03f9e4dadb94be1f39d61bf0b5e1533f4f,"algorithm USED-FOR ( k, ε)-coreset. decision trees PART-OF machine learning. decision trees CONJUNCTION partition trees. partition trees CONJUNCTION decision trees. computational geometry FEATURE-OF partition trees. sklearn CONJUNCTION lightGBM. lightGBM CONJUNCTION sklearn. coresets USED-FOR random forests. computation time EVALUATE-FOR random forests. random forests CONJUNCTION parameter tuning. parameter tuning CONJUNCTION random forests. lightGBM EVALUATE-FOR coresets. sklearn EVALUATE-FOR coresets. computation time EVALUATE-FOR parameter tuning. real - world data - sets EVALUATE-FOR random forests. coresets USED-FOR parameter tuning. computation time EVALUATE-FOR coresets. accuracy EVALUATE-FOR parameter tuning. real - world data - sets EVALUATE-FOR coresets. accuracy EVALUATE-FOR coresets. Method is k - tree. OtherScientificTerm are axis - parallel rectangles, error parameter, tree, optimal k - tree, and coreset. Metric is regression or classification loss. Generic is loss. ","This paper proposes a new algorithm for computing the (k, ε)-coreset of decision trees in machine learning. The coreset of a k-tree is defined as a set of axis-parallel rectangles, where the error parameter is the number of times that the regression or classification loss of the tree converges to a point in the coreset, and the loss is the sum of the log-likelihood of all points in the tree. The authors show that the optimal k-trees can be found by computing the log of the error of each point in a coreset. They also show that decision trees and partition trees with the same computational geometry can be computed in a similar way. The coresets are shown to be able to reduce the computation time of random forests and parameter tuning on two real-world data-sets (sklearn and lightGBM). ","This paper proposes a new algorithm for computing the (k, ε)-coreset of decision trees in machine learning. The coreset of a k-tree is defined as a set of axis-parallel rectangles, where the error parameter is the number of times that the regression or classification loss of the tree converges to a point in the coreset, and the loss is the sum of the log-likelihood of all points in the tree. The authors show that the optimal k-trees can be found by computing the log of the error of each point in a coreset. They also show that decision trees and partition trees with the same computational geometry can be computed in a similar way. The coresets are shown to be able to reduce the computation time of random forests and parameter tuning on two real-world data-sets (sklearn and lightGBM). "
11893,SP:f3ece96b15ec06d703925df2061ed9694ec3bca5,"δ - correct algorithm USED-FOR Top - m identification problem. sample complexity EVALUATE-FOR δ - correct algorithm. tractable lower bound USED-FOR δ - correct algorithm. sample complexity EVALUATE-FOR tractable lower bound. algorithm USED-FOR setting. sample complexity FEATURE-OF upper bound. algorithm COMPARE baselines. baselines COMPARE algorithm. synthetic and real - world data EVALUATE-FOR algorithm. Method are fixed - confidence Top - m identification ), misspecified linear bandit models, and linear models. Generic are problem, and algorithms. Task is medicine and recommendation systems. OtherScientificTerm are linearity, structure of the problem, misspecification, and lower bound. ","This paper studies the problem of Top-m identification (i.e., fixed-confidence Top-M identification) in misspecified linear bandit models. The problem is important in both medicine and recommendation systems, where the linearity and the structure of the problem are important factors. The authors propose a tractable lower bound for the sample complexity of the δ-correct algorithm for the Top- m identification problem. They also provide an algorithm for this setting that is tractable in general.    The main contribution of the paper is to provide a tractible lower bound on the sample cost of the proposed algorithm. The lower bound is based on the observation that the upper bound of the sample efficiency of the algorithm in this setting depends on the number of samples, the structure, and the misspecification of the data. The algorithm is tested on both synthetic and real-world data, and is shown to outperform the baselines in both cases.","This paper studies the problem of Top-m identification (i.e., fixed-confidence Top-M identification) in misspecified linear bandit models. The problem is important in both medicine and recommendation systems, where the linearity and the structure of the problem are important factors. The authors propose a tractable lower bound for the sample complexity of the δ-correct algorithm for the Top- m identification problem. They also provide an algorithm for this setting that is tractable in general.    The main contribution of the paper is to provide a tractible lower bound on the sample cost of the proposed algorithm. The lower bound is based on the observation that the upper bound of the sample efficiency of the algorithm in this setting depends on the number of samples, the structure, and the misspecification of the data. The algorithm is tested on both synthetic and real-world data, and is shown to outperform the baselines in both cases."
11929,SP:e71c5e39b8d8d1640d6de2352ac51ddd52eea89d,"self - supervised learning USED-FOR graph neural networks ( GNNs ). self - supervised learning USED-FOR representation of graph - structure data. self - supervised learning methods USED-FOR GNNs. self - supervised learning USED-FOR disentangled graph representations. Disentangled Graph Contrastive Learning ( DGCL ) method USED-FOR disentangled graph - level representations. self - supervision USED-FOR disentangled graph - level representations. factorized representations USED-FOR latent and disentangled aspect. latent factor FEATURE-OF graph. factorized representations USED-FOR expressive information. contrastive learning manner FEATURE-OF factor - wise discrimination objective. latent factors USED-FOR expressive information. synthetic and real - world datasets EVALUATE-FOR method. method COMPARE state - of - the - art baselines. state - of - the - art baselines COMPARE method. synthetic and real - world datasets EVALUATE-FOR state - of - the - art baselines. OtherScientificTerm are real - world graph, and entanglement of the latent factors. Generic is learned representations. Task is Learning disentangled graph representations. ",This paper proposes a new self-supervised learning method for graph contrastive learning. The key idea is to learn disentangled representations of the graph structure of a real-world graph. This is achieved by learning a contrastive loss that encourages the disentanglement of the latent factors in the graph representation. The proposed method is evaluated on both synthetic and real world graph datasets. ,This paper proposes a new self-supervised learning method for graph contrastive learning. The key idea is to learn disentangled representations of the graph structure of a real-world graph. This is achieved by learning a contrastive loss that encourages the disentanglement of the latent factors in the graph representation. The proposed method is evaluated on both synthetic and real world graph datasets. 
11965,SP:0a7edbbdabab11273689c40c517001eb46491113,"robustness EVALUATE-FOR network. Statistical Reliability Engineering FEATURE-OF stochastic simulation. stochastic simulation USED-FOR network. stochastic simulation USED-FOR robustness. statistical hypothesis test USED-FOR robustness assessment. Importance Splitting simulation USED-FOR procedure. OtherScientificTerm are theoretical guarantees, sample size, and network function. Method is large scale networks. Generic is method. ","This paper studies the robustness of a network trained with stochastic simulation in Statistical Reliability Engineering. The authors provide theoretical guarantees for robustness under the assumption that the sample size is small and the network function is well-behaved. They also propose a statistical hypothesis test to improve robustness assessment. The proposed procedure is based on the Importance Splitting simulation, which is a well-known technique in large scale networks. Experiments are conducted to validate the effectiveness of the proposed method.","This paper studies the robustness of a network trained with stochastic simulation in Statistical Reliability Engineering. The authors provide theoretical guarantees for robustness under the assumption that the sample size is small and the network function is well-behaved. They also propose a statistical hypothesis test to improve robustness assessment. The proposed procedure is based on the Importance Splitting simulation, which is a well-known technique in large scale networks. Experiments are conducted to validate the effectiveness of the proposed method."
12001,SP:c1db485ff1ff9573daa421e167225654babb55ac,Generative modeling USED-FOR machine learning. Deep polynomial neural networks ( PNNs ) USED-FOR unsupervised image generation. PNNs USED-FOR conditional generation tasks. super - resolution HYPONYM-OF conditional generation tasks. noise variable CONJUNCTION conditional variable. conditional variable CONJUNCTION noise variable. single - variable polynomial expansions USED-FOR PNNs. conditional variable HYPONYM-OF two - variable inputs. noise variable HYPONYM-OF two - variable inputs. framework USED-FOR autoand cross - correlations. framework USED-FOR polynomial expansion. input variables USED-FOR CoPE. edges - to - image translation CONJUNCTION image - to - image translation. image - to - image translation CONJUNCTION edges - to - image translation. inverse problems CONJUNCTION edges - to - image translation. edges - to - image translation CONJUNCTION inverse problems. class - conditional generation CONJUNCTION inverse problems. inverse problems CONJUNCTION class - conditional generation. image - to - image translation CONJUNCTION attributeguided generation. attributeguided generation CONJUNCTION image - to - image translation. class - conditional generation CONJUNCTION edges - to - image translation. edges - to - image translation CONJUNCTION class - conditional generation. tasks EVALUATE-FOR CoPE. class - conditional generation EVALUATE-FOR CoPE. attributeguided generation HYPONYM-OF tasks. class - conditional generation HYPONYM-OF tasks. inverse problems HYPONYM-OF tasks. image - to - image translation HYPONYM-OF tasks. edges - to - image translation HYPONYM-OF tasks. CoPE USED-FOR conditional generation tasks. CoPE USED-FOR polynomial_nets_for_conditional_generation. OtherScientificTerm is noise. Material is synthesized image. ,"This paper proposes a new generative modeling for machine learning. Deep polynomial neural networks (PNNs) have been widely used for unsupervised image generation. However, PNNs are typically trained with single-variable polynemic expansions, which are not suitable for conditional generation tasks (e.g., super-resolution). This paper proposes to use two-variable inputs, i.e., noise variable and conditional variable, to train two-variance inputs: noise and conditional. The authors propose a framework for autoand cross-correlation between the input variables and the output of the PNN, which is then used to train a polynomorphic expansion of the input variable. The proposed CoPE is evaluated on three different tasks: class-conditional generation, inverse problems, and edges-to-image translation. CoPE achieves state-of-the-art performance on all three of these tasks, outperforming the previous best results. The paper also shows that CoPE can be applied to a variety of conditional generation task, including attributeguided generation, class-conditioned generation, edge-to image translation, and image-to -image translation, where the noise is added to the synthesized image.   ","This paper proposes a new generative modeling for machine learning. Deep polynomial neural networks (PNNs) have been widely used for unsupervised image generation. However, PNNs are typically trained with single-variable polynemic expansions, which are not suitable for conditional generation tasks (e.g., super-resolution). This paper proposes to use two-variable inputs, i.e., noise variable and conditional variable, to train two-variance inputs: noise and conditional. The authors propose a framework for autoand cross-correlation between the input variables and the output of the PNN, which is then used to train a polynomorphic expansion of the input variable. The proposed CoPE is evaluated on three different tasks: class-conditional generation, inverse problems, and edges-to-image translation. CoPE achieves state-of-the-art performance on all three of these tasks, outperforming the previous best results. The paper also shows that CoPE can be applied to a variety of conditional generation task, including attributeguided generation, class-conditioned generation, edge-to image translation, and image-to -image translation, where the noise is added to the synthesized image.   "
12037,SP:5a75bc7a3ea0ce971cfceebbc1c2434e3aa2584d,neural tangent kernel ( NTK ) CONJUNCTION MMD. MMD CONJUNCTION neural tangent kernel ( NTK ). approach USED-FOR MMD statistic. connection USED-FOR approach. memory and computational complexity EVALUATE-FOR MMD statistic. MMD statistic USED-FOR online implementation. approach USED-FOR NTK based two - sample tests. connection USED-FOR NTK based two - sample tests. theories USED-FOR kernel MMD. connection USED-FOR NTK test statistic properties. Type - I error CONJUNCTION testing power. testing power CONJUNCTION Type - I error. testing power HYPONYM-OF NTK test statistic properties. Type - I error HYPONYM-OF NTK test statistic properties. synthetic and real - world datasets EVALUATE-FOR theory. synthetic and real - world datasets EVALUATE-FOR NTK - MMD statistic. OtherScientificTerm is two - sample test. ,"This paper studies the connection between neural tangent kernel (NTK) and MMD, and proposes a new approach to compute the MMD statistic with a connection between the two. The main contribution of this paper is that it proposes a two-sample test based on the NTK, which can be used to reduce the memory and computational complexity of computing the standard two sample test. The proposed approach is able to compute NTK based two sample tests with the connection, which is more efficient than the standard online implementation based on MMD. The authors also extend the theories to kernel MMD and show that the connection improves NTK test statistic properties such as Type-I error and testing power. Experiments on synthetic and real-world datasets validate the effectiveness of the theory and the proposed NTK-MMD statistic.  ","This paper studies the connection between neural tangent kernel (NTK) and MMD, and proposes a new approach to compute the MMD statistic with a connection between the two. The main contribution of this paper is that it proposes a two-sample test based on the NTK, which can be used to reduce the memory and computational complexity of computing the standard two sample test. The proposed approach is able to compute NTK based two sample tests with the connection, which is more efficient than the standard online implementation based on MMD. The authors also extend the theories to kernel MMD and show that the connection improves NTK test statistic properties such as Type-I error and testing power. Experiments on synthetic and real-world datasets validate the effectiveness of the theory and the proposed NTK-MMD statistic.  "
12073,SP:1df2ffbbe56b8018067820980b93af2a8b57f891,"minimum necessary information USED-FOR neural net D ( · ). class - disentanglement USED-FOR variational autoencoder G ( · ). former COMPARE latter. latter COMPARE former. variational autoencoder G ( · ) USED-FOR class - dependent information. class - disentanglement USED-FOR class - dependent information. classification PART-OF x − G(x ). latter USED-FOR classification. clean images CONJUNCTION adversarial images. adversarial images CONJUNCTION clean images. it USED-FOR adversarial images. it USED-FOR clean images. adversarial attacks USED-FOR perturbations. class - dependent part USED-FOR perturbations. adversarial detection CONJUNCTION adversarial defense. adversarial defense CONJUNCTION adversarial detection. adversarial defense USED-FOR G(x ). detection CONJUNCTION defense. defense CONJUNCTION detection. approach USED-FOR adversarial attacks. defense USED-FOR adversarial attacks. approach USED-FOR defense. detection EVALUATE-FOR approach. Task are detection and defense of adversarial attacks, and classification and attack models. ","This paper studies the detection and defense of adversarial attacks. The authors propose to use the minimum necessary information to train a neural net D( ·) and a variational autoencoder G( ·). The former uses class-disentanglement, while the latter uses the class-dependent information from the classifier G(x). The classification in x-G(x) is then used to train the adversarial attack. The paper shows that it can detect both clean images and adversarial images, and it can also defend against adversarial perturbations in both cases. In addition, the paper also shows that the perturbation generated by the attack can be used as a regularizer for the classifying and attack models.   The paper also proposes a new approach to detect and defend against the class adversarial detection and the defense of G(X). The approach is based on the observation that adversarial examples are more likely to be perturbed in the class dependent part of the class, and that the class of the perturbed examples is the one that is most likely to have the most information.  The authors show that the proposed approach is effective for detection and robustness to adversarial defenses.","This paper studies the detection and defense of adversarial attacks. The authors propose to use the minimum necessary information to train a neural net D( ·) and a variational autoencoder G( ·). The former uses class-disentanglement, while the latter uses the class-dependent information from the classifier G(x). The classification in x-G(x) is then used to train the adversarial attack. The paper shows that it can detect both clean images and adversarial images, and it can also defend against adversarial perturbations in both cases. In addition, the paper also shows that the perturbation generated by the attack can be used as a regularizer for the classifying and attack models.   The paper also proposes a new approach to detect and defend against the class adversarial detection and the defense of G(X). The approach is based on the observation that adversarial examples are more likely to be perturbed in the class dependent part of the class, and that the class of the perturbed examples is the one that is most likely to have the most information.  The authors show that the proposed approach is effective for detection and robustness to adversarial defenses."
12109,SP:2789874561620ba7894c4672f935056bb911e919,Bayesian optimization ( BO ) USED-FOR federated learning ( FL ) setting. federated Thompson sampling ( FTS ) algorithm USED-FOR applications. federated hyperparameter tuning HYPONYM-OF applications. federated Thompson sampling ( FTS ) algorithm USED-FOR Bayesian optimization ( BO ). federated Thompson sampling ( FTS ) algorithm USED-FOR federated learning ( FL ) setting. privacy guarantee FEATURE-OF FL. privacy guarantee FEATURE-OF FTS. differential privacy ( DP ) USED-FOR deep neural networks. DP USED-FOR iterative algorithms. DP USED-FOR user - level privacy. FTS USED-FOR user - level privacy. DP USED-FOR FTS. local modeling USED-FOR BO. DP framework USED-FOR parameter vectors. utility EVALUATE-FOR algorithm. local modeling USED-FOR algorithm. distributed exploration ( DE ) USED-FOR utility. distributed exploration ( DE ) USED-FOR algorithm. privacy CONJUNCTION utility. utility CONJUNCTION privacy. theoretical guarantees FEATURE-OF privacy. theoretical guarantees FEATURE-OF utility. theoretical guarantees FEATURE-OF differentially private FTS. privacy CONJUNCTION utility. utility CONJUNCTION privacy. utility CONJUNCTION privacy guarantee. privacy guarantee CONJUNCTION utility. real - world experiments EVALUATE-FOR DP - FTS - DE. utility EVALUATE-FOR DP - FTS - DE. OtherScientificTerm is privacy - utility trade - off. ,"This paper proposes a federated Thompson sampling (FTS) algorithm for Bayesian optimization (BO) in the federated learning (FL) setting. The authors show that the FTS is differentially private under differential privacy (DP) for deep neural networks. They also show that FTS can be used to improve the privacy-utility trade-off in two applications: (1) federated hyperparameter tuning, and (2) the use of DP for iterative algorithms.    The authors propose the Federated Thompson Sampling (FT) algorithm, which is a variant of the popular Federated Bayesian Optimization (FBO) algorithm. In BO, the privacy guarantee of FL is the same as that of FTS. However, in FTS, the user-level privacy is guaranteed by DP. In this paper, the authors propose to use DP for FTS to achieve the same privacy guarantee as in BO. The main idea is to use local modeling in BO to learn the parameter vectors in the DP framework. The proposed algorithm is based on distributed exploration (DE) to optimize the utility of the algorithm. The theoretical guarantees of privacy and utility of DP-FTS-DE are provided. Experiments on several real-world experiments show that DP-FT-DE achieves better privacy and better utility than FTS-BO. ","This paper proposes a federated Thompson sampling (FTS) algorithm for Bayesian optimization (BO) in the federated learning (FL) setting. The authors show that the FTS is differentially private under differential privacy (DP) for deep neural networks. They also show that FTS can be used to improve the privacy-utility trade-off in two applications: (1) federated hyperparameter tuning, and (2) the use of DP for iterative algorithms.    The authors propose the Federated Thompson Sampling (FT) algorithm, which is a variant of the popular Federated Bayesian Optimization (FBO) algorithm. In BO, the privacy guarantee of FL is the same as that of FTS. However, in FTS, the user-level privacy is guaranteed by DP. In this paper, the authors propose to use DP for FTS to achieve the same privacy guarantee as in BO. The main idea is to use local modeling in BO to learn the parameter vectors in the DP framework. The proposed algorithm is based on distributed exploration (DE) to optimize the utility of the algorithm. The theoretical guarantees of privacy and utility of DP-FTS-DE are provided. Experiments on several real-world experiments show that DP-FT-DE achieves better privacy and better utility than FTS-BO. "
12145,SP:be7d6b81736a2c3f89abd8771b41b18802e88832,"it USED-FOR real - world problems. data annotation USED-FOR MLC models. informative samples USED-FOR cost - effective annotation. BM USED-FOR label correlations. mixture component USED-FOR global pattern of label correlations. Bayesian Bernoulli mixture of label clusters USED-FOR BM. Bayesian Bernoulli mixture of label clusters USED-FOR label correlations. predictive GP USED-FOR feature - component - label mapping. BM CONJUNCTION predictive GP. predictive GP CONJUNCTION BM. BM USED-FOR feature - component - label mapping. predictive GP USED-FOR data features. AL USED-FOR sparse labels. BM USED-FOR sparse labels. GP USED-FOR mixture components. auxiliary variable based variational inference algorithm USED-FOR non - conjugacy. mapping process USED-FOR end - to - end posterior inference. predictive distribution USED-FOR label prediction. model USED-FOR predictive distribution. feature uncertainty CONJUNCTION label covariance. label covariance CONJUNCTION feature uncertainty. label covariance USED-FOR data sampling. BM ) USED-FOR data sampling. label covariance CONJUNCTION BM ). BM ) CONJUNCTION label covariance. GP USED-FOR feature uncertainty. real - world multi - label datasets EVALUATE-FOR model. AL EVALUATE-FOR model. real - world multi - label datasets EVALUATE-FOR AL. Task is Multi - label classification ( MLC ). OtherScientificTerm are correlated ( hence non - exclusive ) labels, sparse label space, correlated label space, inductive bias, and label covariance matrix. ","Multi-label classification (MLC) is an important problem where there are many labels with correlated (non-exclusive) labels, and it is one of the most important real-world problems where it is important to train MLC models with data annotation. However, there is a lack of cost-effective annotation with informative samples, which can be problematic due to the sparse label space. To address this problem, this paper proposes a Bayesian Bernoulli mixture of label clusters (BM) to model the global pattern of label correlations, where each label is represented as a mixture component. The BM is used to learn the label correlations between the data features, and a predictive GP is applied to the feature-component-label mapping between the BM and the predictive GP to learn data features. The authors show that the BM can be used to model sparse labels, which is useful for AL to learn sparse labels with sparse labels. The paper also shows that the GP can learn the mixture components of the data, and that the feature uncertainty and the label covariance (i.e. the label covariances) of data sampling can be learned from the BM.   The authors also propose an auxiliary variable based variational inference algorithm to tackle the problem of non-conjugacy in the correlated label space, and propose an inductive bias to ensure that the predictive distribution of the label prediction is invariant to the mapping process, which allows for end-to-end posterior inference. The proposed model is evaluated on three real-life multi-label datasets, and achieves state-of-the-art performance on AL on all of them. The model is also shown to be able to learn a predictive distribution for label prediction, and is able to generalize well to unseen labels. In addition, the model is shown to learn feature uncertainty (measured by the GP) for data sampling, as well as label covariate (measurement of feature uncertainty) and label covariation (measuring the correlation between label covariates). ","Multi-label classification (MLC) is an important problem where there are many labels with correlated (non-exclusive) labels, and it is one of the most important real-world problems where it is important to train MLC models with data annotation. However, there is a lack of cost-effective annotation with informative samples, which can be problematic due to the sparse label space. To address this problem, this paper proposes a Bayesian Bernoulli mixture of label clusters (BM) to model the global pattern of label correlations, where each label is represented as a mixture component. The BM is used to learn the label correlations between the data features, and a predictive GP is applied to the feature-component-label mapping between the BM and the predictive GP to learn data features. The authors show that the BM can be used to model sparse labels, which is useful for AL to learn sparse labels with sparse labels. The paper also shows that the GP can learn the mixture components of the data, and that the feature uncertainty and the label covariance (i.e. the label covariances) of data sampling can be learned from the BM.   The authors also propose an auxiliary variable based variational inference algorithm to tackle the problem of non-conjugacy in the correlated label space, and propose an inductive bias to ensure that the predictive distribution of the label prediction is invariant to the mapping process, which allows for end-to-end posterior inference. The proposed model is evaluated on three real-life multi-label datasets, and achieves state-of-the-art performance on AL on all of them. The model is also shown to be able to learn a predictive distribution for label prediction, and is able to generalize well to unseen labels. In addition, the model is shown to learn feature uncertainty (measured by the GP) for data sampling, as well as label covariate (measurement of feature uncertainty) and label covariation (measuring the correlation between label covariates). "
12181,SP:2b7270b0370c193300bcbbb5fb0a4101b3329d99,"wedge - shaped point cloud sectors COMPARE point cloud. point cloud COMPARE wedge - shaped point cloud sectors. end - to - end latency EVALUATE-FOR lidar perception models. lidars HYPONYM-OF streaming data source. cartesian coordinate systems USED-FOR methods. multi - scale padding USED-FOR spatial context. feature undistortion CONJUNCTION range stratified convolutions. range stratified convolutions CONJUNCTION feature undistortion. feature undistortion USED-FOR core polar convolutional architecture. range stratified convolutions USED-FOR core polar convolutional architecture. nuScenes dataset EVALUATE-FOR streaming based methods. OtherScientificTerm are sectors, and rectangular regions. Method are polar coordinate system, and non - streaming methods. ","This paper proposes a new streaming data source, namely lidar, to improve the end-to-end latency of lidar perception models. The key idea is to use wedge-shaped point cloud sectors instead of point cloud, which is more efficient than a point cloud. The proposed methods are based on cartesian coordinate systems, where the sectors are partitioned into rectangular regions, and a polar coordinate system is used to map each region to rectangular regions. A core polar convolutional architecture is proposed based on feature undistortion and range stratified convolutions, and multi-scale padding is applied to the spatial context. Experiments on nuScenes dataset show that the proposed streaming based methods outperform non-streaming methods.","This paper proposes a new streaming data source, namely lidar, to improve the end-to-end latency of lidar perception models. The key idea is to use wedge-shaped point cloud sectors instead of point cloud, which is more efficient than a point cloud. The proposed methods are based on cartesian coordinate systems, where the sectors are partitioned into rectangular regions, and a polar coordinate system is used to map each region to rectangular regions. A core polar convolutional architecture is proposed based on feature undistortion and range stratified convolutions, and multi-scale padding is applied to the spatial context. Experiments on nuScenes dataset show that the proposed streaming based methods outperform non-streaming methods."
12217,SP:7ae2c5b7d9c8a6c8f4a353606aa419929c47f31b,"Structured latent variables USED-FOR deep learning models. prior knowledge PART-OF deep learning models. variables USED-FOR learning. differentiable surrogate USED-FOR training. learning approach USED-FOR latent variable. Gumbel - Max trick USED-FOR distributions. structured domains FEATURE-OF distributions. score function estimators USED-FOR optimization. score function estimators USED-FOR differentiable surrogates. stochastic invariant HYPONYM-OF recursive algorithms. gradient estimates CONJUNCTION control variates. control variates CONJUNCTION gradient estimates. feature USED-FOR gradient estimates. feature USED-FOR control variates. structured latent variable models COMPARE relaxation - based counterparts. relaxation - based counterparts COMPARE structured latent variable models. OtherScientificTerm are surrogate, and biased gradients. Generic is model. ","This paper studies the problem of learning a differentiable surrogate for structured latent variables in deep learning models that incorporate prior knowledge. The authors propose a learning approach to learn a latent variable that is invariant to changes in the underlying distribution of the underlying variables. They use the Gumbel-Max trick to learn distributions that are invariant in structured domains. They show that the differentiable surrogates can be learned using standard score function estimators for optimization. They also propose two differentiable algorithms, one called stochastic invariant and one called Stochastic Invariant, to learn these variables for efficient learning. They demonstrate that the gradient estimates of gradient estimates and control variates based on the learned feature are biased, and that the surrogate can be used to correct the biased gradients. Finally, they show that structured latent variable models outperform their relaxation-based counterparts. ","This paper studies the problem of learning a differentiable surrogate for structured latent variables in deep learning models that incorporate prior knowledge. The authors propose a learning approach to learn a latent variable that is invariant to changes in the underlying distribution of the underlying variables. They use the Gumbel-Max trick to learn distributions that are invariant in structured domains. They show that the differentiable surrogates can be learned using standard score function estimators for optimization. They also propose two differentiable algorithms, one called stochastic invariant and one called Stochastic Invariant, to learn these variables for efficient learning. They demonstrate that the gradient estimates of gradient estimates and control variates based on the learned feature are biased, and that the surrogate can be used to correct the biased gradients. Finally, they show that structured latent variable models outperform their relaxation-based counterparts. "
12253,SP:415d363c66a6967c1daca9dc02001b85bf7f0752,"Deep convolutional neural networks ( CNNs ) USED-FOR image denoising. large datasets USED-FOR Deep convolutional neural networks ( CNNs ). noisy image USED-FOR denoisers. models USED-FOR features. large datasets USED-FOR CNN models. convolutional layers PART-OF CNN. multiplicative scaling parameter USED-FOR GainTuning. GainTuning COMPARE CNNs. CNNs COMPARE GainTuning. denoising EVALUATE-FOR GainTuning. image - denoising benchmarks EVALUATE-FOR CNNs. image - denoising benchmarks EVALUATE-FOR GainTuning. noise level CONJUNCTION image type. image type CONJUNCTION noise level. adaptive GainTuning USED-FOR transmission - electronmicroscope images. synthetic data USED-FOR CNN. CNN USED-FOR adaptive GainTuning. GainTuning USED-FOR structure of catalytic nanoparticles. methodology COMPARE GainTuning. GainTuning COMPARE methodology. low signal - to - noise ratios FEATURE-OF data. data USED-FOR GainTuning. data USED-FOR structure of catalytic nanoparticles. Generic are they, and them. OtherScientificTerm is overfitting. ","Deep convolutional neural networks (CNNs) are commonly used for image denoising on large datasets, but they suffer from overfitting due to the large amount of noise in the original image. In this paper, the authors propose to train denoisers on a noisy image and then apply a multiplicative scaling parameter to the weights of the denoiser to make it more robust to noise. The authors show that GainTuning, which is a variant of CNN models trained with large datasets and trained on a large variety of image-denoising benchmarks, outperforms standard CNNs on denoizing on a number of popular image-denoising benchmarks. They also show that the models are able to learn features that are robust to the noise level, image type, and noise level of the input image.    The authors also propose an adaptive variant of their method, GainTune, that adaptively scales the number of layers in a CNN to avoid overfitting. This is done by removing the convolutions of the convolution layers in the CNN and replacing them with a single layer that has the same noise level as the original one. They show that this method can be applied to any CNN trained on synthetic data, and that it is more robust than the standard CNN.  Finally, they show that their methodology outperforms the state-of-the-art GainTuned method on the same data with low signal-to-noise ratios.  They also demonstrate that the adaptive gainTuning is also applicable to transmission-electronmicroscope images, and they use this data to demonstrate the effectiveness of their methodology.  In addition, they use their data to study the structure of catalytic nanoparticles, and show that adaptive GainTunting is able to improve the performance on this data. ","Deep convolutional neural networks (CNNs) are commonly used for image denoising on large datasets, but they suffer from overfitting due to the large amount of noise in the original image. In this paper, the authors propose to train denoisers on a noisy image and then apply a multiplicative scaling parameter to the weights of the denoiser to make it more robust to noise. The authors show that GainTuning, which is a variant of CNN models trained with large datasets and trained on a large variety of image-denoising benchmarks, outperforms standard CNNs on denoizing on a number of popular image-denoising benchmarks. They also show that the models are able to learn features that are robust to the noise level, image type, and noise level of the input image.    The authors also propose an adaptive variant of their method, GainTune, that adaptively scales the number of layers in a CNN to avoid overfitting. This is done by removing the convolutions of the convolution layers in the CNN and replacing them with a single layer that has the same noise level as the original one. They show that this method can be applied to any CNN trained on synthetic data, and that it is more robust than the standard CNN.  Finally, they show that their methodology outperforms the state-of-the-art GainTuned method on the same data with low signal-to-noise ratios.  They also demonstrate that the adaptive gainTuning is also applicable to transmission-electronmicroscope images, and they use this data to demonstrate the effectiveness of their methodology.  In addition, they use their data to study the structure of catalytic nanoparticles, and show that adaptive GainTunting is able to improve the performance on this data. "
12289,SP:90afa1102683b456bc72a54abef466326827546a,convolutional neural network CONJUNCTION asymmetric multiway cut problem solver. asymmetric multiway cut problem solver CONJUNCTION convolutional neural network. fully differentiable architecture USED-FOR simultaneous semantic and instance segmentation. panoptic segmentation PART-OF fully differentiable architecture. convolutional neural network PART-OF fully differentiable architecture. asymmetric multiway cut problem solver PART-OF fully differentiable architecture. latter USED-FOR combinatorial optimization problem. combinatorial optimization problem USED-FOR panoptic labeling. semantic and boundary predictions USED-FOR panoptic labeling. semantic and boundary predictions PART-OF combinatorial optimization problem. formulation USED-FOR smooth surrogate of the panoptic quality metric. gradient USED-FOR optimization problem. Cityscapes and COCO datasets EVALUATE-FOR approaches. combinatorial optimization USED-FOR panoptic segmentation ( COPS ). optimization USED-FOR large scale real - world problem. optimization CONJUNCTION deep learning. deep learning CONJUNCTION optimization. deep learning USED-FOR large scale real - world problem. approach USED-FOR combinatorial optimization. optimization USED-FOR approach. Generic is architecture. ,This paper proposes a fully differentiable architecture for simultaneous semantic and instance segmentation that combines a convolutional neural network and an asymmetric multiway cut problem solver. The latter is used to solve a combinatorial optimization problem that combines semantic and boundary predictions for panoptic labeling. The paper also proposes a new formulation for learning a smooth surrogate of the Panoptic quality metric. Experiments on Cityscapes and COCO datasets show that the proposed approaches outperform existing approaches. The authors also show that their approach can be applied to the problem of combinatorially optimization in panoptIC segmentation (COPS) and that the optimization problem can be solved efficiently with a simple gradient. The proposed approach is a good combination of optimization and deep learning.   ,This paper proposes a fully differentiable architecture for simultaneous semantic and instance segmentation that combines a convolutional neural network and an asymmetric multiway cut problem solver. The latter is used to solve a combinatorial optimization problem that combines semantic and boundary predictions for panoptic labeling. The paper also proposes a new formulation for learning a smooth surrogate of the Panoptic quality metric. Experiments on Cityscapes and COCO datasets show that the proposed approaches outperform existing approaches. The authors also show that their approach can be applied to the problem of combinatorially optimization in panoptIC segmentation (COPS) and that the optimization problem can be solved efficiently with a simple gradient. The proposed approach is a good combination of optimization and deep learning.   
12325,SP:1952e174d9ec7b83ad1d394ece7fe77ea1f6d78d,"Probabilistic context - free grammars ( PCFGs ) CONJUNCTION dynamic Bayesian networks ( DBNs ). dynamic Bayesian networks ( DBNs ) CONJUNCTION Probabilistic context - free grammars ( PCFGs ). dynamic Bayesian networks ( DBNs ) HYPONYM-OF sequence models. Probabilistic context - free grammars ( PCFGs ) HYPONYM-OF sequence models. PCFGs USED-FOR nested hierarchical dependencies ( tree structures ). continuous latent variables USED-FOR DBNs. PCFGs CONJUNCTION DBNs. DBNs CONJUNCTION PCFGs. PCFGs USED-FOR Recursive Bayesian Networks ( RBNs ). RBNs USED-FOR joint distribution. discrete or continuous latent variables FEATURE-OF tree - structured Bayesian networks. tree - structured Bayesian networks USED-FOR joint distribution. exponential number of possible structures CONJUNCTION continuous variables. continuous variables CONJUNCTION exponential number of possible structures. exponential number of possible structures USED-FOR joint inference. maximum posterior estimates USED-FOR continuous latent variables. PCFGs USED-FOR inside and outside probabilities. inside and outside probabilities USED-FOR RBNs. gradient descent USED-FOR maximum posterior estimates. robust parameter optimisation CONJUNCTION Bayesian inference. Bayesian inference CONJUNCTION robust parameter optimisation. marginal data likelihood ( evidence ) CONJUNCTION marginal posterior distribution. marginal posterior distribution CONJUNCTION marginal data likelihood ( evidence ). change point detection CONJUNCTION hierarchical clustering. hierarchical clustering CONJUNCTION change point detection. RBNs USED-FOR segmentation. RBNs COMPARE change point detection. change point detection COMPARE RBNs. RBNs COMPARE hierarchical clustering. hierarchical clustering COMPARE RBNs. noisy sequences USED-FOR RBNs. examples EVALUATE-FOR RBNs. musical data USED-FOR hierarchical music analysis. raw note level USED-FOR hierarchical music analysis. OtherScientificTerm are dependencies, latent variables, nested hierarchical dependency structure, mixed discrete - continuous case, network structures, and expert annotations. Generic is neither. Method is Gaussian RBNs. Material is synthetic data. ","This paper proposes two sequence models: Probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs). PCFGs model nested hierarchical dependencies (tree structures) and DBNs model continuous latent variables.    The authors propose Recursive Bayesian Networks (RBNs) based on the use of PCFGS and the extension of DBNS to the case of discrete latent variables, where the latent variables are assumed to have a nested hierarchical dependency structure.  The joint distribution of the joint distribution over the discrete or continuous latent variable of two tree-structured bayesian networks can be expressed as a function of the number of possible structures in the nested structure of the network structures, which is a special case of the mixed discrete-continuous case.  In this case, the joint inference can be done by maximizing the maximum posterior estimates obtained by gradient descent between the inside and outside probabilities of RBNs, which are obtained by computing the marginal data likelihood (evidence) and the marginal posterior distribution over all possible structures. The authors show that the PCFG and the DBN can be combined to form a Gaussian RBN, which can be used as a generalization of the RBN.  Experiments are conducted on synthetic data and on real-world data, showing that the joint probability of the pair of latent variables can be approximated by the maximum likelihood of the inside or outside of the latent variable, and that PCFGFs and Dbns can approximate the joint probabilities of the two latent variables in a similar way.  Finally, experiments show that RBNB are able to learn to segmentate noisy sequences, and can be applied to both robust parameter optimisation and Bayesian inference, and perform better than either of these methods.  They also show how RBNS and RBNN perform well in terms of change point detection, hierarchical clustering, and segmentation on musical data at the raw note level, and show that their performance is comparable to the performance of previous work (e.g. [1]. ","This paper proposes two sequence models: Probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs). PCFGs model nested hierarchical dependencies (tree structures) and DBNs model continuous latent variables.    The authors propose Recursive Bayesian Networks (RBNs) based on the use of PCFGS and the extension of DBNS to the case of discrete latent variables, where the latent variables are assumed to have a nested hierarchical dependency structure.  The joint distribution of the joint distribution over the discrete or continuous latent variable of two tree-structured bayesian networks can be expressed as a function of the number of possible structures in the nested structure of the network structures, which is a special case of the mixed discrete-continuous case.  In this case, the joint inference can be done by maximizing the maximum posterior estimates obtained by gradient descent between the inside and outside probabilities of RBNs, which are obtained by computing the marginal data likelihood (evidence) and the marginal posterior distribution over all possible structures. The authors show that the PCFG and the DBN can be combined to form a Gaussian RBN, which can be used as a generalization of the RBN.  Experiments are conducted on synthetic data and on real-world data, showing that the joint probability of the pair of latent variables can be approximated by the maximum likelihood of the inside or outside of the latent variable, and that PCFGFs and Dbns can approximate the joint probabilities of the two latent variables in a similar way.  Finally, experiments show that RBNB are able to learn to segmentate noisy sequences, and can be applied to both robust parameter optimisation and Bayesian inference, and perform better than either of these methods.  They also show how RBNS and RBNN perform well in terms of change point detection, hierarchical clustering, and segmentation on musical data at the raw note level, and show that their performance is comparable to the performance of previous work (e.g. [1]. "
12361,SP:5f29b169d3e4bbaeeec85e1aeebe2094fae4be6e,"objective functions FEATURE-OF deep neural networks. Backward propagation of errors ( backpropagation ) USED-FOR objective functions. loss functions HYPONYM-OF objective functions. pseudo - Lagrange multiplier method USED-FOR constrained backpropagation ( CBP ) algorithm. two - bit shift weight constraints HYPONYM-OF constraints. binary HYPONYM-OF constraints. ResNet-18 CONJUNCTION ResNet-50. ResNet-50 CONJUNCTION ResNet-18. ResNet-50 CONJUNCTION GoogLeNet. GoogLeNet CONJUNCTION ResNet-50. AlexNet CONJUNCTION ResNet-18. ResNet-18 CONJUNCTION AlexNet. CBP USED-FOR AlexNet. CBP USED-FOR GoogLeNet. CBP USED-FOR ResNet-18. posttraining method USED-FOR CBP. GoogLeNet PART-OF ImageNet. ImageNet USED-FOR CBP. backpropagation USED-FOR CBP. ResNet-18 CONJUNCTION ResNet50. ResNet50 CONJUNCTION ResNet-18. ResNet50 CONJUNCTION GoogLeNet. GoogLeNet CONJUNCTION ResNet50. algorithm COMPARE methods. methods COMPARE algorithm. top-1 accuracy EVALUATE-FOR ResNet-18. binary weights USED-FOR GoogLeNet. ImageNet EVALUATE-FOR methods. ImageNet EVALUATE-FOR algorithm. top-1 accuracy EVALUATE-FOR algorithm. CBP USED-FOR learning algorithm. constraint functions USED-FOR learning algorithm. constraint functions USED-FOR CBP. OtherScientificTerm are weight precision, objective function, and minimal performance loss. Generic is algorithms. Method is CBP algorithm. ","This paper proposes a novel constrained backpropagation (CBP) algorithm for training deep neural networks with different objective functions (backward propagation of errors) and loss functions (different loss functions). The authors propose a pseudo-Lagrange multiplier method to learn the objective function, and propose a new constraint called two-bit shift weight constraints (i.e., the weight precision is two bits larger than that of the original objective function). They also propose a posttraining method to improve the performance of the proposed CBP algorithm.    The authors show that the proposed constraints (binary and two-bits shift weight) are sufficient to train AlexNet with CBP, ResNet-18, and GoogLeNet. The authors also show that CBP can be used to train a deep neural network with a minimal performance loss.  The paper also shows that using CBP on ImageNet with a post-training method, the proposed algorithm achieves better top-1 accuracy than existing methods.  In addition, the authors demonstrate that the learning algorithm can be learned with the proposed constraint functions, and that the CBP is able to learn a learning algorithm with different constraint functions (e.g., binary vs two bit shift).   Finally, the paper shows that the resulting algorithm can also be used for training a deep network with binary weights. The proposed algorithm is shown to be able to achieve better performance than existing algorithms.  On ImageNet, it is shown that the learned algorithm achieves the best performance with the constraint functions and achieves a top-of-the-articulate performance. ","This paper proposes a novel constrained backpropagation (CBP) algorithm for training deep neural networks with different objective functions (backward propagation of errors) and loss functions (different loss functions). The authors propose a pseudo-Lagrange multiplier method to learn the objective function, and propose a new constraint called two-bit shift weight constraints (i.e., the weight precision is two bits larger than that of the original objective function). They also propose a posttraining method to improve the performance of the proposed CBP algorithm.    The authors show that the proposed constraints (binary and two-bits shift weight) are sufficient to train AlexNet with CBP, ResNet-18, and GoogLeNet. The authors also show that CBP can be used to train a deep neural network with a minimal performance loss.  The paper also shows that using CBP on ImageNet with a post-training method, the proposed algorithm achieves better top-1 accuracy than existing methods.  In addition, the authors demonstrate that the learning algorithm can be learned with the proposed constraint functions, and that the CBP is able to learn a learning algorithm with different constraint functions (e.g., binary vs two bit shift).   Finally, the paper shows that the resulting algorithm can also be used for training a deep network with binary weights. The proposed algorithm is shown to be able to achieve better performance than existing algorithms.  On ImageNet, it is shown that the learned algorithm achieves the best performance with the constraint functions and achieves a top-of-the-articulate performance. "
12397,SP:3ddf8e2e108fb261bb23aec8a27a25aba7523dc1,"acquisition function USED-FOR data / label efficiency. acquisition function USED-FOR Active learning. discrete instance set ( pool - based scenario CONJUNCTION continuous instance space ( query synthesis scenario ). continuous instance space ( query synthesis scenario ) CONJUNCTION discrete instance set ( pool - based scenario. active learning scenarios USED-FOR Gaussian Process Classification ( GPC ). active learning strategies USED-FOR classification error. active learning strategies USED-FOR Estimated Error Reduction ( EER ). gradient - based optimization techniques USED-FOR continuous instance space. continuous instance space USED-FOR query synthesis. it COMPARE gradient - based optimization techniques. gradient - based optimization techniques COMPARE it. gradient - based optimization techniques USED-FOR query synthesis. algorithms USED-FOR EER - based active learning. GPC USED-FOR EER - based active learning. one - dimensional integral USED-FOR joint predictive distribution of label pairs. gradient chain rule USED-FOR gradient of the acquisition function. query synthesis active learning algorithm USED-FOR EER - based strategies. algorithms COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE algorithms. sampling efficiency EVALUATE-FOR state - of - the - art algorithms. synthetic and real - world datasets EVALUATE-FOR state - of - the - art algorithms. sampling efficiency EVALUATE-FOR algorithms. synthetic and real - world datasets EVALUATE-FOR algorithms. Task is labeling. Method is onestep - look - ahead manner. OtherScientificTerm are EER - based acquisition functions, and EER. Metric is computational overhead. ","Active learning uses an acquisition function to improve data/label efficiency. Active learning can be seen as an extension of existing active learning scenarios for Gaussian Process Classification (GPC) where the goal is to reduce the amount of labeled data for labeling in an onestep-look-ahead manner.   This paper considers the discrete instance set (pool-based scenario) and the continuous instance space (query synthesis scenario).   The authors propose two active learning strategies for estimating the classification error, called Estimated Error Reduction (EER), based on two existing algorithms for EER-based active learning.  In particular, it is shown that it outperforms existing gradient-based optimization techniques for query synthesis in the continuously instance space, and it is also shown that the query synthesis active learning algorithm can be applied to existing EER strategies.  The main contribution of this paper is to propose two algorithms for estimating EER in the case of GPC.  First, the authors propose to use the gradient of the acquisition function as a gradient chain rule, which is a one-dimensional integral to the joint predictive distribution of label pairs.  Second, they show that the proposed algorithms outperform existing state-of-the-art algorithms in terms of sampling efficiency on both synthetic and real-world datasets.  They also show that their algorithms are more computationally efficient than the state of the art algorithms.  Finally, they provide theoretical analysis of the properties of EER based acquisition functions and show that they are more robust to computational overhead. ","Active learning uses an acquisition function to improve data/label efficiency. Active learning can be seen as an extension of existing active learning scenarios for Gaussian Process Classification (GPC) where the goal is to reduce the amount of labeled data for labeling in an onestep-look-ahead manner.   This paper considers the discrete instance set (pool-based scenario) and the continuous instance space (query synthesis scenario).   The authors propose two active learning strategies for estimating the classification error, called Estimated Error Reduction (EER), based on two existing algorithms for EER-based active learning.  In particular, it is shown that it outperforms existing gradient-based optimization techniques for query synthesis in the continuously instance space, and it is also shown that the query synthesis active learning algorithm can be applied to existing EER strategies.  The main contribution of this paper is to propose two algorithms for estimating EER in the case of GPC.  First, the authors propose to use the gradient of the acquisition function as a gradient chain rule, which is a one-dimensional integral to the joint predictive distribution of label pairs.  Second, they show that the proposed algorithms outperform existing state-of-the-art algorithms in terms of sampling efficiency on both synthetic and real-world datasets.  They also show that their algorithms are more computationally efficient than the state of the art algorithms.  Finally, they provide theoretical analysis of the properties of EER based acquisition functions and show that they are more robust to computational overhead. "
12433,SP:fa1fac04cd4ccb1f3eaf80807db09f9683ce6b50,"energy functions USED-FOR bounded gradients. gradients USED-FOR numerical instabilities. regularization FEATURE-OF autoencoder - based architectures. low - dimensional manifold FEATURE-OF data. natural images HYPONYM-OF data. natural images HYPONYM-OF low - dimensional manifold. VAE models HYPONYM-OF autoencoder - based architectures. over - regularization CONJUNCTION underregularization. underregularization CONJUNCTION over - regularization. infinite gradients FEATURE-OF autoencoder - based energy function. Method are continuous variational autoencoder ( VAE ) models, and VAE energy function. OtherScientificTerm are parameter gradients, unbounded gradients, posterior collapse, overand under - regularization, and large gradients. Generic is model. Task is suboptimal feature selection. ","This paper considers continuous variational autoencoder (VAE) models, where the parameter gradients of the energy function of the model are bounded. The authors show that the energy functions of VAE models have bounded gradients, and that unbounded gradients can lead to posterior collapse. They also show that under- and over-regularization of the VAE energy function can cause numerical instabilities in the presence of large gradients. Finally, they show that such gradients are responsible for the numerical instability of autoencoders trained with regularization on data on a low-dimensional manifold (e.g. natural images).    The authors also provide a theoretical analysis of the over- and under-regularisation of VAEs. They show that autoencied-based architectures (i.e. VAE-based models with infinite gradients) suffer from over-and underregularization, and show that there is a trade-off between the number of samples and the amount of regularization. They further show that a model trained with infinite regularization can suffer from suboptimal feature selection, which can be attributed to the infinite gradient of the autoenced energy function.","This paper considers continuous variational autoencoder (VAE) models, where the parameter gradients of the energy function of the model are bounded. The authors show that the energy functions of VAE models have bounded gradients, and that unbounded gradients can lead to posterior collapse. They also show that under- and over-regularization of the VAE energy function can cause numerical instabilities in the presence of large gradients. Finally, they show that such gradients are responsible for the numerical instability of autoencoders trained with regularization on data on a low-dimensional manifold (e.g. natural images).    The authors also provide a theoretical analysis of the over- and under-regularisation of VAEs. They show that autoencied-based architectures (i.e. VAE-based models with infinite gradients) suffer from over-and underregularization, and show that there is a trade-off between the number of samples and the amount of regularization. They further show that a model trained with infinite regularization can suffer from suboptimal feature selection, which can be attributed to the infinite gradient of the autoenced energy function."
12469,SP:2611cfd6e0696a57d061687993cef1fe5c95999d,"graph feedback USED-FOR bandit problem. directed graph USED-FOR bandit problem. min - max regret EVALUATE-FOR graph. fractional weak domination number CONJUNCTION k - packing independence number. k - packing independence number CONJUNCTION fractional weak domination number. linear program USED-FOR them. fractional vertex packing set HYPONYM-OF linear program. strong duality theorem USED-FOR regret upper bound. integrality gap FEATURE-OF dual linear program. trees CONJUNCTION graphs. graphs CONJUNCTION trees. bounded integrality gap FEATURE-OF vertex packing problem. graphs FEATURE-OF vertex packing problem. trees FEATURE-OF vertex packing problem. bounded degree FEATURE-OF graphs. OtherScientificTerm are bandit arms, lower bound, and optimal regret. Metric is regret. Generic are notions, and bounds. ","This paper studies the bandit problem with graph feedback in the setting of a directed graph, where the goal is to maximize the min-max regret of a given graph. In this setting, there are a finite number of bandit arms and each arm has a discrete number of vertices. The authors consider the problem of partitioning the graph into sub-graphs and solving them using a linear program, i.e., a fractional vertex packing set. They show that under certain assumptions, the regret of the algorithm is bounded by the fractional weak domination number and the k-packing independence number. The regret upper bound is based on the strong duality theorem, and the lower bound relies on the fact that the integrality gap of the dual linear program is bounded. They also show that for trees and graphs with bounded degree, the vertex packing problem with bounded integrality can be solved with bounded regret.    The paper is well-written and well-motivated. The notions are well-grounded. The bounds are tight. The proof of the bounds is clear.","This paper studies the bandit problem with graph feedback in the setting of a directed graph, where the goal is to maximize the min-max regret of a given graph. In this setting, there are a finite number of bandit arms and each arm has a discrete number of vertices. The authors consider the problem of partitioning the graph into sub-graphs and solving them using a linear program, i.e., a fractional vertex packing set. They show that under certain assumptions, the regret of the algorithm is bounded by the fractional weak domination number and the k-packing independence number. The regret upper bound is based on the strong duality theorem, and the lower bound relies on the fact that the integrality gap of the dual linear program is bounded. They also show that for trees and graphs with bounded degree, the vertex packing problem with bounded integrality can be solved with bounded regret.    The paper is well-written and well-motivated. The notions are well-grounded. The bounds are tight. The proof of the bounds is clear."
12505,SP:e50dec57af337839cbde4b65fb7b431785fda44d,"Shapley values USED-FOR model agnostic feature attributions. global population distribution USED-FOR feature absence. neighbourhood reference distributions USED-FOR Shapley values. Nadaraya - Watson estimator HYPONYM-OF kernel regressor. self - normalised importance sampling estimator USED-FOR Nadaraya - Watson estimator. Neighbourhood Shapley values USED-FOR sparse feature relevance attributions. on - manifold explainability CONJUNCTION robustness. robustness CONJUNCTION on - manifold explainability. robustness EVALUATE-FOR adversarial classifiers. They USED-FOR adversarial classifiers. robustness EVALUATE-FOR They. on - manifold explainability EVALUATE-FOR They. OtherScientificTerm are global population, and local model behaviour. Method is Shapley analysis. ","This paper proposes to use Shapley values for model agnostic feature attributions based on neighbourhood reference distributions. The idea is to use the global population distribution to model feature absence, and then use the Shapley analysis to estimate feature absence from a global population. The authors use the Nadaraya-Watson estimator, which is a kernel regressor based on the self-normalised importance sampling estimator. Neighbourhood Shapley value is then used for sparse feature relevance attributions. They are shown to improve on-manifold explainability and robustness against adversarial classifiers. ","This paper proposes to use Shapley values for model agnostic feature attributions based on neighbourhood reference distributions. The idea is to use the global population distribution to model feature absence, and then use the Shapley analysis to estimate feature absence from a global population. The authors use the Nadaraya-Watson estimator, which is a kernel regressor based on the self-normalised importance sampling estimator. Neighbourhood Shapley value is then used for sparse feature relevance attributions. They are shown to improve on-manifold explainability and robustness against adversarial classifiers. "
12541,SP:35bdeb78f9fe74e754177fb54b48e7399dc8590d,"feature representations USED-FOR deep reinforcement learning ( RL ). them USED-FOR feature learning. state - action sequences HYPONYM-OF un - experienced or less - experienced trajectories. data efficiency EVALUATE-FOR RL feature representation learning. backward dynamics model USED-FOR trajectory cycle. dynamics model USED-FOR PlayVirtual. actions USED-FOR virtual state - action trajectories. cycle consistency constraint FEATURE-OF trajectory. Atari and DeepMind Control Suite benchmarks EVALUATE-FOR designs. benchmarks EVALUATE-FOR method. Method is RL. OtherScientificTerm are data inefficiency, cycle - consistent virtual trajectories, latent space, and groudtruth state supervision. ","This paper proposes a novel approach to learning feature representations for deep reinforcement learning (RL) by learning virtual state-action sequences (i.e. state-actions) from un-experienced or less-experimental trajectories (e.g. from a limited number of demonstrations). The authors argue that RL is prone to data inefficiency due to the lack of cycle-consistent virtual trajectories. To address this issue, the authors propose PlayVirtual, which learns a backward dynamics model for each trajectory cycle, and uses them for feature learning to improve the data efficiency of RL feature representation learning. The dynamics model is trained to learn the dynamics of each trajectory in the latent space, and then the learned dynamics model can be used to learn a dynamics model of the entire trajectory. The authors also introduce a cycle consistency constraint to ensure that the trajectory is consistent across all possible actions. The proposed method is evaluated on the Atari and DeepMind Control Suite benchmarks, and the results show that the proposed designs outperform the baselines. The method is also shown to be robust to groudtruth state supervision.   ","This paper proposes a novel approach to learning feature representations for deep reinforcement learning (RL) by learning virtual state-action sequences (i.e. state-actions) from un-experienced or less-experimental trajectories (e.g. from a limited number of demonstrations). The authors argue that RL is prone to data inefficiency due to the lack of cycle-consistent virtual trajectories. To address this issue, the authors propose PlayVirtual, which learns a backward dynamics model for each trajectory cycle, and uses them for feature learning to improve the data efficiency of RL feature representation learning. The dynamics model is trained to learn the dynamics of each trajectory in the latent space, and then the learned dynamics model can be used to learn a dynamics model of the entire trajectory. The authors also introduce a cycle consistency constraint to ensure that the trajectory is consistent across all possible actions. The proposed method is evaluated on the Atari and DeepMind Control Suite benchmarks, and the results show that the proposed designs outperform the baselines. The method is also shown to be robust to groudtruth state supervision.   "
12577,SP:ca09e472cbcf2ac8c8c9b192a87df2ed59218210,Noisy labels FEATURE-OF large real - world datasets. noisy labels FEATURE-OF robustness. robustness EVALUATE-FOR network ’s architecture. framework USED-FOR robustness. robustness EVALUATE-FOR network. architecture CONJUNCTION target / noise functions. target / noise functions CONJUNCTION architecture. predictive power EVALUATE-FOR representations. predictive power EVALUATE-FOR network ’s robustness. representations USED-FOR linear model. clean labels USED-FOR linear model. architecture COMPARE noise. noise COMPARE architecture. architecture USED-FOR network. predictive power COMPARE methods. methods COMPARE predictive power. predictive power USED-FOR representations. predictive power COMPARE noisy - label - training methods. noisy - label - training methods COMPARE predictive power. representations COMPARE noisy - label - training methods. noisy - label - training methods COMPARE representations. test accuracy EVALUATE-FOR noisy - label - training methods. test accuracy EVALUATE-FOR predictive power. clean labels USED-FOR methods. Method is neural network architectures. ,"This paper studies the problem of robustness to noisy labels on large real-world datasets. The authors propose a framework to evaluate the robustness of a network’s architecture and the target/noise functions. They show that the predictive power of the representations of a linear model trained with clean labels is the same as that of the network trained with noisy labels. They also show that for neural network architectures that are more robust to noise, the architecture is more robust than the noise. Finally, they show that predictive power for representations trained with the same predictive power as methods that use clean labels outperform noisy-label-training methods on test accuracy.","This paper studies the problem of robustness to noisy labels on large real-world datasets. The authors propose a framework to evaluate the robustness of a network’s architecture and the target/noise functions. They show that the predictive power of the representations of a linear model trained with clean labels is the same as that of the network trained with noisy labels. They also show that for neural network architectures that are more robust to noise, the architecture is more robust than the noise. Finally, they show that predictive power for representations trained with the same predictive power as methods that use clean labels outperform noisy-label-training methods on test accuracy."
12613,SP:903727fe028684623a8ccadec210e641ecffc685,"RL algorithm USED-FOR reward function. method USED-FOR value function. transitions USED-FOR value function. hyperparameters USED-FOR method. data - driven Bellman equation USED-FOR method. approach COMPARE prior methods. prior methods COMPARE approach. Method are Reinforcement learning ( RL ) algorithms, RL algorithms, and control algorithm. Generic are process, and two - stage process. OtherScientificTerm are intermediate reward function, and reward function term. ","This paper considers the problem of learning a reward function in reinforcement learning (RL) algorithms. The authors propose a method to learn a value function that is invariant to transitions in RL algorithms. This is achieved by learning an RL algorithm that learns the reward function for a sequence of transitions. The proposed method is based on a data-driven Bellman equation, where the intermediate reward function is learned in a two-stage process. In the first stage, a control algorithm is used to control the transition, and in the second stage, the method learns the value function using a set of hyperparameters. The paper shows that the proposed approach outperforms prior methods in terms of the number of transitions required to learn the final reward function term. ","This paper considers the problem of learning a reward function in reinforcement learning (RL) algorithms. The authors propose a method to learn a value function that is invariant to transitions in RL algorithms. This is achieved by learning an RL algorithm that learns the reward function for a sequence of transitions. The proposed method is based on a data-driven Bellman equation, where the intermediate reward function is learned in a two-stage process. In the first stage, a control algorithm is used to control the transition, and in the second stage, the method learns the value function using a set of hyperparameters. The paper shows that the proposed approach outperforms prior methods in terms of the number of transitions required to learn the final reward function term. "
12649,SP:39ccbd5909a1d7ed212fe92d8d6843c2c70dfe1f,"convex and non - convex settings FEATURE-OF differentially private stochastic optimization. algorithm USED-FOR l2 setting. differentially private algorithms USED-FOR general convex losses. algorithm USED-FOR optimal excess population risk. near - linear time FEATURE-OF algorithm. super - linear time FEATURE-OF differentially private algorithms. algorithm USED-FOR l1 setting. algorithm USED-FOR dimension dependent lower bound. nearly - optimal excess population risk FEATURE-OF algorithm. algorithms USED-FOR differentially private non - convex setting. smooth losses CONJUNCTION polyhedral constraint. polyhedral constraint CONJUNCTION smooth losses. smooth losses FEATURE-OF l1 - case. polyhedral constraint FEATURE-OF l1 - case. linear time FEATURE-OF nearly dimension independent rate. smooth losses FEATURE-OF constrained l2 - case. linear - time algorithm USED-FOR constrained l2 - case. method USED-FOR non - smooth weakly convex stochastic optimization. method COMPARE non - private algorithm. non - private algorithm COMPARE method. method USED-FOR l2 - case. non - convex l2 setting CONJUNCTION lp setting. lp setting CONJUNCTION non - convex l2 setting. Material is convex case. OtherScientificTerm are general non - smooth convex losses, and polylogarithmic ( in the dimension ) overhead. ","This paper studies differentially private stochastic optimization in both convex and non-convex settings. In the convex case, the authors show that the optimal excess population risk of an algorithm in near-linear time for general convex losses is nearly the same as that of the algorithm in the l2 setting, and the algorithm for the l1 setting has a dimension dependent lower bound. The authors also show that for general non-smooth convex loss, the algorithm has a nearly dimension independent rate in linear time.   In the non convex setting, the paper shows that the algorithm with nearly-optimal excess population loss is also nearly-linear in the number of samples.  The paper also shows that for certain special cases of differentiallyprivate algorithms in super-lineartime, the algorithms for the differentially public l1-case and for the polyhedral constraint l2-case are nearly-parametric.  Finally, the proposed algorithms are applied to the non-differentially private non-consistent l2 and lp-case, and are shown to have nearly-dimension independent rates.  For the constrained l2 - case, a linear-time algorithm is shown to be nearly-approximate for the smooth losses and the polyhedron constraint.  In addition, the method is applied to non-strongly convex weakly-concave optimization, where the proposed method outperforms a non-private algorithm by a factor of 2.5 to 3. The proposed method is also applied for non-sparse lp setting, which is a generalization of the proposed by the authors of the previous work. The main contribution of the paper is the polylogarithmical (in the dimension) overhead. ","This paper studies differentially private stochastic optimization in both convex and non-convex settings. In the convex case, the authors show that the optimal excess population risk of an algorithm in near-linear time for general convex losses is nearly the same as that of the algorithm in the l2 setting, and the algorithm for the l1 setting has a dimension dependent lower bound. The authors also show that for general non-smooth convex loss, the algorithm has a nearly dimension independent rate in linear time.   In the non convex setting, the paper shows that the algorithm with nearly-optimal excess population loss is also nearly-linear in the number of samples.  The paper also shows that for certain special cases of differentiallyprivate algorithms in super-lineartime, the algorithms for the differentially public l1-case and for the polyhedral constraint l2-case are nearly-parametric.  Finally, the proposed algorithms are applied to the non-differentially private non-consistent l2 and lp-case, and are shown to have nearly-dimension independent rates.  For the constrained l2 - case, a linear-time algorithm is shown to be nearly-approximate for the smooth losses and the polyhedron constraint.  In addition, the method is applied to non-strongly convex weakly-concave optimization, where the proposed method outperforms a non-private algorithm by a factor of 2.5 to 3. The proposed method is also applied for non-sparse lp setting, which is a generalization of the proposed by the authors of the previous work. The main contribution of the paper is the polylogarithmical (in the dimension) overhead. "
12685,SP:99a476f71e6901aefe281f11fb72ff78265a5b6e,"cooperative bandit problem USED-FOR large - scale decision - making. arbitrary corruptions CONJUNCTION delays. delays CONJUNCTION arbitrary corruptions. arbitrary corruptions FEATURE-OF stochastic networks. stochastic networks USED-FOR communication. cooperative bandit learning HYPONYM-OF real - world communication scenarios. adversarially corrupted rewards FEATURE-OF message - passing. random delays FEATURE-OF network. byzantine communication PART-OF message - passing. network USED-FOR instantaneous rewardsharing. stochastic time - varying networks USED-FOR message - passing. message - passing HYPONYM-OF real - world communication scenarios. instantaneous rewardsharing HYPONYM-OF real - world communication scenarios. message - passing HYPONYM-OF real - world communication scenarios. near - optimal guarantees FEATURE-OF incurred group regret. decentralized algorithms CONJUNCTION near - optimal guarantees. near - optimal guarantees CONJUNCTION decentralized algorithms. decentralized algorithms USED-FOR environments. delayed - update algorithm COMPARE state - of - the - art. state - of - the - art COMPARE delayed - update algorithm. network topologies EVALUATE-FOR delayed - update algorithm. network topologies EVALUATE-FOR state - of - the - art. tight network - dependent minimax lower bounds FEATURE-OF group regret. Generic are problem, and algorithms. OtherScientificTerm is perfect communication. Task is real - world distributed settings. ","This paper studies the cooperative bandit problem for large-scale decision-making in the setting of stochastic networks with arbitrary corruptions and delays. The authors consider two real-world communication scenarios, i.e., cooperative bandits and message-passing, in which the communication is performed by a group of agents. In the first case, the authors show that the communication between agents can be done via a set of discrete stochastastic networks, and in the second case, agents are able to communicate with each other in a distributed setting.    The authors study the problem in the case where the agents have access to adversarially corrupted rewards. In particular, they consider two scenarios: (1) when the agent has access to instantaneous rewardsharing via a network with random delays, and (2) when there is a byzantine communication among agents.  In both cases, the agents are allowed to share information about the current state of the network, and the goal is to minimize the incurred group regret. In both scenarios, the algorithms are decentralized, and decentralized algorithms have near-optimal guarantees on the incurred regret.  The paper also considers two real world communication scenarios:  (a) when agents are in a stochastically time-varying networks, which is a common setting in real-real-world distributed settings (e.g., message- passing), and (b) where agents are not allowed to have perfect communication, but can only share information in a limited time window. In this setting, they show that a delayed-update algorithm is more efficient than the state-of-the-of the-art, and achieves tighter network-dependent minimax lower bounds on the group regret, and outperforms state-and-noise algorithms in both cases. They also show that their algorithm is able to achieve better network topologies compared to the previous state-only or delayed-updates-only algorithms, and show that it is also able to outperform a state- of-the art algorithm that is not decentralized.","This paper studies the cooperative bandit problem for large-scale decision-making in the setting of stochastic networks with arbitrary corruptions and delays. The authors consider two real-world communication scenarios, i.e., cooperative bandits and message-passing, in which the communication is performed by a group of agents. In the first case, the authors show that the communication between agents can be done via a set of discrete stochastastic networks, and in the second case, agents are able to communicate with each other in a distributed setting.    The authors study the problem in the case where the agents have access to adversarially corrupted rewards. In particular, they consider two scenarios: (1) when the agent has access to instantaneous rewardsharing via a network with random delays, and (2) when there is a byzantine communication among agents.  In both cases, the agents are allowed to share information about the current state of the network, and the goal is to minimize the incurred group regret. In both scenarios, the algorithms are decentralized, and decentralized algorithms have near-optimal guarantees on the incurred regret.  The paper also considers two real world communication scenarios:  (a) when agents are in a stochastically time-varying networks, which is a common setting in real-real-world distributed settings (e.g., message- passing), and (b) where agents are not allowed to have perfect communication, but can only share information in a limited time window. In this setting, they show that a delayed-update algorithm is more efficient than the state-of-the-of the-art, and achieves tighter network-dependent minimax lower bounds on the group regret, and outperforms state-and-noise algorithms in both cases. They also show that their algorithm is able to achieve better network topologies compared to the previous state-only or delayed-updates-only algorithms, and show that it is also able to outperform a state- of-the art algorithm that is not decentralized."
12721,SP:d3e896a65470f2439bc7753b4f66e152306b2d6f,"transformer USED-FOR computer vision applications. architectures USED-FOR feature representations. vision transformers USED-FOR feature representations. convolutional neural networks COMPARE vision transformers. vision transformers COMPARE convolutional neural networks. architectures FEATURE-OF vision transformers. mobile devices USED-FOR feature representations. post - training quantization algorithm USED-FOR vision transformers. optimal low - bit quantization intervals USED-FOR quantization task. ranking loss USED-FOR quantization objective. quantization loss CONJUNCTION feature diversity. feature diversity CONJUNCTION quantization loss. nuclear norm FEATURE-OF attention map. nuclear norm USED-FOR mixedprecision quantization scheme. method COMPARE posttraining quantization algorithms. posttraining quantization algorithms COMPARE method. benchmark models EVALUATE-FOR method. top-1 accuracy EVALUATE-FOR DeiT - B model. ImageNet dataset EVALUATE-FOR DeiT - B model. Method is attention mechanism. OtherScientificTerm are self - attention, and quantization. ","This paper proposes a post-training quantization algorithm for vision transformers for computer vision applications. The authors show that convolutional neural networks have similar architectures and feature representations on mobile devices, but their architectures are more suitable for feature representations that can be quantized by a transformer. They propose a mixedprecision quantization scheme based on the nuclear norm of the attention mechanism. They show that the optimal low-bit quantization intervals for the quantization task can be found by optimizing the self-attention. They also propose a ranking loss to further improve the performance of the proposed quantization objective. Finally, they show that quantization loss and feature diversity can be further improved by the proposed nuclear norm in the attention map. The proposed method is shown to outperform existing posttraining quantitative quantization algorithms on several benchmark models and achieves top-1 accuracy on the DeiT-B model on the ImageNet dataset.","This paper proposes a post-training quantization algorithm for vision transformers for computer vision applications. The authors show that convolutional neural networks have similar architectures and feature representations on mobile devices, but their architectures are more suitable for feature representations that can be quantized by a transformer. They propose a mixedprecision quantization scheme based on the nuclear norm of the attention mechanism. They show that the optimal low-bit quantization intervals for the quantization task can be found by optimizing the self-attention. They also propose a ranking loss to further improve the performance of the proposed quantization objective. Finally, they show that quantization loss and feature diversity can be further improved by the proposed nuclear norm in the attention map. The proposed method is shown to outperform existing posttraining quantitative quantization algorithms on several benchmark models and achieves top-1 accuracy on the DeiT-B model on the ImageNet dataset."
12757,SP:aa6b1328585b5916267a3ff4f9119e7aa4ce2bb5,"Double Q - learning USED-FOR overestimation issue of Q - learning. polynomial learning rate USED-FOR analysis. polynomial learning rate USED-FOR slower convergence rate. analytical tools USED-FOR convergence rate. sampling strategy USED-FOR asynchronous double Q - learning. synchronous double Q - learning USED-FOR global optimum. time complexity EVALUATE-FOR asynchronous algorithm. fast convergence FEATURE-OF double - Q learning. Method are Q - learning, double Q - learning, and finite - time analysis. OtherScientificTerm are constant learning rate, state - action space, and discount factor. ","This paper studies the overestimation issue of Q-learning in the setting where the constant learning rate is polynomial in the state-action space. The authors show that double Q - learning can be seen as a special case of this problem, and propose a finite-time analysis of the convergence rate of double Q-learners. The main contribution of this paper is to show that the slower convergence rate is due to the exponential decay of the polynomical learning rate, and that the global optimum is a synchronous double-learning. They also propose a new sampling strategy for asynchronous double-Q-learning, which has a time complexity of O(1/\sqrt{n}) for a finite number of samples. The paper also provides a finite time analysis of their asynchronous algorithm, and shows that the time complexity is O(n^{-1/2}) for the case where the discount factor is constant. Finally, the authors provide some analytical tools to further improve their convergence rate.   ","This paper studies the overestimation issue of Q-learning in the setting where the constant learning rate is polynomial in the state-action space. The authors show that double Q - learning can be seen as a special case of this problem, and propose a finite-time analysis of the convergence rate of double Q-learners. The main contribution of this paper is to show that the slower convergence rate is due to the exponential decay of the polynomical learning rate, and that the global optimum is a synchronous double-learning. They also propose a new sampling strategy for asynchronous double-Q-learning, which has a time complexity of O(1/\sqrt{n}) for a finite number of samples. The paper also provides a finite time analysis of their asynchronous algorithm, and shows that the time complexity is O(n^{-1/2}) for the case where the discount factor is constant. Finally, the authors provide some analytical tools to further improve their convergence rate.   "
12793,SP:04fd4d83717c4f7e1a4b5651a59200151f33411d,"unlabeled and test data COMPARE labeled data. labeled data COMPARE unlabeled and test data. SSL algorithms USED-FOR real - world applications. semi - supervised OOD detection HYPONYM-OF setting. labeled data CONJUNCTION in - distribution data. in - distribution data CONJUNCTION labeled data. approach STEP USED-FOR OOD detection. technique USED-FOR approach STEP. Structure - Keep Unzipping HYPONYM-OF technique. It USED-FOR representation space. representation space USED-FOR OOD samples. STEP approach COMPARE methods. methods COMPARE STEP approach. OOD detection benchmarks EVALUATE-FOR STEP approach. benchmarks EVALUATE-FOR detection. detection EVALUATE-FOR STEP approach. benchmarks EVALUATE-FOR STEP approach. Task are semi - supervised learning ( SSL ) studies, OOD detection settings, and training. OtherScientificTerm are distribution of labeled data, and unknown distribution. Method is optimization algorithm. ","This paper studies semi-supervised learning (SSL) studies in the setting of out-of-distribution (OOD) detection. In this setting, the distribution of labeled data is unknown to the learner, and the goal is to detect OOD samples from the unknown distribution. The authors consider two OOD detection settings: unlabeled and test data, and a setting where the unlabeling data comes from the same distribution as the labeled data, but the test data is from a different distribution.    The authors propose two SSL algorithms for real-world applications. The first approach, called Structure-Keep Unzipping (Stephanie et al., 2018), is an optimization algorithm that tries to find a good trade-off between the quality of the unzipped representation of the labeled and test samples. The second approach, named STEP, is an extension of the technique proposed in [1], which is a technique that is applied to the problem of out of distribution detection. It tries to learn a representation space that can be used to distinguish between OOD and not-out of distribution (OD) samples in the training data. The paper shows that the proposed approach outperforms existing methods in detection on three benchmarks for detection.  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10]","This paper studies semi-supervised learning (SSL) studies in the setting of out-of-distribution (OOD) detection. In this setting, the distribution of labeled data is unknown to the learner, and the goal is to detect OOD samples from the unknown distribution. The authors consider two OOD detection settings: unlabeled and test data, and a setting where the unlabeling data comes from the same distribution as the labeled data, but the test data is from a different distribution.    The authors propose two SSL algorithms for real-world applications. The first approach, called Structure-Keep Unzipping (Stephanie et al., 2018), is an optimization algorithm that tries to find a good trade-off between the quality of the unzipped representation of the labeled and test samples. The second approach, named STEP, is an extension of the technique proposed in [1], which is a technique that is applied to the problem of out of distribution detection. It tries to learn a representation space that can be used to distinguish between OOD and not-out of distribution (OD) samples in the training data. The paper shows that the proposed approach outperforms existing methods in detection on three benchmarks for detection.  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10]"
12829,SP:6bf8b94483b26033795b0eda9649518027f5e1c2,phrase localization CONJUNCTION referring expression comprehension / segmentation. referring expression comprehension / segmentation CONJUNCTION phrase localization. visual grounding USED-FOR visual reasoning. referring expression comprehension / segmentation HYPONYM-OF visual grounding. phrase localization HYPONYM-OF visual grounding. approaches USED-FOR referring expression comprehension ( REC ). approaches USED-FOR segmentation ( RES ). referring expression comprehension ( REC ) CONJUNCTION segmentation ( RES ). segmentation ( RES ) CONJUNCTION referring expression comprehension ( REC ). one - stage multi - task framework USED-FOR visual grounding tasks. modalities PART-OF visual - lingual encoder. modalities PART-OF transformer architecture. model USED-FOR contextualized lingual queries. segmentation mask USED-FOR referred regions. model USED-FOR decoder. contextualized model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE contextualized model. REC and RES tasks EVALUATE-FOR contextualized model. REC and RES tasks EVALUATE-FOR state - of - the - art methods. external dataset EVALUATE-FOR pre - training schedule. contextualized information CONJUNCTION multi - task training. multi - task training CONJUNCTION contextualized information. contextualized information USED-FOR model. multi - task training USED-FOR model. Generic is two - stage setup. Method is complex task - specific one - stage architectures. OtherScientificTerm is bounding box. ,"This paper proposes a multi-task learning framework for visual reasoning tasks that require visual grounding in visual reasoning, such as phrase localization, referring expression comprehension/segmentation, and visual reasoning. The authors propose a two-stage setup, where a visual-lingual encoder is trained with two modalities in a transformer architecture, and a decoder is used to learn a model for contextualized lingual queries. The two approaches are applied to the tasks of phrase localization (REX) and segmentation (RES). The authors also propose a one-stage multi-tasking framework for the visual grounding tasks, where the model is trained on a pre-trained dataset and then fine-tuned on a post-training dataset. The decoder learns a bounding box for each task and a segmentation mask for the referred regions. The contextualized model is compared to state-of-the-art methods on the REC and RES tasks, and is shown to outperform complex task-specific one-staged architectures. The pre-training schedule is also evaluated on an external dataset, and the authors also show that the model can benefit from contextualized information and multi-target training.","This paper proposes a multi-task learning framework for visual reasoning tasks that require visual grounding in visual reasoning, such as phrase localization, referring expression comprehension/segmentation, and visual reasoning. The authors propose a two-stage setup, where a visual-lingual encoder is trained with two modalities in a transformer architecture, and a decoder is used to learn a model for contextualized lingual queries. The two approaches are applied to the tasks of phrase localization (REX) and segmentation (RES). The authors also propose a one-stage multi-tasking framework for the visual grounding tasks, where the model is trained on a pre-trained dataset and then fine-tuned on a post-training dataset. The decoder learns a bounding box for each task and a segmentation mask for the referred regions. The contextualized model is compared to state-of-the-art methods on the REC and RES tasks, and is shown to outperform complex task-specific one-staged architectures. The pre-training schedule is also evaluated on an external dataset, and the authors also show that the model can benefit from contextualized information and multi-target training."
12865,SP:29b552b36696c9bda72f3ab4f31605d98880fd6b,"Boosting HYPONYM-OF algorithmic approach. weak learner HYPONYM-OF agnostic PAC learner. classification loss EVALUATE-FOR agnostic PAC learner. boosting algorithm USED-FOR weak hypotheses. booster CONJUNCTION weak learner. weak learner CONJUNCTION booster. OtherScientificTerm are weak and moderately inaccurate hypotheses, and weak - learner calls. Method are multiclass boosting, Multiclass boosting, boosting, and AdaBoost. Metric is weak learner ’s accuracy parameter. ","This paper proposes a new algorithmic approach called Boosting. Boosting is an algorithm that can be applied to both weak and moderately inaccurate hypotheses. In particular, a weak learner (i.e., an agnostic PAC learner that is agnostic to the classification loss) is proposed, which is a variant of multiclass boosting. Multiclass boosting is an extension of boosting to the case of weak-learner calls. The authors propose a boosting algorithm for weak hypotheses, which they call AdaBoost. The idea is to combine the benefits of both the booster and the weak learners, where the booster is the one that learns the weak hypothesis and the learner is the agnostic one. The paper also shows that AdaBoost can be used to improve the performance of a weak learners in the sense that it can be shown to be more robust to changes in the number of weak learners and to the change in the weak learners’ accuracy parameter. ","This paper proposes a new algorithmic approach called Boosting. Boosting is an algorithm that can be applied to both weak and moderately inaccurate hypotheses. In particular, a weak learner (i.e., an agnostic PAC learner that is agnostic to the classification loss) is proposed, which is a variant of multiclass boosting. Multiclass boosting is an extension of boosting to the case of weak-learner calls. The authors propose a boosting algorithm for weak hypotheses, which they call AdaBoost. The idea is to combine the benefits of both the booster and the weak learners, where the booster is the one that learns the weak hypothesis and the learner is the agnostic one. The paper also shows that AdaBoost can be used to improve the performance of a weak learners in the sense that it can be shown to be more robust to changes in the number of weak learners and to the change in the weak learners’ accuracy parameter. "
12901,SP:f63b050773871338c48b778c362172e4b72477a4,"methods USED-FOR unsupervised object segmentation. methods USED-FOR interpretable object - centric scene generation. unsupervised object segmentation CONJUNCTION interpretable object - centric scene generation. interpretable object - centric scene generation CONJUNCTION unsupervised object segmentation. limited visual complexity FEATURE-OF simulated and real - world datasets. simulated and real - world datasets EVALUATE-FOR methods. RNNs USED-FOR object representations. paradigms COMPARE embedding - based approach. embedding - based approach COMPARE paradigms. clustering procedure USED-FOR randomly ordered object representations. iterative refinement COMPARE clustering procedure. clustering procedure COMPARE iterative refinement. RNNs CONJUNCTION iterative refinement. iterative refinement CONJUNCTION RNNs. GENESIS - V2 USED-FOR variable number of object representations. GENESIS - V2 HYPONYM-OF model. RNNs USED-FOR variable number of object representations. iterative refinement USED-FOR variable number of object representations. GENESIS - V2 COMPARE baselines. baselines COMPARE GENESIS - V2. unsupervised image segmentation CONJUNCTION object - centric scene generation. object - centric scene generation CONJUNCTION unsupervised image segmentation. synthetic datasets CONJUNCTION real - world datasets. real - world datasets CONJUNCTION synthetic datasets. GENESIS - V2 USED-FOR unsupervised image segmentation. GENESIS - V2 USED-FOR object - centric scene generation. baselines USED-FOR unsupervised image segmentation. synthetic datasets USED-FOR unsupervised image segmentation. object - centric scene generation EVALUATE-FOR baselines. real - world datasets USED-FOR object - centric scene generation. synthetic datasets USED-FOR object - centric scene generation. synthetic datasets EVALUATE-FOR GENESIS - V2. real - world datasets EVALUATE-FOR GENESIS - V2. real - world datasets EVALUATE-FOR baselines. Method are unsupervised learning of object - representations, and stochastic stick - breaking process. OtherScientificTerm is unnatural ordering. ","This paper proposes two methods for unsupervised object segmentation and object-centric scene generation. The proposed methods are evaluated on simulated and real-world datasets with limited visual complexity. The authors propose two paradigms: (1) an embedding-based approach where the object representations are learned using RNNs, and (2) an iterative refinement of randomly ordered object representations using a clustering procedure.  The proposed model, called GENESIS-V2, is an extension of the work of [1]. The authors show that, in the case of an unnatural ordering of objects, the iterative learning of object-representations results in a stochastic stick-breaking process, where the number of objects in the scene is not fixed, but changes over time.  Experiments show that the proposed model GENESis- V2 is able to learn a variable number of object representations with the help of a few different types of RNN, and that it is more robust to unnatural ordering than baselines.   The authors also compare the performance of the proposed baselines on synthetic datasets and real -world datasets, and demonstrate that the method outperforms the baselines in terms of performance for both image segmentation (unsupervised segmentation) and for object-centric scene generation (object-centric scenes).  ","This paper proposes two methods for unsupervised object segmentation and object-centric scene generation. The proposed methods are evaluated on simulated and real-world datasets with limited visual complexity. The authors propose two paradigms: (1) an embedding-based approach where the object representations are learned using RNNs, and (2) an iterative refinement of randomly ordered object representations using a clustering procedure.  The proposed model, called GENESIS-V2, is an extension of the work of [1]. The authors show that, in the case of an unnatural ordering of objects, the iterative learning of object-representations results in a stochastic stick-breaking process, where the number of objects in the scene is not fixed, but changes over time.  Experiments show that the proposed model GENESis- V2 is able to learn a variable number of object representations with the help of a few different types of RNN, and that it is more robust to unnatural ordering than baselines.   The authors also compare the performance of the proposed baselines on synthetic datasets and real -world datasets, and demonstrate that the method outperforms the baselines in terms of performance for both image segmentation (unsupervised segmentation) and for object-centric scene generation (object-centric scenes).  "
12937,SP:408deb9e5577ee7118b836fee77135df641fe545,"black box method USED-FOR point predictions. conformal inference USED-FOR framework. conformal inference methods COMPARE adaptive approach. adaptive approach COMPARE conformal inference methods. coverage frequency EVALUATE-FOR adaptive approach. learning problem USED-FOR distribution shift. adaptive conformal inference HYPONYM-OF method. real world datasets EVALUATE-FOR adaptive conformal inference. real world datasets EVALUATE-FOR method. Generic is methods. OtherScientificTerm are data generating distribution, data generating process, and distribution shifts. ","This paper proposes a new black box method for point predictions. The proposed framework is based on conformal inference, which is an extension of previous methods that consider the data generating distribution as a function of the underlying data generating process. The authors show that the performance of existing conformal inferences can be improved by adapting the learning problem to deal with distribution shift. They also show that their method, called adaptive conformal infer, outperforms existing adaptive approach in terms of coverage frequency. They evaluate their method on several real world datasets and show that it outperforms state-of-the-art methods.   ","This paper proposes a new black box method for point predictions. The proposed framework is based on conformal inference, which is an extension of previous methods that consider the data generating distribution as a function of the underlying data generating process. The authors show that the performance of existing conformal inferences can be improved by adapting the learning problem to deal with distribution shift. They also show that their method, called adaptive conformal infer, outperforms existing adaptive approach in terms of coverage frequency. They evaluate their method on several real world datasets and show that it outperforms state-of-the-art methods.   "
12973,SP:e6e5b1e2428abcf1a163ec1cce15cd299f9a544f,"crowded scenes USED-FOR Multi - person pose estimation. bounding box detection CONJUNCTION keypoint grouping. keypoint grouping CONJUNCTION bounding box detection. bounding box detection PART-OF direct pose - level inference strategy. keypoint grouping PART-OF direct pose - level inference strategy. Pose - level Inference Network ( PINet ) USED-FOR complete pose cues. visible body parts USED-FOR complete pose cues. Part - based Pose Generation ( PPG ) USED-FOR coarse poses. PINet USED-FOR coarse poses. Part - based Pose Generation ( PPG ) USED-FOR PINet. pose priors USED-FOR Pose Refinement module. Pose Refinement module USED-FOR coarse poses. visual body cues USED-FOR global pose cues. visual body cues USED-FOR PINet. discriminative body parts USED-FOR PINet. crowded scenes pose estimation benchmarks EVALUATE-FOR PINet. AP EVALUATE-FOR it. OCHuman dataset EVALUATE-FOR it. OtherScientificTerm are overlapping and occlusions, person bounding boxes, and pose cues. Method is Pose Fusion module. ","Multi-person pose estimation in a crowded scenes is challenging due to overlapping and occlusions. This paper proposes a direct pose-level inference strategy that combines bounding box detection and keypoint grouping. The Pose-level Inference Network (PINet) learns complete pose cues based on visible body parts, and a Pose Fusion module is used to fuse the person bounding boxes and keypoints. PINet uses visual body cues to generate global pose cues, and then uses Part-based Pose Generation (PPG) to generate coarse poses using PINet. A Pose Refinement module is also used to refine the coarse poses based on the pose priors. The proposed PINet is evaluated on several crowded scenes pose estimation benchmarks, and it is shown to achieve state-of-the-art AP on the OCHuman dataset. The paper also shows that PINet can learn discriminative body parts that are more likely to be identified by PINet, which is a nice contribution. ","Multi-person pose estimation in a crowded scenes is challenging due to overlapping and occlusions. This paper proposes a direct pose-level inference strategy that combines bounding box detection and keypoint grouping. The Pose-level Inference Network (PINet) learns complete pose cues based on visible body parts, and a Pose Fusion module is used to fuse the person bounding boxes and keypoints. PINet uses visual body cues to generate global pose cues, and then uses Part-based Pose Generation (PPG) to generate coarse poses using PINet. A Pose Refinement module is also used to refine the coarse poses based on the pose priors. The proposed PINet is evaluated on several crowded scenes pose estimation benchmarks, and it is shown to achieve state-of-the-art AP on the OCHuman dataset. The paper also shows that PINet can learn discriminative body parts that are more likely to be identified by PINet, which is a nice contribution. "
13022,SP:e76f048c3dccffcb8bcc6a66f6165fc19d175610,Robust Markov decision processes ( RMDPs ) PART-OF robust reinforcement learning algorithms. algorithm USED-FOR Bellman operator. Bellman operator USED-FOR S - rectangular robust Markov decision processes. L∞-constrained rectangular ambiguity sets FEATURE-OF S - rectangular robust Markov decision processes. homotopy continuation method CONJUNCTION bisection method. bisection method CONJUNCTION homotopy continuation method. algorithm USED-FOR S - rectangular ambiguity. bisection method USED-FOR S - rectangular ambiguity. homotopy continuation method USED-FOR S - rectangular ambiguity. quasi - linear time FEATURE-OF S - rectangular ambiguity. bisection method PART-OF algorithm. homotopy continuation method PART-OF algorithm. cubic time FEATURE-OF leading general linear programming methods. leading general linear programming methods USED-FOR algorithm. cubic time EVALUATE-FOR algorithm. it COMPARE leading commercial optimization package. leading commercial optimization package COMPARE it. Generic is method. ,"Robust Markov decision processes (RMDPs) are an important component of robust reinforcement learning algorithms. This paper proposes an algorithm for learning the Bellman operator for S-rectangular robust Markov Decision Processes with L∞-constrained rectangular ambiguity sets. The proposed algorithm combines the homotopy continuation method and the bisection method to solve the S-triangular ambiguity in quasi-linear time. The algorithm achieves cubic time in comparison to leading general linear programming methods, and it outperforms the leading commercial optimization package. The method is well-motivated and well-written.","Robust Markov decision processes (RMDPs) are an important component of robust reinforcement learning algorithms. This paper proposes an algorithm for learning the Bellman operator for S-rectangular robust Markov Decision Processes with L∞-constrained rectangular ambiguity sets. The proposed algorithm combines the homotopy continuation method and the bisection method to solve the S-triangular ambiguity in quasi-linear time. The algorithm achieves cubic time in comparison to leading general linear programming methods, and it outperforms the leading commercial optimization package. The method is well-motivated and well-written."
13071,SP:c4af66a64a5c2bd58ca2e29dbc4b27d5bf4b63b8,machine - learned predictions USED-FOR online algorithms. generalized one - way trading CONJUNCTION two - stage online knapsack. two - stage online knapsack CONJUNCTION generalized one - way trading. competitive ratio EVALUATE-FOR online algorithms. Task is online knapsack problem. OtherScientificTerm is upper and lower bound. ,"This paper studies the online knapsack problem with machine-learned predictions for online algorithms. In particular, the authors consider generalized one-way trading and two-stage online knapack. The competitive ratio of the proposed online algorithms is shown to be upper and lower bound. The paper is well-written and easy to follow.","This paper studies the online knapsack problem with machine-learned predictions for online algorithms. In particular, the authors consider generalized one-way trading and two-stage online knapack. The competitive ratio of the proposed online algorithms is shown to be upper and lower bound. The paper is well-written and easy to follow."
13120,SP:1d478d4fa3f5df0ded963ef164325667fd744dbb,"Episodic control USED-FOR reinforcement learning. model - based episodic memory of trajectories USED-FOR episodic control. memory USED-FOR agent. memory USED-FOR complementary learning model. model - based, episodic and habitual learning PART-OF architecture. dynamic hybrid control CONJUNCTION model - based, episodic and habitual learning. model - based, episodic and habitual learning CONJUNCTION dynamic hybrid control. dynamic hybrid control USED-FOR complementary learning model. model - based, episodic and habitual learning USED-FOR complementary learning model. model COMPARE reinforcement learning agents. reinforcement learning agents COMPARE model. OtherScientificTerm are episodic memory, and stochastic and non - Markovian settings. ","Episodic control is an important problem in reinforcement learning. This paper proposes a model-based episodic memory of trajectories for episodic control. This memory is used to train an agent to learn a complementary learning model that can leverage the memory. The proposed architecture is a combination of dynamic hybrid control, model- based, episodic and habitual learning. Experiments show that the proposed model outperforms existing reinforcement learning agents in both stochastic and non-Markovian settings. ","Episodic control is an important problem in reinforcement learning. This paper proposes a model-based episodic memory of trajectories for episodic control. This memory is used to train an agent to learn a complementary learning model that can leverage the memory. The proposed architecture is a combination of dynamic hybrid control, model- based, episodic and habitual learning. Experiments show that the proposed model outperforms existing reinforcement learning agents in both stochastic and non-Markovian settings. "
13169,SP:551174c1266b5f4b6aaf5432a4c713386f90898c,labeled data USED-FOR deep learning. Semi - supervised learning ( SSL ) USED-FOR unlabeled data. pseudo labels USED-FOR unlabeled data. data programming ( DP ) scheme USED-FOR probabilistic labels. probabilistic labels USED-FOR unlabeled data. DP - SSL HYPONYM-OF SSL method. data programming ( DP ) scheme USED-FOR SSL method. LFs PART-OF SSL style. DP methods USED-FOR initial labeling functions ( LFs ). human experts USED-FOR DP methods. noisy labels USED-FOR label model. probabilistic labels USED-FOR unlabeled samples. LFs USED-FOR noisy labels. DP - SSL COMPARE SSL methods. SSL methods COMPARE DP - SSL. classification EVALUATE-FOR SSL methods. SSL benchmarks EVALUATE-FOR DP - SSL. test sets EVALUATE-FOR SSL methods. DP - SSL USED-FOR unlabeled data. test sets EVALUATE-FOR classification. classification EVALUATE-FOR DP - SSL. test sets EVALUATE-FOR DP - SSL. CIFAR-10 EVALUATE-FOR DP - SSL. classification accuracy EVALUATE-FOR test data. unlabeled data EVALUATE-FOR DP - SSL. test data EVALUATE-FOR DP - SSL. classification accuracy EVALUATE-FOR DP - SSL. annotation accuracy EVALUATE-FOR DP - SSL. Method is SSL. Material is labeled samples. ,"Semi-supervised learning (SSL) is a popular technique for learning from labeled data for deep learning. Semi-supervision learning (SSL) aims to learn from unlabeled data with pseudo labels. This paper proposes a new SSL method called DP-SSL, which is based on the data programming (DP) scheme to learn probabilistic labels for unlabeling data from the labeled data. In SSL, the initial labeling functions (LFs) are learned by human experts. In this paper, the authors propose to add LFs to the SSL style, where the LFs are learned using DP methods. The authors show that DP methods can learn the initial labels for the unlabelled data from noisy labels, which can be used to train a label model with noisy labels. They also show that learning from noisy labeled samples can be more efficient than learning from the original labeled samples.  The authors further propose a DP method based on DP methods to learn the LF for the labeled samples, which they call DP-SSL. They show that the proposed DP method is able to learn LFs that are robust to noisy labels and can be trained with DP methods without the need to use human experts to train the model. They demonstrate that their DP method outperforms the state-of-the-art SSL methods on a number of SSL benchmarks and test sets for classification and classification on CIFAR-10. The paper also shows that DP- SSL can learn to learn to classify unlabel data with unlabelable labels. Finally, they show that for classification on the test sets, DP-DSL outperforms other SSL methods.    On the test data, the paper shows that the DP method performs better on classification accuracy on the unlabelated data, and on test data on test sets. On the other hand, on the unlabeled data, it outperforms all the SSL methods in terms of classification accuracy. On test data that has been labeled correctly, it also outperforms them all.  Finally, on a subset of test sets that have been labeled incorrectly, it is shown that DP - SSL performs well on classification performance on the labelled data. On a subset that has not been labeled properly, it performs worse on classification. However, it does perform better on the labeled and unlabelized data. ","Semi-supervised learning (SSL) is a popular technique for learning from labeled data for deep learning. Semi-supervision learning (SSL) aims to learn from unlabeled data with pseudo labels. This paper proposes a new SSL method called DP-SSL, which is based on the data programming (DP) scheme to learn probabilistic labels for unlabeling data from the labeled data. In SSL, the initial labeling functions (LFs) are learned by human experts. In this paper, the authors propose to add LFs to the SSL style, where the LFs are learned using DP methods. The authors show that DP methods can learn the initial labels for the unlabelled data from noisy labels, which can be used to train a label model with noisy labels. They also show that learning from noisy labeled samples can be more efficient than learning from the original labeled samples.  The authors further propose a DP method based on DP methods to learn the LF for the labeled samples, which they call DP-SSL. They show that the proposed DP method is able to learn LFs that are robust to noisy labels and can be trained with DP methods without the need to use human experts to train the model. They demonstrate that their DP method outperforms the state-of-the-art SSL methods on a number of SSL benchmarks and test sets for classification and classification on CIFAR-10. The paper also shows that DP- SSL can learn to learn to classify unlabel data with unlabelable labels. Finally, they show that for classification on the test sets, DP-DSL outperforms other SSL methods.    On the test data, the paper shows that the DP method performs better on classification accuracy on the unlabelated data, and on test data on test sets. On the other hand, on the unlabeled data, it outperforms all the SSL methods in terms of classification accuracy. On test data that has been labeled correctly, it also outperforms them all.  Finally, on a subset of test sets that have been labeled incorrectly, it is shown that DP - SSL performs well on classification performance on the labelled data. On a subset that has not been labeled properly, it performs worse on classification. However, it does perform better on the labeled and unlabelized data. "
13218,SP:d1d6a40a8bde62a21da4fc18a076e344c84ab0d0,"Multi - view Pose transformer ( MvP ) USED-FOR estimating multi - person 3D poses. multi - view images USED-FOR Multi - view Pose transformer ( MvP ). multi - view images USED-FOR estimating multi - person 3D poses. MvP USED-FOR multi - person 3D poses. volumetric representation USED-FOR per - person 3D pose. volumetric representation USED-FOR estimating 3D joint locations. detected 2D poses USED-FOR per - person 3D pose. query embeddings USED-FOR MvP. query embeddings USED-FOR skeleton joints. accuracy EVALUATE-FOR pipeline. hierarchical scheme USED-FOR query embeddings of multi - person skeleton joints. accuracy EVALUATE-FOR MvP. hierarchical scheme USED-FOR MvP. geometrically guided attention mechanism USED-FOR cross - view information. MvP USED-FOR geometrically guided attention mechanism. projective attention HYPONYM-OF geometrically guided attention mechanism. feature representations USED-FOR projective attention. view - dependent camera geometry PART-OF feature representations. RayConv operation USED-FOR MvP. MvP model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE MvP model. it COMPARE approach. approach COMPARE it. Panoptic dataset EVALUATE-FOR AP25. AP25 EVALUATE-FOR it. Panoptic dataset EVALUATE-FOR it. MvP USED-FOR recovering human mesh. MvP USED-FOR modeling multi - person body shapes. SMPL model USED-FOR modeling multi - person body shapes. SMPL model USED-FOR recovering human mesh. Generic is intermediate tasks. OtherScientificTerm are multi - view information, 3D joint locations, human mesh, and multi - person body shapes. Method is inputdependent query adaptation approach. ","This paper proposes Multi-view Pose transformer (MVPT) for estimating multi-person 3D poses from multi-view images. MVPT learns a volumetric representation for estimating 3D joint locations from detected 2D poses, and then uses query embeddings of the skeleton joints from multiple views of the same joint to generate multi-perspective queries for intermediate tasks. The proposed pipeline achieves state-of-the-art accuracy on the Panoptic dataset.    MVP uses a hierarchical scheme to learn a query embedding of multi-people skeleton joints, and a geometrically guided attention mechanism, called projective attention, to capture cross-view information. The feature representations of the feature representations contain view-dependent camera geometry, which is used as input to the inputdependent query adaptation approach.  The MVP model is trained using the RayConv operation, where the query is extracted from multiple view pairs, and the volumetries of the multi- view information are concatenated together to produce a single query.  Experiments show that MVP is able to recover a human mesh from multiple images, and that it outperforms the previous approach on the AP25 dataset. In addition, MVP can also be used for recovering human mesh using the SMPL model, and is also able to perform well for modeling multiple-person body shapes, and for recovering multi-body shapes. ","This paper proposes Multi-view Pose transformer (MVPT) for estimating multi-person 3D poses from multi-view images. MVPT learns a volumetric representation for estimating 3D joint locations from detected 2D poses, and then uses query embeddings of the skeleton joints from multiple views of the same joint to generate multi-perspective queries for intermediate tasks. The proposed pipeline achieves state-of-the-art accuracy on the Panoptic dataset.    MVP uses a hierarchical scheme to learn a query embedding of multi-people skeleton joints, and a geometrically guided attention mechanism, called projective attention, to capture cross-view information. The feature representations of the feature representations contain view-dependent camera geometry, which is used as input to the inputdependent query adaptation approach.  The MVP model is trained using the RayConv operation, where the query is extracted from multiple view pairs, and the volumetries of the multi- view information are concatenated together to produce a single query.  Experiments show that MVP is able to recover a human mesh from multiple images, and that it outperforms the previous approach on the AP25 dataset. In addition, MVP can also be used for recovering human mesh using the SMPL model, and is also able to perform well for modeling multiple-person body shapes, and for recovering multi-body shapes. "
13267,SP:2e147bd5321e25bb27d2531fd58c46460a1e5320,"sparse vectors PART-OF family. error - free responses USED-FOR sparse vectors. support recovery CONJUNCTION approximate recovery problems. approximate recovery problems CONJUNCTION support recovery. approximate recovery problems USED-FOR problems. support recovery USED-FOR problems. 1 - bit compressed sensing USED-FOR approximate recovery problems. 1 - bit compressed sensing USED-FOR problems. learning algorithms USED-FOR problem. learning algorithms USED-FOR problem. Task is learning problems. OtherScientificTerm are noisy responses, and unknown vectors. Method is learning model. Metric is query complexity. ","This paper studies the problem of learning problems with noisy responses. The authors propose a new family of sparse vectors, which is a family that includes sparse vectors with error-free responses. They show that these problems can be seen as support recovery and approximate recovery problems with 1-bit compressed sensing. They also propose learning algorithms to solve this problem. The main contribution of the paper is that the learning model is able to learn sparse vectors that have noisy responses, and that the query complexity is bounded by the number of unknown vectors.","This paper studies the problem of learning problems with noisy responses. The authors propose a new family of sparse vectors, which is a family that includes sparse vectors with error-free responses. They show that these problems can be seen as support recovery and approximate recovery problems with 1-bit compressed sensing. They also propose learning algorithms to solve this problem. The main contribution of the paper is that the learning model is able to learn sparse vectors that have noisy responses, and that the query complexity is bounded by the number of unknown vectors."
13316,SP:e3388e479a825be429f3a878e2c4d8b05903ff10,"sensors USED-FOR detecting abrupt changes in temporal behavior patterns. detecting abrupt changes in temporal behavior patterns FEATURE-OF industrial and security applications. sensors USED-FOR industrial and security applications. information - theoretic lower bound USED-FOR finitely parameterized probability distributions. information - theoretic lower bound FEATURE-OF detection delay. bounds COMPARE information - theoretic lower bounds. information - theoretic lower bounds COMPARE bounds. expected delay bounds USED-FOR scheme. synthetic and real datasets EVALUATE-FOR method. OtherScientificTerm are abrupt changes in temporal behavior patterns, abrupt changes, sensing actions, and exploitation of querying informative actions. Task is bandit quickest changepoint detection problem. Method is online sensing scheme. Metric is false alarm rates. ","This paper considers the problem of detecting abrupt changes in temporal behavior patterns from sensors for industrial and security applications. The authors consider the bandit quickest changepoint detection problem and propose a novel online sensing scheme. The proposed scheme is based on the expected delay bounds, which are tighter than existing information-theoretic lower bound for finitely parameterized probability distributions. The main contribution of the paper is that the detection delay matches the information-thrighly lower bound in the case of sensing actions, and that the exploitation of querying informative actions can be avoided. The method is evaluated on both synthetic and real datasets, and the false alarm rates are shown to be very low. ","This paper considers the problem of detecting abrupt changes in temporal behavior patterns from sensors for industrial and security applications. The authors consider the bandit quickest changepoint detection problem and propose a novel online sensing scheme. The proposed scheme is based on the expected delay bounds, which are tighter than existing information-theoretic lower bound for finitely parameterized probability distributions. The main contribution of the paper is that the detection delay matches the information-thrighly lower bound in the case of sensing actions, and that the exploitation of querying informative actions can be avoided. The method is evaluated on both synthetic and real datasets, and the false alarm rates are shown to be very low. "
13365,SP:268260e9452ba2bc57e50a6b7b3328233137ac9b,"stochastic bilevel CONJUNCTION min - max. min - max CONJUNCTION stochastic bilevel. min - max CONJUNCTION compositional optimization. compositional optimization CONJUNCTION min - max. Stochastic nested optimization USED-FOR machine learning applications. compositional optimization HYPONYM-OF Stochastic nested optimization. stochastic bilevel HYPONYM-OF Stochastic nested optimization. min - max HYPONYM-OF Stochastic nested optimization. nested structure FEATURE-OF problems. SGD - type updates USED-FOR nested problems. they COMPARE non - nested problems. non - nested problems COMPARE they. convergence rate EVALUATE-FOR they. SGD - type updates USED-FOR stochastic nested problems. ALternating Stochastic gradient dEscenT ( ALSET ) method HYPONYM-OF SGD approach. ALSET USED-FOR stochastic nested problems. hidden smoothness FEATURE-OF problem. SGD - type algorithms USED-FOR stochastic nested problems. Method is problem - specific algorithms and analyses. Generic are analysis, and it. OtherScientificTerm are nested problem, and regularity conditions. Metric is sample complexity. ","This paper studies the problem of stochastic nested optimization (stochastic bilevel, min-max, and compositional optimization) in machine learning applications. Stochastic nested optimisation is a special case of the class of problem-specific algorithms and analyses where the goal is to find a solution to a nested problem that minimizes the total number of iterations needed to solve the original problem.    The authors consider three problems with a nested structure:   1.  2. 3. 4. 5.  They show that under certain regularity conditions, SGD-type updates to these nested problems have a better convergence rate than they do for non-nested problems.  The main contribution of the paper is the analysis of the sample complexity of solving the nested problem, and how it is related to the number of iterates required to find the optimal solution.  In particular, the authors show that for any SGD approach (e.g., the ALternating SGD gradient dEscenT (ALSET) method, which is an extension of the work of Zhang et al. (2020) and Liu et al., 2018), the number $O(\sqrt{T})$ of iterations required to solve a given nested problem is at most $O(T)$, where $T$ is the dimension of the problem. The authors also show that this sample complexity does not depend on the hidden smoothness of the underlying problem, but rather on the regularity of the SGD updates.  Finally, they show that SGD type algorithms can be used to solve these stochedastic nested problems, and that ALSET is a generalization of the previous work (Zhang et al, 2018). ","This paper studies the problem of stochastic nested optimization (stochastic bilevel, min-max, and compositional optimization) in machine learning applications. Stochastic nested optimisation is a special case of the class of problem-specific algorithms and analyses where the goal is to find a solution to a nested problem that minimizes the total number of iterations needed to solve the original problem.    The authors consider three problems with a nested structure:   1.  2. 3. 4. 5.  They show that under certain regularity conditions, SGD-type updates to these nested problems have a better convergence rate than they do for non-nested problems.  The main contribution of the paper is the analysis of the sample complexity of solving the nested problem, and how it is related to the number of iterates required to find the optimal solution.  In particular, the authors show that for any SGD approach (e.g., the ALternating SGD gradient dEscenT (ALSET) method, which is an extension of the work of Zhang et al. (2020) and Liu et al., 2018), the number $O(\sqrt{T})$ of iterations required to solve a given nested problem is at most $O(T)$, where $T$ is the dimension of the problem. The authors also show that this sample complexity does not depend on the hidden smoothness of the underlying problem, but rather on the regularity of the SGD updates.  Finally, they show that SGD type algorithms can be used to solve these stochedastic nested problems, and that ALSET is a generalization of the previous work (Zhang et al, 2018). "
13414,SP:82ad52361bc5b2c421f1dc6b76e1a5520570fc6c,"supervised learning USED-FOR transformer - based model. siamese sampling mechanism USED-FOR sparse and similar clips. interdependent knowledge PART-OF network. reasoning strategy USED-FOR interdependent knowledge. interdependent knowledge PART-OF network inference. siamese clips HYPONYM-OF sparse and similar clips. Siamese Sampling and Reasoning ( SiaSamRea ) approach USED-FOR interdependent knowledge. reasoning strategy PART-OF Siamese Sampling and Reasoning ( SiaSamRea ) approach. reasoning strategy USED-FOR network inference. siamese sampling mechanism PART-OF Siamese Sampling and Reasoning ( SiaSamRea ) approach. siamese knowledge reasoning USED-FOR soft label. siamese knowledge reasoning HYPONYM-OF modules. siamese knowledge generation HYPONYM-OF modules. modules PART-OF reasoning strategy. siamese knowledge reasoning PART-OF reasoning strategy. siamese knowledge generation PART-OF reasoning strategy. SiaSamRea USED-FOR multimodal reasoning paradigm. ActivityNet - QA CONJUNCTION How2QA. How2QA CONJUNCTION ActivityNet - QA. How2QA CONJUNCTION TGIF - QA. TGIF - QA CONJUNCTION How2QA. MSVD - QA CONJUNCTION ActivityNet - QA. ActivityNet - QA CONJUNCTION MSVD - QA. MSRVTT - QA CONJUNCTION MSVD - QA. MSVD - QA CONJUNCTION MSRVTT - QA. VideoQA benchmarks EVALUATE-FOR SiaSamRea. Task is VideoQA ) task. OtherScientificTerm are inter - relationship, and soft labels. ","This paper proposes a new transformer-based model for supervised learning on the multi-modal QA (VideoQA) task. The authors propose a new reasoning strategy called Siamese Sampling and Reasoning (SiaSamRea) approach that combines the siamese sampling mechanism to generate sparse and similar clips (i.e., siamesese clips, i.e. soft labels). The reasoning strategy is composed of two modules: (1) siamesed knowledge generation, where the inter-relationship between the input and output of a pair of modalities is considered, and (2) a soft label generation module, which uses siameser knowledge reasoning to generate the soft label for each modality.  The authors show that the proposed reasoning strategy improves network inference by incorporating the interdependent knowledge in the network. They also show that SiaSam rea can be applied to the multimodal reasoning paradigm and achieve state-of-the-art performance on the MSRVTT-QA, MSVD-qA, ActivityNet-QAs, How2QA and TGIF-Qa.   The paper is well-written, well-motivated, and well-structured. The experiments on the standard VideoQA benchmarks demonstrate the effectiveness of the proposed method. ","This paper proposes a new transformer-based model for supervised learning on the multi-modal QA (VideoQA) task. The authors propose a new reasoning strategy called Siamese Sampling and Reasoning (SiaSamRea) approach that combines the siamese sampling mechanism to generate sparse and similar clips (i.e., siamesese clips, i.e. soft labels). The reasoning strategy is composed of two modules: (1) siamesed knowledge generation, where the inter-relationship between the input and output of a pair of modalities is considered, and (2) a soft label generation module, which uses siameser knowledge reasoning to generate the soft label for each modality.  The authors show that the proposed reasoning strategy improves network inference by incorporating the interdependent knowledge in the network. They also show that SiaSam rea can be applied to the multimodal reasoning paradigm and achieve state-of-the-art performance on the MSRVTT-QA, MSVD-qA, ActivityNet-QAs, How2QA and TGIF-Qa.   The paper is well-written, well-motivated, and well-structured. The experiments on the standard VideoQA benchmarks demonstrate the effectiveness of the proposed method. "
13463,SP:160022e2cd61159da92f92e85520b7062a337a8d,"Structured distributions USED-FOR latent probabilistic representations. observed data USED-FOR latent probabilistic representations. computational and memory complexity EVALUATE-FOR latent representations. Hidden Markov Models ( HMMs ) CONJUNCTION Probabilistic Context - Free Grammars ( PCFGs ). Probabilistic Context - Free Grammars ( PCFGs ) CONJUNCTION Hidden Markov Models ( HMMs ). Probabilistic Context - Free Grammars ( PCFGs ) HYPONYM-OF models. Hidden Markov Models ( HMMs ) HYPONYM-OF models. computational and memory complexity EVALUATE-FOR structured models. approach USED-FOR structured models. computational and memory complexity EVALUATE-FOR approach. rank USED-FOR speed. matrix - vector product USED-FOR central inference step. polyphonic music modeling CONJUNCTION unsupervised grammar induction. unsupervised grammar induction CONJUNCTION polyphonic music modeling. language modeling CONJUNCTION polyphonic music modeling. polyphonic music modeling CONJUNCTION language modeling. neural parameterized structured models USED-FOR language modeling. unsupervised grammar induction CONJUNCTION video modeling. video modeling CONJUNCTION unsupervised grammar induction. neural parameterized structured models USED-FOR polyphonic music modeling. accuracy EVALUATE-FOR models. approach COMPARE models. models COMPARE approach. neural parameterized structured models EVALUATE-FOR approach. neural parameterized structured models USED-FOR unsupervised grammar induction. unsupervised grammar induction EVALUATE-FOR approach. large state spaces FEATURE-OF models. accuracy EVALUATE-FOR approach. OtherScientificTerm are combinatorial spaces, hidden states, and low - rank constraint. ","This paper proposes a new approach to learn structured distributions for learning latent probabilistic representations from observed data. The approach aims to reduce the computational and memory complexity of learning such latent representations by reducing the number of hidden states in the combinatorial spaces. The authors propose two models: Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs). The authors show that the proposed approach can learn structured models with lower computational complexity than existing structured models in terms of the rank of the hidden states and speed. The central inference step is based on the matrix-vector product between the hidden state and the true hidden state, and the authors propose to use a low-rank constraint. The proposed approach is evaluated on language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling using neural parameterized structured models, and shows improved accuracy over existing models with large state spaces. ","This paper proposes a new approach to learn structured distributions for learning latent probabilistic representations from observed data. The approach aims to reduce the computational and memory complexity of learning such latent representations by reducing the number of hidden states in the combinatorial spaces. The authors propose two models: Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs). The authors show that the proposed approach can learn structured models with lower computational complexity than existing structured models in terms of the rank of the hidden states and speed. The central inference step is based on the matrix-vector product between the hidden state and the true hidden state, and the authors propose to use a low-rank constraint. The proposed approach is evaluated on language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling using neural parameterized structured models, and shows improved accuracy over existing models with large state spaces. "
13512,SP:238592ad73927194cdf0c0cf9ae2e48ca86e182c,"exploration USED-FOR Reinforcement Learning. Thompson Sampling HYPONYM-OF Bayesian exploration strategies. technique USED-FOR complex environments. computational intractability FEATURE-OF probability distributions. deep neural network models CONJUNCTION approximate posterior methods. approximate posterior methods CONJUNCTION deep neural network models. approximation techniques USED-FOR exploration - exploitation trade - offs. Sample Average Uncertainty ( SAU ) HYPONYM-OF uncertainty measure. uncertainty measure USED-FOR contextual bandits. SAU HYPONYM-OF frequentist approach. Bayesian approaches USED-FOR outcomes uncertainty. Thompson Sampling HYPONYM-OF Bayesian approaches. SAU USED-FOR uncertainty measure. SAU USED-FOR deep contextual bandits. drop - in replacement USED-FOR epsilongreedy exploration. drop - in replacement USED-FOR deep contextual bandits. SAU - based exploration COMPARE deep Bayesian bandit methods. deep Bayesian bandit methods COMPARE SAU - based exploration. modest computation cost EVALUATE-FOR deep Bayesian bandit methods. real - world datasets EVALUATE-FOR deep Bayesian bandit methods. real - world datasets EVALUATE-FOR SAU - based exploration. Task is exploration - exploitation dilemma. OtherScientificTerm are action - value function, value predictions, and regret bounds. Method are outcome models, and outcome model. Metric is complexity. Material is deep bandit scenario. ","This paper considers the exploration-exploitation dilemma in Reinforcement Learning, where the goal is to learn an action-value function that maximizes the expected return of the current state-action pair, while minimizing the expected regret of the action taken by the agent. The authors propose two Bayesian exploration strategies: Sample Average Uncertainty (SAU) and Thompson Sampling, both of which are Bayes-based approaches for learning outcome models. The main contribution of this paper is to extend the technique to more complex environments where the probability distributions have computational intractability (e.g., deep neural network models and approximate posterior methods). The authors provide regret bounds for both exploration and exploitation trade-offs based on approximation techniques. They also propose a new uncertainty measure, Sample Average Ununcertainty, for contextual bandits, which is an extension of SAU, a frequentist approach to the uncertainty measure used by previous Bayesian approaches for outcomes uncertainty (such as Thompson Sampled). They show that SAU can be used as an uncertainty measure for deep contextual bandits as a drop-in replacement for epsilongreedy exploration, and show that the proposed SAU-based exploration outperforms existing deep Bayesian bandit methods on several real-world datasets with modest computation cost.    The authors also provide a theoretical analysis of the complexity of the SAU in the deep bandit scenario, which shows that the regret bounds are tight. ","This paper considers the exploration-exploitation dilemma in Reinforcement Learning, where the goal is to learn an action-value function that maximizes the expected return of the current state-action pair, while minimizing the expected regret of the action taken by the agent. The authors propose two Bayesian exploration strategies: Sample Average Uncertainty (SAU) and Thompson Sampling, both of which are Bayes-based approaches for learning outcome models. The main contribution of this paper is to extend the technique to more complex environments where the probability distributions have computational intractability (e.g., deep neural network models and approximate posterior methods). The authors provide regret bounds for both exploration and exploitation trade-offs based on approximation techniques. They also propose a new uncertainty measure, Sample Average Ununcertainty, for contextual bandits, which is an extension of SAU, a frequentist approach to the uncertainty measure used by previous Bayesian approaches for outcomes uncertainty (such as Thompson Sampled). They show that SAU can be used as an uncertainty measure for deep contextual bandits as a drop-in replacement for epsilongreedy exploration, and show that the proposed SAU-based exploration outperforms existing deep Bayesian bandit methods on several real-world datasets with modest computation cost.    The authors also provide a theoretical analysis of the complexity of the SAU in the deep bandit scenario, which shows that the regret bounds are tight. "
13561,SP:ffc5b18f7e18607b2934e5aa199e7542005d79f4,"behavior CONJUNCTION neural activity. neural activity CONJUNCTION behavior. deep learning USED-FOR automated analysis of behavior. computer vision CONJUNCTION deep learning. deep learning CONJUNCTION computer vision. computer vision USED-FOR automated analysis of behavior. Disentangled Behavior Embedding ( DBE ) USED-FOR robust behavioral embeddings. DBE CONJUNCTION stochastic temporal model. stochastic temporal model CONJUNCTION DBE. end - to - end approach USED-FOR discrete behavior representations. models USED-FOR consistent behavior representations. dynamic behavioral factors ( pose ) PART-OF deep autoencoder. temporal structures of pose dynamics USED-FOR models. fine - grained behavioral motif generation CONJUNCTION behavior decoding. behavior decoding CONJUNCTION fine - grained behavioral motif generation. approaches COMPARE DBE. DBE COMPARE approaches. approaches COMPARE VDBE. VDBE COMPARE approaches. DBE CONJUNCTION VDBE. VDBE CONJUNCTION DBE. DBE USED-FOR tasks. tasks EVALUATE-FOR VDBE. tasks EVALUATE-FOR approaches. behavior decoding HYPONYM-OF tasks. fine - grained behavioral motif generation HYPONYM-OF tasks. Material are neuroscience, large and high - quality video datasets, and interpretable behavioral videos. Task is motor task. ","This paper proposes Disentangled Behavior Embedding (DBDE), a method for learning behavior embeddings from video. The authors propose to use a deep autoencoder with dynamic behavioral factors (pose) as a prior, and use an end-to-end approach to learn discrete behavior representations. They show that DBE can be combined with a stochastic temporal model to achieve robust behavior embedding. They also show that models trained on temporal structures of pose dynamics are able to learn consistent behavior representations that are robust to changes in the temporal structure of a motor task. They evaluate DBE and VDBE on two tasks: fine-grained behavioral motif generation and behavior decoding, and show that their approaches outperform previous approaches on both tasks.","This paper proposes Disentangled Behavior Embedding (DBDE), a method for learning behavior embeddings from video. The authors propose to use a deep autoencoder with dynamic behavioral factors (pose) as a prior, and use an end-to-end approach to learn discrete behavior representations. They show that DBE can be combined with a stochastic temporal model to achieve robust behavior embedding. They also show that models trained on temporal structures of pose dynamics are able to learn consistent behavior representations that are robust to changes in the temporal structure of a motor task. They evaluate DBE and VDBE on two tasks: fine-grained behavioral motif generation and behavior decoding, and show that their approaches outperform previous approaches on both tasks."
13610,SP:bf78a450e4aad6b87fdeb8ec0d68adaaff7b595b,"DMTET HYPONYM-OF deep 3D conditional generative model. user guides USED-FOR DMTET. coarse voxels HYPONYM-OF user guides. hybrid 3D representation USED-FOR implicit and explicit 3D representations. DMTET USED-FOR reconstructed surface. implicit approaches COMPARE DMTET. DMTET COMPARE implicit approaches. deep 3D generative models USED-FOR explicit representations. deep 3D generative models COMPARE model. model COMPARE deep 3D generative models. meshes HYPONYM-OF explicit representations. deformable tetrahedral grid USED-FOR discretized signed distance function. implicit signed distance representation CONJUNCTION explicit surface mesh representation. explicit surface mesh representation CONJUNCTION implicit signed distance representation. discretized signed distance function CONJUNCTION differentiable marching tetrahedra layer. differentiable marching tetrahedra layer CONJUNCTION discretized signed distance function. differentiable marching tetrahedra layer USED-FOR implicit signed distance representation. deformable tetrahedral grid CONJUNCTION differentiable marching tetrahedra layer. differentiable marching tetrahedra layer CONJUNCTION deformable tetrahedral grid. deformable tetrahedral grid PART-OF DMTET. differentiable marching tetrahedra layer PART-OF DMTET. reconstruction CONJUNCTION adversarial losses. adversarial losses CONJUNCTION reconstruction. surface mesh USED-FOR adversarial losses. adversarial losses USED-FOR generation of the hierarchy of subdivisions. reconstruction USED-FOR generation of the hierarchy of subdivisions. approach USED-FOR conditional shape synthesis. coarse voxel inputs USED-FOR conditional shape synthesis. OtherScientificTerm are signed distance values, finer geometric details, arbitrary topology, and hierarchy of subdivisions. Material is complex 3D animal shapes. ","This paper proposes DMTET, a deep 3D conditional generative model based on user guides (e.g., coarse voxels). The authors propose a hybrid 3D representation for both implicit and explicit 3D representations, where the signed distance values are used to represent finer geometric details, and the reconstructed surface is represented as a mesh. Compared to previous implicit approaches, the authors show that the proposed model is able to generate complex 3D animal shapes with complex topology. The authors also show that their model is more robust to adversarial attacks compared to other deep generative models for explicit representations (i.e., meshes).    The authors introduce a deformable tetrahedral grid, a discretized signed distance function, a differentiable marching tetrahedra layer, and an implicit signed distance representation and an explicit surface mesh representation. The proposed approach is applied to the problem of conditional shape synthesis with coarse vauxel inputs, and is shown to be robust to reconstruction and adversarial losses on the surface mesh, and robust to the generation of the hierarchy of subdivisions, and to the arbitrary topology of the surface.","This paper proposes DMTET, a deep 3D conditional generative model based on user guides (e.g., coarse voxels). The authors propose a hybrid 3D representation for both implicit and explicit 3D representations, where the signed distance values are used to represent finer geometric details, and the reconstructed surface is represented as a mesh. Compared to previous implicit approaches, the authors show that the proposed model is able to generate complex 3D animal shapes with complex topology. The authors also show that their model is more robust to adversarial attacks compared to other deep generative models for explicit representations (i.e., meshes).    The authors introduce a deformable tetrahedral grid, a discretized signed distance function, a differentiable marching tetrahedra layer, and an implicit signed distance representation and an explicit surface mesh representation. The proposed approach is applied to the problem of conditional shape synthesis with coarse vauxel inputs, and is shown to be robust to reconstruction and adversarial losses on the surface mesh, and robust to the generation of the hierarchy of subdivisions, and to the arbitrary topology of the surface."
13659,SP:2bc0bd6aa2a12691b16145f0d23542c4c86e3a44,"information theory CONJUNCTION statistics. statistics CONJUNCTION information theory. statistics CONJUNCTION machine learning. machine learning CONJUNCTION statistics. statistical dependence FEATURE-OF Mutual information ( MI ). structural properties FEATURE-OF it. sliced MI ( SMI ) USED-FOR surrogate measure of dependence. it USED-FOR structural properties. scalable computation CONJUNCTION estimation. estimation CONJUNCTION scalable computation. structural properties FEATURE-OF MI. estimation EVALUATE-FOR it. scalable computation EVALUATE-FOR it. MI COMPARE SMI. SMI COMPARE MI. deterministic transformations USED-FOR SMI. SMI USED-FOR feature extraction. processing functions of raw data USED-FOR it. independence testing CONJUNCTION feature extraction. feature extraction CONJUNCTION independence testing. MI USED-FOR high - dimensional inference. SMI COMPARE MI. MI COMPARE SMI. SMI USED-FOR high - dimensional inference. independence testing USED-FOR theory. feature extraction USED-FOR theory. Task is estimation of highdimensional MI. OtherScientificTerm are statistical scalability, and one - dimensional random projections. ","This paper studies the estimation of highdimensional Mutual information (MI) in the context of information theory, statistics, and machine learning. The authors propose a surrogate measure of dependence, sliced MI (SMI), which is a measure of statistical dependence between two pairs of data points. They show that it captures the structural properties of MI and that it can be used for both scalable computation and estimation. They also show that SMI can be applied to deterministic transformations, which is an important property of MI.    The paper also shows that the SMI is more efficient than the original SMI in terms of statistical scalability, as it is based on the processing functions of raw data. The theory is tested on independence testing, feature extraction, and high-dimensional inference, where SMI outperforms the original MI in all cases, and is particularly useful for one-dimensional random projections. ","This paper studies the estimation of highdimensional Mutual information (MI) in the context of information theory, statistics, and machine learning. The authors propose a surrogate measure of dependence, sliced MI (SMI), which is a measure of statistical dependence between two pairs of data points. They show that it captures the structural properties of MI and that it can be used for both scalable computation and estimation. They also show that SMI can be applied to deterministic transformations, which is an important property of MI.    The paper also shows that the SMI is more efficient than the original SMI in terms of statistical scalability, as it is based on the processing functions of raw data. The theory is tested on independence testing, feature extraction, and high-dimensional inference, where SMI outperforms the original MI in all cases, and is particularly useful for one-dimensional random projections. "
13708,SP:e220b348901b476c2afd95f97630fb5400582f40,"query efficiency EVALUATE-FOR myopic methods. non - myopic Bayesian optimization COMPARE myopic methods. myopic methods COMPARE non - myopic Bayesian optimization. expected improvement HYPONYM-OF myopic methods. query efficiency EVALUATE-FOR non - myopic Bayesian optimization. unreliable bruteforce derivative - free optimization USED-FOR Monte Carlo rollout acquisition function. unreliable bruteforce derivative - free optimization USED-FOR multi - step lookahead constrained BO method. sample average approximation CONJUNCTION infinitesimal perturbation analysis. infinitesimal perturbation analysis CONJUNCTION sample average approximation. reparameterization trick USED-FOR Methods. likelihoodratio - based unbiased estimator USED-FOR acquisition function optimization. 2 - OPT - C COMPARE methods. methods COMPARE 2 - OPT - C. query efficiency EVALUATE-FOR methods. query efficiency EVALUATE-FOR 2 - OPT - C. Metric is computational cost. Method is unconstrained BO methods. Material is unconstrained setting. OtherScientificTerm are constraints, sampled acquisition function surface, feasible and infeasible regions, and tight constraints. Task are constrained problems, and sequential and batch settings. ","This paper studies the query efficiency of non-myopic Bayesian optimization (i.e., expected improvement) compared to myopic methods on the problem of query efficiency. The authors consider the unconstrained setting, where there are no constraints on the sampled acquisition function surface, but there is a constraint on the number of feasible and infeasible regions. They propose a multi-step lookahead constrained BO method based on unreliable bruteforce derivative-free optimization for the Monte Carlo rollout acquisition function. They show that their method 2-OPT-C outperforms existing methods on query efficiency in both sequential and batch settings. They also show that the computational cost is much lower than that of unconstrain BO methods. Methods based on the reparameterization trick are also shown to outperform existing methods.  The authors also propose a likelihoodratio-based unbiased estimator for the acquisition function optimization, which is based on sample average approximation and infinitesimal perturbation analysis.  ","This paper studies the query efficiency of non-myopic Bayesian optimization (i.e., expected improvement) compared to myopic methods on the problem of query efficiency. The authors consider the unconstrained setting, where there are no constraints on the sampled acquisition function surface, but there is a constraint on the number of feasible and infeasible regions. They propose a multi-step lookahead constrained BO method based on unreliable bruteforce derivative-free optimization for the Monte Carlo rollout acquisition function. They show that their method 2-OPT-C outperforms existing methods on query efficiency in both sequential and batch settings. They also show that the computational cost is much lower than that of unconstrain BO methods. Methods based on the reparameterization trick are also shown to outperform existing methods.  The authors also propose a likelihoodratio-based unbiased estimator for the acquisition function optimization, which is based on sample average approximation and infinitesimal perturbation analysis.  "
13757,SP:51fbd861422647912f275b48861ea3c4812afdc8,scalar value functions PART-OF value network. distributional RL USED-FOR return distribution. return distribution COMPARE scalar value. scalar value COMPARE return distribution. hybrid reward architectures ( HRA ) USED-FOR source - specific value functions. hybrid reward architectures ( HRA ) USED-FOR RL. source - specific value functions USED-FOR reward. distributional RL CONJUNCTION hybrid reward architectures. hybrid reward architectures CONJUNCTION distributional RL. Multi - Dimensional Distributional DQN ( MD3QN ) USED-FOR joint return distribution. distributional RL USED-FOR joint return distribution. distributional RL USED-FOR Multi - Dimensional Distributional DQN ( MD3QN ). MD3QN USED-FOR randomness in returns. MD3QN USED-FOR rich reward correlation. joint return distribution CONJUNCTION Bellman target. Bellman target CONJUNCTION joint return distribution. Maximum Mean Discrepancy FEATURE-OF joint return distribution. Maximum Mean Discrepancy USED-FOR empirical algorithm. method USED-FOR joint return distribution. method COMPARE RL methods. RL methods COMPARE method. multi - dimensional reward functions USED-FOR control setting. richly correlated reward functions FEATURE-OF joint return distribution. control setting EVALUATE-FOR RL methods. multi - dimensional reward functions USED-FOR method. multi - dimensional reward functions USED-FOR RL methods. Method is joint distribution modeling. OtherScientificTerm is joint distributional Bellman operator. ,"This paper proposes a new joint distributional DQN (JDQN) model for RL, which is based on distributional RL and hybrid reward architectures (HRA) to learn source-specific value functions in the value network. The authors show that the return distribution of the joint distribution can be asymptotically different from the scalar value of the whole value network, and that this is due to the use of joint distribution modeling. They propose Multi-Dimensional Distributional Distributional Bellman operator (MD3QN), which is a variant of Distributional RL that learns a joint return distribution over the joint return and the Bellman target. They also show that this joint return can be approximated using Maximum Mean Discrepancy (MMD) as an empirical algorithm. They further show that MD3Qn is able to capture the rich reward correlation between the joint reward function and the reward function of the source. They show that in the control setting, the proposed method can be used to learn a joint reward distribution over richly correlated reward functions, and they show that their method outperforms existing RL methods using multi-dimensional reward functions in a control setting.","This paper proposes a new joint distributional DQN (JDQN) model for RL, which is based on distributional RL and hybrid reward architectures (HRA) to learn source-specific value functions in the value network. The authors show that the return distribution of the joint distribution can be asymptotically different from the scalar value of the whole value network, and that this is due to the use of joint distribution modeling. They propose Multi-Dimensional Distributional Distributional Bellman operator (MD3QN), which is a variant of Distributional RL that learns a joint return distribution over the joint return and the Bellman target. They also show that this joint return can be approximated using Maximum Mean Discrepancy (MMD) as an empirical algorithm. They further show that MD3Qn is able to capture the rich reward correlation between the joint reward function and the reward function of the source. They show that in the control setting, the proposed method can be used to learn a joint reward distribution over richly correlated reward functions, and they show that their method outperforms existing RL methods using multi-dimensional reward functions in a control setting."
13806,SP:1f85c93d6bbfd65bf497c92c9cd534d799753097,"CorticalFlow HYPONYM-OF geometric deep - learning model. diffeomorphic transformations USED-FOR model. numeric conditions USED-FOR manifoldness. discrete resolution USED-FOR topological errors. numeric conditions USED-FOR topological errors. CorticalFlow USED-FOR brain cortical surface reconstruction. its USED-FOR brain cortical surface reconstruction. its EVALUATE-FOR CorticalFlow. computation time EVALUATE-FOR CorticalFlow. CorticalFlow USED-FOR generation of anatomically plausible surfaces. Material is 3 - dimensional image. OtherScientificTerm are template mesh ’s topological properties, and GPU memory footprint. Method are flow Ordinary Differential Equation ( ODE ) framework, and surface reconstruction methods. Task is generation of surfaces. ","This paper proposes a geometric deep-learning model called CorticalFlow, which is based on the flow Ordinary Differential Equation (ODE) framework. The model is trained on diffeomorphic transformations of a 3-dimensional image, where the model is able to learn to generate a 3D surface from a template mesh’s topological properties. The topological errors are modeled as discrete resolution, and numeric conditions on the manifoldness of the manifold are used to ensure that the manifold is well-manifolded. The paper shows that the generation of surfaces from the template mesh is computationally efficient, with a GPU memory footprint much smaller than existing surface reconstruction methods, and its performance on brain cortical surface reconstruction is comparable to its state-of-the-art counterpart, but with much faster computation time.    The paper also shows that, in addition to the GPU memory savings, the authors also show that the use of numeric conditions to ensure manifoldness is also beneficial, and that this can be used to reduce the number of computationally expensive computations.  Finally, the paper shows how the proposed corticalFlow can be applied to the generation and generation of anatomically plausible surfaces, and how it can be combined with existing methods.","This paper proposes a geometric deep-learning model called CorticalFlow, which is based on the flow Ordinary Differential Equation (ODE) framework. The model is trained on diffeomorphic transformations of a 3-dimensional image, where the model is able to learn to generate a 3D surface from a template mesh’s topological properties. The topological errors are modeled as discrete resolution, and numeric conditions on the manifoldness of the manifold are used to ensure that the manifold is well-manifolded. The paper shows that the generation of surfaces from the template mesh is computationally efficient, with a GPU memory footprint much smaller than existing surface reconstruction methods, and its performance on brain cortical surface reconstruction is comparable to its state-of-the-art counterpart, but with much faster computation time.    The paper also shows that, in addition to the GPU memory savings, the authors also show that the use of numeric conditions to ensure manifoldness is also beneficial, and that this can be used to reduce the number of computationally expensive computations.  Finally, the paper shows how the proposed corticalFlow can be applied to the generation and generation of anatomically plausible surfaces, and how it can be combined with existing methods."
13855,SP:2f31d9cf4ad17ad08344439ca0aef7ec91944545,"computational cost EVALUATE-FOR models. differential privacy CONJUNCTION max information. max information CONJUNCTION differential privacy. deletion guarantees CONJUNCTION deletion guarantees. deletion guarantees CONJUNCTION deletion guarantees. deletion guarantees FEATURE-OF non - adaptive sequences. deletion guarantees FEATURE-OF adaptive sequences. provable deletion guarantees FEATURE-OF adaptive deletion sequences. attack USED-FOR SISA algorithm. non - convex models USED-FOR adaptive deletion sequences. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. Method is Data deletion algorithms. Task is non - convex setting. OtherScientificTerm are update sequence, non - adaptive deletion sequences, and training methodologies. ","Data deletion algorithms have been a hot topic of interest in the recent years. However, the computational cost of these models is expensive due to the non-convex setting. This paper aims to address this issue by proposing a novel attack on the SISA algorithm.    The authors consider the setting of differential privacy and max information, where the update sequence is non-adaptive. They show that under certain assumptions on differential privacy, there are provable deletion guarantees for adaptive deletion sequences with provably provable deletions. They also show that there are deletion guarantees in non-consvex cases for adaptive sequences and non-autoregressive deletion guarantees. Finally, they show that non-vanishingly adaptive deletions are provably non-trivial under certain training methodologies.  The paper also shows that the attack can be applied to non-comprehensive models and can be used to attack the adaptive deleting sequences. Experiments are conducted on CIFAR-10, MNIST, and Fashion-MNIST. ","Data deletion algorithms have been a hot topic of interest in the recent years. However, the computational cost of these models is expensive due to the non-convex setting. This paper aims to address this issue by proposing a novel attack on the SISA algorithm.    The authors consider the setting of differential privacy and max information, where the update sequence is non-adaptive. They show that under certain assumptions on differential privacy, there are provable deletion guarantees for adaptive deletion sequences with provably provable deletions. They also show that there are deletion guarantees in non-consvex cases for adaptive sequences and non-autoregressive deletion guarantees. Finally, they show that non-vanishingly adaptive deletions are provably non-trivial under certain training methodologies.  The paper also shows that the attack can be applied to non-comprehensive models and can be used to attack the adaptive deleting sequences. Experiments are conducted on CIFAR-10, MNIST, and Fashion-MNIST. "
13904,SP:7150006590e268ab732c9be6c9048f67a377f956,epistemic uncertainty CONJUNCTION aleatoric uncertainty. aleatoric uncertainty CONJUNCTION epistemic uncertainty. prior distribution FEATURE-OF MDPs. policy optimising CVaR USED-FOR setting. aleatoric uncertainty CONJUNCTION inherent stochasticity of MDPs. inherent stochasticity of MDPs CONJUNCTION aleatoric uncertainty. prior distribution FEATURE-OF epistemic uncertainty. Monte Carlo tree search CONJUNCTION Bayesian optimisation. Bayesian optimisation CONJUNCTION Monte Carlo tree search. two - player stochastic game USED-FOR problem. Monte Carlo tree search USED-FOR approximate algorithm. Bayesian optimisation USED-FOR approximate algorithm. approach COMPARE baseline approaches. baseline approaches COMPARE approach. baseline approaches USED-FOR problem. approach USED-FOR problem. Task is risk - averse Bayes - adaptive reinforcement learning. OtherScientificTerm is conditional value at risk ( CVaR ). ,"This paper studies risk-averse Bayes-adaptive reinforcement learning in the setting where the epistemic uncertainty (i.e., the prior distribution of the MDPs) and aleatoric uncertainty (the inherent stochasticity of the prior) are known, and the conditional value at risk (CVaR) is known. In this setting, the authors consider policy optimising CVaR. They propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation to solve the problem in a two-player stochastically game. They show that their approach outperforms baseline approaches in this problem. ","This paper studies risk-averse Bayes-adaptive reinforcement learning in the setting where the epistemic uncertainty (i.e., the prior distribution of the MDPs) and aleatoric uncertainty (the inherent stochasticity of the prior) are known, and the conditional value at risk (CVaR) is known. In this setting, the authors consider policy optimising CVaR. They propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation to solve the problem in a two-player stochastically game. They show that their approach outperforms baseline approaches in this problem. "
13953,SP:a94f39406f73d7483ddd744ed2f03c78b8bc5d44,"binary classification data USED-FOR shallow ReLU networks. logistic loss USED-FOR shallow ReLU networks. gradient descent USED-FOR logistic loss. gradient descent USED-FOR shallow ReLU networks. sigmoid mapping USED-FOR conditional distribution. gradient descent USED-FOR population risk. early stopping USED-FOR gradient descent. complexity measure EVALUATE-FOR conditional model. local interpolation property FEATURE-OF univariate classifier. Deep networks USED-FOR arbitrary prediction problems. gradient descent USED-FOR Deep networks. constant step size FEATURE-OF vanilla gradient descent. vanilla gradient descent USED-FOR inner ( inputfacing ) weights. shallow ReLU networks HYPONYM-OF networks. induced conditional model COMPARE model. model COMPARE induced conditional model. optimality FEATURE-OF population misclassification rate. sigmoid mapping USED-FOR induced conditional model. gradient descent USED-FOR restricted conditional models. network nodes CONJUNCTION gradient descent iterations. gradient descent iterations CONJUNCTION network nodes. optimal test error FEATURE-OF noisy distributions. optimal test error EVALUATE-FOR univariate predictor. local interpolation property FEATURE-OF univariate predictor. multiplicative error property FEATURE-OF logistic loss. technique USED-FOR large network width. empirical logistic risk CONJUNCTION logistic risk. logistic risk CONJUNCTION empirical logistic risk. logistic loss CONJUNCTION empirical logistic risk. empirical logistic risk CONJUNCTION logistic loss. compactly - supported marginal FEATURE-OF Borel measure. gradient descent USED-FOR optimal test error. it USED-FOR learning task. universal approximation properties FEATURE-OF neural networks. Bayes ( convex ) risk USED-FOR conditional model. agnostic learning setting HYPONYM-OF predictors. shallow ReLU networks USED-FOR predictors. gradient descent USED-FOR shallow ReLU networks. OtherScientificTerm are data distribution, joint distribution, training time, measurable functions, training risk, data simplicity, distribution, finite sample, sphere, function f, R, and computational and statistical obstructions. Metric are Bayes risk, logistic and misclassification losses, calibration, Bayes risk R, and misclassification loss. Method are stalwart methods, infinite - width random feature model, classification calibration, and universal","This paper studies deep ReLU networks trained on binary classification data. Deep networks have been shown to be universal approximators of arbitrary prediction problems when the data distribution is non-i.i.d. (i.e., when the joint distribution of the input and the output of the network is a finite sample from a joint distribution). Deep networks are typically trained using gradient descent with a constant step size.    This paper considers the problem of training shallow ReLU neural networks with logistic loss on a finite set of samples. The authors show that gradient descent can be used to reduce the population risk of the population misclassification rate of a classifier trained with gradient descent for restricted conditional models. This is done by using a sigmoid mapping from the conditional distribution to the true conditional distribution, which is defined as a function of the number of network nodes and gradient descent iterations.  The authors also show that the complexity measure of the conditional model can be expressed as the sum of the logistic and logistic losses of the classifier, and show that this complexity measure is a lower bound of the Bayes risk of a trained classifier.  In addition, they show that for any infinite-width random feature model, gradient descent on the inner (inputfacing) weights of a deep network with vanilla gradient descent converges to the optimal test error for a class of networks (shallow reLU networks, i.e. networks with ReLU activation functions).   The main contribution of this paper is that the authors show how gradient descent (with early stopping) is able to reduce population risk when the training time is large enough.  They show that, for any finite sample, the induced conditional model has the same local interpolation property as the original model, which means that the data simplicity of the training risk is the same as the optimality of the univariate classifier with respect to the data. They also show how this technique can be applied to large network width.  Finally, the authors prove that, under the assumption that the distribution of samples is a sphere around a point on the sphere, any univariate predictor with the same number of samples has a local interpolated property (which they call “universal interpolation”), and that this univariate predictors can be learned using deep networks.  This is achieved by gradient descent in the following way:   1. For any infinite set of data points, we can learn a Bayes (convex) risk for a conditional model  2. We can learn an infinite number of parameters of the infinite set, and we can use gradient descent to learn a class-conditional model.  3. For a finite number of training samples, we are able to learn the optimal classifier (with infinite width).  4. We are allowed to sample from an infinite set with infinite width and infinite depth.  We can also sample from this infinite set. ","This paper studies deep ReLU networks trained on binary classification data. Deep networks have been shown to be universal approximators of arbitrary prediction problems when the data distribution is non-i.i.d. (i.e., when the joint distribution of the input and the output of the network is a finite sample from a joint distribution). Deep networks are typically trained using gradient descent with a constant step size.    This paper considers the problem of training shallow ReLU neural networks with logistic loss on a finite set of samples. The authors show that gradient descent can be used to reduce the population risk of the population misclassification rate of a classifier trained with gradient descent for restricted conditional models. This is done by using a sigmoid mapping from the conditional distribution to the true conditional distribution, which is defined as a function of the number of network nodes and gradient descent iterations.  The authors also show that the complexity measure of the conditional model can be expressed as the sum of the logistic and logistic losses of the classifier, and show that this complexity measure is a lower bound of the Bayes risk of a trained classifier.  In addition, they show that for any infinite-width random feature model, gradient descent on the inner (inputfacing) weights of a deep network with vanilla gradient descent converges to the optimal test error for a class of networks (shallow reLU networks, i.e. networks with ReLU activation functions).   The main contribution of this paper is that the authors show how gradient descent (with early stopping) is able to reduce population risk when the training time is large enough.  They show that, for any finite sample, the induced conditional model has the same local interpolation property as the original model, which means that the data simplicity of the training risk is the same as the optimality of the univariate classifier with respect to the data. They also show how this technique can be applied to large network width.  Finally, the authors prove that, under the assumption that the distribution of samples is a sphere around a point on the sphere, any univariate predictor with the same number of samples has a local interpolated property (which they call “universal interpolation”), and that this univariate predictors can be learned using deep networks.  This is achieved by gradient descent in the following way:   1. For any infinite set of data points, we can learn a Bayes (convex) risk for a conditional model  2. We can learn an infinite number of parameters of the infinite set, and we can use gradient descent to learn a class-conditional model.  3. For a finite number of training samples, we are able to learn the optimal classifier (with infinite width).  4. We are allowed to sample from an infinite set with infinite width and infinite depth.  We can also sample from this infinite set. "
14002,SP:a9c786cbb61e1f10f3542161b13e43a1a68ab34d,"misinformation campaigns USED-FOR social outcomes. misinformation campaigns USED-FOR coordinated accounts. social media FEATURE-OF coordinated accounts. coordinated group detection USED-FOR misinformation. methodology USED-FOR misinformation. methodology USED-FOR coordinated group detection. social media USED-FOR misinformation. social media FEATURE-OF sparsity of account activities. limited expressive power EVALUATE-FOR detectors. prior knowledge USED-FOR detectors. temporal logic CONJUNCTION pre - defined filtering functions. pre - defined filtering functions CONJUNCTION temporal logic. prior knowledge FEATURE-OF neural temporal point process. neural temporal point process PART-OF coordination detection framework. pre - defined filtering functions HYPONYM-OF prior knowledge. temporal logic HYPONYM-OF prior knowledge. account embedding space CONJUNCTION prior knowledge. prior knowledge CONJUNCTION account embedding space. theoretically guaranteed variational inference approach USED-FOR mean - field approximation. mean - field approximation USED-FOR it. theoretically guaranteed variational inference approach USED-FOR it. real - world dataset EVALUATE-FOR method. method COMPARE model. model COMPARE method. real - world dataset EVALUATE-FOR model. unsupervised and semi - supervised settings EVALUATE-FOR model. unsupervised and semi - supervised settings EVALUATE-FOR method. COVID-19 Vaccine Tweets dataset EVALUATE-FOR model. COVID-19 vaccines FEATURE-OF spreading misinformation. Method is deep learning based coordination detectors. Generic are they, and distribution. OtherScientificTerm is Gibbs distribution of group assignment. Task is detection. ","This paper proposes a methodology for detecting coordinated group detection of misinformation on social media for social outcomes from misinformation campaigns. The authors argue that deep learning based coordination detectors are limited in their expressive power because they rely on prior knowledge (e.g., temporal logic and pre-defined filtering functions) that has limited expressive power, and propose a methodology to mitigate this issue. The proposed coordination detection framework incorporates the neural temporal point process, a prior knowledge of the Gibbs distribution of group assignment, into the detection framework. The method is evaluated on a real-world dataset where the sparsity of account activities in social media is observed, and is compared to a model that does not rely on account embedding space or prior knowledge. The model is also evaluated on the COVID-19 Vaccine Tweets dataset, where spreading misinformation on COVID19 vaccines is also observed. In addition, it uses a theoretically guaranteed variational inference approach to obtain a mean-field approximation to the distribution of the distribution, and it is shown to outperform a previous model on this dataset. ","This paper proposes a methodology for detecting coordinated group detection of misinformation on social media for social outcomes from misinformation campaigns. The authors argue that deep learning based coordination detectors are limited in their expressive power because they rely on prior knowledge (e.g., temporal logic and pre-defined filtering functions) that has limited expressive power, and propose a methodology to mitigate this issue. The proposed coordination detection framework incorporates the neural temporal point process, a prior knowledge of the Gibbs distribution of group assignment, into the detection framework. The method is evaluated on a real-world dataset where the sparsity of account activities in social media is observed, and is compared to a model that does not rely on account embedding space or prior knowledge. The model is also evaluated on the COVID-19 Vaccine Tweets dataset, where spreading misinformation on COVID19 vaccines is also observed. In addition, it uses a theoretically guaranteed variational inference approach to obtain a mean-field approximation to the distribution of the distribution, and it is shown to outperform a previous model on this dataset. "
14051,SP:b5c6e967a26a02861db2ecd620e9061db0c03e59,"binary classification task HYPONYM-OF model problem. unit sphere FEATURE-OF smooth curves. structure USED-FOR model problem. deep fully - connected neural network USED-FOR binary classification task. network depth COMPARE geometric properties. geometric properties COMPARE network depth. generalization guarantee EVALUATE-FOR deep networks. nonlinear data USED-FOR deep networks. fitting resource USED-FOR classification problem. network depth HYPONYM-OF fitting resource. neural tangent kernel ( NTK ) regime FEATURE-OF reduction to dynamics. convergence CONJUNCTION generalization. generalization CONJUNCTION convergence. decay properties FEATURE-OF NTK. fine - grained control USED-FOR decay properties. fine - grained control USED-FOR NTK. manifolds FEATURE-OF translationally invariant operator. smooth functions USED-FOR NTK. translationally invariant operator USED-FOR NTK. OtherScientificTerm are low - dimensional nonlinear structure, mild regularity conditions, network width, intrinsic data properties, and network. Task is engineering and scientific problems. Method is randomly - initialized gradient descent. ","This paper studies the generalization guarantee of deep networks on nonlinear data with a low-dimensional nonlinear structure. The model problem is defined as a binary classification task over a set of smooth curves on a unit sphere, where the structure of the model problem depends on a deep fully-connected neural network. The authors show that under mild regularity conditions, the network depth is a fitting resource for the classification problem, while the geometric properties of the neural tangent kernel (NTK) regime of the reduction to dynamics are non-trivial. They show that the NTK is a translationally invariant operator on manifolds and that the decay properties of NTK can be controlled by fine-grained control on the network width and the intrinsic data properties. They also provide convergence and generalization results under randomly-initialized gradient descent. ","This paper studies the generalization guarantee of deep networks on nonlinear data with a low-dimensional nonlinear structure. The model problem is defined as a binary classification task over a set of smooth curves on a unit sphere, where the structure of the model problem depends on a deep fully-connected neural network. The authors show that under mild regularity conditions, the network depth is a fitting resource for the classification problem, while the geometric properties of the neural tangent kernel (NTK) regime of the reduction to dynamics are non-trivial. They show that the NTK is a translationally invariant operator on manifolds and that the decay properties of NTK can be controlled by fine-grained control on the network width and the intrinsic data properties. They also provide convergence and generalization results under randomly-initialized gradient descent. "
14100,SP:8f6bee3be43df6b6e80804974014caaafe08c49e,"class information PART-OF GAN. auxiliary classifier GAN HYPONYM-OF cGANs. softmax cross - entropy loss ( ACGAN ) FEATURE-OF auxiliary classifier GAN. relational information FEATURE-OF class - labeled dataset. Tiny - ImageNet CONJUNCTION CUB200. CUB200 CONJUNCTION Tiny - ImageNet. CIFAR10 CONJUNCTION Tiny - ImageNet. Tiny - ImageNet CONJUNCTION CIFAR10. CUB200 CONJUNCTION ImageNet datasets. ImageNet datasets CONJUNCTION CUB200. Tiny - ImageNet EVALUATE-FOR ReACGAN. CUB200 EVALUATE-FOR ReACGAN. CIFAR10 EVALUATE-FOR ReACGAN. ImageNet datasets EVALUATE-FOR ReACGAN. D2D - CE CONJUNCTION StyleGAN2 architecture. StyleGAN2 architecture CONJUNCTION D2D - CE. differentiable augmentations USED-FOR ReACGAN. software package USED-FOR representative cGANs. Model weights CONJUNCTION software package. software package CONJUNCTION Model weights. Method are Conditional Generative Adversarial Networks ( cGAN ), ACGAN, and classifier. OtherScientificTerm are diversity, and unit hypersphere. ","This paper proposes ReACGAN, an extension of Conditional Generative Adversarial Networks (cGAN) that incorporates class information into the training of a GAN. The authors argue that existing cGANs (e.g., auxiliary classifier GAN, softmax cross-entropy loss (ACGAN) and D2D-CE) do not capture the relational information of the class-labeled dataset, and that the diversity of a cGAN can be improved by incorporating this relational information. To achieve this, the authors propose a modification of ACGAN, where the classifier is trained on a subset of the training data, and the diversity is encouraged to be high on the unit hypersphere. The proposed re-parameterized ACGAN is evaluated on CIFAR10, Tiny-ImageNet, CUB200, and ImageNet datasets, and shows superior performance compared to ReacGAN with differentiable augmentations. Model weights and a software package are also proposed to train representative cGAN, which can be combined with any existing software package. Experiments are conducted on D2d-CE and StyleGAN2 architecture.  ","This paper proposes ReACGAN, an extension of Conditional Generative Adversarial Networks (cGAN) that incorporates class information into the training of a GAN. The authors argue that existing cGANs (e.g., auxiliary classifier GAN, softmax cross-entropy loss (ACGAN) and D2D-CE) do not capture the relational information of the class-labeled dataset, and that the diversity of a cGAN can be improved by incorporating this relational information. To achieve this, the authors propose a modification of ACGAN, where the classifier is trained on a subset of the training data, and the diversity is encouraged to be high on the unit hypersphere. The proposed re-parameterized ACGAN is evaluated on CIFAR10, Tiny-ImageNet, CUB200, and ImageNet datasets, and shows superior performance compared to ReacGAN with differentiable augmentations. Model weights and a software package are also proposed to train representative cGAN, which can be combined with any existing software package. Experiments are conducted on D2d-CE and StyleGAN2 architecture.  "
14149,SP:080e80746a87228b156408ff649ab7a17f44e92d,Policy Space Response Oracles ( PSRO ) HYPONYM-OF reinforcement learning ( RL ) algorithm. reinforcement learning ( RL ) algorithm USED-FOR two - player zero - sum games. large games FEATURE-OF approximate Nash equilibria. PSRO USED-FOR continuous actions. PSRO USED-FOR approximate Nash equilibrium. Extensive - Form Double Oracle ( XDO ) HYPONYM-OF extensive - form double oracle algorithm. extensive - form double oracle algorithm USED-FOR two - player zero - sum games. PSRO COMPARE XDO. XDO COMPARE PSRO. best responses USED-FOR XDO. deep RL USED-FOR Neural XDO ( NXDO ). deep RL USED-FOR best response. XDO COMPARE PSRO. PSRO COMPARE XDO. XDO USED-FOR approximate Nash equilibrium. XDO COMPARE CFR. CFR COMPARE XDO. Leduc poker game CONJUNCTION Oshi - Zumo. Oshi - Zumo CONJUNCTION Leduc poker game. exploitability EVALUATE-FOR CFR. exploitability EVALUATE-FOR XDO. NXDO COMPARE PSRO. PSRO COMPARE NXDO. NXDO COMPARE NFSP. NFSP COMPARE NXDO. PSRO CONJUNCTION NFSP. NFSP CONJUNCTION PSRO. sequential multidimensional continuous - action game EVALUATE-FOR NFSP. sequential multidimensional continuous - action game EVALUATE-FOR PSRO. sequential multidimensional continuous - action game EVALUATE-FOR NXDO. NXDO HYPONYM-OF deep RL method. deep RL method USED-FOR approximate Nash equilibrium. NXDO USED-FOR approximate Nash equilibrium. high - dimensional continuous - action sequential games FEATURE-OF approximate Nash equilibrium. OtherScientificTerm is infostates. Material is Leduc poker. ,"This paper proposes a reinforcement learning (RL) algorithm called Policy Space Response Oracles (PSRO) which is an extension of the RL algorithm called Neural XDO (NXDO) to two-player zero-sum games. PSRO is an extensive-form double oracle algorithm, which is a variant of the Extensive-Form Double Oracle (XDO), a well-studied RL algorithm for large games. In this paper, the authors show that PSRO converges to approximate Nash equilibria in large games with infostates, while XDO converges directly to the Nash equilibrium with best responses. The authors also show that deep RL can be used to learn the best response in a similar way as XDO.   The authors evaluate PSRO on a sequential multidimensional continuous-action game, a Leduc poker game, and an Oshi-Zumo poker game. They show that XDO outperforms PSRO and NFSP in terms of exploitability, while PSRO outperforms XDO in the case of continuous actions. They also compare the exploitability of XDO to PSRO, NFSP, and a deep RL method called NXDO (Neural XDO) which uses deep RL to learn an approximate Nash equilibrium in a high-dimensional continuous action sequential games.  In addition, they show that the best-response in XDO can be learned using deep RL. ","This paper proposes a reinforcement learning (RL) algorithm called Policy Space Response Oracles (PSRO) which is an extension of the RL algorithm called Neural XDO (NXDO) to two-player zero-sum games. PSRO is an extensive-form double oracle algorithm, which is a variant of the Extensive-Form Double Oracle (XDO), a well-studied RL algorithm for large games. In this paper, the authors show that PSRO converges to approximate Nash equilibria in large games with infostates, while XDO converges directly to the Nash equilibrium with best responses. The authors also show that deep RL can be used to learn the best response in a similar way as XDO.   The authors evaluate PSRO on a sequential multidimensional continuous-action game, a Leduc poker game, and an Oshi-Zumo poker game. They show that XDO outperforms PSRO and NFSP in terms of exploitability, while PSRO outperforms XDO in the case of continuous actions. They also compare the exploitability of XDO to PSRO, NFSP, and a deep RL method called NXDO (Neural XDO) which uses deep RL to learn an approximate Nash equilibrium in a high-dimensional continuous action sequential games.  In addition, they show that the best-response in XDO can be learned using deep RL. "
14198,SP:bda04facef4f34679fc4e17b8ea1aae74c3d649f,"graph structured data USED-FOR deep neural networks. node - level unsupervised learning HYPONYM-OF nodeor graph - level supervised learning. node clustering HYPONYM-OF node - level unsupervised learning. representation complexity EVALUATE-FOR graphs. adjacency matrices USED-FOR graphs. permutation - invariant variational autoencoder USED-FOR graph structured data. model USED-FOR node order. model USED-FOR graph reconstruction. extracted representations USED-FOR downstream graph - level classification and regression. Method are graph - level unsupervised representation learning, and graph matching. ",This paper studies the problem of graph structured data for training deep neural networks. The authors consider node-level unsupervised learning (node-level clustering) and graph-level supervised learning (graph-level learning with adjacency matrices). The authors show that the representation complexity of graphs with the same number of nodes is the same as that of the graphs with different number of edges. They propose a permutation-invariant variational autoencoder to learn graph structured information. They also propose a model to learn the node order for graph reconstruction. The extracted representations are then used for downstream graph -level classification and regression.   ,This paper studies the problem of graph structured data for training deep neural networks. The authors consider node-level unsupervised learning (node-level clustering) and graph-level supervised learning (graph-level learning with adjacency matrices). The authors show that the representation complexity of graphs with the same number of nodes is the same as that of the graphs with different number of edges. They propose a permutation-invariant variational autoencoder to learn graph structured information. They also propose a model to learn the node order for graph reconstruction. The extracted representations are then used for downstream graph -level classification and regression.   
14247,SP:e17ea6aeba78c9dfc25596d8b35a2a4f1f1f6763,"limited scalability EVALUATE-FOR Graph Neural Networks ( GNNs ). subgraph USED-FOR GNN. bounded - size scope FEATURE-OF localized subgraph. critical neighbors PART-OF subgraph. GNN USED-FOR informative representation. GNN USED-FOR local neighborhood. function approximation ( GraphSAGE ) CONJUNCTION topological learning ( GIN ). topological learning ( GIN ) CONJUNCTION function approximation ( GraphSAGE ). graph signal processing ( GCN ) CONJUNCTION function approximation ( GraphSAGE ). function approximation ( GraphSAGE ) CONJUNCTION graph signal processing ( GCN ). decoupling USED-FOR GNN expressive power. graphs CONJUNCTION backbone GNN architectures. backbone GNN architectures CONJUNCTION graphs. backbone GNN architectures EVALUATE-FOR design. graphs EVALUATE-FOR design. OtherScientificTerm are graph and model sizes, model depth, receptive field, degraded expressivity, oversmoothing, neighborhood explosion, node, and global graph. Task is expensive computation. Method is GNNs. ","Graph Neural Networks (GNNs) have been shown to have limited scalability due to the limited graph and model sizes. This paper studies the issue of oversmoothing in GNNs. In particular, the paper shows that a GNN trained on a subgraph with bounded-size scope (i.e. bounded-width and bounded-depth) can only express a localized subgraph of bounded size, which can be decomposed into subgraphs with bounded number of critical neighbors. The paper proposes to decouple the informative representation learned by the GNN from the local neighborhood of the subgraph, which is then decoupled from the global graph. This decoupling is motivated by graph signal processing (GCN), function approximation (GraphSAGE), and topological learning (GIN). The paper also shows that the model depth and the receptive field of GNN can be reduced to zero, which leads to degraded expressivity. The authors also show that the decouplings can be used to reduce the neighborhood explosion, which occurs when a node is added to the local graph, which results in expensive computation. Experiments on graphs and backbone GNN architectures demonstrate the effectiveness of the proposed design on several graphs.","Graph Neural Networks (GNNs) have been shown to have limited scalability due to the limited graph and model sizes. This paper studies the issue of oversmoothing in GNNs. In particular, the paper shows that a GNN trained on a subgraph with bounded-size scope (i.e. bounded-width and bounded-depth) can only express a localized subgraph of bounded size, which can be decomposed into subgraphs with bounded number of critical neighbors. The paper proposes to decouple the informative representation learned by the GNN from the local neighborhood of the subgraph, which is then decoupled from the global graph. This decoupling is motivated by graph signal processing (GCN), function approximation (GraphSAGE), and topological learning (GIN). The paper also shows that the model depth and the receptive field of GNN can be reduced to zero, which leads to degraded expressivity. The authors also show that the decouplings can be used to reduce the neighborhood explosion, which occurs when a node is added to the local graph, which results in expensive computation. Experiments on graphs and backbone GNN architectures demonstrate the effectiveness of the proposed design on several graphs."
14296,SP:4890f251db559a0a572afc66e0c1f899b577d9ff,"Normalizing flows HYPONYM-OF latent - variable generative models. tractable likelihood FEATURE-OF latent - variable generative models. Jacobian FEATURE-OF latent - to - observable - variable transformation. linear time FEATURE-OF likelihood. nearly - singular Jacobian FEATURE-OF networks. affine couplings USED-FOR regular distributions. well - conditioned affine - coupling flows USED-FOR log - concave distribution. underdamped Langevin dynamics CONJUNCTION Hénon maps. Hénon maps CONJUNCTION underdamped Langevin dynamics. Hénon maps HYPONYM-OF structured dynamical system. underdamped Langevin dynamics HYPONYM-OF stochastic differential equation. affine coupling architectures CONJUNCTION underdamped Langevin dynamics. underdamped Langevin dynamics CONJUNCTION affine coupling architectures. symplectic diffeomorphisms FEATURE-OF structured dynamical system. iid Gaussians USED-FOR padded version of the input distribution. Gaussian padding USED-FOR normalizing flows. Method is Affine - coupling models. Generic is architecture. OtherScientificTerm are representational power, ill - conditioned Jacobians, well - conditioned affine coupling flows, and Gibbs measures. Task are universal approximation, likelihood - based training, and training affine couplings. ","This paper studies the problem of learning a tractable likelihood for latent-variable generative models, such as Normalizing flows. Affine-coupling models have been shown to be universal approximators of the log-concave distribution of the Jacobian of the latent-to-observable-variable transformation, and the authors propose a new architecture, called ""affine coupling models"". The authors show that the likelihood of the likelihood in linear time can be approximated by affine couplings for regular distributions, and that networks with a nearly-singular Jacobian have representational power. The authors also show that well-conditioned affine-condensing flows can be used to approximate a log-consistent distribution, and show that underdamped Langevin dynamics and Hénon maps (a stochastic differential equation with symplectic diffeomorphisms) are universal approximation for affine coupling architectures.    The main contribution of the paper is that the authors provide a theoretical analysis of the generalization ability of affine couplings, and they show that for a certain class of networks with Gaussian padding, the likelihood-based training converges to the true distribution.  The authors further show that, for a class of normalizing flows that use iid Gaussians, a padded version of the input distribution is equivalent to a normalizing flow that is a function of ill-conditioning Jacobians.  Finally, the authors show empirically that, under certain conditions, affine fourier networks are able to approximate the true distributions of a structured dynamical system, and can generalize to a more general class of structures.  In addition, they also provide an empirical analysis of how well-conditional affine matching works for different types of networks. They show that in a few cases, well-constrained networks can be learned in a single step.  They also provide some theoretical analysis that shows that, in the presence of Gibbs measures that are non-trivial to compute, the learned representations are not universal, but can be learnt in an unsupervised way. ","This paper studies the problem of learning a tractable likelihood for latent-variable generative models, such as Normalizing flows. Affine-coupling models have been shown to be universal approximators of the log-concave distribution of the Jacobian of the latent-to-observable-variable transformation, and the authors propose a new architecture, called ""affine coupling models"". The authors show that the likelihood of the likelihood in linear time can be approximated by affine couplings for regular distributions, and that networks with a nearly-singular Jacobian have representational power. The authors also show that well-conditioned affine-condensing flows can be used to approximate a log-consistent distribution, and show that underdamped Langevin dynamics and Hénon maps (a stochastic differential equation with symplectic diffeomorphisms) are universal approximation for affine coupling architectures.    The main contribution of the paper is that the authors provide a theoretical analysis of the generalization ability of affine couplings, and they show that for a certain class of networks with Gaussian padding, the likelihood-based training converges to the true distribution.  The authors further show that, for a class of normalizing flows that use iid Gaussians, a padded version of the input distribution is equivalent to a normalizing flow that is a function of ill-conditioning Jacobians.  Finally, the authors show empirically that, under certain conditions, affine fourier networks are able to approximate the true distributions of a structured dynamical system, and can generalize to a more general class of structures.  In addition, they also provide an empirical analysis of how well-conditional affine matching works for different types of networks. They show that in a few cases, well-constrained networks can be learned in a single step.  They also provide some theoretical analysis that shows that, in the presence of Gibbs measures that are non-trivial to compute, the learned representations are not universal, but can be learnt in an unsupervised way. "
14345,SP:5ffa81488ed1092deb89bd5e150fa146325057ce,"Lagrangian problem USED-FOR coupons allocation. method USED-FOR coupons allocation policy. λ - generalization method USED-FOR policy learning process. λ USED-FOR policy learning process. offline reinforcement learning method CONJUNCTION off - policy evaluation algorithm. off - policy evaluation algorithm CONJUNCTION offline reinforcement learning method. policy learning CONJUNCTION policy evaluation. policy evaluation CONJUNCTION policy learning. offline reinforcement learning method USED-FOR policy learning. off - policy evaluation algorithm USED-FOR policy learning. off - policy evaluation algorithm USED-FOR policy evaluation. offline reinforcement learning method USED-FOR policy evaluation. simulation platform CONJUNCTION real - world e - commerce market. real - world e - commerce market CONJUNCTION simulation platform. real - world e - commerce market EVALUATE-FOR approach. simulation platform EVALUATE-FOR approach. Task are Coupons allocation, and online e - commerce environment. Material are e - commerce market, and e - commerce platform. OtherScientificTerm are coupons, Lagrangian multiplier variable λ, and policy space. Method are coupons allocation policy learning, and λ - generalization ( BCORLE(λ ) ) framework. Metric are computation overhead, and users ’ retention rate. Generic are policy, and problem. ","This paper considers the problem of coupon allocation in the online e-commerce market. Coupons allocation is an important problem in this setting, and the authors propose a method to learn a coupons allocation policy in an offline setting, where the coupons are available in an online fashion and the goal is to allocate the best coupon to the best buyer. The authors consider the Lagrangian problem of the coupons allocation, which is an interesting problem with a large computation overhead.    In this paper, the authors introduce the concept of the coupon allocation policy learning, and propose the λ-generalization (BCORLE(+)) framework. The idea is to learn the policy that maximizes the probability that the buyer will receive a coupon with the best price, and that the coupon will be allocated to the buyer.  The authors show that the policy learning process can be decomposed into two steps: (1) learning the policy space, and (2) learning a Lagrangeian multiplier variable $\lambda$.  The proposed method is evaluated on a simulation platform and a real-world e-commerchants market, and it is shown that the proposed method outperforms the existing offline reinforcement learning method and an off-policy evaluation algorithm for policy learning and policy evaluation. The paper also shows that the algorithm is able to improve the users’ retention rate. ","This paper considers the problem of coupon allocation in the online e-commerce market. Coupons allocation is an important problem in this setting, and the authors propose a method to learn a coupons allocation policy in an offline setting, where the coupons are available in an online fashion and the goal is to allocate the best coupon to the best buyer. The authors consider the Lagrangian problem of the coupons allocation, which is an interesting problem with a large computation overhead.    In this paper, the authors introduce the concept of the coupon allocation policy learning, and propose the λ-generalization (BCORLE(+)) framework. The idea is to learn the policy that maximizes the probability that the buyer will receive a coupon with the best price, and that the coupon will be allocated to the buyer.  The authors show that the policy learning process can be decomposed into two steps: (1) learning the policy space, and (2) learning a Lagrangeian multiplier variable $\lambda$.  The proposed method is evaluated on a simulation platform and a real-world e-commerchants market, and it is shown that the proposed method outperforms the existing offline reinforcement learning method and an off-policy evaluation algorithm for policy learning and policy evaluation. The paper also shows that the algorithm is able to improve the users’ retention rate. "
14394,SP:6b04cc7b4e45b9e65a1d34c15e3f75a2ef27d601,"local affinity FEATURE-OF label consistency. local affinity USED-FOR intrinsic structure. self regularization loss USED-FOR noisy neighbors. inherent structure USED-FOR domain adaptation. local neighbors CONJUNCTION reciprocal neighbors. reciprocal neighbors CONJUNCTION local neighbors. reciprocal neighbors CONJUNCTION expanded neighborhood. expanded neighborhood CONJUNCTION reciprocal neighbors. reciprocal neighbors USED-FOR local structure. local neighbors USED-FOR local structure. Task is Domain adaptation ( DA ). Method are DA methods, source pretrained model, and source domain classifier. Generic is method. OtherScientificTerm are affinity, and expanded neighborhoods. Material is 2D image and 3D point cloud recognition datasets. "," Domain adaptation (DA) is an important problem in machine learning, and many existing DA methods rely on the assumption that a source pretrained model is invariant to label noise in the target domain. However, the affinity between source and target labels is not always the same. This paper proposes to use local affinity as a measure of label consistency to measure the intrinsic structure in the source domain. The authors propose a self regularization loss to mitigate the problem of noisy neighbors in a source domain classifier. The proposed method is based on the observation that the inherent structure in a domain adaptation is highly correlated with the affinity of the source and the target labels. To mitigate this issue, the authors propose to learn local neighbors, reciprocal neighbors, and an expanded neighborhood. The local structure is learned by learning local neighbors and reciprocal neighbors in the original domain, and expanded neighborhoods. Experiments are conducted on 2D image and 3D point cloud recognition datasets. "," Domain adaptation (DA) is an important problem in machine learning, and many existing DA methods rely on the assumption that a source pretrained model is invariant to label noise in the target domain. However, the affinity between source and target labels is not always the same. This paper proposes to use local affinity as a measure of label consistency to measure the intrinsic structure in the source domain. The authors propose a self regularization loss to mitigate the problem of noisy neighbors in a source domain classifier. The proposed method is based on the observation that the inherent structure in a domain adaptation is highly correlated with the affinity of the source and the target labels. To mitigate this issue, the authors propose to learn local neighbors, reciprocal neighbors, and an expanded neighborhood. The local structure is learned by learning local neighbors and reciprocal neighbors in the original domain, and expanded neighborhoods. Experiments are conducted on 2D image and 3D point cloud recognition datasets. "
14443,SP:ac1bf04ff782e5892a0bc5fe5949848ca8e731c2,graph learning CONJUNCTION image / video recognition. image / video recognition CONJUNCTION graph learning. image / video recognition CONJUNCTION object detection. object detection CONJUNCTION image / video recognition. point cloud processing CONJUNCTION graph learning. graph learning CONJUNCTION point cloud processing. Learning representations from sets USED-FOR point cloud processing. point cloud processing CONJUNCTION image / video recognition. image / video recognition CONJUNCTION point cloud processing. geometrically - interpretable and generic pooling mechanism USED-FOR fixed - dimensional representation. geometrically - interpretable and generic pooling mechanism USED-FOR features. end - to - end trainable Euclidean embedding USED-FOR sliced - Wasserstein distance. end - to - end trainable Euclidean embedding USED-FOR set - structured data. method COMPARE set representation learning approaches. set representation learning approaches COMPARE method. pooling method COMPARE method. method COMPARE pooling method. point - cloud HYPONYM-OF set - structured data. set - structured data EVALUATE-FOR pooling method. OtherScientificTerm is probability distribution. ,"This paper proposes to learn representations from sets for point cloud processing, graph learning, image/video recognition, object detection, and graph learning. The authors propose a geometrically-interpretable and generic pooling mechanism to learn a fixed-dimensional representation. The proposed pooling method is based on an end-to-end trainable Euclidean embedding for the sliced-Wasserstein distance between the probability distribution and the set-structured data (e.g., point-cloud). Experiments show that the proposed method outperforms existing set representation learning approaches. ","This paper proposes to learn representations from sets for point cloud processing, graph learning, image/video recognition, object detection, and graph learning. The authors propose a geometrically-interpretable and generic pooling mechanism to learn a fixed-dimensional representation. The proposed pooling method is based on an end-to-end trainable Euclidean embedding for the sliced-Wasserstein distance between the probability distribution and the set-structured data (e.g., point-cloud). Experiments show that the proposed method outperforms existing set representation learning approaches. "
14492,SP:6cb2f0cbc076f8680cb00411790629f8e1478053,"training stability FEATURE-OF recurrent neural networks ( RNNs ). SBO - RNN HYPONYM-OF RNNs. stochastic bilevel optimization ( SBO ) USED-FOR RNNs. feedforward and backpropagation USED-FOR lower and upper - level optimization. stochastic gradient descent ( SGD ) USED-FOR SBO problem. stochastic gradient descent ( SGD ) USED-FOR RNN. RNN USED-FOR SBO problem. benchmark datasets EVALUATE-FOR approach. OtherScientificTerm are hidden states, hyperparameters, and vanishing or exploding gradient. Material is training data. ","This paper studies the problem of training stability of recurrent neural networks (RNNs) and proposes a new RNN called SBO-RNN, which is an extension of stochastic bilevel optimization (SBO) for RNNs. The main idea is to use feedforward and backpropagation for both the lower and upper-level optimization. The authors show that the SBO problem can be solved by stochedastic gradient descent (SGD) on an RNN, and that the hidden states of the RNN are invariant to the choice of hyperparameters. The paper also shows that the vanishing or exploding gradient can be avoided if the training data is sufficiently large. The approach is tested on several benchmark datasets and shows promising results. ","This paper studies the problem of training stability of recurrent neural networks (RNNs) and proposes a new RNN called SBO-RNN, which is an extension of stochastic bilevel optimization (SBO) for RNNs. The main idea is to use feedforward and backpropagation for both the lower and upper-level optimization. The authors show that the SBO problem can be solved by stochedastic gradient descent (SGD) on an RNN, and that the hidden states of the RNN are invariant to the choice of hyperparameters. The paper also shows that the vanishing or exploding gradient can be avoided if the training data is sufficiently large. The approach is tested on several benchmark datasets and shows promising results. "
14541,SP:d3a4300e21ca215334f256f0467a428470548fe4,"online problem USED-FOR minimizing power consumption. algorithm USED-FOR power - saving states. energy consumption CONJUNCTION wake - up costs. wake - up costs CONJUNCTION energy consumption. wake - up costs FEATURE-OF power - saving states. energy consumption FEATURE-OF power - saving states. predicted lengths of the idle periods USED-FOR learning - augmented online algorithm. worst - case guarantee EVALUATE-FOR algorithm. algorithm USED-FOR online ski rental problem. learning augmented setting USED-FOR online ski rental problem. OtherScientificTerm is prediction error. Generic are problem, and approach. ",This paper considers the online problem of minimizing power consumption in an online setting. The authors propose a learning-augmented online algorithm based on the predicted lengths of the idle periods. The algorithm is shown to be able to find power-saving states with low energy consumption and wake-up costs. The paper also provides a worst-case guarantee for the proposed algorithm for the online ski rental problem in the learning augmented setting. ,This paper considers the online problem of minimizing power consumption in an online setting. The authors propose a learning-augmented online algorithm based on the predicted lengths of the idle periods. The algorithm is shown to be able to find power-saving states with low energy consumption and wake-up costs. The paper also provides a worst-case guarantee for the proposed algorithm for the online ski rental problem in the learning augmented setting. 
14590,SP:22aba6284123af0ecd6605ee4e89b351bd7e10a3,sample sizes FEATURE-OF tasks. task similarities CONJUNCTION sample complexity. sample complexity CONJUNCTION task similarities. mathematical framework USED-FOR transferability. sample complexity EVALUATE-FOR learning models. transferability FEATURE-OF multi - source transfer learning problems. optimal combining coefficients USED-FOR transferability. models USED-FOR tasks. models USED-FOR task. model complexity CONJUNCTION similarities. similarities CONJUNCTION model complexity. sample sizes CONJUNCTION model complexity. model complexity CONJUNCTION sample sizes. analytical expression USED-FOR transferability measure. sample sizes FEATURE-OF analytical expression. model complexity FEATURE-OF analytical expression. sample sizes FEATURE-OF transferability measure. model complexity FEATURE-OF transferability measure. analyses USED-FOR practical learning tasks. parameterized model USED-FOR quantifiable transferability measure. deep neural networks USED-FOR multi - source transfer learning tasks. alternating iterative algorithm USED-FOR deep neural networks. approach COMPARE transfer learning algorithms. transfer learning algorithms COMPARE approach. image classification tasks EVALUATE-FOR approach. image classification tasks EVALUATE-FOR transfer learning algorithms. transfer learning algorithms USED-FOR multi - source and few - shot scenarios. multi - source and few - shot scenarios EVALUATE-FOR approach. Method is transfer learning algorithm designs. Task is knowledge transferring mechanism. ,"This paper proposes a mathematical framework to quantify the transferability of learning models in terms of sample sizes for different tasks, task similarities, and sample complexity. The authors consider transferability for multi-source transfer learning problems, where the goal is to learn models that can transfer knowledge across different tasks.   The authors propose to measure transferability by optimal combining coefficients, which is a generalization of previous transfer learning algorithm designs.  The main contribution of the paper is to provide an analytical expression for the trade-off between sample sizes, model complexity, and similarities between two tasks, and to provide a quantifiable transferability measure based on a parameterized model.  In addition, the authors propose an alternating iterative algorithm for training deep neural networks to solve multi- source transfer learning tasks, which can be used as a knowledge transferring mechanism.  Experiments on image classification tasks show that the proposed approach outperforms existing transfer learning algorithms in both multi-distribution and few-shot scenarios, and is competitive with state-of-the-art methods on a number of practical learning tasks using similar analyses.","This paper proposes a mathematical framework to quantify the transferability of learning models in terms of sample sizes for different tasks, task similarities, and sample complexity. The authors consider transferability for multi-source transfer learning problems, where the goal is to learn models that can transfer knowledge across different tasks.   The authors propose to measure transferability by optimal combining coefficients, which is a generalization of previous transfer learning algorithm designs.  The main contribution of the paper is to provide an analytical expression for the trade-off between sample sizes, model complexity, and similarities between two tasks, and to provide a quantifiable transferability measure based on a parameterized model.  In addition, the authors propose an alternating iterative algorithm for training deep neural networks to solve multi- source transfer learning tasks, which can be used as a knowledge transferring mechanism.  Experiments on image classification tasks show that the proposed approach outperforms existing transfer learning algorithms in both multi-distribution and few-shot scenarios, and is competitive with state-of-the-art methods on a number of practical learning tasks using similar analyses."
14639,SP:0fb8dcf15e0d43547d566fdba7bc70b3bb600005,"asymmetry FEATURE-OF search tasks. search image USED-FOR computational model. eccentricity - dependent visual recognition CONJUNCTION target - dependent top - down cues. target - dependent top - down cues CONJUNCTION eccentricity - dependent visual recognition. eccentricity - dependent visual recognition USED-FOR model. model COMPARE human behavior. human behavior COMPARE model. human behavior USED-FOR paradigmatic search tasks. asymmetry FEATURE-OF paradigmatic search tasks. paradigmatic search tasks EVALUATE-FOR model. model USED-FOR search asymmetry. ImageNet USED-FOR model. developmental diet USED-FOR model. classical perceptual properties FEATURE-OF neural network models. Task are Visual search, and visual search. OtherScientificTerm are eye movements, polarity of search asymmetry, and VisualSearchAsymmetry. Method is task - specific training. Material is natural images. ","This paper studies the problem of search asymmetry in the context of visual search. Visual search is an important problem in many real-world applications, and the asymmetry of search tasks is of great interest to the community. The authors propose a computational model that can be trained on a search image, and show that a model trained on ImageNet with eccentricity-dependent visual recognition and target-dependent top-down cues outperforms human behavior on a number of paradigmatic search tasks. The model is trained using ImageNet and a developmental diet, where the model is shown to be able to learn to find the optimal solution to a task-specific search problem.  The authors also show that neural network models with classical perceptual properties (e.g. eye movements) are more sensitive to the polarity of the input images, and that the model can learn to identify the optimal search solution for a particular search task from a single input image.   The paper is well-written, well-motivated, and well-structured. The idea of the paper is interesting, and it is interesting to see that the proposed method VisualSearchAsymmetry can be applied to a wide range of search problems. The paper also shows that the performance of the model on a model that is trained on natural images is comparable to human behavior.  However, there are some issues with the paper:   (1) The paper does not provide a thorough discussion of the role of polarity in visual search, and does not clearly state the contribution of the proposed model to the problem. (2) There is no discussion of how the model’s performance is related to the design of the search task, or the task specific training. ","This paper studies the problem of search asymmetry in the context of visual search. Visual search is an important problem in many real-world applications, and the asymmetry of search tasks is of great interest to the community. The authors propose a computational model that can be trained on a search image, and show that a model trained on ImageNet with eccentricity-dependent visual recognition and target-dependent top-down cues outperforms human behavior on a number of paradigmatic search tasks. The model is trained using ImageNet and a developmental diet, where the model is shown to be able to learn to find the optimal solution to a task-specific search problem.  The authors also show that neural network models with classical perceptual properties (e.g. eye movements) are more sensitive to the polarity of the input images, and that the model can learn to identify the optimal search solution for a particular search task from a single input image.   The paper is well-written, well-motivated, and well-structured. The idea of the paper is interesting, and it is interesting to see that the proposed method VisualSearchAsymmetry can be applied to a wide range of search problems. The paper also shows that the performance of the model on a model that is trained on natural images is comparable to human behavior.  However, there are some issues with the paper:   (1) The paper does not provide a thorough discussion of the role of polarity in visual search, and does not clearly state the contribution of the proposed model to the problem. (2) There is no discussion of how the model’s performance is related to the design of the search task, or the task specific training. "
14688,SP:f0cc968ea9da4884dcdaf6d0c75ea9f1511bdfc3,"adversarial examples USED-FOR certifiably robust models. tightness of the upper bound USED-FOR certifiably robust models. Interval Bound Propagation ( IBP ) training COMPARE models. models COMPARE Interval Bound Propagation ( IBP ) training. looser bounds USED-FOR Interval Bound Propagation ( IBP ) training. tighter bounds USED-FOR models. loss landscapes FEATURE-OF linear relaxation - based methods. tightness CONJUNCTION smoothness. smoothness CONJUNCTION tightness. tightness USED-FOR method. smoothness USED-FOR method. Method are Certifiable training, certifiable training, and certifiable training method. OtherScientificTerm are worst - case loss, and loss landscape. Generic is state - of - the - arts method. ","Certifiable training is a popular technique to train robust models against adversarial examples. In this paper, the authors study the tightness of the upper bound on the worst-case loss for certifiable training. They show that the best-known Interval Bound Propagation (IBP) training with looser bounds outperforms existing models with tighter bounds. They also show that linear relaxation-based methods have similar loss landscapes to those of IBP, which is a state-of-the-arts method. Finally, they propose a new method based on tightness and smoothness.   ","Certifiable training is a popular technique to train robust models against adversarial examples. In this paper, the authors study the tightness of the upper bound on the worst-case loss for certifiable training. They show that the best-known Interval Bound Propagation (IBP) training with looser bounds outperforms existing models with tighter bounds. They also show that linear relaxation-based methods have similar loss landscapes to those of IBP, which is a state-of-the-arts method. Finally, they propose a new method based on tightness and smoothness.   "
14737,SP:a158f8772a9dada059ffd1d6d7838ed40d8483da,"stochastic setting FEATURE-OF online linear regression. online ridge regression CONJUNCTION forward algorithm. forward algorithm CONJUNCTION online ridge regression. high probability regret bounds USED-FOR online ridge regression. high probability regret bounds USED-FOR forward algorithm. robustness FEATURE-OF regularization parameter. ridge USED-FOR forward algorithm. it PART-OF algorithms. linear function approximation PART-OF algorithms. it USED-FOR regret bounds. modification USED-FOR linear bandit settings. Method is online regression algorithms. OtherScientificTerm are bounded observations, boundedness assumption, and theoretical bounds. ","This paper studies the problem of online linear regression in the stochastic setting with bounded observations. In this setting, online regression algorithms have been shown to achieve high probability regret bounds for both the online ridge regression and the forward algorithm. The authors consider the case where the robustness of the regularization parameter is assumed to be bounded. They show that under this boundedness assumption, the ridge can be approximated by a linear function approximation, and they show that a forward algorithm with ridge is guaranteed to converge to the optimal solution with high probability. They also show that this modification holds for linear bandit settings. Finally, they extend their theoretical bounds to the case when the regularizer is not bounded. ","This paper studies the problem of online linear regression in the stochastic setting with bounded observations. In this setting, online regression algorithms have been shown to achieve high probability regret bounds for both the online ridge regression and the forward algorithm. The authors consider the case where the robustness of the regularization parameter is assumed to be bounded. They show that under this boundedness assumption, the ridge can be approximated by a linear function approximation, and they show that a forward algorithm with ridge is guaranteed to converge to the optimal solution with high probability. They also show that this modification holds for linear bandit settings. Finally, they extend their theoretical bounds to the case when the regularizer is not bounded. "
14786,SP:17ff9a2133aebf2d1b1787e8efc49d709389c0e7,"generative adversarial network CONJUNCTION adversarial training. adversarial training CONJUNCTION generative adversarial network. method USED-FOR setting. nonconvex - nonconcave setting FEATURE-OF minimax problems. adversarial training HYPONYM-OF minimax problems. generative adversarial network HYPONYM-OF minimax problems. two - time - scale variant PART-OF EG. slowO(1 / k ) rate FEATURE-OF squared gradient norm. smooth structured nonconvexnonconcave setting FEATURE-OF two - time - scale variant. EG+ HYPONYM-OF EG. EG+ HYPONYM-OF two - time - scale variant. slowO(1 / k ) rate EVALUATE-FOR two - time - scale variant. O(1 / k ) rate FEATURE-OF squared gradient norm. anchoring technique USED-FOR EG. EG+ CONJUNCTION EAG. EAG CONJUNCTION EG+. fast O(1 / k ) rate FEATURE-OF squared gradient norm. squared gradient norm FEATURE-OF smooth structured nonconvex - nonconcave problems. EG+ USED-FOR two - time - scale EG. negative comonotonicity condition FEATURE-OF saddle - gradient operator. fast extragradient ( FEG ) HYPONYM-OF two - time - scale EG. FEG - A HYPONYM-OF backtracking line - search version. Method are extragradient ( EG ) method, extra anchored gradient ( EAG ), and FEG. OtherScientificTerm are smooth convex - concave setting, and problem parameters. ","This paper proposes an extension of the extragradient (EG) method to the nonconvex-nonconcave setting. The proposed method can be applied to this setting to solve minimax problems, such as generative adversarial network and adversarial training. The authors propose a two-time-scale variant of EG, called EG+, which is an O(1/k) rate for the squared gradient norm in the smooth structured nonconvolutional setting. They also propose an extra anchored gradient (EAG) method, which uses the anchoring technique in EG to anchor the problem parameters to the ground truth. They show that EG+ and EAG converge to a fast O(O(1 / k) rate on the squared gradients norm in a smooth convex-concentrated setting. Finally, they show that the two-scale EG, EG++ and FEG, can be used to solve two-times-scale versions of EG. In particular, FEG-A is a backtracking line-search version where the saddle-gradient operator has a negative comonotonicity condition, and the authors show that FEG+ and EG+ converges to a good solution at a faster rate than FEG.","This paper proposes an extension of the extragradient (EG) method to the nonconvex-nonconcave setting. The proposed method can be applied to this setting to solve minimax problems, such as generative adversarial network and adversarial training. The authors propose a two-time-scale variant of EG, called EG+, which is an O(1/k) rate for the squared gradient norm in the smooth structured nonconvolutional setting. They also propose an extra anchored gradient (EAG) method, which uses the anchoring technique in EG to anchor the problem parameters to the ground truth. They show that EG+ and EAG converge to a fast O(O(1 / k) rate on the squared gradients norm in a smooth convex-concentrated setting. Finally, they show that the two-scale EG, EG++ and FEG, can be used to solve two-times-scale versions of EG. In particular, FEG-A is a backtracking line-search version where the saddle-gradient operator has a negative comonotonicity condition, and the authors show that FEG+ and EG+ converges to a good solution at a faster rate than FEG."
14835,SP:4e38973033de24fc183c6112e1146f8eef0ddaea,"statistical data USED-FOR uniformity testing. rankings FEATURE-OF statistical data. uniform distribution COMPARE Mallows model. Mallows model COMPARE uniform distribution. pairwise statistics USED-FOR uniform distribution. pairwise statistics USED-FOR Mallows model. uniformity testing algorithm USED-FOR local DP scenario. ranking data USED-FOR binary statistics. binary statistics USED-FOR it. OtherScientificTerm are alternative class, large domain, uniformity, ✏ 0, and privacy budget parameter. Method are Mallows models, central DP algorithm, and uniformity testing algorithms. Task is Testing ranking data. ","This paper studies the problem of uniformity testing on ranking data in statistical data such as rankings. The authors consider the setting where the alternative class is a large domain and the uniform distribution is a uniform distribution over pairwise statistics, and the Mallows model is trained on pairwise measurements of the ranking data. They show that the uniformity of the pairwise distribution of the two Mallows models is a function of the number of pairs of pairs in the dataset and the privacy budget parameter. They then propose a central DP algorithm for testing uniformity and show that it can be trained on binary statistics from ranking data, and they also provide a uniformity test algorithm for the local DP scenario.  Testing ranking data is a challenging problem, and this paper is an attempt to address this problem in a practical way. The main contribution of the paper is to provide a theoretical analysis of the privacy cost of uniforming algorithms. ","This paper studies the problem of uniformity testing on ranking data in statistical data such as rankings. The authors consider the setting where the alternative class is a large domain and the uniform distribution is a uniform distribution over pairwise statistics, and the Mallows model is trained on pairwise measurements of the ranking data. They show that the uniformity of the pairwise distribution of the two Mallows models is a function of the number of pairs of pairs in the dataset and the privacy budget parameter. They then propose a central DP algorithm for testing uniformity and show that it can be trained on binary statistics from ranking data, and they also provide a uniformity test algorithm for the local DP scenario.  Testing ranking data is a challenging problem, and this paper is an attempt to address this problem in a practical way. The main contribution of the paper is to provide a theoretical analysis of the privacy cost of uniforming algorithms. "
14884,SP:99a835191a3ba8372e391b6d3316e9b68e543295,Greedy algorithms USED-FOR learning graphical models. worst - case exponential runtime EVALUATE-FOR greedy algorithms. greedy algorithms USED-FOR learning directed acyclic graphs. greedy scorebased algorithm USED-FOR learning DAGs. edge - greedy algorithms COMPARE approach. approach COMPARE edge - greedy algorithms. vertex - greedy USED-FOR approach. GES and hill - climbing algorithms HYPONYM-OF edge - greedy algorithms. score evaluations USED-FOR approach. polynomial - time algorithms USED-FOR learning DAG models. polynomial - time algorithms PART-OF algorithm. score - based algorithms USED-FOR order - based algorithms. Bregman divergences CONJUNCTION exponential families. exponential families CONJUNCTION Bregman divergences. score functions CONJUNCTION optimality conditions. optimality conditions CONJUNCTION score functions. algorithm USED-FOR score. Task is learning statistical models with sparse structure. Generic is they. OtherScientificTerm is DAGs. Method is DAG models. Metric is sample and computational complexity bounds. ,"Greedy algorithms for learning graphical models with sparse structure have been shown to have the worst-case exponential runtime. This paper studies the problem of learning directed acyclic graphs (DAGs). The authors propose a greedy scorebased algorithm for learning DAGs. Compared to edge-greedy algorithms (e.g., GES and hill-climbing algorithms), the proposed approach is based on score evaluations. The algorithm consists of two polynomial-time algorithms, which are used in the previous work, to compute a score for each edge in the DAG. The score functions and optimality conditions are derived, and the authors prove sample and computational complexity bounds for the algorithm. The authors also show that the score-based algorithms can be used to improve the sample and complexity of order based algorithms.    The authors consider Bregman divergences and exponential families. ","Greedy algorithms for learning graphical models with sparse structure have been shown to have the worst-case exponential runtime. This paper studies the problem of learning directed acyclic graphs (DAGs). The authors propose a greedy scorebased algorithm for learning DAGs. Compared to edge-greedy algorithms (e.g., GES and hill-climbing algorithms), the proposed approach is based on score evaluations. The algorithm consists of two polynomial-time algorithms, which are used in the previous work, to compute a score for each edge in the DAG. The score functions and optimality conditions are derived, and the authors prove sample and computational complexity bounds for the algorithm. The authors also show that the score-based algorithms can be used to improve the sample and complexity of order based algorithms.    The authors consider Bregman divergences and exponential families. "
14933,SP:b60989706296b963b6671c01f22384978a334be1,"accuracy CONJUNCTION adversarial robustness. adversarial robustness CONJUNCTION accuracy. adversarial robustness EVALUATE-FOR CNNs. adversarial training USED-FOR CNNs. adversarial training USED-FOR adversarial robustness. adversarial robustness FEATURE-OF backbone CNNs. dilation architecture USED-FOR backbone CNN. accuracy CONJUNCTION adversarial robustness. adversarial robustness CONJUNCTION accuracy. real - world datasets CONJUNCTION benchmark neural networks. benchmark neural networks CONJUNCTION real - world datasets. real - world datasets EVALUATE-FOR algorithm. benchmark neural networks EVALUATE-FOR algorithm. adversarial robustness EVALUATE-FOR algorithm. accuracy EVALUATE-FOR algorithm. Method are convolutional neural networks ( CNNs ), and neural architecture dilation algorithm. Generic is they. OtherScientificTerm are adversarial attacks, and minimal computational overhead. ",This paper proposes a neural architecture dilation algorithm to improve the robustness of convolutional neural networks (CNNs) against adversarial attacks. The authors argue that adversarial training improves the accuracy and adversarial robustness for CNNs when they are trained with a dilation architecture. The proposed algorithm is evaluated on several real-world datasets and benchmark neural networks and shows that the proposed algorithm can improve accuracy and robustness with minimal computational overhead. ,This paper proposes a neural architecture dilation algorithm to improve the robustness of convolutional neural networks (CNNs) against adversarial attacks. The authors argue that adversarial training improves the accuracy and adversarial robustness for CNNs when they are trained with a dilation architecture. The proposed algorithm is evaluated on several real-world datasets and benchmark neural networks and shows that the proposed algorithm can improve accuracy and robustness with minimal computational overhead. 
14982,SP:77ed765e911a4e5f2bfba13cbd2403500a5d05e6,"linear function approximation USED-FOR episodic Markov decision processes ( MDPs ). model - based reward - free reinforcement learning USED-FOR episodic Markov decision processes ( MDPs ). linear function approximation USED-FOR model - based reward - free reinforcement learning. transition probability kernel PART-OF MDP. feature mappings FEATURE-OF linear function. Linear Mixture MDP assumption USED-FOR algorithm. linear function USED-FOR transition probability kernel. reward function USED-FOR ε - optimal policy. UCRL - RFE USED-FOR ε - optimal policy. Bernstein - type bonus USED-FOR UCRL - RFE. upper bound COMPARE lower bound. lower bound COMPARE upper bound. Task are planning phase, and exploration phase. Generic is policy. OtherScientificTerm is feature mapping. Method are linear Mixture MDPs, and reward - free algorithm. ","This paper studies the problem of model-based reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs). The authors propose an algorithm based on the Linear Mixture MDP assumption, where the transition probability kernel of the MDP is approximated by a linear function with feature mappings, and the planning phase is followed by an exploration phase, where a policy is learned that maximizes the probability of the next state given the current state and the current feature mapping. The authors show that under the linear Mixtures MDPs, UCRL-RFE is able to learn a ε-optimal policy with respect to the reward function. The main contribution of the paper is the introduction of a Bernstein-type bonus, which is used to improve the upper bound of the lower bound. The paper also shows that the reward of the reward-based algorithm is a function of the number of features and the number the feature mapping is.  ","This paper studies the problem of model-based reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs). The authors propose an algorithm based on the Linear Mixture MDP assumption, where the transition probability kernel of the MDP is approximated by a linear function with feature mappings, and the planning phase is followed by an exploration phase, where a policy is learned that maximizes the probability of the next state given the current state and the current feature mapping. The authors show that under the linear Mixtures MDPs, UCRL-RFE is able to learn a ε-optimal policy with respect to the reward function. The main contribution of the paper is the introduction of a Bernstein-type bonus, which is used to improve the upper bound of the lower bound. The paper also shows that the reward of the reward-based algorithm is a function of the number of features and the number the feature mapping is.  "
15031,SP:28563ba0975f56ddb662cd46e85de78bb6024d36,seasonal patterns FEATURE-OF data stream of events. Shifting Seasonal Matrix Factorization approach USED-FOR seasonal patterns. SSMF HYPONYM-OF Shifting Seasonal Matrix Factorization approach. it USED-FOR regime shifts. regime shifts PART-OF seasonal patterns. regime shifts USED-FOR it. lossless data compression scheme USED-FOR it. lossless data compression scheme USED-FOR method. algorithm COMPARE baseline methods. baseline methods COMPARE algorithm. real - world data streams EVALUATE-FOR baseline methods. real - world data streams EVALUATE-FOR algorithm. OtherScientificTerm is human intervention. ,"This paper proposes a Shifting Seasonal Matrix Factorization approach, called SSMF, to learn seasonal patterns in a data stream of events. The method is based on a lossless data compression scheme, and it can be applied to regime shifts in seasonal patterns. Experiments on real-world data streams show that the proposed algorithm outperforms several baseline methods without human intervention. ","This paper proposes a Shifting Seasonal Matrix Factorization approach, called SSMF, to learn seasonal patterns in a data stream of events. The method is based on a lossless data compression scheme, and it can be applied to regime shifts in seasonal patterns. Experiments on real-world data streams show that the proposed algorithm outperforms several baseline methods without human intervention. "
15080,SP:e4bb07033001be4d04695ef058f426d49fe440be,"Assignment HYPONYM-OF informatics. exact solvers USED-FOR assignment problems. objective functions CONJUNCTION prior assumptions. prior assumptions CONJUNCTION objective functions. algorithms USED-FOR real problems. WeaveNet HYPONYM-OF neural network architecture. feature weaving layer PART-OF core module. strongly NP - hard settings USED-FOR stable matching. stable matching HYPONYM-OF non - linear assignment problems. OtherScientificTerm is NP - hardness or incomplete input. Method are approximation algorithms, learning - based 7 method, learning - based baselines, and algorithmic method. Task are real - world assignment problems, and combinatorial problem of assignment. Generic is model. ","Assignment is an important problem in informatics, and exact solvers for assignment problems with NP-hardness or incomplete input are of great interest. However, there are many existing approximation algorithms for this problem, which are computationally expensive. In this paper, the authors consider the combinatorial problem of assignment, and propose a learning-based 7 method that is computationally efficient. The authors show that their algorithms can be used to solve real problems in which the objective functions and prior assumptions are differentiable. They also propose a neural network architecture called WeaveNet, which is a core module that consists of a feature weaving layer and a learning module. They show that this model can solve non-linear assignment problems (e.g., stable matching in the strongly NP -hard settings) and generalize well to real-world assignment problems. The paper also shows that their algorithmic method can be applied to a variety of problems where there is a large gap between the true and approximation algorithms.    The authors also show that the learning module of their model can be seen as a special case of the learning-by-7 method. ","Assignment is an important problem in informatics, and exact solvers for assignment problems with NP-hardness or incomplete input are of great interest. However, there are many existing approximation algorithms for this problem, which are computationally expensive. In this paper, the authors consider the combinatorial problem of assignment, and propose a learning-based 7 method that is computationally efficient. The authors show that their algorithms can be used to solve real problems in which the objective functions and prior assumptions are differentiable. They also propose a neural network architecture called WeaveNet, which is a core module that consists of a feature weaving layer and a learning module. They show that this model can solve non-linear assignment problems (e.g., stable matching in the strongly NP -hard settings) and generalize well to real-world assignment problems. The paper also shows that their algorithmic method can be applied to a variety of problems where there is a large gap between the true and approximation algorithms.    The authors also show that the learning module of their model can be seen as a special case of the learning-by-7 method. "
15129,SP:8a559e21d45661eef427b310e5fe8488d5749137,3D point cloud data USED-FOR safety - critical applications. autonomous driving HYPONYM-OF safety - critical applications. robustness EVALUATE-FOR 3D deep learning models. adversarial attacks FEATURE-OF 3D deep learning models. threat models USED-FOR 3D point clouds. self - supervised learning proxy tasks USED-FOR threat models. adversarial training USED-FOR self - supervised learning proxy tasks. adversarial training USED-FOR threat models. MLP - based ( PointNet ) CONJUNCTION convolution - based ( DGCNN ). convolution - based ( DGCNN ) CONJUNCTION MLP - based ( PointNet ). convolution - based ( DGCNN ) CONJUNCTION transformer - based ( PCT ) 3D architectures. transformer - based ( PCT ) 3D architectures CONJUNCTION convolution - based ( DGCNN ). self - supervision USED-FOR 3D point cloud recognition. self - supervision COMPARE adversarial training baseline. adversarial training baseline COMPARE self - supervision. robustness EVALUATE-FOR 3D point cloud recognition. robustness EVALUATE-FOR self - supervision. it USED-FOR adversarial propagation. local feature learning USED-FOR adversarial robustness. local feature learning USED-FOR point clouds. adversarial robustness FEATURE-OF point clouds. DGCNN CONJUNCTION jigsaw proxy task. jigsaw proxy task CONJUNCTION DGCNN. jigsaw proxy task USED-FOR 3D adversarial robustness. 3D adversarial robustness EVALUATE-FOR DGCNN. OtherScientificTerm is point - level input perturbations. ,"This paper investigates the robustness of 3D deep learning models under adversarial attacks on point cloud data for safety-critical applications (e.g. autonomous driving). The authors show that existing threat models for 3D point clouds are robust to point-level input perturbations, and that self-supervised learning proxy tasks are also robust to adversarial training on self-surrogate learning of such threat models. The authors also show that self supervised learning of point cloud recognition with self-submission improves robustness against 3D adversarial robustness for MLP-based (PointNet), convolution-based, DGCNN, and transformer-based(PCT) 3D architectures. Finally, the authors also demonstrate that the adversarial perturbation of point clouds generated by local feature learning improves 3D robustness to point clouds produced by point clouds trained with self supervision.    The paper also shows that the robust performance of self-distributional self supervision improves when the number of points in the point cloud is small, and when it is used for adversarial propagation.  The authors further show that the performance improvement is due to a combination of self supervision and adversarial learning, which improves the performance of the self- supervision over the standard adversarial testing baseline. The paper further shows that 3D-adversarially robustness is improved when the point clouds in a point cloud are trained with DGCnn or a jigsaw proxy task. ","This paper investigates the robustness of 3D deep learning models under adversarial attacks on point cloud data for safety-critical applications (e.g. autonomous driving). The authors show that existing threat models for 3D point clouds are robust to point-level input perturbations, and that self-supervised learning proxy tasks are also robust to adversarial training on self-surrogate learning of such threat models. The authors also show that self supervised learning of point cloud recognition with self-submission improves robustness against 3D adversarial robustness for MLP-based (PointNet), convolution-based, DGCNN, and transformer-based(PCT) 3D architectures. Finally, the authors also demonstrate that the adversarial perturbation of point clouds generated by local feature learning improves 3D robustness to point clouds produced by point clouds trained with self supervision.    The paper also shows that the robust performance of self-distributional self supervision improves when the number of points in the point cloud is small, and when it is used for adversarial propagation.  The authors further show that the performance improvement is due to a combination of self supervision and adversarial learning, which improves the performance of the self- supervision over the standard adversarial testing baseline. The paper further shows that 3D-adversarially robustness is improved when the point clouds in a point cloud are trained with DGCnn or a jigsaw proxy task. "
15178,SP:657c5a1114c0d054b9e767d85990bbbb0492912d,"FISTA CONJUNCTION mirror descent. mirror descent CONJUNCTION FISTA. projected Newton ’s method CONJUNCTION FISTA. FISTA CONJUNCTION projected Newton ’s method. near - optimal regret bounds CONJUNCTION convergence rates. convergence rates CONJUNCTION near - optimal regret bounds. projected Newton ’s method CONJUNCTION mirror descent. mirror descent CONJUNCTION projected Newton ’s method. O(T ) regret EVALUATE-FOR online mirror descent. near - optimal regret bounds FEATURE-OF Optimization algorithms. mirror descent HYPONYM-OF Optimization algorithms. projected Newton ’s method HYPONYM-OF Optimization algorithms. FISTA HYPONYM-OF Optimization algorithms. conditional gradient variants USED-FOR linear optimization. O(T ) regret HYPONYM-OF suboptimal rates. toolkit USED-FOR projections. discrete and continuous perspectives USED-FOR toolkit. discrete and continuous perspectives USED-FOR projections. early termination USED-FOR away - step Frank - Wolfe algorithm. runtime EVALUATE-FOR Bregman projections. OtherScientificTerm are computational bottleneck, iterative projections, and cardinality based submodular polytopes. Metric is runtime v / s convergence rates. ","This paper studies the problem of online mirror descent in the setting where the computational bottleneck is high. Optimization algorithms such as projected Newton’s method, FISTA and mirror descent have been shown to have near-optimal regret bounds and convergence rates. This paper extends the results to linear optimization with conditional gradient variants. The main contribution of this paper is a toolkit for learning such projections from discrete and continuous perspectives. The toolkit is based on iterative projections from cardinality based submodular polytopes. The authors show that the runtime v/s convergence rates of these algorithms are suboptimal (in terms of O(T) regret), but that the O(S) regret of online Mirror descent can be improved to O(O(1/\sqrt{T}) with early termination. They also provide an away-step Frank-Wolfe algorithm with an early termination, and show that Bregman projections have a similar runtime. ","This paper studies the problem of online mirror descent in the setting where the computational bottleneck is high. Optimization algorithms such as projected Newton’s method, FISTA and mirror descent have been shown to have near-optimal regret bounds and convergence rates. This paper extends the results to linear optimization with conditional gradient variants. The main contribution of this paper is a toolkit for learning such projections from discrete and continuous perspectives. The toolkit is based on iterative projections from cardinality based submodular polytopes. The authors show that the runtime v/s convergence rates of these algorithms are suboptimal (in terms of O(T) regret), but that the O(S) regret of online Mirror descent can be improved to O(O(1/\sqrt{T}) with early termination. They also provide an away-step Frank-Wolfe algorithm with an early termination, and show that Bregman projections have a similar runtime. "
15227,SP:8dae43d6b5cebb7ef6c39437d997b390c2380536,"natural parameters PART-OF k - parameter minimal exponential family. i.i.d. samples USED-FOR natural parameters. maximum likelihood estimator USED-FOR exponential family. finite sample guarantees USED-FOR parameter estimation. maximum likelihood estimation USED-FOR re - parameterized distribution. exponential family FEATURE-OF re - parameterized distribution. maximum likelihood estimation USED-FOR method. re - parameterized distribution USED-FOR method. Generic are it, and estimator. Metric are sample complexity, and computational complexity. ","This paper considers the problem of estimating the natural parameters of the k-parameter minimal exponential family with i.i.d. samples. The natural parameters are assumed to be of the form $p(x,y) = \mathcal{O}(\log p(x|y)$, where $p$ is the number of samples and $y$ is a function of $p$. The paper proposes a method to estimate the natural parameter of the exponential family by using a maximum likelihood estimator. The main contribution of the paper is that the proposed method is based on the maximum likelihood estimation of a re-parametrized distribution of an exponential family, with finite sample guarantees for parameter estimation. The sample complexity of the method is shown to be polynomial in $O(\log n)$, and it is shown that the estimator is polynomially efficient. The paper also shows that the computational complexity is bounded by $O(n^2)$. ","This paper considers the problem of estimating the natural parameters of the k-parameter minimal exponential family with i.i.d. samples. The natural parameters are assumed to be of the form $p(x,y) = \mathcal{O}(\log p(x|y)$, where $p$ is the number of samples and $y$ is a function of $p$. The paper proposes a method to estimate the natural parameter of the exponential family by using a maximum likelihood estimator. The main contribution of the paper is that the proposed method is based on the maximum likelihood estimation of a re-parametrized distribution of an exponential family, with finite sample guarantees for parameter estimation. The sample complexity of the method is shown to be polynomial in $O(\log n)$, and it is shown that the estimator is polynomially efficient. The paper also shows that the computational complexity is bounded by $O(n^2)$. "
15276,SP:4f9ddb697e86356fb293ef34a69ca3702c4e8164,"differentiable renderers USED-FOR predicting intrinsic object properties. learning - based approaches USED-FOR inverse graphics. rasterization - based renderers USED-FOR learning - based approaches. rasterization CONJUNCTION ray - tracing. ray - tracing CONJUNCTION rasterization. DIBR++ HYPONYM-OF hybrid differentiable renderer. speed CONJUNCTION realism. realism CONJUNCTION speed. hybrid differentiable renderer USED-FOR photorealistic effects. ray - tracing PART-OF hybrid differentiable renderer. direct estimation CONJUNCTION spherical basis functions. spherical basis functions CONJUNCTION direct estimation. renderer USED-FOR light transport. environmental lighting and spatially - varying material models PART-OF renderer. direct estimation USED-FOR light transport. spherical basis functions USED-FOR light transport. learning frameworks USED-FOR geometry, reflectance and lighting prediction. physics - based differentiable renderers COMPARE DIB - R++. DIB - R++ COMPARE physics - based differentiable renderers. compact and expressive shading model USED-FOR DIB - R++. path tracing USED-FOR physics - based differentiable renderers. approach COMPARE rasterization - based approaches. rasterization - based approaches COMPARE approach. material editing CONJUNCTION relighting. relighting CONJUNCTION material editing. approach USED-FOR artistic applications. material and lighting disentanglement FEATURE-OF synthetic and real data. rasterization - based approaches USED-FOR artistic applications. material and lighting disentanglement EVALUATE-FOR rasterization - based approaches. relighting HYPONYM-OF artistic applications. material editing HYPONYM-OF artistic applications. material and lighting disentanglement EVALUATE-FOR approach. synthetic and real data EVALUATE-FOR approach. Method is naive lighting and material models. OtherScientificTerm are non - Lambertian, specular reflections, and ground - truth. ","This paper proposes a new differentiable renderers for predicting intrinsic object properties, which is a natural extension of previous learning-based approaches for inverse graphics. The authors propose DIBR++, a hybrid differentiable rendering model that combines rasterization and ray-tracing to achieve photorealistic effects. The renderer is composed of environmental lighting and spatially-varying material models, where naive lighting and material models are replaced by non-Lambertian, specular reflections.  The authors show that the renderer can be used to model light transport by direct estimation and spherical basis functions, and that the learning frameworks for geometry, reflectance and lighting prediction can be applied to learn geometry and reflectance directly. They show that DIB-R++ is able to learn a compact and expressive shading model, which allows for speed and realism. They also show that physics-based differentiable rendererers that use path tracing are able to achieve better results than DIB -R++.  Finally, the authors demonstrate that the proposed approach outperforms previous state-of-the-art rasterisation and ray tracing based approaches in terms of material and lighting disentanglement in both synthetic and real data, and in artistic applications such as material editing, relighting and relighting.","This paper proposes a new differentiable renderers for predicting intrinsic object properties, which is a natural extension of previous learning-based approaches for inverse graphics. The authors propose DIBR++, a hybrid differentiable rendering model that combines rasterization and ray-tracing to achieve photorealistic effects. The renderer is composed of environmental lighting and spatially-varying material models, where naive lighting and material models are replaced by non-Lambertian, specular reflections.  The authors show that the renderer can be used to model light transport by direct estimation and spherical basis functions, and that the learning frameworks for geometry, reflectance and lighting prediction can be applied to learn geometry and reflectance directly. They show that DIB-R++ is able to learn a compact and expressive shading model, which allows for speed and realism. They also show that physics-based differentiable rendererers that use path tracing are able to achieve better results than DIB -R++.  Finally, the authors demonstrate that the proposed approach outperforms previous state-of-the-art rasterisation and ray tracing based approaches in terms of material and lighting disentanglement in both synthetic and real data, and in artistic applications such as material editing, relighting and relighting."
15325,SP:6ac1c8556e7131939cc582f513bc9921470e1b09,"Soft - argmax operation USED-FOR detection - based methods. soft - argmax USED-FOR neural network. sampling - argmax HYPONYM-OF differentiable training method. continuous formulation USED-FOR output distribution. continuous formulation USED-FOR differentiable sampling process. continuous formulation USED-FOR expectation. sampling - argmax USED-FOR localization tasks. soft - argmax operation USED-FOR localization tasks. sampling - argmax COMPARE soft - argmax operation. soft - argmax operation COMPARE sampling - argmax. OtherScientificTerm are differentiable manner, probability map, pixel - wise supervision, map, implicit constraints, and expectation of the localization error. Generic are model, and method. Metric is average error. ","This paper proposes a new differentiable training method called sampling-argmax, which is a variant of the popular soft-arg max operation in detection-based methods. In a differentiable manner, a probability map is learned for each pixel in the input image, and pixel-wise supervision is applied to this map. This map is then used to learn implicit constraints on the output of the model. The main idea is to learn the expectation of the localization error using a continuous formulation of the output distribution, and then use this expectation to guide the differentiable sampling process. Experiments show that the proposed method is able to reduce the average error of the neural network trained with soft -argmax to a neural network that is trained with no implicit constraints. In addition, the authors show that, in localization tasks, the proposed sampling-armmax outperforms the standard soft- argmax operation. ","This paper proposes a new differentiable training method called sampling-argmax, which is a variant of the popular soft-arg max operation in detection-based methods. In a differentiable manner, a probability map is learned for each pixel in the input image, and pixel-wise supervision is applied to this map. This map is then used to learn implicit constraints on the output of the model. The main idea is to learn the expectation of the localization error using a continuous formulation of the output distribution, and then use this expectation to guide the differentiable sampling process. Experiments show that the proposed method is able to reduce the average error of the neural network trained with soft -argmax to a neural network that is trained with no implicit constraints. In addition, the authors show that, in localization tasks, the proposed sampling-armmax outperforms the standard soft- argmax operation. "
15374,SP:478c05c90090f9d80b72ac352c488073b45a5d8b,"Graph Contrastive Learning ( GCL ) USED-FOR generalizable representations. contrastive views USED-FOR generalizable representations. data augmentation USED-FOR contrastive views. incomplete structure information USED-FOR models learning. directional structure FEATURE-OF directed graphs. hand - picking parameters FEATURE-OF predefined contrastive views. data augmentation USED-FOR contrastive information. directional structure HYPONYM-OF intrinsic graph structural information. data augmentation USED-FOR graph structure. predefined contrastive views USED-FOR GCL. hand - picking parameters USED-FOR GCL. it USED-FOR contrastive information. Laplacian perturbation HYPONYM-OF directed graph data augmentation method. contrastive views USED-FOR directed graph contrastive learning framework. Laplacian perturbation USED-FOR contrastive views. multi - task curriculum learning USED-FOR it. model COMPARE GCL models. GCL models COMPARE model. structural features of directed graphs EVALUATE-FOR model. benchmarks EVALUATE-FOR state - of - the - art approaches. Method is message passing scheme. OtherScientificTerm are graph changing action, directed graph structure, and easy - to - difficult contrastive views. ","Graph Contrastive Learning (GCL) aims to learn generalizable representations from contrastive views using data augmentation. In contrast to the message passing scheme, GCL uses a graph changing action, i.e., a directed graph changing the graph structure of the input graph. The authors argue that existing models learning from incomplete structure information can be problematic due to the lack of intrinsic graph structural information, e.g., the directional structure of directed graphs. To address this issue, they propose GCL with hand-picking parameters for the predefined contrastive view in GCL, which is based on the observation that contrastive information is often lost due to poor performance of contrastive representation learning from data augmentations. The proposed directed graph contrastive learning framework (DGCL) leverages the use of the contrastive features of the directed graph structure to improve the performance of GCL. Specifically, the authors propose to use Laplacian perturbation (a directed graph data augmentation method), which is an extension of the work of (Zhang et al., 2017) to the context of directed graph representation learning, and it augments contrastive images from the directed graphs with different hand-picked parameters. They also propose a multi-task curriculum learning approach to train the proposed model, and show that it is able to learn contrastive representations from easy-to-difficult contrastive perspectives. They show that their model outperforms previous state-of-the-art GCL models in terms of performance on a number of benchmarks on three benchmarks, and that the model can learn to learn the structural features of a graph in a better than previous state of the art. ","Graph Contrastive Learning (GCL) aims to learn generalizable representations from contrastive views using data augmentation. In contrast to the message passing scheme, GCL uses a graph changing action, i.e., a directed graph changing the graph structure of the input graph. The authors argue that existing models learning from incomplete structure information can be problematic due to the lack of intrinsic graph structural information, e.g., the directional structure of directed graphs. To address this issue, they propose GCL with hand-picking parameters for the predefined contrastive view in GCL, which is based on the observation that contrastive information is often lost due to poor performance of contrastive representation learning from data augmentations. The proposed directed graph contrastive learning framework (DGCL) leverages the use of the contrastive features of the directed graph structure to improve the performance of GCL. Specifically, the authors propose to use Laplacian perturbation (a directed graph data augmentation method), which is an extension of the work of (Zhang et al., 2017) to the context of directed graph representation learning, and it augments contrastive images from the directed graphs with different hand-picked parameters. They also propose a multi-task curriculum learning approach to train the proposed model, and show that it is able to learn contrastive representations from easy-to-difficult contrastive perspectives. They show that their model outperforms previous state-of-the-art GCL models in terms of performance on a number of benchmarks on three benchmarks, and that the model can learn to learn the structural features of a graph in a better than previous state of the art. "
15423,SP:85b383d2f722f7bff438840e423f5cb4c67d5980,common interface FEATURE-OF grounded language learning environments. RTFM CONJUNCTION Messenger. Messenger CONJUNCTION RTFM. grid - world environments CONJUNCTION symbolic counterparts of visual worlds. symbolic counterparts of visual worlds CONJUNCTION grid - world environments. interpreting rich natural language USED-FOR complex scenes. interpreting rich natural language USED-FOR symbolic counterparts of visual worlds. RTFM HYPONYM-OF grid - world environments. ALFWorld HYPONYM-OF complex scenes. Messenger HYPONYM-OF grid - world environments. grid - world environments PART-OF SILG. symbolic counterparts of visual worlds PART-OF SILG. action space CONJUNCTION language specification. language specification CONJUNCTION action space. richness of observation space CONJUNCTION action space. action space CONJUNCTION richness of observation space. language specification CONJUNCTION plan complexity. plan complexity CONJUNCTION language specification. shared model architecture USED-FOR RL. recurrent state - tracking CONJUNCTION entity - centric attention. entity - centric attention CONJUNCTION recurrent state - tracking. egocentric local convolution CONJUNCTION recurrent state - tracking. recurrent state - tracking CONJUNCTION egocentric local convolution. entity - centric attention CONJUNCTION pretrained LM. pretrained LM CONJUNCTION entity - centric attention. shared model architecture USED-FOR environments. SILG USED-FOR pretrained LM. shared architecture COMPARE environment - specific architectures. environment - specific architectures COMPARE shared architecture. SILG EVALUATE-FOR models. SILG USED-FOR language grounding. Method is unified models. OtherScientificTerm is entities. Material is multi - environment benchmark. ,"This paper proposes SILG, a set of grounded language learning environments with a common interface. The SILG consists of three grid-world environments (RTFM, Messenger, and ALFWorld) and three symbolic counterparts of visual worlds, where the goal is to learn language by interpreting rich natural language for complex scenes. The authors propose to use a shared model architecture for RL across all environments, which is based on egocentric local convolution, recurrent state-tracking, entity-centric attention, and a pretrained LM.   The authors show that the shared architecture is more robust to the richness of observation space, richness of action space, language specification, and plan complexity, and that the unified models are able to generalize to unseen entities. They also show that SILG is able to learn a shared architecture for all environments better than other environment-specific architectures. The models are evaluated on the multi-environment benchmark, and SILG shows that the learned language grounding is more effective than existing methods. ","This paper proposes SILG, a set of grounded language learning environments with a common interface. The SILG consists of three grid-world environments (RTFM, Messenger, and ALFWorld) and three symbolic counterparts of visual worlds, where the goal is to learn language by interpreting rich natural language for complex scenes. The authors propose to use a shared model architecture for RL across all environments, which is based on egocentric local convolution, recurrent state-tracking, entity-centric attention, and a pretrained LM.   The authors show that the shared architecture is more robust to the richness of observation space, richness of action space, language specification, and plan complexity, and that the unified models are able to generalize to unseen entities. They also show that SILG is able to learn a shared architecture for all environments better than other environment-specific architectures. The models are evaluated on the multi-environment benchmark, and SILG shows that the learned language grounding is more effective than existing methods. "
15472,SP:23c8db56f59f778fe812a5dd161f7a1f21c3cdba,"Vision MoE ( V - MoE ) COMPARE dense networks. dense networks COMPARE Vision MoE ( V - MoE ). Vision MoE ( V - MoE ) HYPONYM-OF Vision Transformer. V - MoE COMPARE networks. networks COMPARE V - MoE. V - MoE USED-FOR image recognition. extension USED-FOR adaptive per - image compute. routing algorithm USED-FOR extension. V - MoE USED-FOR scale vision models. V - MoE USED-FOR 15B parameter model. ImageNet EVALUATE-FOR 15B parameter model. Task are Natural Language Processing, and Computer Vision. Method is vision models. ","This paper proposes Vision MoE (V-MoE), a variant of Vision Transformer, which can be seen as a generalization of Vision Transformers (MoE). The authors argue that Vision MoEs (MoEs) are more efficient than dense networks, and demonstrate that V-MoEs can be used to improve the performance of existing vision models. The authors also show that a 15B parameter model trained on ImageNet is able to outperform the state-of-the-art on a number of tasks, including image recognition and language processing.   The paper also proposes an extension to the adaptive per-image compute, which is based on a routing algorithm, and shows that the proposed extension can be applied to a wide range of vision models, and that the performance gain is significant.  Finally, the authors show that the results are consistent across a range of tasks and datasets, and show that V - MoE outperforms networks trained on a variety of tasks. ","This paper proposes Vision MoE (V-MoE), a variant of Vision Transformer, which can be seen as a generalization of Vision Transformers (MoE). The authors argue that Vision MoEs (MoEs) are more efficient than dense networks, and demonstrate that V-MoEs can be used to improve the performance of existing vision models. The authors also show that a 15B parameter model trained on ImageNet is able to outperform the state-of-the-art on a number of tasks, including image recognition and language processing.   The paper also proposes an extension to the adaptive per-image compute, which is based on a routing algorithm, and shows that the proposed extension can be applied to a wide range of vision models, and that the performance gain is significant.  Finally, the authors show that the results are consistent across a range of tasks and datasets, and show that V - MoE outperforms networks trained on a variety of tasks. "
15521,SP:c5235f41dfb8b5cc478f11c5d5e0ab0b8676871e,"benign optimization landscape FEATURE-OF loss function. n ( sample size ) neurons FEATURE-OF 1 - hidden - layer networks. zero training loss FEATURE-OF global minimizer. local - min or saddle points FEATURE-OF nice local region. global minimizer FEATURE-OF KKT point. projected gradient methods USED-FOR KKT points. SGD USED-FOR narrow neural nets. projected gradient methods USED-FOR narrow neural nets. projected gradient methods COMPARE SGD. SGD COMPARE projected gradient methods. projected gradient methods USED-FOR constrained formulation. Method are neural networks, narrow networks, and gradient descent. Generic is network. OtherScientificTerm are activation, expressivity, and feasible region. Task is constrained optimization formulation. ","This paper considers the problem of training neural networks with 1-hidden-layer networks with n (sample size) neurons, where the loss function lies on a benign optimization landscape. The authors show that for narrow networks, there exists a global minimizer with zero training loss that is a KKT point of the network, which is a function of the size of the activation and the expressivity of the weights. They show that this point is a feasible region of the optimal solution of gradient descent. They also show that there exist local-min or saddle points in which a nice local region can be found, and they show that the local minimizer of the KKT points is a local-max or saddle point.   The authors also provide a constrained optimization formulation, which shows that projected gradient methods can be used to find KKT pointed out by SGD for training narrow neural nets, and that this constrained formulation can be extended to the case where the feasible region is not known. ","This paper considers the problem of training neural networks with 1-hidden-layer networks with n (sample size) neurons, where the loss function lies on a benign optimization landscape. The authors show that for narrow networks, there exists a global minimizer with zero training loss that is a KKT point of the network, which is a function of the size of the activation and the expressivity of the weights. They show that this point is a feasible region of the optimal solution of gradient descent. They also show that there exist local-min or saddle points in which a nice local region can be found, and they show that the local minimizer of the KKT points is a local-max or saddle point.   The authors also provide a constrained optimization formulation, which shows that projected gradient methods can be used to find KKT pointed out by SGD for training narrow neural nets, and that this constrained formulation can be extended to the case where the feasible region is not known. "
15570,SP:0be529f5254afd59dcfa6b34a359c7037e7a8323,"risk measures USED-FOR risk - aware multi - armed bandit models. variance HYPONYM-OF risk measures. correlated options FEATURE-OF real - world online decision making problems. learner PART-OF CMCB. full - bandit feedback HYPONYM-OF feedback settings. full - information HYPONYM-OF feedback settings. logarithmic factors FEATURE-OF optimal regrets. matching lower bounds USED-FOR algorithms. optimal regrets FEATURE-OF algorithms. option correlation FEATURE-OF risk - aware bandits. analytical techniques USED-FOR bandit analysis. analytical techniques USED-FOR concentration. estimated covariance USED-FOR concentration. analytical techniques USED-FOR risk of selected actions. analytical techniques USED-FOR estimated covariance. sampling strategy properties USED-FOR bandit analysis. sampling strategy properties USED-FOR analytical techniques. Generic is they. OtherScientificTerm are weight vectors, random feedback, option covariance, reward observation scenarios, and covariance structures. ","This paper studies the problem of learning risk measures for risk-aware multi-armed bandit models, where the risk measures (variance, etc.) are correlated with the number of options available to the learner. The authors show that in real-world online decision making problems with correlated options, the optimal regret of a learner in a CMCB can be approximated by the concentration of the weight vectors of the options.    The authors provide matching lower bounds for algorithms with optimal regrets that depend on logarithmic factors.  They show that under certain feedback settings (e.g., full-information and full-bandit feedback), the optimal regrets of algorithms with random feedback are bounded by a constant factor that depends on the option covariance. They also provide analytical techniques to estimate the concentration based on the estimated covariance of the risk of selected actions.  Finally, the authors provide a theoretical analysis of the option correlation in risk- aware bandits, which shows that the concentration can be estimated based on analytical techniques based on sampling strategy properties of the bandit analysis.  The paper also shows that under some reward observation scenarios, there is a trade-off between the concentration and the covariance structures. ","This paper studies the problem of learning risk measures for risk-aware multi-armed bandit models, where the risk measures (variance, etc.) are correlated with the number of options available to the learner. The authors show that in real-world online decision making problems with correlated options, the optimal regret of a learner in a CMCB can be approximated by the concentration of the weight vectors of the options.    The authors provide matching lower bounds for algorithms with optimal regrets that depend on logarithmic factors.  They show that under certain feedback settings (e.g., full-information and full-bandit feedback), the optimal regrets of algorithms with random feedback are bounded by a constant factor that depends on the option covariance. They also provide analytical techniques to estimate the concentration based on the estimated covariance of the risk of selected actions.  Finally, the authors provide a theoretical analysis of the option correlation in risk- aware bandits, which shows that the concentration can be estimated based on analytical techniques based on sampling strategy properties of the bandit analysis.  The paper also shows that under some reward observation scenarios, there is a trade-off between the concentration and the covariance structures. "
15619,SP:472a90bb175b0286765c5a47b040e1a58f594a05,"r × r - dimensional PSD matrices PART-OF Positive Semidefinite ( PSD ) factorization. PSD factorizations USED-FOR semidefinite programs. quantum resources USED-FOR information theory. Nonnegative Matrix Factorization ( NMF ) problem USED-FOR PSD factorization task. algorithm USED-FOR NMFs. Multiplicative Update algorithm HYPONYM-OF algorithm. positive diagonal matrices USED-FOR non - negativity. non - commutative extension USED-FOR PSD factorizations. Lee - Seung ’s algorithm USED-FOR non - commutative extension. Matrix Multiplicative Update ( MMU ) algorithm HYPONYM-OF non - commutative extension. multiplicative update algorithm USED-FOR NMF. squared loss objective EVALUATE-FOR update scheme. blockdiagonal PSD factorizations CONJUNCTION tensor PSD factorizations. tensor PSD factorizations CONJUNCTION blockdiagonal PSD factorizations. MMU algorithm USED-FOR blockdiagonal PSD factorizations. MMU algorithm USED-FOR tensor PSD factorizations. MMU algorithm USED-FOR primitive. primitive USED-FOR blockdiagonal PSD factorizations. primitive USED-FOR tensor PSD factorizations. real and synthetic data EVALUATE-FOR method. OtherScientificTerm are r - dimensional non - negative vectors, and matrix geometric mean of appropriate PSD matrices. Generic are problem, and it. Method are PSD factorization, MajorizationMinimization framework, and Lieb ’s Concavity Theorem. ","This paper studies the Nonnegative Matrix Factorization (NMF) problem in the context of the Positive Semidefinite (PSD) factorization, i.e., the r × r-dimensional PSD matrices in the original PSD factorization. In this problem, the objective is to factorize an arbitrary matrix into r–r-dimensional non-negative vectors, and the non-negativity of the matrix geometric mean of appropriate PSD matrix is assumed to be non-zero.    The authors consider the problem of nonnegative matrix factorization as a non-convex optimization problem, which is motivated by information theory and quantum resources.  In particular, the authors study the nonnegative NMF problem as the nonnegativity in the case of positive diagonal matrices.  The nonnegative problem is formulated as the NonNegative Matrix Factorisation (NNF) problem, where the NMF matrix is nonnegative. The authors propose an algorithm for solving NMFs that is based on the MajorizationMinimization framework, and show that the algorithm is non-asymptotically non-commutative. The non-computative extension to PSD is the Matrix Multiplicative Update (MMU) algorithm, which extends Lee-Seung’s algorithm to non-consistent PSD.  This algorithm is a simple extension of Lee-Segung's algorithm, and it uses a multiplicative update algorithm to compute the non negation of NMF. The update scheme has a simple squared loss objective, which can be solved efficiently.  in the sense that it does not require the use of any additional information.  Experiments are conducted on real and synthetic data, showing that the proposed MMU algorithm is able to compute a primitive for blockdiagonal PSD and tensor PSD factors, and that the primitive can be used as a primitive in the MMU method to compute blockdiagonality of the PSD function. The proposed method is shown to be computationally tractable, and can be combined with existing methods. The paper also provides a proof of the existence of Lieb's Concavity Theorem.","This paper studies the Nonnegative Matrix Factorization (NMF) problem in the context of the Positive Semidefinite (PSD) factorization, i.e., the r × r-dimensional PSD matrices in the original PSD factorization. In this problem, the objective is to factorize an arbitrary matrix into r–r-dimensional non-negative vectors, and the non-negativity of the matrix geometric mean of appropriate PSD matrix is assumed to be non-zero.    The authors consider the problem of nonnegative matrix factorization as a non-convex optimization problem, which is motivated by information theory and quantum resources.  In particular, the authors study the nonnegative NMF problem as the nonnegativity in the case of positive diagonal matrices.  The nonnegative problem is formulated as the NonNegative Matrix Factorisation (NNF) problem, where the NMF matrix is nonnegative. The authors propose an algorithm for solving NMFs that is based on the MajorizationMinimization framework, and show that the algorithm is non-asymptotically non-commutative. The non-computative extension to PSD is the Matrix Multiplicative Update (MMU) algorithm, which extends Lee-Seung’s algorithm to non-consistent PSD.  This algorithm is a simple extension of Lee-Segung's algorithm, and it uses a multiplicative update algorithm to compute the non negation of NMF. The update scheme has a simple squared loss objective, which can be solved efficiently.  in the sense that it does not require the use of any additional information.  Experiments are conducted on real and synthetic data, showing that the proposed MMU algorithm is able to compute a primitive for blockdiagonal PSD and tensor PSD factors, and that the primitive can be used as a primitive in the MMU method to compute blockdiagonality of the PSD function. The proposed method is shown to be computationally tractable, and can be combined with existing methods. The paper also provides a proof of the existence of Lieb's Concavity Theorem."
15668,SP:83abd6d149d88cc6e96cbc4d488e4fe9dc2a4fcb,"Domain Generalization ( DG ) USED-FOR model. DG approaches USED-FOR domaininvariant information. DG approaches USED-FOR generalization capability. features PART-OF latent space. domain - specific representation USED-FOR generalization. meta - learning framework USED-FOR domain - specific representation. mDSDI COMPARE state - of - the - art techniques. state - of - the - art techniques COMPARE mDSDI. state - of - the - art techniques USED-FOR DG. mDSDI USED-FOR DG. domain - specific COMPARE domain - invariant. domain - invariant COMPARE domain - specific. Background - Colored - MNIST HYPONYM-OF dataset. OtherScientificTerm are domain - specific information, invariance view, and domain - invariant and domainspecific features. Generic is framework. Method is unified framework. ","This paper proposes Domain Generalization (DG) to improve the generalization capability of a model by leveraging domain-invariant information. Previous DG approaches do not consider domaininvariance information, and the authors propose a meta-learning framework to learn a domain-specific representation for better generalization.   The authors propose mDSDI, an extension of the invariance view to the case where features in the latent space are shared across all domains.  The paper shows that the proposed framework is able to generalize better than previous DG approaches that do not take into account the invariant view. The authors also show that mDS DI outperforms state-of-the-art techniques for DG on a dataset called Background-Colored-MNIST.  In addition, the authors also propose a unified framework that unifies the use of domain-agnostic and domainspecific features.  Experiments are conducted on a few datasets to show that the performance of the proposed DG is better than that of a single DG on the same dataset, and that the generalisation performance of a DG trained on a different dataset is also better than the performance on a single dataset.","This paper proposes Domain Generalization (DG) to improve the generalization capability of a model by leveraging domain-invariant information. Previous DG approaches do not consider domaininvariance information, and the authors propose a meta-learning framework to learn a domain-specific representation for better generalization.   The authors propose mDSDI, an extension of the invariance view to the case where features in the latent space are shared across all domains.  The paper shows that the proposed framework is able to generalize better than previous DG approaches that do not take into account the invariant view. The authors also show that mDS DI outperforms state-of-the-art techniques for DG on a dataset called Background-Colored-MNIST.  In addition, the authors also propose a unified framework that unifies the use of domain-agnostic and domainspecific features.  Experiments are conducted on a few datasets to show that the performance of the proposed DG is better than that of a single DG on the same dataset, and that the generalisation performance of a DG trained on a different dataset is also better than the performance on a single dataset."
15717,SP:4191474c75e2fedf514f0f3001a67a047eb74c30,"diffusion models COMPARE generative models. generative models COMPARE diffusion models. image sample quality EVALUATE-FOR generative models. image sample quality EVALUATE-FOR diffusion models. architecture USED-FOR unconditional image synthesis. diversity CONJUNCTION fidelity. fidelity CONJUNCTION diversity. method USED-FOR diversity. gradients USED-FOR classifier. sample quality USED-FOR conditional image synthesis. classifier guidance USED-FOR sample quality. classifier USED-FOR method. gradients USED-FOR method. classifier guidance USED-FOR conditional image synthesis. classifier guidance CONJUNCTION upsampling diffusion models. upsampling diffusion models CONJUNCTION classifier guidance. FID EVALUATE-FOR classifier guidance. Material are ImageNet 128 ⇥ 128, ImageNet 256 ⇥ 256, and ImageNet 512 ⇥ 512. Method is BigGAN - deep. ","This paper proposes a new architecture for unconditional image synthesis based on the observation that diffusion models have better image sample quality than other generative models. The proposed method is based on classifier guidance to improve the sample quality for conditional image synthesis using gradients from a classifier. The authors show that the proposed method improves the diversity and fidelity of the generated images compared to ImageNet 128 ⇥ 128, ImageNet 256 ⇥ 256, and ImageNet 512 ⇥ 512.  The authors also show that using the proposed architecture, BigGAN-deep can achieve FID of $O(\sqrt{n\log n})$, which is a significant improvement over the previous state-of-the-art FID achieved by the same architecture.   The paper also shows that the benefits of the proposed approach are not only in terms of FID, but also in the diversity of generated images, and that the method can be applied to improve diversity and improve the fidelity of existing diffusion models as well.  Finally, the authors show how to combine the benefits from the proposed model with upsampling diffusion models and show that their method can improve the diversity. ","This paper proposes a new architecture for unconditional image synthesis based on the observation that diffusion models have better image sample quality than other generative models. The proposed method is based on classifier guidance to improve the sample quality for conditional image synthesis using gradients from a classifier. The authors show that the proposed method improves the diversity and fidelity of the generated images compared to ImageNet 128 ⇥ 128, ImageNet 256 ⇥ 256, and ImageNet 512 ⇥ 512.  The authors also show that using the proposed architecture, BigGAN-deep can achieve FID of $O(\sqrt{n\log n})$, which is a significant improvement over the previous state-of-the-art FID achieved by the same architecture.   The paper also shows that the benefits of the proposed approach are not only in terms of FID, but also in the diversity of generated images, and that the method can be applied to improve diversity and improve the fidelity of existing diffusion models as well.  Finally, the authors show how to combine the benefits from the proposed model with upsampling diffusion models and show that their method can improve the diversity. "
15766,SP:fe3cab08596cde4c14ecf6fca8d0f95b02bab229,"out - of - distribution samples USED-FOR few - shot learning. unlabeled samples HYPONYM-OF out - of - distribution samples. out - of - distribution samples USED-FOR classifier. query data HYPONYM-OF in - distribution samples. approach USED-FOR inductive and transductive settings. method COMPARE pretrained networks. pretrained networks COMPARE method. architectures FEATURE-OF pretrained networks. OtherScientificTerm are irrelevant features, prototypes, and feature extractors. Task is pre - training. ","This paper studies few-shot learning with out-of-distribution samples, i.e. unlabeled samples that contain irrelevant features. The authors propose a novel approach to learn a classifier from such out- of distribution samples, which they call prototypes. The idea is that the classifier can be trained on out-out-of distribution samples (i.e., query data) and learn a good classifier on in-distributions. The proposed approach is applicable to both inductive and transductive settings. Experiments show that the proposed method is able to outperform existing pretrained networks with different architectures and architectures. The paper also shows that pre-training with prototypes can be more efficient than using feature extractors.  ","This paper studies few-shot learning with out-of-distribution samples, i.e. unlabeled samples that contain irrelevant features. The authors propose a novel approach to learn a classifier from such out- of distribution samples, which they call prototypes. The idea is that the classifier can be trained on out-out-of distribution samples (i.e., query data) and learn a good classifier on in-distributions. The proposed approach is applicable to both inductive and transductive settings. Experiments show that the proposed method is able to outperform existing pretrained networks with different architectures and architectures. The paper also shows that pre-training with prototypes can be more efficient than using feature extractors.  "
15815,SP:b1f65724926f136979829b7a6c870bc31f38f591,experience replay USED-FOR reinforcement learning. Prioritized sampling USED-FOR samples. recentness CONJUNCTION corrective feedback. corrective feedback CONJUNCTION recentness. TD error CONJUNCTION recentness. recentness CONJUNCTION TD error. criteria PART-OF prioritization. TD error HYPONYM-OF criteria. recentness HYPONYM-OF prioritization. corrective feedback HYPONYM-OF criteria. corrective feedback PART-OF prioritization. recentness HYPONYM-OF criteria. optimal prioritization strategy USED-FOR Bellman update. on - policiness CONJUNCTION Q value. Q value CONJUNCTION on - policiness. methods USED-FOR prioritization weight. ReMERN CONJUNCTION ReMERT. ReMERT CONJUNCTION ReMERN. ReMERT HYPONYM-OF methods. ReMERN HYPONYM-OF methods. ReMERT USED-FOR temporal ordering of states. ReMERN HYPONYM-OF error network. MuJoCo CONJUNCTION Atari. Atari CONJUNCTION MuJoCo. Atari CONJUNCTION Meta - World. Meta - World CONJUNCTION Atari. methods COMPARE prioritized sampling algorithms. prioritized sampling algorithms COMPARE methods. RL benchmarks EVALUATE-FOR prioritized sampling algorithms. RL benchmarks EVALUATE-FOR methods. Meta - World HYPONYM-OF RL benchmarks. MuJoCo HYPONYM-OF RL benchmarks. Atari HYPONYM-OF RL benchmarks. Metric is regret minimization objective. OtherScientificTerm is hindsight TD error. Task is sampling. Generic is strategy. ,"Prioritized sampling is a popular technique in experience replay for reinforcement learning. Prioritized sampling aims to generate samples that minimize the regret minimization objective. The paper proposes three criteria in prioritization: TD error, recentness, and corrective feedback. The authors propose an optimal prioritization strategy for the Bellman update, which minimizes the hindsight TD error. They also propose two methods to optimize the prioritization weight based on on-policiness and Q value. The proposed methods, ReMERN and ReMERT, are both methods that optimize the temporal ordering of states. They show that the proposed methods outperform other prioritized sampling algorithms on standard RL benchmarks such as MuJoCo, Atari, and Meta-World.   The authors also propose a new error network called ReMerner, which is a variant of ReMert, and show that ReMBERT can be used to improve the quality of sampling.  The paper is well-written and well-motivated. However, there are a few issues that prevent me from recommending acceptance of this paper. ","Prioritized sampling is a popular technique in experience replay for reinforcement learning. Prioritized sampling aims to generate samples that minimize the regret minimization objective. The paper proposes three criteria in prioritization: TD error, recentness, and corrective feedback. The authors propose an optimal prioritization strategy for the Bellman update, which minimizes the hindsight TD error. They also propose two methods to optimize the prioritization weight based on on-policiness and Q value. The proposed methods, ReMERN and ReMERT, are both methods that optimize the temporal ordering of states. They show that the proposed methods outperform other prioritized sampling algorithms on standard RL benchmarks such as MuJoCo, Atari, and Meta-World.   The authors also propose a new error network called ReMerner, which is a variant of ReMert, and show that ReMBERT can be used to improve the quality of sampling.  The paper is well-written and well-motivated. However, there are a few issues that prevent me from recommending acceptance of this paper. "
15864,SP:601ebf30b3c6aa35fcef49633aa8eb0acd0f2c66,nonstationary environment FEATURE-OF expert advice. nonstationary environment FEATURE-OF sequential prediction. expert advice USED-FOR sequential prediction. regret bounds USED-FOR linear - time algorithm. relative entropy projection step PART-OF algorithm. projection COMPARE weight - sharing approaches. weight - sharing approaches COMPARE projection. implicit costs FEATURE-OF weight updates. algorithm USED-FOR projection step. linear time FEATURE-OF projection step. OtherScientificTerm is long - term memory guarantees. Task is portfolio optimization. ,"This paper studies the problem of sequential prediction in a nonstationary environment with expert advice. The authors provide regret bounds for a linear-time algorithm with a relative entropy projection step, which is an algorithm that combines the benefits of long-term memory guarantees with the advantages of weight-sharing approaches. In particular, the authors show that the projection step in linear time is equivalent to weight-sharing approaches in terms of implicit costs of weight updates, and that the proposed algorithm can be used as an algorithm for any projection step with linear time. They also provide a theoretical analysis for portfolio optimization. ","This paper studies the problem of sequential prediction in a nonstationary environment with expert advice. The authors provide regret bounds for a linear-time algorithm with a relative entropy projection step, which is an algorithm that combines the benefits of long-term memory guarantees with the advantages of weight-sharing approaches. In particular, the authors show that the projection step in linear time is equivalent to weight-sharing approaches in terms of implicit costs of weight updates, and that the proposed algorithm can be used as an algorithm for any projection step with linear time. They also provide a theoretical analysis for portfolio optimization. "
15913,SP:b2439973063e827b3cbe92306a2fdee3286b6b44,navigational engines CONJUNCTION recommendation systems. recommendation systems CONJUNCTION navigational engines. routing applications USED-FOR recommendation systems. routing applications USED-FOR navigational engines. contextual linear bandits USED-FOR routing applications. routing applications USED-FOR variant. contextual linear bandits USED-FOR variant. algorithms USED-FOR problem. O(d log d ) regret CONJUNCTION list size poly(d ). list size poly(d ) CONJUNCTION O(d log d ) regret. O(d log d ) regret FEATURE-OF algorithm. list size poly(d ) FEATURE-OF algorithm. nearly tight algorithms USED-FOR problem. Steiner ’s formula USED-FOR centroid of a convex set. algorithmic techniques USED-FOR convex geometry. Steiner ’s formula HYPONYM-OF algorithmic techniques. OtherScientificTerm is hidden d - dimensional value. Method is cutting - plane algorithms. Metric is regret. ,"This paper proposes a variant of contextual linear bandits for routing applications in both navigational engines and recommendation systems, where the hidden d-dimensional value is a linear function of the input d. The authors propose two algorithms to solve this problem. The first algorithm has O(d log d) regret and a list size poly(d), while the second algorithm has a regret of O(log d) and a poly(D) regret.  The authors also propose two nearly tight algorithms for this problem, which are based on two algorithmic techniques: (1) Steiner’s formula for finding the centroid of a convex set, and (2) cutting-plane algorithms.  ","This paper proposes a variant of contextual linear bandits for routing applications in both navigational engines and recommendation systems, where the hidden d-dimensional value is a linear function of the input d. The authors propose two algorithms to solve this problem. The first algorithm has O(d log d) regret and a list size poly(d), while the second algorithm has a regret of O(log d) and a poly(D) regret.  The authors also propose two nearly tight algorithms for this problem, which are based on two algorithmic techniques: (1) Steiner’s formula for finding the centroid of a convex set, and (2) cutting-plane algorithms.  "
15962,SP:abe83c7e0bcf4829742609d709637e2f84d8a4d9,"Automated machine learning ( AutoML ) USED-FOR data scientists. combinators USED-FOR compositional code. orthogonal combinators USED-FOR machinelearning operators. orthogonal combinators USED-FOR pipelines. machinelearning operators PART-OF pipelines. search spaces USED-FOR AutoML optimizers. hyperparameter schemas CONJUNCTION search spaces. search spaces CONJUNCTION hyperparameter schemas. translation scheme USED-FOR search spaces. hyperparameter schemas USED-FOR translation scheme. pipelines USED-FOR translation scheme. Lale HYPONYM-OF sklearn - compatible AutoML library. user study EVALUATE-FOR it. Method are machine learning, AutoML, and functional programming. OtherScientificTerm is non - compositional code changes. ","Automated machine learning (AutoML) is becoming a popular tool for data scientists to train data scientists. However, there are several issues with AutoML: (1) AutoML is not compatible with non-compositional code changes, and (2) the search spaces for AutoML optimizers are not well-suited for non-functional programming. To address these issues, the authors propose a sklearn-compatible AutoML library called Lale, which uses orthogonal combinators to transform compositional code into a set of pipelines that contain machinelearning operators. The authors also propose a translation scheme that uses hyperparameter schemas and search spaces from these pipelines. The paper also conducts a user study to show that Lale can be used to improve the performance of AutoML, and that it can be applied to a variety of tasks.","Automated machine learning (AutoML) is becoming a popular tool for data scientists to train data scientists. However, there are several issues with AutoML: (1) AutoML is not compatible with non-compositional code changes, and (2) the search spaces for AutoML optimizers are not well-suited for non-functional programming. To address these issues, the authors propose a sklearn-compatible AutoML library called Lale, which uses orthogonal combinators to transform compositional code into a set of pipelines that contain machinelearning operators. The authors also propose a translation scheme that uses hyperparameter schemas and search spaces from these pipelines. The paper also conducts a user study to show that Lale can be used to improve the performance of AutoML, and that it can be applied to a variety of tasks."
16011,SP:0d7f1cae577ed598048b64617e85ca6bd5c6d7fa,"small datasets USED-FOR neural network weights. problem - byproblem basis FEATURE-OF pattern of sparsity. generalization CONJUNCTION interference. interference CONJUNCTION generalization. interference FEATURE-OF few - shot and continual learning problems. generalization EVALUATE-FOR selective sparsity. meta - learning USED-FOR adaptable features. inductive bias USED-FOR meta - learning systems. Method are weight initialization, learning algorithm, sparse learning, and sparse gradient descent. Metric is generalization error. OtherScientificTerm are patterned sparsity, and learning rates. ","This paper studies the generalization performance of neural network weights on small datasets. The authors consider the problem of weight initialization for few-shot and continual learning, where the goal is to learn a good generalization error on a small number of training examples. They show that a pattern of sparsity on the problem-byproblem basis can be observed on the training data and the learning algorithm. They also show that, under certain assumptions on the learning rate and the number of examples, a learning algorithm will converge to a solution that minimizes generalization and interference.    The authors show that under certain conditions, there is a trade-off between generalization in generalization for selective sparsity and interference in both the generalisation and interference for both the problem setting and the setting where there is patterned sparsity. They further show that meta-learning is able to learn adaptable features that are more robust to the inductive bias of the learning rates, and that sparse gradient descent is more likely to converge to the optimal solution. ","This paper studies the generalization performance of neural network weights on small datasets. The authors consider the problem of weight initialization for few-shot and continual learning, where the goal is to learn a good generalization error on a small number of training examples. They show that a pattern of sparsity on the problem-byproblem basis can be observed on the training data and the learning algorithm. They also show that, under certain assumptions on the learning rate and the number of examples, a learning algorithm will converge to a solution that minimizes generalization and interference.    The authors show that under certain conditions, there is a trade-off between generalization in generalization for selective sparsity and interference in both the generalisation and interference for both the problem setting and the setting where there is patterned sparsity. They further show that meta-learning is able to learn adaptable features that are more robust to the inductive bias of the learning rates, and that sparse gradient descent is more likely to converge to the optimal solution. "
16060,SP:05037e1850003a725a466b64d3e32aa2aed458fb,"shared response modeling HYPONYM-OF multi - view learning problem. multi - set canonical correlation analysis USED-FOR unmixing matrices. sampling noise USED-FOR Multiset CCA. joint diagonalization USED-FOR approach. joint diagonalization USED-FOR Multiset CCA. ShICA - ML HYPONYM-OF maximum - likelihood method. non - Gaussianity USED-FOR ShICA - J. maximum - likelihood method USED-FOR non - Gaussianity. maximum - likelihood method USED-FOR ShICA - J. second - order statistics USED-FOR ShICA - J. method USED-FOR shared components estimation. ShICA USED-FOR shared components estimation. ShICA USED-FOR method. ShICA COMPARE alternatives. alternatives COMPARE ShICA. fMRI and MEG datasets EVALUATE-FOR ShICA. OtherScientificTerm are common components, shared independent components, additive Gaussian noise, and noise variances. Method is Shared Independent Component Analysis ( ShICA ). Generic is model. ","This paper tackles the multi-view learning problem, i.e., shared response modeling, which is a multi-set learning problem where there are multiple views of the same data point, but the common components have shared independent components. The authors propose Shared Independent Component Analysis (ShICA), which is based on multi-sets canonical correlation analysis for unmixing matrices. Multiset CCA uses sampling noise for each view, and the proposed approach uses joint diagonalization. The proposed method, ShICA-J, uses a maximum-likelihood method called ShICA - ML, which uses the non-Gaussianity of the shared independent component and the additive Gaussian noise for the shared component, and uses second-order statistics for the joint component. Experiments on fMRI and MEG datasets show that ShICA outperforms other alternatives, and that the proposed method can be applied to the problem of shared components estimation using ShICA. ","This paper tackles the multi-view learning problem, i.e., shared response modeling, which is a multi-set learning problem where there are multiple views of the same data point, but the common components have shared independent components. The authors propose Shared Independent Component Analysis (ShICA), which is based on multi-sets canonical correlation analysis for unmixing matrices. Multiset CCA uses sampling noise for each view, and the proposed approach uses joint diagonalization. The proposed method, ShICA-J, uses a maximum-likelihood method called ShICA - ML, which uses the non-Gaussianity of the shared independent component and the additive Gaussian noise for the shared component, and uses second-order statistics for the joint component. Experiments on fMRI and MEG datasets show that ShICA outperforms other alternatives, and that the proposed method can be applied to the problem of shared components estimation using ShICA. "
16109,SP:44dd1faa1813c433fd7581d05cae3df440bfb93e,"self - play ( SP ) CONJUNCTION population play ( PP ). population play ( PP ) CONJUNCTION self - play ( SP ). multi - agent reinforcement learning techniques USED-FOR agents. population play ( PP ) HYPONYM-OF multi - agent reinforcement learning techniques. self - play ( SP ) HYPONYM-OF multi - agent reinforcement learning techniques. model USED-FOR human - aware ” agents. behavioral cloning play ” CONJUNCTION BCP. BCP CONJUNCTION behavioral cloning play ”. behavioral cloning play ” HYPONYM-OF human - aware ” agents. BCP HYPONYM-OF human - aware ” agents. behavioral cloning USED-FOR human model. generalization EVALUATE-FOR agents. agents USED-FOR human co - players. approach USED-FOR agents. generalization EVALUATE-FOR approach. multi - agent approaches USED-FOR competitive domains. self - play agents USED-FOR agent partner. FCP agents COMPARE SP. SP COMPARE FCP agents. SP CONJUNCTION PP. PP CONJUNCTION SP. PP CONJUNCTION BCP. BCP CONJUNCTION PP. FCP agents COMPARE BCP. BCP COMPARE FCP agents. FCP agents COMPARE PP. PP COMPARE FCP agents. Material is human data. Generic are it, and method. Method are Fictitious Co - Play ( FCP ), and two - player collaborative cooking simulator. OtherScientificTerm is subjective preference. ","This paper proposes a new multi-agent reinforcement learning technique called Fictitious Co-Play (FCP) to learn a human-aware agent from human data. The proposed method is based on the observation that human data is highly correlated with human data, and that it can be used to train “human-aware” agents, i.e. agents that are able to generalize well to unseen environments. The authors propose to use two existing multi- agent reinforcement learning techniques, self-play (SP) and population play (PP), to train agents that can generalize to unseen domains. They also propose a new model called “Behavioral Cloning Play” (BCP) that is able to train two types of agents, namely “behavioral cloning play” and “bouncing ball play,” which can be trained using behavioral cloning to train a human model.  The authors demonstrate that their approach improves the generalization performance of the agents trained with human co-players in a two-player collaborative cooking simulator. They show that their FCP agents outperform SP, PP, and BCP in terms of generalization, and they also show that the agents can generalise to unseen tasks. The paper also shows that the proposed method can be applied to other multi-agents in competitive domains. Finally, the authors show that FCP can be combined with other multi agent approaches to improve the performance of agents trained to learn to play with an agent partner.    The main contribution of the paper is the introduction of FCP, a new method for learning to learn agents that generalize from unseen environments, which is a natural extension of the work of [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,19].   [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] ","This paper proposes a new multi-agent reinforcement learning technique called Fictitious Co-Play (FCP) to learn a human-aware agent from human data. The proposed method is based on the observation that human data is highly correlated with human data, and that it can be used to train “human-aware” agents, i.e. agents that are able to generalize well to unseen environments. The authors propose to use two existing multi- agent reinforcement learning techniques, self-play (SP) and population play (PP), to train agents that can generalize to unseen domains. They also propose a new model called “Behavioral Cloning Play” (BCP) that is able to train two types of agents, namely “behavioral cloning play” and “bouncing ball play,” which can be trained using behavioral cloning to train a human model.  The authors demonstrate that their approach improves the generalization performance of the agents trained with human co-players in a two-player collaborative cooking simulator. They show that their FCP agents outperform SP, PP, and BCP in terms of generalization, and they also show that the agents can generalise to unseen tasks. The paper also shows that the proposed method can be applied to other multi-agents in competitive domains. Finally, the authors show that FCP can be combined with other multi agent approaches to improve the performance of agents trained to learn to play with an agent partner.    The main contribution of the paper is the introduction of FCP, a new method for learning to learn agents that generalize from unseen environments, which is a natural extension of the work of [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,19].   [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] "
16158,SP:21c84bd720b1e90ea0f88fbf8fd24dbcb49b547c,"method USED-FOR cooperative multi - agent reinforcement learning. discrete and continuous action spaces FEATURE-OF cooperative multi - agent reinforcement learning. deep deterministic policy gradients USED-FOR policies. approach USED-FOR policies. MADDPG HYPONYM-OF multi - agent actor - critic method. deep deterministic policy gradients USED-FOR approach. QMIX HYPONYM-OF multi - agent Q - learning algorithm. FACMAC USED-FOR centralised but factored critic. per - agent utilities CONJUNCTION joint action - value function. joint action - value function CONJUNCTION per - agent utilities. joint action - value function FEATURE-OF centralised but factored critic. per - agent utilities PART-OF centralised but factored critic. non - linear monotonic function USED-FOR centralised but factored critic. non - linear monotonic function USED-FOR joint action - value function. it USED-FOR tasks. representational capacity USED-FOR it. monolithic, or monotonically factored critics USED-FOR tasks. joint action space FEATURE-OF centralised policy gradient estimator. centralised policy gradient estimator USED-FOR FACMAC. multi - agent MuJoCo benchmark CONJUNCTION StarCraft II micromanagement tasks. StarCraft II micromanagement tasks CONJUNCTION multi - agent MuJoCo benchmark. multi - agent particle environments CONJUNCTION multi - agent MuJoCo benchmark. multi - agent MuJoCo benchmark CONJUNCTION multi - agent particle environments. multi - agent MuJoCo benchmark EVALUATE-FOR FACMAC. multi - agent particle environments EVALUATE-FOR FACMAC. StarCraft II micromanagement tasks EVALUATE-FOR FACMAC. FACMAC COMPARE baselines. baselines COMPARE FACMAC. FACMAC COMPARE MADDPG. MADDPG COMPARE FACMAC. MADDPG COMPARE baselines. baselines COMPARE MADDPG. OtherScientificTerm are critic, and coordinated policy changes. Method are nonmonotonic factorisation, and centralised critic. ","This paper proposes a novel method for cooperative multi-agent reinforcement learning in discrete and continuous action spaces. The approach is based on deep deterministic policy gradients to learn policies, which is similar to MADDPG, a multi-agents actor-critic method. The authors propose a novel approach called FACMAC to learn a centralised but factored critic that combines per-agent utilities and a joint action-value function. The centralised critic is a non-linear monotonic function, which allows for non-monotonic factorisation of the policy gradient. The paper also proposes a variant of QMIX, a well-studied and well-motivated multi-manual Q-learning algorithm, which can be used in the proposed approach.   The paper shows that FACMAC can learn the centralised policy gradient estimator in the joint action space, and it can be applied to a wide range of tasks that require monolithic, or monotonically factored critics. It also shows that it can learn a representational capacity that allows for coordinated policy changes across agents. The proposed FACMAC is evaluated on the multi- agent particle environments, the single-agent MuJoCo benchmark, and StarCraft II micromanagement tasks, and shows superior performance to other baselines. In addition, FACMAC shows that the proposed method is more robust to nonmonotonicity of the critic, and that it is able to learn better representations of the environment.  The authors also show that the performance of FACMAC matches that of MADDPGS and shows that, in some cases, it outperforms the state of the art. ","This paper proposes a novel method for cooperative multi-agent reinforcement learning in discrete and continuous action spaces. The approach is based on deep deterministic policy gradients to learn policies, which is similar to MADDPG, a multi-agents actor-critic method. The authors propose a novel approach called FACMAC to learn a centralised but factored critic that combines per-agent utilities and a joint action-value function. The centralised critic is a non-linear monotonic function, which allows for non-monotonic factorisation of the policy gradient. The paper also proposes a variant of QMIX, a well-studied and well-motivated multi-manual Q-learning algorithm, which can be used in the proposed approach.   The paper shows that FACMAC can learn the centralised policy gradient estimator in the joint action space, and it can be applied to a wide range of tasks that require monolithic, or monotonically factored critics. It also shows that it can learn a representational capacity that allows for coordinated policy changes across agents. The proposed FACMAC is evaluated on the multi- agent particle environments, the single-agent MuJoCo benchmark, and StarCraft II micromanagement tasks, and shows superior performance to other baselines. In addition, FACMAC shows that the proposed method is more robust to nonmonotonicity of the critic, and that it is able to learn better representations of the environment.  The authors also show that the performance of FACMAC matches that of MADDPGS and shows that, in some cases, it outperforms the state of the art. "
16207,SP:1c8351b8a6cdf1212840388e19a596729b3bfda4,Hebbian plasticity USED-FOR storage. Hebbian plasticity CONJUNCTION attractor dynamics. attractor dynamics CONJUNCTION Hebbian plasticity. storage CONJUNCTION attractor dynamics. attractor dynamics CONJUNCTION storage. Hopfield networks PART-OF neuroscience. memory - augmented neural networks USED-FOR machine learning. key - value mechanism USED-FOR memory - augmented neural networks. augmented networks COMPARE variants. variants COMPARE augmented networks. three - factor plasticity rules USED-FOR basic key - value memory. network parameters USED-FOR rules. heteroassociative memory CONJUNCTION sequence learning. sequence learning CONJUNCTION heteroassociative memory. continual recall CONJUNCTION heteroassociative memory. heteroassociative memory CONJUNCTION continual recall. network COMPARE Hopfield networks. Hopfield networks COMPARE network. network USED-FOR continual recall. network USED-FOR heteroassociative memory. Hopfield networks USED-FOR autoassociative memory tasks. network USED-FOR sequence learning. autoassociative memory tasks EVALUATE-FOR network. Hopfield network USED-FOR model of biological long - term memory. ,"This paper studies the use of memory-augmented neural networks (MAMNets) in the context of long-term memory in neuroscience. The authors propose a novel key-value mechanism for memory augmentation that is based on Hebbian plasticity and attractor dynamics. The key idea is to augment the memory of a MAMNet with an augmented version of the Hopfield network, which is a well-studied model in neuroscience and has been shown to be a good memory augmenter. The paper shows that the proposed method can be used to augment memory in a variety of tasks, including continual recall, heteroassociative memory, and sequence learning.","This paper studies the use of memory-augmented neural networks (MAMNets) in the context of long-term memory in neuroscience. The authors propose a novel key-value mechanism for memory augmentation that is based on Hebbian plasticity and attractor dynamics. The key idea is to augment the memory of a MAMNet with an augmented version of the Hopfield network, which is a well-studied model in neuroscience and has been shown to be a good memory augmenter. The paper shows that the proposed method can be used to augment memory in a variety of tasks, including continual recall, heteroassociative memory, and sequence learning."
16256,SP:7ad6da2c63859d64970e9b35326e9ceab48add47,"Pairwise learning HYPONYM-OF learning tasks. bipartite ranking CONJUNCTION metric learning. metric learning CONJUNCTION bipartite ranking. It USED-FOR machine learning tasks. metric learning HYPONYM-OF machine learning tasks. bipartite ranking HYPONYM-OF machine learning tasks. approach USED-FOR streaming data. streaming data USED-FOR pairwise learning. online gradient descent ( OGD ) algorithm USED-FOR approach. stochastic and online gradient descent methods USED-FOR pairwise learning. optimization CONJUNCTION generalization error bounds. generalization error bounds CONJUNCTION optimization. stability results CONJUNCTION optimization. optimization CONJUNCTION stability results. generalization error bounds USED-FOR smooth and nonsmooth problems. generalization error bounds USED-FOR convex and nonconvex. convex and nonconvex CONJUNCTION smooth and nonsmooth problems. smooth and nonsmooth problems CONJUNCTION convex and nonconvex. optimization CONJUNCTION generalization analysis. generalization analysis CONJUNCTION optimization. techniques USED-FOR optimization. techniques USED-FOR generalization analysis. generalization bounds USED-FOR OGD. buffering set USED-FOR OGD. buffering set USED-FOR generalization bounds. algorithms USED-FOR differentially private SGD algorithms. differentially private SGD algorithms USED-FOR pairwise learning. stability analysis USED-FOR differentially private SGD algorithms. algorithms CONJUNCTION stability analysis. stability analysis CONJUNCTION algorithms. OtherScientificTerm are loss function, scalability issue, and gradient direction. Metric is storage and computational complexity. ","This paper considers the problem of learning tasks such as bipartite ranking, metric learning, and pairwise learning. It considers two machine learning tasks where the loss function is a function of the number of training samples and the amount of streaming data. The authors propose an approach for streaming data based on the online gradient descent (OGD) algorithm. They show that the scalability issue with OGD is that the gradient direction of the algorithm is non-differentially private, which causes the storage and computational complexity to be high. To address this issue, the authors propose to use stochastic and online gradients for the problem.    The authors provide stability results, optimization, and generalization error bounds for convex and nonsmooth problems, as well as generalization bounds for OGD with and without a buffering set. They also provide algorithms and stability analysis for differentially private SGD algorithms for both algorithms.  The paper is well-written and well-motivated. The techniques used for optimization and the generalization analysis are well-grounded, and the paper is clearly written.","This paper considers the problem of learning tasks such as bipartite ranking, metric learning, and pairwise learning. It considers two machine learning tasks where the loss function is a function of the number of training samples and the amount of streaming data. The authors propose an approach for streaming data based on the online gradient descent (OGD) algorithm. They show that the scalability issue with OGD is that the gradient direction of the algorithm is non-differentially private, which causes the storage and computational complexity to be high. To address this issue, the authors propose to use stochastic and online gradients for the problem.    The authors provide stability results, optimization, and generalization error bounds for convex and nonsmooth problems, as well as generalization bounds for OGD with and without a buffering set. They also provide algorithms and stability analysis for differentially private SGD algorithms for both algorithms.  The paper is well-written and well-motivated. The techniques used for optimization and the generalization analysis are well-grounded, and the paper is clearly written."
16305,SP:cb11dacc930d71a616ee2fbe4acfae030f9dca59,"REDO HYPONYM-OF class - agnostic framework. class - agnostic framework USED-FOR Dynamic Objects. RGBD or calibrated videos USED-FOR REDO. RGBD or calibrated videos USED-FOR class - agnostic framework. rigid motion CONJUNCTION non - rigid motion. non - rigid motion CONJUNCTION rigid motion. non - rigid motion CONJUNCTION articulation. articulation CONJUNCTION non - rigid motion. occlusion CONJUNCTION camera settings. camera settings CONJUNCTION occlusion. unified framework USED-FOR problem setting. articulation HYPONYM-OF object dynamics. rigid motion HYPONYM-OF object dynamics. non - rigid motion HYPONYM-OF object dynamics. aggregated temporal visual cues USED-FOR canonical 4D implicit function. 4D transformation module USED-FOR object dynamics. REDO COMPARE dynamic reconstruction methods. dynamic reconstruction methods COMPARE REDO. Generic are modules, and component. Material is real - world video data 3DPW. ","This paper proposes REDO, a class-agnostic framework for Dynamic Objects based on RGBD or calibrated videos. The core idea is to learn a canonical 4D implicit function based on aggregated temporal visual cues, which can be applied to any object dynamics, including rigid motion, non-rigid motion, and non-ruling motion (accommodating occlusion and camera settings). The paper proposes a unified framework for this problem setting, and introduces two modules. The first component is a 4D transformation module that transforms the object dynamics into a single object, and the second component is an implicit function that is learned from aggregated visual cues. Experiments on real-world video data 3DPW show that REDO outperforms existing dynamic reconstruction methods. ","This paper proposes REDO, a class-agnostic framework for Dynamic Objects based on RGBD or calibrated videos. The core idea is to learn a canonical 4D implicit function based on aggregated temporal visual cues, which can be applied to any object dynamics, including rigid motion, non-rigid motion, and non-ruling motion (accommodating occlusion and camera settings). The paper proposes a unified framework for this problem setting, and introduces two modules. The first component is a 4D transformation module that transforms the object dynamics into a single object, and the second component is an implicit function that is learned from aggregated visual cues. Experiments on real-world video data 3DPW show that REDO outperforms existing dynamic reconstruction methods. "
16354,SP:8ae97752e74b4395774575009031abcb6ba5cea7,"fixed stepsize FEATURE-OF linear stochastic approximation ( LSA ) algorithms. methods USED-FOR machine learning tasks. high probability bounds USED-FOR LSA. covariance matrices PART-OF central limit theorems. Method are random estimates, polynomial concentration bounds, and Gaussian or exponential high probability bounds. OtherScientificTerm are products of matrices, stepsize, and random matrices. Generic is bounds. ","This paper studies the linear stochastic approximation (LSA) algorithms with fixed stepsize. The authors provide high probability bounds for LSA under the assumption that the products of matrices are non-convex. They also provide polynomial concentration bounds for random estimates. The main contribution of this paper is to extend the central limit theorems on the covariance matrices to the case where the stepsize is fixed.   The authors also provide Gaussian or exponential high probability bound for the case when the step size is large enough.  The paper is well-written and well-motivated, and the methods are well-suited for machine learning tasks. The bounds are also well-supported by experiments. ","This paper studies the linear stochastic approximation (LSA) algorithms with fixed stepsize. The authors provide high probability bounds for LSA under the assumption that the products of matrices are non-convex. They also provide polynomial concentration bounds for random estimates. The main contribution of this paper is to extend the central limit theorems on the covariance matrices to the case where the stepsize is fixed.   The authors also provide Gaussian or exponential high probability bound for the case when the step size is large enough.  The paper is well-written and well-motivated, and the methods are well-suited for machine learning tasks. The bounds are also well-supported by experiments. "
16403,SP:86c1e937755e35efafecc09dfe2606ffb1653a41,"options framework USED-FOR temporal abstraction. options framework USED-FOR reinforcement learning. temporal abstraction USED-FOR reinforcement learning. discounted Markov decision processes ( MDPs ) CONJUNCTION average - reward MDPs. average - reward MDPs CONJUNCTION discounted Markov decision processes ( MDPs ). samplebased planning variants PART-OF learning algorithms. intra - option algorithms CONJUNCTION samplebased planning variants. samplebased planning variants CONJUNCTION intra - option algorithms. algorithms CONJUNCTION convergence proofs. convergence proofs CONJUNCTION algorithms. those USED-FOR algorithms. those USED-FOR convergence proofs. Four - Room domain EVALUATE-FOR algorithms. OtherScientificTerm is option - interrupting behavior. Method are discounted, and average - reward formulation. ","This paper proposes a new options framework for temporal abstraction in reinforcement learning. The authors consider discounted Markov decision processes (MDPs) and average-reward MDPs, where the option-interrupting behavior is discounted. They show that existing algorithms (i.e., intra-option algorithms and samplebased planning variants in learning algorithms) can be combined with existing algorithms and convergence proofs, and show that those algorithms can converge to the optimal solution in the Four-Room domain. They also show that the discounted MDP formulation is equivalent to the average reward formulation. ","This paper proposes a new options framework for temporal abstraction in reinforcement learning. The authors consider discounted Markov decision processes (MDPs) and average-reward MDPs, where the option-interrupting behavior is discounted. They show that existing algorithms (i.e., intra-option algorithms and samplebased planning variants in learning algorithms) can be combined with existing algorithms and convergence proofs, and show that those algorithms can converge to the optimal solution in the Four-Room domain. They also show that the discounted MDP formulation is equivalent to the average reward formulation. "
16452,SP:7e4e1e20e7c253d02c6ae58457fb30029f130f0c,"Visual Transformers ( VTs ) COMPARE Convolutional networks ( CNNs ). Convolutional networks ( CNNs ) COMPARE Visual Transformers ( VTs ). CNNs COMPARE VTs. VTs COMPARE CNNs. VTs USED-FOR global relations between image elements. representation capacity FEATURE-OF they. models COMPARE common CNNs. common CNNs COMPARE models. local properties FEATURE-OF visual domain. local properties USED-FOR VTs. local properties PART-OF CNN architectural design. small training set regime FEATURE-OF robustness. robustness EVALUATE-FOR VTs. images USED-FOR auxiliary selfsupervised task. VTs USED-FOR spatial relations. task USED-FOR VT training. task USED-FOR VTs. task CONJUNCTION ( supervised ) training. ( supervised ) training CONJUNCTION task. it PART-OF VTs. accuracy EVALUATE-FOR VTs. VTs EVALUATE-FOR method. accuracy EVALUATE-FOR method. OtherScientificTerm are convolutional inductive bias, and computational overhead. Material is ImageNet. Method is VTs - Drloc. ","This paper presents a theoretical analysis of why Visual Transformers (VTs) perform better than Convolutional networks (CNNs) in terms of the convolutional inductive bias. In particular, the authors show that, unlike CNNs, VTs are able to capture global relations between image elements, and that they have a better representation capacity. The authors also show that the models are more robust than common CNNs when the local properties of the visual domain are incorporated into the CNN architectural design. The paper also shows that the robustness of VTs in the small training set regime is enhanced by the use of an auxiliary selfsupervised task that uses images from the training set as an auxiliary task. Finally, the paper shows that VTs can capture spatial relations more effectively than CNNs.   The paper further proposes a new task for VT training, called Drloc, which is a combination of the task from ImageNet and a variant of the auxiliary self-supervised learning task. The proposed method, called VTs-Drloc, combines the benefits of both the task and (supervised) training, and it is shown that the proposed method improves the accuracy of the VTs on ImageNet with a reduced computational overhead. The experiments show that Drloc outperforms the state-of-the-art.","This paper presents a theoretical analysis of why Visual Transformers (VTs) perform better than Convolutional networks (CNNs) in terms of the convolutional inductive bias. In particular, the authors show that, unlike CNNs, VTs are able to capture global relations between image elements, and that they have a better representation capacity. The authors also show that the models are more robust than common CNNs when the local properties of the visual domain are incorporated into the CNN architectural design. The paper also shows that the robustness of VTs in the small training set regime is enhanced by the use of an auxiliary selfsupervised task that uses images from the training set as an auxiliary task. Finally, the paper shows that VTs can capture spatial relations more effectively than CNNs.   The paper further proposes a new task for VT training, called Drloc, which is a combination of the task from ImageNet and a variant of the auxiliary self-supervised learning task. The proposed method, called VTs-Drloc, combines the benefits of both the task and (supervised) training, and it is shown that the proposed method improves the accuracy of the VTs on ImageNet with a reduced computational overhead. The experiments show that Drloc outperforms the state-of-the-art."
16501,SP:0132ef17585e293b23e9dc45189c0989d829b52a,datasets USED-FOR Label - free alignment. Hyperbolic spaces USED-FOR informative representations of hierarchical data. geometric approach USED-FOR label - free alignment of hierarchical datasets. translation CONJUNCTION scaling. scaling CONJUNCTION translation. scaling CONJUNCTION rotation. rotation CONJUNCTION scaling. Riemannian geometry USED-FOR Lorentz model of hyperbolic space. Riemannian geometry USED-FOR HPA. theoretical properties CONJUNCTION stability. stability CONJUNCTION theoretical properties. stability CONJUNCTION computational efficiency. computational efficiency CONJUNCTION stability. theoretical properties EVALUATE-FOR HPA. gene expression and mass cytometry data FEATURE-OF batch correction tasks. batch correction tasks EVALUATE-FOR its. methods USED-FOR label - free alignment in hyperbolic spaces. data USED-FOR unsupervised batch effect removal. Method is hyperbolic Procrustes analysis ( HPA ). Generic is components. Task is alignment. OtherScientificTerm is hyperbolic spaces. ,"Label-free alignment of hierarchical datasets is a well-studied problem. Hyperbolic spaces provide informative representations of hierarchical data, and this paper proposes a geometric approach for label-free alignments. The authors propose the hyperbolic Procrustes analysis (HPA), which is based on the Riemannian geometry of the Lorentz model of the data.   The key idea of HPA is to decompose the data into three components: alignment, translation, scaling, and rotation.  The authors provide theoretical properties, stability, and computational efficiency for HPA, and evaluate its performance on several batch correction tasks on gene expression and mass cytometry data. The paper also proposes methods to perform label-based alignments in the same way as existing methods.  Experiments on unsupervised batch effect removal on the same data show that the proposed methods perform well.","Label-free alignment of hierarchical datasets is a well-studied problem. Hyperbolic spaces provide informative representations of hierarchical data, and this paper proposes a geometric approach for label-free alignments. The authors propose the hyperbolic Procrustes analysis (HPA), which is based on the Riemannian geometry of the Lorentz model of the data.   The key idea of HPA is to decompose the data into three components: alignment, translation, scaling, and rotation.  The authors provide theoretical properties, stability, and computational efficiency for HPA, and evaluate its performance on several batch correction tasks on gene expression and mass cytometry data. The paper also proposes methods to perform label-based alignments in the same way as existing methods.  Experiments on unsupervised batch effect removal on the same data show that the proposed methods perform well."
16550,SP:3580ac64f09e3021de5d4c92411bcc0f3c5d10f3,"Privacy - protected microdata USED-FOR differentially private algorithm. logarithmic factor FEATURE-OF accuracy. accuracy EVALUATE-FOR differentially private query answering systems. sum query CONJUNCTION point queries. point queries CONJUNCTION sum query. noisy answers USED-FOR sum query. noisy answers USED-FOR point queries. log(d ) factor COMPARE O(d ) factor. O(d ) factor COMPARE log(d ) factor. log(d ) factor USED-FOR point queries. O(d ) factor USED-FOR sum query. lower bounds USED-FOR pure, approximate, and concentrated differential privacy. Material are microdata, and benchmark datasets. Method are uncertainty principle, pure differential privacy, and mitigation strategies. OtherScientificTerm is microdata requirement. ","Privacy-protected microdata is used to train a differentially private algorithm. The paper provides lower bounds on the accuracy of the accuracy (in terms of the logarithmic factor) of differential private query answering systems.   The paper is motivated by the uncertainty principle, which states that there is a trade-off between pure differential privacy and privacy with respect to the amount of microdata required to satisfy the microdata requirement.  The authors provide lower bounds for pure, approximate, and concentrated differential privacy, and show that under certain assumptions on the number of queries, the accuracy can be improved by a factor of O(log(d) for sum query and O(D) for point queries with noisy answers. The authors also provide mitigation strategies to mitigate the effect of the noise in the sum query, and provide lower bound on the O(d ) factor for the point queries under the assumption that the noisy answers are from the same set of queries.  Experiments are conducted on several benchmark datasets, and the results show that the lower bounds are tight.","Privacy-protected microdata is used to train a differentially private algorithm. The paper provides lower bounds on the accuracy of the accuracy (in terms of the logarithmic factor) of differential private query answering systems.   The paper is motivated by the uncertainty principle, which states that there is a trade-off between pure differential privacy and privacy with respect to the amount of microdata required to satisfy the microdata requirement.  The authors provide lower bounds for pure, approximate, and concentrated differential privacy, and show that under certain assumptions on the number of queries, the accuracy can be improved by a factor of O(log(d) for sum query and O(D) for point queries with noisy answers. The authors also provide mitigation strategies to mitigate the effect of the noise in the sum query, and provide lower bound on the O(d ) factor for the point queries under the assumption that the noisy answers are from the same set of queries.  Experiments are conducted on several benchmark datasets, and the results show that the lower bounds are tight."
16599,SP:c0e64dc8acfaed3e4d7745af12fd34003d0e5017,"sparse reward CONJUNCTION inefficient exploration. inefficient exploration CONJUNCTION sparse reward. inefficient exploration FEATURE-OF long - horizon tasks. RL CONJUNCTION planning. planning CONJUNCTION RL. path - planner CONJUNCTION RL agent. RL agent CONJUNCTION path - planner. dense feedback USED-FOR curriculum of tree - structured sub - tasks. RL agent USED-FOR dense feedback. dense feedback USED-FOR path - planner. planner USED-FOR long - horizon task. easy - to - hard curriculum USED-FOR planner. bottom - up traversal of the tree USED-FOR RL agent. RL agent CONJUNCTION planner. planner CONJUNCTION RL agent. mutual training USED-FOR CO - PILOT. SAC CONJUNCTION PPO. PPO CONJUNCTION SAC. RL CONJUNCTION planning ( RRT *. planning ( RRT * CONJUNCTION RL. CO - PILOT COMPARE RL. RL COMPARE CO - PILOT. CO - PILOT COMPARE combination ( SoRB ). combination ( SoRB ) COMPARE CO - PILOT. navigation and continuous control tasks EVALUATE-FOR combination ( SoRB ). SAC HYPONYM-OF RL. navigation and continuous control tasks EVALUATE-FOR CO - PILOT. PPO HYPONYM-OF RL. success rate CONJUNCTION sample efficiency. sample efficiency CONJUNCTION success rate. sample efficiency EVALUATE-FOR CO - PILOT. success rate EVALUATE-FOR CO - PILOT. Method are Goal - conditioned reinforcement learning ( RL ), Planning, environment model, and planning policy. OtherScientificTerm are dense reward / guidance, tree of sub - tasks, sub - tasks, and RRT *. Generic is task. ","Goal-conditioned reinforcement learning (RL) and planning (RRT) are two popular approaches to tackle the problem of sparse reward and inefficient exploration in long-horizon tasks with dense reward/guidance. Planning is typically done by learning a curriculum of tree-structured sub-tasks, where dense feedback is used to guide a path-planner and an RL agent through the curriculum. In this paper, the authors propose CO-PILOT, which combines RL, planning and RL with dense feedback in order to address the issue of inefficient exploration and sparse reward in sparse reward environments.   Planning is done by training a planner that learns a planner for a given task from an easy-to-hard curriculum. The planner is trained in a way that encourages the RL agent to learn a dense feedback to guide the path-plunger and RL agent. The RL agent learns a bottom-up traversal of the tree, and the planner learns a top-down traversal from the top to the bottom. The goal is to learn an environment model that can be used as a guide for RL and planning.  The authors propose to train a RL agent and a planner in parallel, and then use mutual training between RL and planner to train the planner for the long-term goal.  In order to learn the environment model, the planner is first trained on a tree of sub-task, and RL is then trained to learn to explore the sub-trajectories of the planning policy.  Experiments on navigation and continuous control tasks show that CO-based RL outperforms RL (SAC and PPOE) in terms of success rate and sample efficiency, and RRT* in the case of RL (PPO). In the authors also show that the proposed CO-PUILOT outperform RL and RL when RL is used in combination with RRT*. The authors also compare the performance of CO-PIILOT with RL and RRR in navigation and continual control tasks, and show that it outperforms the combination (SoRB).    The paper is well-written and well-motivated. The authors have done a good job of combining RL with planning to improve the success rate, sample efficiency and improve the sample efficiency. ","Goal-conditioned reinforcement learning (RL) and planning (RRT) are two popular approaches to tackle the problem of sparse reward and inefficient exploration in long-horizon tasks with dense reward/guidance. Planning is typically done by learning a curriculum of tree-structured sub-tasks, where dense feedback is used to guide a path-planner and an RL agent through the curriculum. In this paper, the authors propose CO-PILOT, which combines RL, planning and RL with dense feedback in order to address the issue of inefficient exploration and sparse reward in sparse reward environments.   Planning is done by training a planner that learns a planner for a given task from an easy-to-hard curriculum. The planner is trained in a way that encourages the RL agent to learn a dense feedback to guide the path-plunger and RL agent. The RL agent learns a bottom-up traversal of the tree, and the planner learns a top-down traversal from the top to the bottom. The goal is to learn an environment model that can be used as a guide for RL and planning.  The authors propose to train a RL agent and a planner in parallel, and then use mutual training between RL and planner to train the planner for the long-term goal.  In order to learn the environment model, the planner is first trained on a tree of sub-task, and RL is then trained to learn to explore the sub-trajectories of the planning policy.  Experiments on navigation and continuous control tasks show that CO-based RL outperforms RL (SAC and PPOE) in terms of success rate and sample efficiency, and RRT* in the case of RL (PPO). In the authors also show that the proposed CO-PUILOT outperform RL and RL when RL is used in combination with RRT*. The authors also compare the performance of CO-PIILOT with RL and RRR in navigation and continual control tasks, and show that it outperforms the combination (SoRB).    The paper is well-written and well-motivated. The authors have done a good job of combining RL with planning to improve the success rate, sample efficiency and improve the sample efficiency. "
16648,SP:9911693a04a300b5a93634fb0267ef83e5489d77,"black box explanations USED-FOR model credibility. techniques USED-FOR explanations. hyper - parameter tuning USED-FOR methods. Bayesian framework USED-FOR generating local explanations. LIME CONJUNCTION KernelSHAP. KernelSHAP CONJUNCTION LIME. credible intervals USED-FOR feature importances. real world datasets CONJUNCTION user studies. user studies CONJUNCTION real world datasets. OtherScientificTerm are local explanations, and feature importance. Generic are framework, and uncertainty. ","This paper studies the problem of black box explanations for model credibility. The authors propose a Bayesian framework for generating local explanations, and propose two techniques for generating explanations that are robust to hyper-parameter tuning. The framework is based on the idea that local explanations are a function of feature importance, and the authors show that LIME and KernelSHAP can be seen as credible intervals for feature importances. The paper also shows that the proposed framework is robust to uncertainty. Experiments are conducted on several real world datasets and user studies.","This paper studies the problem of black box explanations for model credibility. The authors propose a Bayesian framework for generating local explanations, and propose two techniques for generating explanations that are robust to hyper-parameter tuning. The framework is based on the idea that local explanations are a function of feature importance, and the authors show that LIME and KernelSHAP can be seen as credible intervals for feature importances. The paper also shows that the proposed framework is robust to uncertainty. Experiments are conducted on several real world datasets and user studies."
16697,SP:5efb4b81bd37c70640e8768e9dfb5bba14a0cfb8,"energy - efficient neural networks CONJUNCTION hardware accelerations. hardware accelerations CONJUNCTION energy - efficient neural networks. multiplications PART-OF convolutional neural networks ( CNNs ). Adder neural networks ( ANNs ) USED-FOR low energy cost. ANNs COMPARE CNNs. CNNs COMPARE ANNs. accuracy EVALUATE-FOR ANNs. accuracy EVALUATE-FOR CNNs. ANNs CONJUNCTION CNNs. CNNs CONJUNCTION ANNs. knowledge distillation HYPONYM-OF training tricks. filters CONJUNCTION features. features CONJUNCTION filters. similarity measurement FEATURE-OF features. similarity measurement FEATURE-OF filters. similarity measurement USED-FOR property difference. unordered heavy tails PART-OF ANNs. classification EVALUATE-FOR ANNs. feature distributions PART-OF loss function. method USED-FOR heavy tails. angle - based constraint USED-FOR diversity of tails. method USED-FOR ANNs. heavy tails PART-OF ANNs. angle - based constraint USED-FOR distribution parameters. classifier USED-FOR method. approach USED-FOR ANNs. benchmarks EVALUATE-FOR approach. benchmarks EVALUATE-FOR distributions. OtherScientificTerm are fatter tails, feature space, Multivariate Skew Laplace distributions, and ANN features. ","This paper proposes Adder neural networks (ANNs) to reduce the energy cost of training energy-efficient neural networks and hardware accelerations. The authors show that ANNs have fatter tails than CNNs, which is due to the multiplications in convolutional neural networks, but also due to fatter weights in the feature space. They show that Adder ANNs (and CNNs) can achieve a low energy cost while maintaining the same accuracy as CNNs.   The authors propose two training tricks: (1) knowledge distillation, which distills training tricks into a single training step, and (2) Multivariate Skew Laplace distributions (MSP) which is based on the property difference between the similarity measurement of the filters and the features, and the properties of the feature distributions in the loss function.  They also show that unordered heavy tails in ANNs and CNNs are the main reason why ANNs perform better in classification.  The paper also proposes a method to remove heavy tails from ANNs by adding an angle-based constraint to encourage the diversity of tails. The proposed method is applied to the distribution parameters of each classifier. The paper shows that the proposed approach improves the performance of ANNs on a number of benchmarks, and that the distributions of the distributions are more diverse than those of CNNs and ANNs. Finally, the method is shown to be more robust to changes in the number of filters and features.","This paper proposes Adder neural networks (ANNs) to reduce the energy cost of training energy-efficient neural networks and hardware accelerations. The authors show that ANNs have fatter tails than CNNs, which is due to the multiplications in convolutional neural networks, but also due to fatter weights in the feature space. They show that Adder ANNs (and CNNs) can achieve a low energy cost while maintaining the same accuracy as CNNs.   The authors propose two training tricks: (1) knowledge distillation, which distills training tricks into a single training step, and (2) Multivariate Skew Laplace distributions (MSP) which is based on the property difference between the similarity measurement of the filters and the features, and the properties of the feature distributions in the loss function.  They also show that unordered heavy tails in ANNs and CNNs are the main reason why ANNs perform better in classification.  The paper also proposes a method to remove heavy tails from ANNs by adding an angle-based constraint to encourage the diversity of tails. The proposed method is applied to the distribution parameters of each classifier. The paper shows that the proposed approach improves the performance of ANNs on a number of benchmarks, and that the distributions of the distributions are more diverse than those of CNNs and ANNs. Finally, the method is shown to be more robust to changes in the number of filters and features."
16746,SP:cbccb65457564992d534504c0d060da44cafce8c,"gradient descent phenomenon USED-FOR learning proclivity. learning proclivity FEATURE-OF over - parameterized neural networks. features USED-FOR task. feature imbalances FEATURE-OF neural networks. learning dynamics USED-FOR imbalance. learning dynamics USED-FOR gradient descent. guarantees USED-FOR regularization method. formalism USED-FOR regularization method. regularization method USED-FOR feature learning dynamics. formalism USED-FOR guarantees. accuracy EVALUATE-FOR regularization method. robustness EVALUATE-FOR regularization method. OtherScientificTerm are Gradient Starvation, predictive features, statistical structure, and gradient starvation. Metric is cross - entropy loss. Method is Dynamical Systems theory. ","Gradient Starvation: Gradient descent phenomenon is a well-studied phenomenon in the learning proclivity of over-parameterized neural networks. Gradient starvation happens when the cross-entropy loss of a neural network is large enough to cause the predictive features of the network to have a statistical structure that is non-trivial to compute. The authors show that gradient starvation happens in the presence of feature imbalances in neural networks, and that the learning dynamics of gradient descent is responsible for this imbalance. They also provide guarantees for a regularization method that is based on this formalism to regularize the feature learning dynamics and improve the accuracy of the regularized network.   ","Gradient Starvation: Gradient descent phenomenon is a well-studied phenomenon in the learning proclivity of over-parameterized neural networks. Gradient starvation happens when the cross-entropy loss of a neural network is large enough to cause the predictive features of the network to have a statistical structure that is non-trivial to compute. The authors show that gradient starvation happens in the presence of feature imbalances in neural networks, and that the learning dynamics of gradient descent is responsible for this imbalance. They also provide guarantees for a regularization method that is based on this formalism to regularize the feature learning dynamics and improve the accuracy of the regularized network.   "
16795,SP:8f6fe37cb0a332b66e10cc00261a44622841c8c6,"Deep reinforcement learning USED-FOR superhuman AI. superhuman AI USED-FOR competitive games. Go and StarCraft HYPONYM-OF competitive games. learning techniques USED-FOR AI teammate. AI teammate USED-FOR human - machine collaborative games. AI teammates COMPARE those. those COMPARE AI teammates. subjective metrics of trust EVALUATE-FOR those. objective team performance EVALUATE-FOR AI teammates. AI agents PART-OF cooperative card game Hanabi. interpretability CONJUNCTION trust. trust CONJUNCTION interpretability. teamwork CONJUNCTION interpretability. interpretability CONJUNCTION teamwork. interpretability HYPONYM-OF subjective measures. trust HYPONYM-OF subjective measures. teamwork HYPONYM-OF subjective measures. AI design CONJUNCTION reinforcement learning benchmarking. reinforcement learning benchmarking CONJUNCTION AI design. subjective metrics of human - AI teaming CONJUNCTION objective task performance. objective task performance CONJUNCTION subjective metrics of human - AI teaming. Method are rule - based and learning - based agents, rule - based AI teammate ( SmartBot ), and learning - based agent. OtherScientificTerm are game score, and human - AI team performance. Metric is subjective metrics. ","Deep reinforcement learning is used to train superhuman AI to play competitive games (e.g., Go and StarCraft). The paper shows that AI teammates trained with learning techniques outperform those trained with subjective metrics of trust. The paper also shows that a rule-based AI teammate (SmartBot) outperforms the human-machine collaborative games when the game score is low. The authors also show that the AI teammates outperform AI teammates when the objective team performance is high.   The paper is well-written, well-motivated, and well-structured. The AI agents are trained in a cooperative card game Hanabi, where the game is played between humans and AI agents. The game is a cooperative cooperative game, and the AI agents can be rule- based and learning-based agents. There are three subjective measures of human-AI teaming: teamwork, interpretability, and trust. In this paper, the authors show that AI design and reinforcement learning benchmarking can be used to compare AI design to human design, and show that rule-free AI design outperforms AI design when the human design is good enough. They also find that the human AI team performance improves when the AI teammate is good at a task, and that the learning based AI teammate performs better when the learning- based agent is bad at the task. ","Deep reinforcement learning is used to train superhuman AI to play competitive games (e.g., Go and StarCraft). The paper shows that AI teammates trained with learning techniques outperform those trained with subjective metrics of trust. The paper also shows that a rule-based AI teammate (SmartBot) outperforms the human-machine collaborative games when the game score is low. The authors also show that the AI teammates outperform AI teammates when the objective team performance is high.   The paper is well-written, well-motivated, and well-structured. The AI agents are trained in a cooperative card game Hanabi, where the game is played between humans and AI agents. The game is a cooperative cooperative game, and the AI agents can be rule- based and learning-based agents. There are three subjective measures of human-AI teaming: teamwork, interpretability, and trust. In this paper, the authors show that AI design and reinforcement learning benchmarking can be used to compare AI design to human design, and show that rule-free AI design outperforms AI design when the human design is good enough. They also find that the human AI team performance improves when the AI teammate is good at a task, and that the learning based AI teammate performs better when the learning- based agent is bad at the task. "
16844,SP:2a05e333fc1a14057515ef3addde9a40152373db,"visual question generation ( VQG ) USED-FOR human - like neural questions. image CONJUNCTION side information. side information CONJUNCTION image. side information USED-FOR human - like neural questions. image USED-FOR human - like neural questions. double visual and answer hints USED-FOR model. rule - based similarity matching method USED-FOR candidate visual hints. learning approach USED-FOR double - hints based VQG. weakly supervised learning problem USED-FOR learning approach. benchmark datasets EVALUATE-FOR method. automatic machine metrics CONJUNCTION human evaluation. human evaluation CONJUNCTION automatic machine metrics. method COMPARE approaches. approaches COMPARE method. benchmark datasets EVALUATE-FOR approaches. automatic machine metrics HYPONYM-OF metrics. human evaluation HYPONYM-OF metrics. human evaluation EVALUATE-FOR method. metrics EVALUATE-FOR approaches. metrics EVALUATE-FOR method. automatic machine metrics EVALUATE-FOR method. OtherScientificTerm are uninformative and non - referential questions, visual hints, salient visual regions of interest, predicted salient visual regions of interest, and quality of predicted visual hints. Generic is they. Method is generation procedure. ","This paper proposes a new method for visual question generation (VQG) that uses visual hints to guide the generation of human-like neural questions from an image and side information. The idea is to use visual hints for both uninformative and non-referential questions. The model is trained with double visual and answer hints, where the visual hints are generated from salient visual regions of interest and the answer is generated from the same region of interest. The authors propose a rule-based similarity matching method to identify candidate visual hints, and they also propose a learning approach for double-hints based VQG based on a weakly supervised learning problem. The proposed method is evaluated on three benchmark datasets and compared with several existing approaches on three different metrics: automatic machine metrics, human evaluation, and a human evaluation of the quality of the suggested visual hints. The results show that the proposed method outperforms the previous approaches in all three metrics. The paper also shows that the generation procedure is more robust to the number of visual hints and the amount of side information, and that the predicted visual regions are more likely to be in the predicted salient visual region. The quality of predicted visual hints is also improved.","This paper proposes a new method for visual question generation (VQG) that uses visual hints to guide the generation of human-like neural questions from an image and side information. The idea is to use visual hints for both uninformative and non-referential questions. The model is trained with double visual and answer hints, where the visual hints are generated from salient visual regions of interest and the answer is generated from the same region of interest. The authors propose a rule-based similarity matching method to identify candidate visual hints, and they also propose a learning approach for double-hints based VQG based on a weakly supervised learning problem. The proposed method is evaluated on three benchmark datasets and compared with several existing approaches on three different metrics: automatic machine metrics, human evaluation, and a human evaluation of the quality of the suggested visual hints. The results show that the proposed method outperforms the previous approaches in all three metrics. The paper also shows that the generation procedure is more robust to the number of visual hints and the amount of side information, and that the predicted visual regions are more likely to be in the predicted salient visual region. The quality of predicted visual hints is also improved."
16908,SP:15756d6ef47b39ded404acea2135c93bd5ee1062,"Label noise CONJUNCTION class imbalance. class imbalance CONJUNCTION Label noise. label noise CONJUNCTION class imbalance. class imbalance CONJUNCTION label noise. Generalized Data Weighting ( GDW ) USED-FOR class imbalance. Generalized Data Weighting ( GDW ) USED-FOR label noise. class level FEATURE-OF gradients. gradients USED-FOR Generalized Data Weighting ( GDW ). GDW USED-FOR loss gradient. chain rule USED-FOR GDW. GDW COMPARE instance weighting methods. instance weighting methods COMPARE GDW. GDW USED-FOR class - level weights. computational cost EVALUATE-FOR instance weighting methods. gradient descent step USED-FOR class - level weights. class - level weights USED-FOR GDW. gradient descent step USED-FOR GDW. GDW COMPARE state - of - the - art methods. state - of - the - art methods COMPARE GDW. uniform noise setting FEATURE-OF CIFAR10. uniform noise setting EVALUATE-FOR GDW. CIFAR10 EVALUATE-FOR GDW. Material are real - world datasets, and clean and unbiased data. Generic is methods. OtherScientificTerm are class - level information, class - level gradients, and intermediate gradients. ","Label noise, class imbalance, and class imbalance are common problems in real-world datasets. The authors propose Generalized Data Weighting (GDW) to address the problem of class imbalance by using gradients at the class level. GDW uses the gradients of the class-level to compute the loss gradient for each class, and then applies a chain rule to learn the weights of each class.   The authors show that GDW outperforms existing instance weighting methods in terms of computational cost and performance on clean and unbiased data. They also show that the gradient descent step of GDW can be used to learn class-levels of the weights, and GDW is shown to be more efficient than existing methods in the sense that it does not require any additional training data.  In addition, GDW also shows that in the uniform noise setting of CIFAR10, in which GDW performs better than state-of-the-art methods, the authors also demonstrate that the proposed method is more robust to class imbalance and label noise.  The main contribution of the paper is that the authors propose to use class- level information to learn weights at the level of the classes, which is a natural way to incorporate class imbalance into existing methods. The paper also proposes to use a gradient-descent-based method to learn a set of intermediate gradients for the weights at each layer. ","Label noise, class imbalance, and class imbalance are common problems in real-world datasets. The authors propose Generalized Data Weighting (GDW) to address the problem of class imbalance by using gradients at the class level. GDW uses the gradients of the class-level to compute the loss gradient for each class, and then applies a chain rule to learn the weights of each class.   The authors show that GDW outperforms existing instance weighting methods in terms of computational cost and performance on clean and unbiased data. They also show that the gradient descent step of GDW can be used to learn class-levels of the weights, and GDW is shown to be more efficient than existing methods in the sense that it does not require any additional training data.  In addition, GDW also shows that in the uniform noise setting of CIFAR10, in which GDW performs better than state-of-the-art methods, the authors also demonstrate that the proposed method is more robust to class imbalance and label noise.  The main contribution of the paper is that the authors propose to use class- level information to learn weights at the level of the classes, which is a natural way to incorporate class imbalance into existing methods. The paper also proposes to use a gradient-descent-based method to learn a set of intermediate gradients for the weights at each layer. "
16972,SP:7a8f56a01bec51ebf70d9ff689005a62cccfe5c6,"it USED-FOR embodied agents. language USED-FOR embodied agents. sensorimotor modalities USED-FOR language. embodied agent FEATURE-OF spatio - temporal descriptions of behavioral traces. time - extended predicates CONJUNCTION spatio - temporal references. spatio - temporal references CONJUNCTION time - extended predicates. time - extended predicates PART-OF descriptions. spatio - temporal references PART-OF descriptions. architectural biases USED-FOR task. attention computations USED-FOR latter. multimodal Transformer architectures HYPONYM-OF models. generalization CONJUNCTION generalization. generalization CONJUNCTION generalization. generalization EVALUATE-FOR models. randomly held - out sentences USED-FOR generalization. generalization EVALUATE-FOR models. grammar primitives USED-FOR generalization. generalization HYPONYM-OF generalization. generalization HYPONYM-OF generalization. object identity FEATURE-OF attention computation. attention computation PART-OF Transformers. object identity USED-FOR generalization. code CONJUNCTION pretrained models. pretrained models CONJUNCTION code. OtherScientificTerm are Language, grounded language, spatio - temporal linguistic concepts, and truth function. Task are spatio - temporal language grounding task, and language - guided autonomous embodied agents. ","This paper proposes a new spatio-temporal language grounding task. Language grounding is the task of learning a language from sensorimotor modalities that can be used to guide embodied agents to learn a grounded language. The paper proposes to use language to guide an embodied agent to learn spatiotemporal descriptions of behavioral traces of the embodied agent. The descriptions are composed of time-extended predicates and spatio - temporal references. The authors propose two models, multimodal Transformer architectures, for this task. The architectural biases of the two models are that the former uses attention computations, while the latter uses the truth function. The generalization performance of the models is evaluated on generalization on randomly held-out sentences, and generalization using grammar primitives. The results show that Transformers with attention computation that depends on object identity are able to generalize better than models that do not rely on this information.    The paper is well-written, well-motivated, and well-structured. The idea of language-guided autonomous embodied agents is interesting, and the paper is clearly written. However, there are a few issues in the paper:   1. It is not clear to me that the paper addresses the problem of generalization in generalization.  2. There is a lack of code and pretrained models.  3. It does not seem to be clear that there is any connection between the code and the generalization results.  4. It would be good to see a more detailed analysis of the generalizability of the proposed generalization properties of the model. ","This paper proposes a new spatio-temporal language grounding task. Language grounding is the task of learning a language from sensorimotor modalities that can be used to guide embodied agents to learn a grounded language. The paper proposes to use language to guide an embodied agent to learn spatiotemporal descriptions of behavioral traces of the embodied agent. The descriptions are composed of time-extended predicates and spatio - temporal references. The authors propose two models, multimodal Transformer architectures, for this task. The architectural biases of the two models are that the former uses attention computations, while the latter uses the truth function. The generalization performance of the models is evaluated on generalization on randomly held-out sentences, and generalization using grammar primitives. The results show that Transformers with attention computation that depends on object identity are able to generalize better than models that do not rely on this information.    The paper is well-written, well-motivated, and well-structured. The idea of language-guided autonomous embodied agents is interesting, and the paper is clearly written. However, there are a few issues in the paper:   1. It is not clear to me that the paper addresses the problem of generalization in generalization.  2. There is a lack of code and pretrained models.  3. It does not seem to be clear that there is any connection between the code and the generalization results.  4. It would be good to see a more detailed analysis of the generalizability of the proposed generalization properties of the model. "
17036,SP:3d4a9d439bc84c3b0e6600f6985a23bdf95cd67f,"detecting CONJUNCTION tracking. tracking CONJUNCTION detecting. tracking USED-FOR Multiple object tracking and segmentation. detecting USED-FOR Multiple object tracking and segmentation. single frame predictions USED-FOR segmentation mask. temporal dimension USED-FOR association problem. approaches USED-FOR association problem. temporal dimension USED-FOR approaches. Prototypical Cross - Attention Network ( PCAN ) USED-FOR online multiple object tracking and segmentation. rich spatio - temporal information USED-FOR online multiple object tracking and segmentation. rich spatio - temporal information USED-FOR Prototypical Cross - Attention Network ( PCAN ). cross - attention USED-FOR rich information. cross - attention USED-FOR PCAN. prototypical appearance module USED-FOR contrastive foreground and background prototypes. PCAN USED-FOR contrastive foreground and background prototypes. prototypical appearance module USED-FOR PCAN. PCAN COMPARE video instance tracking and segmentation competition winners. video instance tracking and segmentation competition winners COMPARE PCAN. Youtube - VIS and BDD100 K datasets EVALUATE-FOR video instance tracking and segmentation competition winners. Youtube - VIS and BDD100 K datasets EVALUATE-FOR PCAN. OtherScientificTerm are space - time memory, and prototypes. ","This paper proposes a Prototypical Cross-Attention Network (PCAN) for online multiple object tracking and segmentation based on rich spatio-temporal information, which is useful for detecting, tracking, and tracking with a single frame predictions. Previous approaches to the association problem are based on the temporal dimension of the objects in the space-time memory, while PCAN uses cross-attention to capture the rich information. PCAN also uses a prototypical appearance module to learn contrastive foreground and background prototypes, which are then used to learn the segmentation mask from the single frame to the next frame. Experiments on Youtube-VIS and BDD100K datasets show that PCAN outperforms the video instance tracking & segmentation competition winners on the Youtube-VIS, Youtube-BDD100K, and Youtube-DDD100k datasets. ","This paper proposes a Prototypical Cross-Attention Network (PCAN) for online multiple object tracking and segmentation based on rich spatio-temporal information, which is useful for detecting, tracking, and tracking with a single frame predictions. Previous approaches to the association problem are based on the temporal dimension of the objects in the space-time memory, while PCAN uses cross-attention to capture the rich information. PCAN also uses a prototypical appearance module to learn contrastive foreground and background prototypes, which are then used to learn the segmentation mask from the single frame to the next frame. Experiments on Youtube-VIS and BDD100K datasets show that PCAN outperforms the video instance tracking & segmentation competition winners on the Youtube-VIS, Youtube-BDD100K, and Youtube-DDD100k datasets. "
17100,SP:1175ad16382b349ab1a39895150172d266abe571,optimization USED-FOR deep learning. it USED-FOR gradient descent. approximate numerical solution USED-FOR initial value problem of gradient flow. curvature FEATURE-OF gradient flow trajectory. gradient descent USED-FOR initial value problem of gradient flow. gradient descent USED-FOR approximate numerical solution. homogeneous activations FEATURE-OF deep neural networks. favorable curvature FEATURE-OF gradient flow trajectories. gradient descent USED-FOR they. gradient descent USED-FOR global minimum. deep linear neural networks USED-FOR gradient flow. random initialization USED-FOR gradient descent. gradient descent COMPARE gradient flow. gradient flow COMPARE gradient descent. deep neural networks USED-FOR gradient descent. step size FEATURE-OF gradient descent. OtherScientificTerm is Gradient flow. Metric is computational efficiency. Method is gradient flows. ,"Gradient flow is a well-studied optimization technique for deep learning, and it is well-known that it can be used to solve the initial value problem of gradient flow. However, it is not well known that gradient descent can provide an approximate numerical solution to the initial values of the gradient flow trajectory with respect to the curvature of the input. This paper studies gradient flow trajectories with favorable curvature and shows that they can be approximated by gradient descent with homogeneous activations of deep neural networks. In particular, the authors show that gradient flow is approximated with deep linear neural networks, and they show that the gradient descent converges to a global minimum with a small step size. They also show that under certain conditions, gradient descent is equivalent to gradient flow with a random initialization, and that the step size of gradient descent in the case of deep networks can be much smaller than gradient descent for gradient flow in the presence of homogeneous activation functions.    The paper is well written and well-motivated. Gradient flow can be seen as a useful tool for improving the computational efficiency of gradient flows, and the authors have made a number of contributions. ","Gradient flow is a well-studied optimization technique for deep learning, and it is well-known that it can be used to solve the initial value problem of gradient flow. However, it is not well known that gradient descent can provide an approximate numerical solution to the initial values of the gradient flow trajectory with respect to the curvature of the input. This paper studies gradient flow trajectories with favorable curvature and shows that they can be approximated by gradient descent with homogeneous activations of deep neural networks. In particular, the authors show that gradient flow is approximated with deep linear neural networks, and they show that the gradient descent converges to a global minimum with a small step size. They also show that under certain conditions, gradient descent is equivalent to gradient flow with a random initialization, and that the step size of gradient descent in the case of deep networks can be much smaller than gradient descent for gradient flow in the presence of homogeneous activation functions.    The paper is well written and well-motivated. Gradient flow can be seen as a useful tool for improving the computational efficiency of gradient flows, and the authors have made a number of contributions. "
17164,SP:b8412e9ce82ce92125fe7cd3aff7bea8b906d16e,"multi - armed bandits USED-FOR delayed and longterm impact of actions. action history USED-FOR learning. regret EVALUATE-FOR algorithm. OtherScientificTerm are delayed impact of actions, arm rewards, feedback loop, and delayed impacts of historical actions. Generic are setting, and techniques. Task is bandit setting. Metric is matching regret lower bound. Method are bandit literature, and fair algorithms. ","This paper studies the problem of multi-armed bandits in which the delayed and long-term impact of actions are considered in the context of the multi-arm bandit setting. In this setting, there are multiple arms and each arm has a different delayed impact on the reward, and the goal is to minimize the gap between the delayed impact of the previous arm and the current arm rewards. The authors propose two techniques to address this problem. The first is to use action history to guide the learning, which is based on the fact that the delayed impacts of historical actions can be considered as a feedback loop. The second is to learn a matching regret lower bound, which matches the matching lower bound in the bandit literature. The regret of the proposed algorithm is shown to be $O(\sqrt{T})$-approx.   The authors also provide a theoretical analysis of the regret of their algorithm, which shows that their algorithm achieves a regret that matches the lower bound of the fair algorithms in the literature. ","This paper studies the problem of multi-armed bandits in which the delayed and long-term impact of actions are considered in the context of the multi-arm bandit setting. In this setting, there are multiple arms and each arm has a different delayed impact on the reward, and the goal is to minimize the gap between the delayed impact of the previous arm and the current arm rewards. The authors propose two techniques to address this problem. The first is to use action history to guide the learning, which is based on the fact that the delayed impacts of historical actions can be considered as a feedback loop. The second is to learn a matching regret lower bound, which matches the matching lower bound in the bandit literature. The regret of the proposed algorithm is shown to be $O(\sqrt{T})$-approx.   The authors also provide a theoretical analysis of the regret of their algorithm, which shows that their algorithm achieves a regret that matches the lower bound of the fair algorithms in the literature. "
17228,SP:9c1d678dff5f609197dc3cfb67b841827f4a439a,"end - to - end solution USED-FOR video instance segmentation ( VIS ). transformers USED-FOR end - to - end solution. per - clip pipeline COMPARE per - frame methods. per - frame methods COMPARE per - clip pipeline. per - clip models USED-FOR frame - to - frame communications. benchmark sets EVALUATE-FOR method. method USED-FOR near - online inference. Method are Inter - frame Communication Transformers ( IFC ), and offline inference. OtherScientificTerm are overhead, concise memory tokens, and features. Material is YouTube - VIS 2019 val set. ","This paper proposes Inter-frame Communication Transformers (IFC), an end-to-end solution for video instance segmentation (VIS) using transformers. The key idea is to use a per-clip pipeline instead of per-frame methods, which avoids the overhead of each frame and allows for concise memory tokens to be stored. The authors show that the per-clip models are able to handle the issue of frame to frame communications, and that the proposed method is able to perform near-online inference on standard benchmark sets, and offline inference on the YouTube-VIS 2019 val set. The paper also shows that the features learned by IFC are more robust to changes in the number of frames and the amount of data.","This paper proposes Inter-frame Communication Transformers (IFC), an end-to-end solution for video instance segmentation (VIS) using transformers. The key idea is to use a per-clip pipeline instead of per-frame methods, which avoids the overhead of each frame and allows for concise memory tokens to be stored. The authors show that the per-clip models are able to handle the issue of frame to frame communications, and that the proposed method is able to perform near-online inference on standard benchmark sets, and offline inference on the YouTube-VIS 2019 val set. The paper also shows that the features learned by IFC are more robust to changes in the number of frames and the amount of data."
17292,SP:6c922eaa358f6fb9771690b1240e4f6f08a35b69,"vector - space representation USED-FOR machine learning applications. vector - space representation USED-FOR graph analysis. graph analysis CONJUNCTION machine learning applications. machine learning applications CONJUNCTION graph analysis. Graph embedding USED-FOR vector - space representation. Graph embedding USED-FOR graph. sampling of context nodes USED-FOR graph embedding methods. random walks USED-FOR sampling of context nodes. random walks HYPONYM-OF biased sampler. degree FEATURE-OF random walks. residual2vec HYPONYM-OF graph embedding method. random walks ’ bias USED-FOR graph embedding. random graphs USED-FOR residual2vec. link prediction CONJUNCTION clustering. clustering CONJUNCTION link prediction. debiasing USED-FOR structural properties. structural properties PART-OF graph embedding. clustering EVALUATE-FOR debiasing. link prediction EVALUATE-FOR debiasing. OtherScientificTerm are structural properties of graphs, node, and structural biases in graphs. Task is graph representation learning. ","Graph embedding is a vector-space representation that is widely used in graph analysis and machine learning applications. Graph embedding can be used to represent any graph, but the structural properties of graphs are often not well-studied. For example, the sampling of context nodes in existing graph embedding methods is based on the biased sampler, which is called random walks. In this paper, the authors show that random walks (random walks with a certain degree) can be seen as a way to debiasing the structural biases in graphs. The authors propose a new graph embeddings method, called residual2vec, that debiases the random walks’ bias in the graph representation learning. They show that the debiased random walks have a degree that depends on the degree of the node. They also show that a variant of the proposed residual 2vec method, which uses random graphs, can be learned from random walks as well. Experiments on link prediction and clustering are conducted to demonstrate that the de-biasing of structural properties in a graph is beneficial.   ","Graph embedding is a vector-space representation that is widely used in graph analysis and machine learning applications. Graph embedding can be used to represent any graph, but the structural properties of graphs are often not well-studied. For example, the sampling of context nodes in existing graph embedding methods is based on the biased sampler, which is called random walks. In this paper, the authors show that random walks (random walks with a certain degree) can be seen as a way to debiasing the structural biases in graphs. The authors propose a new graph embeddings method, called residual2vec, that debiases the random walks’ bias in the graph representation learning. They show that the debiased random walks have a degree that depends on the degree of the node. They also show that a variant of the proposed residual 2vec method, which uses random graphs, can be learned from random walks as well. Experiments on link prediction and clustering are conducted to demonstrate that the de-biasing of structural properties in a graph is beneficial.   "
17356,SP:851eac96135b577a5014166edcb43db6a190cf4b,"local differential privacy FEATURE-OF estimating non - linear functionals of discrete distributions. quadratic risk USED-FOR power sum functional. plug - in type estimators COMPARE MLE. MLE COMPARE plug - in type estimators. two - step procedure USED-FOR sequentially interactive case. α - LDP mechanisms CONJUNCTION estimators. estimators CONJUNCTION α - LDP mechanisms. private samples USED-FOR estimators. OtherScientificTerm are discrete distribution, non - interactive case, and privacy constraint. Method are privacy mechanisms ( PM ), multinomial model, and Gaussian model. ","This paper studies the problem of estimating non-linear functionals of discrete distributions under local differential privacy. The authors consider the setting where the discrete distribution is non-interactive and the target distribution is a multinomial model. In this setting, the authors consider privacy mechanisms (PM) and show that the power sum functional can be estimated with quadratic risk, which is a non-trivial result in the non-intrusive case. In the sequentially interactive case, they show that plug-in type estimators are asymptotically optimal, while MLE is optimal under a two-step procedure. They also show that under a certain privacy constraint, they can estimate the power of the power under the multinomials in the case of a Gaussian model. Finally, they provide a connection between the α-LDP mechanisms and the proposed estimators with private samples. ","This paper studies the problem of estimating non-linear functionals of discrete distributions under local differential privacy. The authors consider the setting where the discrete distribution is non-interactive and the target distribution is a multinomial model. In this setting, the authors consider privacy mechanisms (PM) and show that the power sum functional can be estimated with quadratic risk, which is a non-trivial result in the non-intrusive case. In the sequentially interactive case, they show that plug-in type estimators are asymptotically optimal, while MLE is optimal under a two-step procedure. They also show that under a certain privacy constraint, they can estimate the power of the power under the multinomials in the case of a Gaussian model. Finally, they provide a connection between the α-LDP mechanisms and the proposed estimators with private samples. "
17420,SP:a0408b54f88a26479f33f36bb27e0a675f637ccd,"directed graph USED-FOR learner ’s feedback. filtering CONJUNCTION label efficient classification. label efficient classification CONJUNCTION filtering. feedback graphs USED-FOR applications. label efficient classification HYPONYM-OF applications. filtering HYPONYM-OF applications. GAPPLETRON HYPONYM-OF online multiclass algorithm. arbitrary feedback graphs USED-FOR online multiclass algorithm. surrogate regret bounds USED-FOR algorithm. domination number HYPONYM-OF graph - theoretic parameter. full information case FEATURE-OF GAPPLETRON. surrogate regret EVALUATE-FOR GAPPLETRON. synthetic data EVALUATE-FOR algorithm. algorithm COMPARE baselines. baselines COMPARE algorithm. synthetic data EVALUATE-FOR baselines. feedback graphs EVALUATE-FOR algorithm. Task is online multiclass classification. OtherScientificTerm are bandit feedback, surrogate losses, prediction space, and time horizon. Generic are bounds, lower bound, and upper bounds. ","This paper considers the problem of online multiclass classification, where the learner’s feedback is given as a directed graph. Two applications of feedback graphs are considered: filtering and label efficient classification. The authors propose GAPPLETRON, which is an efficient and scalable algorithm based on arbitrary feedback graphs, and provide surrogate regret bounds for the algorithm. The main idea is to use bandit feedback as a surrogate loss, and then use the surrogate losses as a regularizer for the prediction space. In particular, the authors consider a graph-theoretic parameter called the ""domination number"", which is a measure of the dominance of a node in a prediction space over a given time horizon. They show that under certain assumptions, GAPPLON achieves a surrogate regret of $O(\sqrt{T})$ for the full information case, and the bounds are tighter than the lower bound. The algorithm is evaluated on synthetic data, and compared with several baselines on different feedback graphs. The results show that the proposed algorithm performs better than the baselines. ","This paper considers the problem of online multiclass classification, where the learner’s feedback is given as a directed graph. Two applications of feedback graphs are considered: filtering and label efficient classification. The authors propose GAPPLETRON, which is an efficient and scalable algorithm based on arbitrary feedback graphs, and provide surrogate regret bounds for the algorithm. The main idea is to use bandit feedback as a surrogate loss, and then use the surrogate losses as a regularizer for the prediction space. In particular, the authors consider a graph-theoretic parameter called the ""domination number"", which is a measure of the dominance of a node in a prediction space over a given time horizon. They show that under certain assumptions, GAPPLON achieves a surrogate regret of $O(\sqrt{T})$ for the full information case, and the bounds are tighter than the lower bound. The algorithm is evaluated on synthetic data, and compared with several baselines on different feedback graphs. The results show that the proposed algorithm performs better than the baselines. "
17484,SP:490262589efce6fb10b913431ec6db8d4e5b2dec,"threshold cut USED-FOR single dimension ( feature ). decision tree USED-FOR it. decision tree USED-FOR k - clustering. algorithm USED-FOR explainable clustering. O(k ) CONJUNCTION O(k ). O(k ) CONJUNCTION O(k ). Ω(k ) lower bound USED-FOR k - means. Ω(log k ) lower bound USED-FOR k - medians. Ω(log k ) lower bound CONJUNCTION Ω(k ) lower bound. Ω(k ) lower bound CONJUNCTION Ω(log k ) lower bound. upper bounds COMPARE O(k ). O(k ) COMPARE upper bounds. O(k ) HYPONYM-OF upper bounds. OtherScientificTerm are cluster, k - means objective, upper and lower bounds, and ` p - norms. Metric is k - medians objective. ","This paper studies the problem of k-clustering, i.e., the problem where a threshold cut is applied to a single dimension (feature) of a cluster, and the goal is to find a cluster that minimizes the k-means objective. The authors propose an algorithm for explainable clustering based on the idea of `p-norms', which is a generalization of the previous work [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,21].    [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29]   The main contribution of this paper is to derive upper and lower bounds for k-medians. The upper bounds are O(k) and O(K), where k is the number of clusters in the cluster. The lower bound is O(O(k^k) for the case where the cluster size is small.   In particular, the authors show that the upper bounds depend on the size of the cluster and on the dimension of the data, and that the lower bound depends on the choice of the threshold cut. They also show that it is equivalent to a decision tree that is constructed from the decision tree.  In addition, they also provide a Ω(log k) lower bound on k-mean, and show that this is a lower bound of the original upper bound.  The authors also provide an upper bound on the number p of clusters that can be found by the algorithm, which depends on k.  Finally, they provide an algorithm that achieves the same upper bounds as [1]. ","This paper studies the problem of k-clustering, i.e., the problem where a threshold cut is applied to a single dimension (feature) of a cluster, and the goal is to find a cluster that minimizes the k-means objective. The authors propose an algorithm for explainable clustering based on the idea of `p-norms', which is a generalization of the previous work [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,21].    [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29]   The main contribution of this paper is to derive upper and lower bounds for k-medians. The upper bounds are O(k) and O(K), where k is the number of clusters in the cluster. The lower bound is O(O(k^k) for the case where the cluster size is small.   In particular, the authors show that the upper bounds depend on the size of the cluster and on the dimension of the data, and that the lower bound depends on the choice of the threshold cut. They also show that it is equivalent to a decision tree that is constructed from the decision tree.  In addition, they also provide a Ω(log k) lower bound on k-mean, and show that this is a lower bound of the original upper bound.  The authors also provide an upper bound on the number p of clusters that can be found by the algorithm, which depends on k.  Finally, they provide an algorithm that achieves the same upper bounds as [1]. "
17548,SP:6a9e47be710ddaf386bffc54d003d7dc2b67fdc3,"pre - trained language model ( PrLM ) USED-FOR downstream natural language processing tasks. multilingual PrLM USED-FOR limited resources. language universality USED-FOR limited resources. limited resources USED-FOR low - resource languages. multilingual PrLM USED-FOR downstream natural language processing tasks. language universality USED-FOR multilingual PrLM. plain text USED-FOR multilingual PrLMs. monolingual linguistic structure knowledge USED-FOR PrLMs. explicit universal dependency parsing CONJUNCTION implicit language modeling. implicit language modeling CONJUNCTION explicit universal dependency parsing. multilingual PrLM USED-FOR explicit universal dependency parsing. multilingual PrLM USED-FOR implicit language modeling. learned representation USED-FOR model. universal dependency parse FEATURE-OF Syntax. model COMPARE multilingual PrLM. multilingual PrLM COMPARE model. model COMPARE multilingual - BERT. multilingual - BERT COMPARE model. model COMPARE approach. approach COMPARE model. linguistic structure parsing datasets EVALUATE-FOR multilingual PrLM. linguistic structure parsing datasets EVALUATE-FOR model. multilingual - BERT HYPONYM-OF multilingual PrLM. OtherScientificTerm are universal linguistic structure clues, and PrLM interpretability. ","This paper proposes a multilingual pre-trained language model (PrLM) for downstream natural language processing tasks. The multilingual PrLM leverages language universality to leverage limited resources for low-resource languages. The authors show that multilingual prLMs can be trained on plain text with monolingual linguistic structure knowledge. Syntax has a universal dependency parse, and the authors propose to use explicit universal dependency parsing and implicit language modeling with multilingual PRLM. The model is trained with a learned representation, and then the model is evaluated on two linguistic structure parsing datasets. The proposed model outperforms multilingual-BERT, a previous model, in terms of universal linguistic structure clues. The paper also shows that the proposed approach is more interpretable than multilingual BERT.  ","This paper proposes a multilingual pre-trained language model (PrLM) for downstream natural language processing tasks. The multilingual PrLM leverages language universality to leverage limited resources for low-resource languages. The authors show that multilingual prLMs can be trained on plain text with monolingual linguistic structure knowledge. Syntax has a universal dependency parse, and the authors propose to use explicit universal dependency parsing and implicit language modeling with multilingual PRLM. The model is trained with a learned representation, and then the model is evaluated on two linguistic structure parsing datasets. The proposed model outperforms multilingual-BERT, a previous model, in terms of universal linguistic structure clues. The paper also shows that the proposed approach is more interpretable than multilingual BERT.  "
17612,SP:94f4b65214a648cbc84f13beba45a825e2e9901a,"deep architecture USED-FOR vehicle routing problems ( VRPs ). Transformer HYPONYM-OF deep architecture. Transformer USED-FOR vehicle routing problems ( VRPs ). positional encoding ( PE ) method USED-FOR representing VRP solutions. learning improvement models USED-FOR VRP. it USED-FOR learning improvement models. Dual - Aspect Collaborative Transformer ( DACT ) USED-FOR embeddings. embeddings USED-FOR node and positional features. Transformer USED-FOR symmetry of VRP solutions. cyclic positional encoding ( CPE ) method USED-FOR Transformer. cyclic sequences HYPONYM-OF symmetry of VRP solutions. cyclic positional encoding ( CPE ) method USED-FOR positional features. curriculum learning strategy USED-FOR sample efficiency. Proximal Policy Optimization USED-FOR DACT. traveling salesman problem ( TSP ) CONJUNCTION capacitated vehicle routing problem ( CVRP ). capacitated vehicle routing problem ( CVRP ) CONJUNCTION traveling salesman problem ( TSP ). DACT USED-FOR capacitated vehicle routing problem ( CVRP ). DACT USED-FOR traveling salesman problem ( TSP ). DACT COMPARE Transformer based improvement models. Transformer based improvement models COMPARE DACT. synthetic and benchmark instances EVALUATE-FOR DACT. OtherScientificTerm are VRP solutions, and incompatible correlations. Generic are them, and ones. ","This paper proposes a deep architecture, called Dual-Aspect Collaborative Transformer (DACT), for solving vehicle routing problems (VRPs) using a Transformer. The authors propose a positional encoding (PE) method for representing VRP solutions, and use it to train learning improvement models for VRP. They also propose a Dual-aspect Collaboration Transformer to learn embeddings for node and positional features. The Transformer is able to preserve the symmetry of VRP problems (e.g. cyclic sequences) by using the cyclic positional encoder (CPE) method. They further propose a curriculum learning strategy to improve sample efficiency. DACT is evaluated on the traveling salesman problem (TSP) and the capacitated vehicle routing problem (CVRP). They show that DACT outperforms other Transformer based improvement models on both synthetic and benchmark instances.    The paper is well-written and well-motivated, and the paper is easy to follow. However, there are a few issues with the paper:   1. The paper does not provide a clear description of the proposed DACT.  2. It does not explain how DACT works.  3. It is not clear whether DACT can be used in conjunction with Proximal Policy Optimization (PPO).   4. There is a lack of experiments. ","This paper proposes a deep architecture, called Dual-Aspect Collaborative Transformer (DACT), for solving vehicle routing problems (VRPs) using a Transformer. The authors propose a positional encoding (PE) method for representing VRP solutions, and use it to train learning improvement models for VRP. They also propose a Dual-aspect Collaboration Transformer to learn embeddings for node and positional features. The Transformer is able to preserve the symmetry of VRP problems (e.g. cyclic sequences) by using the cyclic positional encoder (CPE) method. They further propose a curriculum learning strategy to improve sample efficiency. DACT is evaluated on the traveling salesman problem (TSP) and the capacitated vehicle routing problem (CVRP). They show that DACT outperforms other Transformer based improvement models on both synthetic and benchmark instances.    The paper is well-written and well-motivated, and the paper is easy to follow. However, there are a few issues with the paper:   1. The paper does not provide a clear description of the proposed DACT.  2. It does not explain how DACT works.  3. It is not clear whether DACT can be used in conjunction with Proximal Policy Optimization (PPO).   4. There is a lack of experiments. "
17676,SP:e5c8680d8da9e7548fcb9bb5c073848eb80e1dd0,"Bayes error FEATURE-OF generative models. normalizing flows USED-FOR generative models. invertible transformation USED-FOR Bayes error. it USED-FOR Gaussian base distributions. Bayes error EVALUATE-FOR flow models. Holmes - Diaconis - Ross integration USED-FOR it. it USED-FOR Bayes error. synthetic datasets COMPARE benchmark datasets. benchmark datasets COMPARE synthetic datasets. Bayes error FEATURE-OF synthetic datasets. approach USED-FOR classification models. method USED-FOR benchmark datasets. Task is data - driven classification problem. Metric is classification error. OtherScientificTerm are data distribution, and intractable quantity. Generic are technique, and models. ","This paper considers the data-driven classification problem, where the goal is to minimize the classification error of a model trained on a given data distribution. The authors consider generative models based on normalizing flows. They show that the Bayes error of generative model trained with normalizing flow can be approximated by an invertible transformation, and that it can be used to approximate Gaussian base distributions. They also show that this technique can be applied to any class of flow models, and show that it leads to a lower Bayes ratio for flow models trained with Holmes-Diaconis-Ross integration. The paper also shows that this approach can be extended to classification models trained on synthetic datasets that have a smaller Bayes rate than standard benchmark datasets.    The paper is well-written and well-motivated, and the paper is clearly written. However, the paper suffers from an intractable quantity, which makes it difficult to follow. It is not clear to me that the proposed method is particularly applicable to benchmark datasets, and it is unclear to me whether the proposed technique is applicable to real-world data. ","This paper considers the data-driven classification problem, where the goal is to minimize the classification error of a model trained on a given data distribution. The authors consider generative models based on normalizing flows. They show that the Bayes error of generative model trained with normalizing flow can be approximated by an invertible transformation, and that it can be used to approximate Gaussian base distributions. They also show that this technique can be applied to any class of flow models, and show that it leads to a lower Bayes ratio for flow models trained with Holmes-Diaconis-Ross integration. The paper also shows that this approach can be extended to classification models trained on synthetic datasets that have a smaller Bayes rate than standard benchmark datasets.    The paper is well-written and well-motivated, and the paper is clearly written. However, the paper suffers from an intractable quantity, which makes it difficult to follow. It is not clear to me that the proposed method is particularly applicable to benchmark datasets, and it is unclear to me whether the proposed technique is applicable to real-world data. "
17740,SP:2896679f0472522bc3334178cd7574494cf12b7b,"language modeling CONJUNCTION computer vision. computer vision CONJUNCTION language modeling. neural architectures USED-FOR language modeling. neural architectures USED-FOR computer vision. hyper - parameter choices CONJUNCTION training instability. training instability CONJUNCTION hyper - parameter choices. automated and architecture agnostic method USED-FOR initializing neural networks. GradInit HYPONYM-OF automated and architecture agnostic method. GradInit USED-FOR initializing neural networks. SGD CONJUNCTION Adam. Adam CONJUNCTION SGD. norm FEATURE-OF network layer. heuristic USED-FOR GradInit. numerical scheme USED-FOR variables. GradInit USED-FOR convolutional architectures. skip connections USED-FOR convolutional architectures. learning rates CONJUNCTION momentum coefficients. momentum coefficients CONJUNCTION learning rates. Adam CONJUNCTION SGD. SGD CONJUNCTION Adam. Transformer architecture USED-FOR machine translation. It USED-FOR Transformer architecture. learning rate warmup FEATURE-OF it. Adam USED-FOR learning rate warmup. SGD USED-FOR learning rate warmup. SGD USED-FOR it. Adam USED-FOR it. Generic are architectures, and schemes. OtherScientificTerm are network parameters, hyperparameters, scalar multiplier variable, and normalization layers. Method are architecture - specific initialization schemes, and neural networks. ","This paper proposes an automated and architecture agnostic method, called GradInit, for initializing neural networks with different neural architectures for language modeling and computer vision. The authors argue that existing architecture-specific initialization schemes for different architectures can be problematic due to hyper-parameter choices and training instability. To address this issue, the authors propose Grad Init, a heuristic that can be applied to any convolutional architectures with skip connections. Grad Init is based on the observation that hyperparameters of different architectures tend to be highly correlated in terms of the norm of the network layer, which is a scalar multiplier variable.  The authors propose a numerical scheme to compute these variables, and show that GradInit can be used to compute the norm for any convolutionsal architectures, and that it can be combined with SGD, Adam, and SGD for learning rate warmup. It is also applied to the Transformer architecture for machine translation, and it is shown that it performs better than SGD and Adam for learning rates and momentum coefficients.   The main contribution of the paper is that the authors provide a theoretical analysis of architecture-agnostic initialization schemes, and the authors show that the hyperparameter of different neural networks is highly correlated with the number of normalization layers and the norm. ","This paper proposes an automated and architecture agnostic method, called GradInit, for initializing neural networks with different neural architectures for language modeling and computer vision. The authors argue that existing architecture-specific initialization schemes for different architectures can be problematic due to hyper-parameter choices and training instability. To address this issue, the authors propose Grad Init, a heuristic that can be applied to any convolutional architectures with skip connections. Grad Init is based on the observation that hyperparameters of different architectures tend to be highly correlated in terms of the norm of the network layer, which is a scalar multiplier variable.  The authors propose a numerical scheme to compute these variables, and show that GradInit can be used to compute the norm for any convolutionsal architectures, and that it can be combined with SGD, Adam, and SGD for learning rate warmup. It is also applied to the Transformer architecture for machine translation, and it is shown that it performs better than SGD and Adam for learning rates and momentum coefficients.   The main contribution of the paper is that the authors provide a theoretical analysis of architecture-agnostic initialization schemes, and the authors show that the hyperparameter of different neural networks is highly correlated with the number of normalization layers and the norm. "
17804,SP:f69731403592fa5bdd4ca327708582d615aa131c,"Linear mixed - effect models USED-FOR disease progression. interpretable parameters USED-FOR subject trajectories. diffeomorphism USED-FOR Euclidean metric. diffeomorphism USED-FOR metric. reproducible kernel Hilbert space FEATURE-OF radial basis functions. radial basis functions USED-FOR diffeomorphism. metric update USED-FOR forecasting of imaging and clinical biomarkers. TADPOLE challenge EVALUATE-FOR methods. Material is longitudinal data. Method are interpretable models, and ADNI. OtherScientificTerm are progression profiles, Riemannian manifold, patient - specific trajectories, and central geodesic. Generic is approach. Metric is interpretability. Task is Neural Information Processing Systems. ","Linear mixed-effect models for disease progression are an important problem in longitudinal data, where the goal is to learn interpretable parameters for subject trajectories.  This paper proposes a new approach, called ADNI, to learn such interpretable models. The proposed metric is based on a diffeomorphism in the reproducible kernel Hilbert space of the Riemannian manifold, which is used to define a Euclidean metric. This approach is motivated by the observation that patient-specific trajectories tend to diverge on a central geodesic, which can be interpreted as a measure of interpretability. The authors propose to use radial basis functions in a reproducing way on the reproducing part of the reproducibly kernel space, which allows for a more interpretable surrogate of a patient's progression profiles.  The authors also propose a metric update for the forecasting of imaging and clinical biomarkers.  Experiments on the TADPOLE challenge show that the proposed methods outperform the state-of-the-art methods.    Contributions:  1. The paper proposes an interpretable model for longitudinal data.  2. The idea of Neural Information Processing Systems (NIPS) is interesting.","Linear mixed-effect models for disease progression are an important problem in longitudinal data, where the goal is to learn interpretable parameters for subject trajectories.  This paper proposes a new approach, called ADNI, to learn such interpretable models. The proposed metric is based on a diffeomorphism in the reproducible kernel Hilbert space of the Riemannian manifold, which is used to define a Euclidean metric. This approach is motivated by the observation that patient-specific trajectories tend to diverge on a central geodesic, which can be interpreted as a measure of interpretability. The authors propose to use radial basis functions in a reproducing way on the reproducing part of the reproducibly kernel space, which allows for a more interpretable surrogate of a patient's progression profiles.  The authors also propose a metric update for the forecasting of imaging and clinical biomarkers.  Experiments on the TADPOLE challenge show that the proposed methods outperform the state-of-the-art methods.    Contributions:  1. The paper proposes an interpretable model for longitudinal data.  2. The idea of Neural Information Processing Systems (NIPS) is interesting."
17868,SP:438e906f52c4c0538956b51a2270b3ac498b27a8,"routing - by - memory mechanism USED-FOR CNN architectures. memory head CONJUNCTION procedure. procedure CONJUNCTION memory head. memory head PART-OF PU. procedure PART-OF PU. procedures USED-FOR features. mechanism USED-FOR Networks. four - step training strategy USED-FOR mechanism. four - step training strategy USED-FOR Networks. VGGNet CONJUNCTION ResNet. ResNet CONJUNCTION VGGNet. ResNet CONJUNCTION EfficientNet ’s accuracies. EfficientNet ’s accuracies CONJUNCTION ResNet. Tiny ImageNet CONJUNCTION ImageNet. ImageNet CONJUNCTION Tiny ImageNet. ImageNet CONJUNCTION CIFAR-100 benchmarks. CIFAR-100 benchmarks CONJUNCTION ImageNet. Tiny ImageNet EVALUATE-FOR EfficientNet ’s accuracies. VGGNet EVALUATE-FOR method. ResNet EVALUATE-FOR method. EfficientNet ’s accuracies EVALUATE-FOR method. ImageNet EVALUATE-FOR method. CIFAR-100 benchmarks EVALUATE-FOR method. Tiny ImageNet EVALUATE-FOR method. Method are Convolutional Neural Networks ( CNNs ), parallel procedures, and parallel Procedural Units ( PUs ). OtherScientificTerm are semantic features, procedure sequence, intermediate features, and intermediate feature. Generic are specialized procedures, It, and network. ","This paper proposes a routing-by-memory mechanism for training CNN architectures.   Convolutional Neural Networks (CNNs) are typically trained using parallel procedures, and the authors propose parallel Procedural Units (PUs). PU is a type of multi-head architecture that consists of a memory head, a procedure head, and a procedure sequence. The procedure sequence consists of two steps: (1) the memory head computes a sequence of specialized procedures, (2) the procedure sequence computes intermediate features, (3) and (4) the procedures are combined to produce a set of features. Networks trained with this mechanism are trained using a four-step training strategy.  The method is evaluated on VGGNet, ResNet, and EfficientNet’s accuracies on Tiny ImageNet, ImageNet and CIFAR-100 benchmarks. It is shown that the proposed method is able to improve the performance of the network. ","This paper proposes a routing-by-memory mechanism for training CNN architectures.   Convolutional Neural Networks (CNNs) are typically trained using parallel procedures, and the authors propose parallel Procedural Units (PUs). PU is a type of multi-head architecture that consists of a memory head, a procedure head, and a procedure sequence. The procedure sequence consists of two steps: (1) the memory head computes a sequence of specialized procedures, (2) the procedure sequence computes intermediate features, (3) and (4) the procedures are combined to produce a set of features. Networks trained with this mechanism are trained using a four-step training strategy.  The method is evaluated on VGGNet, ResNet, and EfficientNet’s accuracies on Tiny ImageNet, ImageNet and CIFAR-100 benchmarks. It is shown that the proposed method is able to improve the performance of the network. "
17932,SP:d240173080cd3647dbaa5173a6422396f226775b,"fundamental symmetries CONJUNCTION coordinate freedoms of physical law. coordinate freedoms of physical law CONJUNCTION fundamental symmetries. coordinate freedoms of physical law FEATURE-OF neural networks. fundamental symmetries FEATURE-OF neural networks. irreducible representations USED-FOR frameworks. rotation CONJUNCTION reflection ( parity ). reflection ( parity ) CONJUNCTION rotation. translation CONJUNCTION rotation. rotation CONJUNCTION translation. fundamental symmetries USED-FOR physical laws. scalar products CONJUNCTION scalar contractions. scalar contractions CONJUNCTION scalar products. scalar contractions HYPONYM-OF scalars. scalar products HYPONYM-OF scalars. OtherScientificTerm are high - order tensor objects, symmetry - enforcing constraints, classical physics, permutations, symmetries, and Euclidean, Lorentz, and Poincaré groups. Method are polynomial functions, and scalar - based method. Generic is theory. ","This paper studies the relationship between neural networks with fundamental symmetries and coordinate freedoms of physical law and neural networks trained with irreducible representations. In particular, the authors consider the case of high-order tensor objects that are invariant to symmetry-enforcing constraints. The authors show that such frameworks can be seen as a special case of existing frameworks that can be applied to any set of polynomial functions.   The authors also show that classical physics can be viewed as an extension of these frameworks.  They show that, under certain conditions, physical laws can be expressed as a function of the fundamental symmetry of a set of scalars (e.g., scalar products, scalar contractions, etc.) and that these scalars can be represented as polynomials.  The paper also shows that, for translation, rotation, reflection (parity), and rotation with permutations, the symmetry of the set of permutations can be approximated by a scalar-based method.  Finally, the paper shows that Euclidean, Lorentz, and Poincaré groups can be considered as special cases of the theory. ","This paper studies the relationship between neural networks with fundamental symmetries and coordinate freedoms of physical law and neural networks trained with irreducible representations. In particular, the authors consider the case of high-order tensor objects that are invariant to symmetry-enforcing constraints. The authors show that such frameworks can be seen as a special case of existing frameworks that can be applied to any set of polynomial functions.   The authors also show that classical physics can be viewed as an extension of these frameworks.  They show that, under certain conditions, physical laws can be expressed as a function of the fundamental symmetry of a set of scalars (e.g., scalar products, scalar contractions, etc.) and that these scalars can be represented as polynomials.  The paper also shows that, for translation, rotation, reflection (parity), and rotation with permutations, the symmetry of the set of permutations can be approximated by a scalar-based method.  Finally, the paper shows that Euclidean, Lorentz, and Poincaré groups can be considered as special cases of the theory. "
17996,SP:72c0f47566904deb27d8157da30807ec1d6b5685,Bounding box ( bbox ) regression HYPONYM-OF computer vision. loss functions USED-FOR bbox regression. Intersection over Union ( IoU ) loss HYPONYM-OF loss functions. IoUbased losses USED-FOR power IoU losses. power IoU term CONJUNCTION power regularization term. power regularization term CONJUNCTION power IoU term. power parameter α FEATURE-OF power regularization term. power regularization term FEATURE-OF power IoU losses. power IoU term FEATURE-OF power IoU losses. order preservingness CONJUNCTION loss / gradient reweighting. loss / gradient reweighting CONJUNCTION order preservingness. α - IoU losses HYPONYM-OF losses. loss / gradient reweighting HYPONYM-OF properties. order preservingness HYPONYM-OF properties. α - IoU losses COMPARE IoU - based losses. IoU - based losses COMPARE α - IoU losses. small datasets CONJUNCTION noisy bboxes. noisy bboxes CONJUNCTION small datasets. object detection benchmarks CONJUNCTION models. models CONJUNCTION object detection benchmarks. bbox regression accuracy EVALUATE-FOR detectors. performance margin EVALUATE-FOR α - IoU losses. performance margin EVALUATE-FOR IoU - based losses. OtherScientificTerm is α. ,"This paper studies the problem of Bounding box (bbox) regression, an important problem in computer vision. The authors propose two new loss functions for bbox regression, the Intersection over Union (IoU) loss and the power IoU loss. The IoUbased losses are a generalization of IoU based losses.   The authors show that the power  IoU losses have the following properties:  1) order preservingness, 2) loss/gradient reweighting, and 3) a power regularization term that depends on the power parameter α.  The paper also shows that the proposed losses, called α-IiuU losses, are more efficient than the existing IoU-based losses in terms of performance margin.  Experiments are conducted on several object detection benchmarks and models, and on small datasets and noisy bboxes. The results show that for detectors with a certain level of noise, the proposed α-IIOU losses achieve better performance margin compared to the existing state-of-the-art performance margin of the previous state of the art.","This paper studies the problem of Bounding box (bbox) regression, an important problem in computer vision. The authors propose two new loss functions for bbox regression, the Intersection over Union (IoU) loss and the power IoU loss. The IoUbased losses are a generalization of IoU based losses.   The authors show that the power  IoU losses have the following properties:  1) order preservingness, 2) loss/gradient reweighting, and 3) a power regularization term that depends on the power parameter α.  The paper also shows that the proposed losses, called α-IiuU losses, are more efficient than the existing IoU-based losses in terms of performance margin.  Experiments are conducted on several object detection benchmarks and models, and on small datasets and noisy bboxes. The results show that for detectors with a certain level of noise, the proposed α-IIOU losses achieve better performance margin compared to the existing state-of-the-art performance margin of the previous state of the art."
18060,SP:397125177d7007316d67194ec00d5dc57b44ac79,"imitation learning problem USED-FOR policy. Markov Decision Process ( MDP ) setting FEATURE-OF policy. imitation learning USED-FOR policy. policy USED-FOR problem. adversarial construction USED-FOR policy. DROIL CONJUNCTION Maximum Entropy Inverse Reinforcement Learning. Maximum Entropy Inverse Reinforcement Learning CONJUNCTION DROIL. framework USED-FOR generalized concept of entropy. generalized concept of entropy USED-FOR DROIL. framework USED-FOR DROIL. approach USED-FOR objective function. approach USED-FOR convex optimization problem. state and action spaces FEATURE-OF loss functions. convex optimization problem USED-FOR objective function. polynomial number of variables FEATURE-OF convex optimization problem. approach USED-FOR stationary and non - stationary policies. methods COMPARE it. it COMPARE methods. inner reinforcement learning problem USED-FOR it. synthetic data CONJUNCTION highway driving environment. highway driving environment CONJUNCTION synthetic data. optimization method USED-FOR DROIL. synthetic data EVALUATE-FOR DROIL. highway driving environment EVALUATE-FOR DROIL. synthetic data EVALUATE-FOR optimization method. highway driving environment EVALUATE-FOR optimization method. OtherScientificTerm are reward function, demonstrated behaviors, noisy demonstrations, and optimistic generalizations. Generic is task. Method is Distributionally Robust Imitation Learning ( DROIL ). ","This paper considers the imitation learning problem of learning a policy in a Markov Decision Process (MDP) setting in which the reward function is a function of the demonstrated behaviors. In this task, the authors propose Distributionally Robust Imitation Learning (DROIL), where the goal is to learn a policy that is robust to noisy demonstrations. The problem is formulated as imitation learning in which a policy is learned using an adversarial construction of the learned policy. DROIL is a generalization of DROIL and Maximum Entropy Inverse Reinforcement Learning. The authors propose a framework for learning a generalized concept of entropy for DROIL. They also propose an approach to learn an objective function that is a convex optimization problem over a polynomial number of variables over the state and action spaces of the loss functions. The proposed approach is applied to both stationary and non-stationary policies. The optimization method is evaluated on synthetic data and a highway driving environment, and the authors show that their optimization method outperforms DROIL on both synthetic data, and outperforms it when it is used as an inner reinforcement learning problem. The paper also shows that the proposed approach can be used to learn policies that are robust to noise in noisy demonstrations, and that it can learn optimistic generalizations. ","This paper considers the imitation learning problem of learning a policy in a Markov Decision Process (MDP) setting in which the reward function is a function of the demonstrated behaviors. In this task, the authors propose Distributionally Robust Imitation Learning (DROIL), where the goal is to learn a policy that is robust to noisy demonstrations. The problem is formulated as imitation learning in which a policy is learned using an adversarial construction of the learned policy. DROIL is a generalization of DROIL and Maximum Entropy Inverse Reinforcement Learning. The authors propose a framework for learning a generalized concept of entropy for DROIL. They also propose an approach to learn an objective function that is a convex optimization problem over a polynomial number of variables over the state and action spaces of the loss functions. The proposed approach is applied to both stationary and non-stationary policies. The optimization method is evaluated on synthetic data and a highway driving environment, and the authors show that their optimization method outperforms DROIL on both synthetic data, and outperforms it when it is used as an inner reinforcement learning problem. The paper also shows that the proposed approach can be used to learn policies that are robust to noise in noisy demonstrations, and that it can learn optimistic generalizations. "
18124,SP:58f220bbbed8d3e0633b408fca3b6838c4ad323d,"Post - processing USED-FOR algorithmic fairness. approach USED-FOR ML systems. Post - processing HYPONYM-OF approach. retraining USED-FOR it. post - processing algorithms USED-FOR individual fairness ( IF ). similarity graph USED-FOR fairness constraints. graph Laplacian regularization USED-FOR graph smoothing problem. graph smoothing problem USED-FOR IF post - processing problem. post - processing algorithms USED-FOR individual biases. post - processing algorithms USED-FOR large - scale NLP models. individual biases FEATURE-OF large - scale NLP models. accuracy EVALUATE-FOR post - processing algorithms. BERT HYPONYM-OF large - scale NLP models. Method is postprocessing. Generic is model. OtherScientificTerm are objective function, and individual fairness. ","This paper proposes a new approach to algorithmic fairness in ML systems. Post-processing is an approach that aims to ensure that postprocessing does not bias the model in a way that is harmful to individual fairness. In particular, it does not rely on retraining for individual fairness, but rather on the fact that the objective function is a weighted sum of a set of fairness constraints over a similarity graph. The authors propose two post-processing algorithms for individual fair (IF) that are based on the idea of individual fairness (IF). The IF post-processing problem is formulated as a graph smoothing problem, which is solved by a graph Laplacian regularization. Experiments are conducted to show that the proposed post- processing algorithms are able to mitigate individual biases in large-scale NLP models (e.g., BERT). The authors also show that post-postprocessing algorithms improve accuracy in terms of accuracy on a variety of datasets. ","This paper proposes a new approach to algorithmic fairness in ML systems. Post-processing is an approach that aims to ensure that postprocessing does not bias the model in a way that is harmful to individual fairness. In particular, it does not rely on retraining for individual fairness, but rather on the fact that the objective function is a weighted sum of a set of fairness constraints over a similarity graph. The authors propose two post-processing algorithms for individual fair (IF) that are based on the idea of individual fairness (IF). The IF post-processing problem is formulated as a graph smoothing problem, which is solved by a graph Laplacian regularization. Experiments are conducted to show that the proposed post- processing algorithms are able to mitigate individual biases in large-scale NLP models (e.g., BERT). The authors also show that post-postprocessing algorithms improve accuracy in terms of accuracy on a variety of datasets. "
18188,SP:ef791aa29decd839e7e583c9d1f71e8309ca87ef,"Text - to - SQL task USED-FOR SQL queries. model USED-FOR database schemas. graph structure USED-FOR unified encoding model. unified encoding model USED-FOR natural language question and database schema. graph structure USED-FOR SADGA. question - graph CONJUNCTION schema - graph. schema - graph CONJUNCTION question - graph. unified modeling USED-FOR structure - aware aggregation method. Global Graph Linking CONJUNCTION Local Graph Linking. Local Graph Linking CONJUNCTION Global Graph Linking. Local Graph Linking CONJUNCTION DualGraph Aggregation Mechanism. DualGraph Aggregation Mechanism CONJUNCTION Local Graph Linking. Global Graph Linking USED-FOR structure - aware aggregation method. Local Graph Linking PART-OF structure - aware aggregation method. DualGraph Aggregation Mechanism PART-OF structure - aware aggregation method. Text - to - SQL benchmark Spider EVALUATE-FOR proposal. Task are Text - to - SQL, and cross - domain Text - to - SQL. Method are encoding method, and question - schema linking method. OtherScientificTerm is database schema. ","This paper tackles the Text-to-SQL task for SQL queries. The authors propose a unified encoding model for both natural language question and database schema, where the model is able to link two database schemas in a single model. The encoding method is based on SADGA, which uses the graph structure of the question-graph and the schema-graph as input. The paper proposes a structure-aware aggregation method based on the unified modeling, which combines Global Graph Linking, Local Graph Linked, and DualGraph Aggregation Mechanism. The proposal is evaluated on the well-known standard Text to-SQL benchmark Spider, and is shown to outperform the state-of-the-art on the cross-domains and is competitive with the best performing question-schema linking method. ","This paper tackles the Text-to-SQL task for SQL queries. The authors propose a unified encoding model for both natural language question and database schema, where the model is able to link two database schemas in a single model. The encoding method is based on SADGA, which uses the graph structure of the question-graph and the schema-graph as input. The paper proposes a structure-aware aggregation method based on the unified modeling, which combines Global Graph Linking, Local Graph Linked, and DualGraph Aggregation Mechanism. The proposal is evaluated on the well-known standard Text to-SQL benchmark Spider, and is shown to outperform the state-of-the-art on the cross-domains and is competitive with the best performing question-schema linking method. "
18252,SP:a2fa25a4539a38af61a0993f65ecc14339f26c2e,"models USED-FOR supervised and reinforcement learning. discrete and continuous model components USED-FOR models. approach USED-FOR discrete - continuous computation graphs. discrete probability distributions USED-FOR neural networks. stochastic softmax tricks USED-FOR neural networks. discrete component USED-FOR graph ’s execution paths. discrete component USED-FOR computation graphs. sequential discrete components USED-FOR stochastic computations graphs. small gradients CONJUNCTION local minima. local minima CONJUNCTION small gradients. scale parameter FEATURE-OF Gumbel noise perturbations. scale parameter USED-FOR learning behavior. dropout residual connections USED-FOR stochastic, discrete - continuous computation graphs. complex discrete - stochastic models COMPARE continuous counterparts. continuous counterparts COMPARE complex discrete - stochastic models. benchmark datasets EVALUATE-FOR complex discrete - stochastic models. benchmark datasets EVALUATE-FOR continuous counterparts. Method are discrete - continuous models, and complex discrete - continuous models. Generic is strategies. ","This paper proposes a new approach to learning discrete-continuous computation graphs. The authors propose to use discrete and continuous model components to train models for supervised and reinforcement learning. The discrete component is used to model the graph’s execution paths, while the continuous component is applied to the computation graphs, which are then used to train neural networks with discrete probability distributions.  The authors show that the stochastic computations graphs can be decomposed into sequential discrete components, which can be used to learn models with small gradients and local minima.  They also show that neural networks trained with the same number of discrete components are able to learn with the standard set of discrete softmax tricks, and that the learning behavior can be controlled by adjusting the scale parameter of the Gumbel noise perturbations.  Finally, the authors propose two strategies to improve the performance of complex discrete-stochastic models compared to their continuous counterparts on several benchmark datasets.    The main contribution of the paper is that the authors introduce the use of dropout residual connections for learning stochedastic, discrete, and continuous computation graphs and show that their approach can be applied to a variety of existing approaches to learning such models. ","This paper proposes a new approach to learning discrete-continuous computation graphs. The authors propose to use discrete and continuous model components to train models for supervised and reinforcement learning. The discrete component is used to model the graph’s execution paths, while the continuous component is applied to the computation graphs, which are then used to train neural networks with discrete probability distributions.  The authors show that the stochastic computations graphs can be decomposed into sequential discrete components, which can be used to learn models with small gradients and local minima.  They also show that neural networks trained with the same number of discrete components are able to learn with the standard set of discrete softmax tricks, and that the learning behavior can be controlled by adjusting the scale parameter of the Gumbel noise perturbations.  Finally, the authors propose two strategies to improve the performance of complex discrete-stochastic models compared to their continuous counterparts on several benchmark datasets.    The main contribution of the paper is that the authors introduce the use of dropout residual connections for learning stochedastic, discrete, and continuous computation graphs and show that their approach can be applied to a variety of existing approaches to learning such models. "
18316,SP:bb3ec363e90269db4a2ba99d8107cb56f86e68f0,"Approximate Bayesian inference USED-FOR neural networks. Approximate Bayesian inference COMPARE training. training COMPARE Approximate Bayesian inference. high - fidelity approximate inference FEATURE-OF Bayesian neural networks ( BNNs ). full - batch Hamiltonian Monte Carlo USED-FOR high - fidelity approximate inference. covariate shift FEATURE-OF Bayesian model average. approximate inference procedures CONJUNCTION maximum a - posteriori ( MAP ) training. maximum a - posteriori ( MAP ) training CONJUNCTION approximate inference procedures. priors USED-FOR BNNs. robustness EVALUATE-FOR BNNs. robustness EVALUATE-FOR priors. Material is out - of - distribution data. Method is classical estimation. OtherScientificTerm are linear dependencies, features, and posterior contraction. "," of Bayesian neural networks (BNNs) trained with Hamiltonian Hamiltonian Monte Carlo (HMC) is shown to be robust to covariate shift in out-of-distribution data. Approximate Bayesian inference is used to train neural networks with linear dependencies on features, which is more robust than classical estimation. The authors also show that high-fidelity approximate inference can be achieved with full-batch Hamiltonian Monocoal (HBM) using a variant of classical classical estimation, and that the Bayesian model average is robust to the covariate shifted due to the posterior contraction. Finally, the authors show that approximate inference procedures and maximum a-posteri (MAP) training can be combined to improve the robustness of BNNs."," of Bayesian neural networks (BNNs) trained with Hamiltonian Hamiltonian Monte Carlo (HMC) is shown to be robust to covariate shift in out-of-distribution data. Approximate Bayesian inference is used to train neural networks with linear dependencies on features, which is more robust than classical estimation. The authors also show that high-fidelity approximate inference can be achieved with full-batch Hamiltonian Monocoal (HBM) using a variant of classical classical estimation, and that the Bayesian model average is robust to the covariate shifted due to the posterior contraction. Finally, the authors show that approximate inference procedures and maximum a-posteri (MAP) training can be combined to improve the robustness of BNNs."
18380,SP:f86ec7042e9b73ae071704a6d3ed17d7e3da1b75,"settings PART-OF meta - learning evaluation. in - distribution [ ID ] HYPONYM-OF settings. out - of - distribution [ OOD ] HYPONYM-OF settings. task distribution USED-FOR train and test tasks. metalearning theory CONJUNCTION FSL applications. FSL applications CONJUNCTION metalearning theory. they USED-FOR task generation. few - shot classification benchmarks EVALUATE-FOR OOD evaluation. ID setting USED-FOR metalearning theory. meta - learning methods USED-FOR ID setting. OOD datasets EVALUATE-FOR meta - learning methods. meta - learning method USED-FOR model selection. ID evaluation CONJUNCTION OOD evaluation. OOD evaluation CONJUNCTION ID evaluation. FSL benchmarks USED-FOR ID evaluation. FSL benchmarks USED-FOR OOD evaluation. benchmarks USED-FOR OOD evaluation. Task are OOD setting, and ID vs. OOD evaluation. Generic is methods. ","This paper considers two settings in meta-learning evaluation: in-distribution [ID] and out-of-distributions [OOD]. In the ID setting, the task distribution for both train and test tasks is the same. In the OOD setting, there is a large gap between the training and test distributions. The authors propose two methods to mitigate this issue. First, they use metalearning theory and FSL applications to train a model on the training data and then they use them for task generation. Second, they evaluate OOD evaluation on standard few-shot classification benchmarks.    The authors show that the ID vs. OOD performance of the proposed methods is comparable to that of the standard methods. They also show that meta-learners on OOD datasets are able to outperform the standard ones. Finally, they show that there is no significant difference between the performance of ID and OOD methods on standard benchmarks. The paper also shows that the proposed method can be used for model selection in a meta learning method.  The paper concludes with an analysis of the relationship between the ID evaluation and OLD evaluation on FSL benchmarks.","This paper considers two settings in meta-learning evaluation: in-distribution [ID] and out-of-distributions [OOD]. In the ID setting, the task distribution for both train and test tasks is the same. In the OOD setting, there is a large gap between the training and test distributions. The authors propose two methods to mitigate this issue. First, they use metalearning theory and FSL applications to train a model on the training data and then they use them for task generation. Second, they evaluate OOD evaluation on standard few-shot classification benchmarks.    The authors show that the ID vs. OOD performance of the proposed methods is comparable to that of the standard methods. They also show that meta-learners on OOD datasets are able to outperform the standard ones. Finally, they show that there is no significant difference between the performance of ID and OOD methods on standard benchmarks. The paper also shows that the proposed method can be used for model selection in a meta learning method.  The paper concludes with an analysis of the relationship between the ID evaluation and OLD evaluation on FSL benchmarks."
18444,SP:371f77148b4f00a929f7c118b1bb7c5a6238d264,"rules PART-OF knowledge base ( KB ). language model ( LM)-based rule generation USED-FOR rules. KB - based rule induction CONJUNCTION LM - based rule generation. LM - based rule generation CONJUNCTION KB - based rule induction. data commonalities USED-FOR KB - based methods. LMs USED-FOR free text. rich expressive power FEATURE-OF LMs. methods USED-FOR canned ” rules. open rule induction problem USED-FOR open rules. Orion ( open rule induction ) system USED-FOR open rules. LMs USED-FOR open rules. automatically inducted rules COMPARE manually annotated rules. manually annotated rules COMPARE automatically inducted rules. open rules USED-FOR relation extraction. OtherScientificTerm are Rules, annotated rules, and supervision of annotated rules. Method are inference systems, rule induction systems, and LM - based methods. Generic is they. ","This paper studies the problem of learning rules from rules in a knowledge base (KB) using language model (LM)-based rule generation.    Rules are learned from a set of rules that have been annotated in the knowledge base.  The authors show that existing KB-based rule induction and LM-based rules generation can be learned from data commonalities.  They also show that these methods can be used to learn “canned” rules, i.e. rules that are not automatically inducted by inference systems, but are learned by rule induction systems.  Finally, they show that open rules can be generated by LMs with rich expressive power, and that LMs can be trained to generate free text.  In addition, the authors propose an open rule induction problem to learn open rules using LMs, and show that the Orion (open rule induction) system is able to generate open rules in this setting.  For relation extraction, open rules are also learned using open rules.  Experiments are conducted on a number of datasets, showing that automatically inducting rules is more powerful than manually annotated rules. The authors also find that the supervision of annotated rule is more important than the learning of new rules, as they are able to learn rules that do not need to be annotated.  Overall, the paper is well-written and well-motivated.  However, there are a few issues with the paper:   1. The paper does not provide sufficient discussion of the connection between the open rules and the rule induction system.  2. There is a lack of comparison with existing work on the topic.  3. It is not clear to me that there is a clear connection between open rules for relation extraction and automatically learned rules. 4. It would be better if the authors provide more discussion about the limitations of existing LMs for learning open rules, and how they can be improved.","This paper studies the problem of learning rules from rules in a knowledge base (KB) using language model (LM)-based rule generation.    Rules are learned from a set of rules that have been annotated in the knowledge base.  The authors show that existing KB-based rule induction and LM-based rules generation can be learned from data commonalities.  They also show that these methods can be used to learn “canned” rules, i.e. rules that are not automatically inducted by inference systems, but are learned by rule induction systems.  Finally, they show that open rules can be generated by LMs with rich expressive power, and that LMs can be trained to generate free text.  In addition, the authors propose an open rule induction problem to learn open rules using LMs, and show that the Orion (open rule induction) system is able to generate open rules in this setting.  For relation extraction, open rules are also learned using open rules.  Experiments are conducted on a number of datasets, showing that automatically inducting rules is more powerful than manually annotated rules. The authors also find that the supervision of annotated rule is more important than the learning of new rules, as they are able to learn rules that do not need to be annotated.  Overall, the paper is well-written and well-motivated.  However, there are a few issues with the paper:   1. The paper does not provide sufficient discussion of the connection between the open rules and the rule induction system.  2. There is a lack of comparison with existing work on the topic.  3. It is not clear to me that there is a clear connection between open rules for relation extraction and automatically learned rules. 4. It would be better if the authors provide more discussion about the limitations of existing LMs for learning open rules, and how they can be improved."
18508,SP:8be2e0ea4a83fe32a4859f456007a829e5e9270a,Reinforcement Learning ( RL ) algorithms USED-FOR real - world scenarios. single - agent counterpart COMPARE offline multiagent RL. offline multiagent RL COMPARE single - agent counterpart. state and action space FEATURE-OF agents. agents PART-OF offline multiagent RL. offline RL algorithms USED-FOR multi - agent systems. offline RL algorithm USED-FOR extrapolation error. state - action pairs USED-FOR value estimation. Implicit Constraint Q - learning ( ICQ ) HYPONYM-OF offline RL algorithm. ICQ USED-FOR multi - agent tasks. implicit constraint USED-FOR joint - policy. OtherScientificTerm is accumulated extrapolation error. ,"This paper proposes a new offline RL algorithm called Implicit Constraint Q-learning (ICQ) for multi-agent reinforcement learning (MRL). ICQ is based on the observation that the extrapolation error of RL algorithms in offline multiagent RL is a function of the number of agents and the state and action space of the agents. The authors propose to use this observation as an implicit constraint to learn a joint-policy, which is then used to learn the value estimation of the state-action pairs for value estimation. The proposed algorithm is shown to outperform the single-agent counterpart and outperform offline RL algorithms for a number of real-world scenarios. ","This paper proposes a new offline RL algorithm called Implicit Constraint Q-learning (ICQ) for multi-agent reinforcement learning (MRL). ICQ is based on the observation that the extrapolation error of RL algorithms in offline multiagent RL is a function of the number of agents and the state and action space of the agents. The authors propose to use this observation as an implicit constraint to learn a joint-policy, which is then used to learn the value estimation of the state-action pairs for value estimation. The proposed algorithm is shown to outperform the single-agent counterpart and outperform offline RL algorithms for a number of real-world scenarios. "
18572,SP:1939b24b68970c33ca16ce238deed257f76d009e,"machine learning models USED-FOR security related applications. real - world adversaries USED-FOR neural network based detectors. uniform norm - bounded perturbations USED-FOR adversarial examples ( AEs ). finance CONJUNCTION social networks. social networks CONJUNCTION finance. malware CONJUNCTION finance. finance CONJUNCTION malware. AEs USED-FOR domains. social networks HYPONYM-OF domains. malware HYPONYM-OF domains. finance HYPONYM-OF domains. semantically meaningful dependencies FEATURE-OF features. features USED-FOR applications. non - uniform perturbations USED-FOR feature dependencies. non - uniform perturbations USED-FOR adversarial training. malware classification CONJUNCTION credit risk prediction. credit risk prediction CONJUNCTION malware classification. credit risk prediction CONJUNCTION spam detection. spam detection CONJUNCTION credit risk prediction. approach USED-FOR real - world attacks. certification EVALUATE-FOR non - uniform bounds. non - uniform perturbation bounds USED-FOR robustness certification. Metric is imperceptibility. OtherScientificTerm are uniform perturbations, and empirical data distribution. ","This paper studies the problem of training machine learning models for security related applications against real-world adversaries in the presence of uniform norm-bounded perturbations to neural network based detectors. In particular, the authors propose to use adversarial examples (AEs) that are imperceptible to uniform perturbation, but can be detected by the empirical data distribution. AEs can be classified in three different domains: malware, finance, and social networks. The authors show that the imperceptibility of AEs in these domains is due to semantically meaningful dependencies in the features of the features. They also show that non-uniform adversarial training can be performed in the absence of these features for these applications. The paper also shows that the robustness certification performance of the proposed approach can be improved by using non-utility for certification. Experiments are conducted on malware classification, credit risk prediction, and spam detection. ","This paper studies the problem of training machine learning models for security related applications against real-world adversaries in the presence of uniform norm-bounded perturbations to neural network based detectors. In particular, the authors propose to use adversarial examples (AEs) that are imperceptible to uniform perturbation, but can be detected by the empirical data distribution. AEs can be classified in three different domains: malware, finance, and social networks. The authors show that the imperceptibility of AEs in these domains is due to semantically meaningful dependencies in the features of the features. They also show that non-uniform adversarial training can be performed in the absence of these features for these applications. The paper also shows that the robustness certification performance of the proposed approach can be improved by using non-utility for certification. Experiments are conducted on malware classification, credit risk prediction, and spam detection. "
18636,SP:417b30930b245667d777e5d90ee80dd41546760e,spectral filtering USED-FOR statistical properties. spectral filtering USED-FOR learning with kernels. learning with kernels USED-FOR statistical properties. regularization schemes COMPARE Tikhonov regularization. Tikhonov regularization COMPARE regularization schemes. faster convergence rates FEATURE-OF excess risk. regularization schemes USED-FOR excess risk. regularization schemes USED-FOR least squares. faster convergence rates EVALUATE-FOR regularization schemes. loss functions USED-FOR estimators. Tikhonov regularization USED-FOR generalized self concordant loss functions ( GSC ). logistic loss HYPONYM-OF generalized self concordant loss functions ( GSC ). proximal point method USED-FOR optimization. iterated Tikhonov regularization scheme CONJUNCTION proximal point method. proximal point method CONJUNCTION iterated Tikhonov regularization scheme. fast and optimal rates USED-FOR GSC. iterated Tikhonov regularization scheme USED-FOR fast and optimal rates. iterated Tikhonov regularization scheme USED-FOR GSC. OtherScientificTerm is source and capacity conditions. Task is learning task. ,"This paper studies spectral filtering in learning with kernels to improve the statistical properties of spectral filtering. The authors show that existing regularization schemes for least squares have faster convergence rates compared to Tikhonov regularization, and that the excess risk is also reduced. They also show that under certain source and capacity conditions, regularization strategies can be used to achieve faster convergence of the excess loss.    The authors propose two new estimators based on loss functions, the generalized self concordant loss functions (GSC) and the logistic loss, which are based on the results of Tikhonsky et al. (2017). They also propose an iterated version of the proximal point method for optimization. They show that the iterated Tikhonis regularization scheme is able to achieve fast and optimal rates for the GSC, and they also provide a theoretical analysis of the convergence rate of the learned estimators.  Finally, the authors provide some numerical experiments on the learning task. ","This paper studies spectral filtering in learning with kernels to improve the statistical properties of spectral filtering. The authors show that existing regularization schemes for least squares have faster convergence rates compared to Tikhonov regularization, and that the excess risk is also reduced. They also show that under certain source and capacity conditions, regularization strategies can be used to achieve faster convergence of the excess loss.    The authors propose two new estimators based on loss functions, the generalized self concordant loss functions (GSC) and the logistic loss, which are based on the results of Tikhonsky et al. (2017). They also propose an iterated version of the proximal point method for optimization. They show that the iterated Tikhonis regularization scheme is able to achieve fast and optimal rates for the GSC, and they also provide a theoretical analysis of the convergence rate of the learned estimators.  Finally, the authors provide some numerical experiments on the learning task. "
18700,SP:1caeee4f00b52fe356ff4e5dd004d0203e838370,"linear transform USED-FOR input - output dimensions. butterfly matrices USED-FOR linear transform. Deformable Butterfly ( DeBut ) HYPONYM-OF linear transform. It USED-FOR neural networks. sparsity FEATURE-OF DeBut layer. sparsity USED-FOR network compression. light weight CONJUNCTION inference complexity. inference complexity CONJUNCTION light weight. DeBut COMPARE fully connected and convolutional layers. fully connected and convolutional layers COMPARE DeBut. OtherScientificTerm are butterflies, and natural complexity - accuracy tradeoff. Method is neural network. Generic is it. Metric is accuracy. ","This paper proposes a new linear transform, called Deformable butterfly (Deformable Butterfly), which is a linear transform over the input-output dimensions of butterfly matrices. It can be used to compress neural networks. The authors show that the sparsity of the DeBut layer can be leveraged for network compression. They also show that DeBut is more efficient than fully connected and convolutional layers in terms of the natural complexity-accuracy tradeoff between the light weight and the inference complexity. The paper also shows that a neural network can be trained with DeBut, and that it can also be trained to achieve better accuracy. ","This paper proposes a new linear transform, called Deformable butterfly (Deformable Butterfly), which is a linear transform over the input-output dimensions of butterfly matrices. It can be used to compress neural networks. The authors show that the sparsity of the DeBut layer can be leveraged for network compression. They also show that DeBut is more efficient than fully connected and convolutional layers in terms of the natural complexity-accuracy tradeoff between the light weight and the inference complexity. The paper also shows that a neural network can be trained with DeBut, and that it can also be trained to achieve better accuracy. "
18764,SP:d345ce1d7afc367ee1a9fb68d50ff1b2219f02cb,"weights PART-OF network. method USED-FOR weight reusability. shared weights USED-FOR MARK. common Knowledge Base ( KB ) USED-FOR shared weights. metalearning approach USED-FOR weight reusability. metalearning approach USED-FOR KB. benchmarks EVALUATE-FOR MARK. average accuracy EVALUATE-FOR methods. MARK USED-FOR reusable knowledge. Method are artificial neural networks, and MetA Reusable Knowledge. Task are Catastrophic Forgetting ( CF ), and overwriting. OtherScientificTerm are forgetting of old information, trainable masks, and KB relevant weights. Generic are task, and model. Material is 20 - Split - MiniImageNet dataset. Metric is forgetfulness. ","This paper addresses the problem of Catastrophic Forgetting (CF) in artificial neural networks. The authors propose a method called MetA Reusable Knowledge (MARK) to address the issue of forgetting of old information. The proposed method is based on the idea of weight reusability, i.e. the reusing of weights in a network that have been trained on the same task. To achieve this, Mark uses shared weights from a common Knowledge Base (KB) and trainable masks. A metalearning approach to the KB is used to find the KB relevant weights, and the model is trained to re-use these weights. Mark is evaluated on the 20-Split-MiniImageNet dataset, and is shown to outperform existing methods in terms of average accuracy, forgetting, and overwriting. The paper also shows that Mark can be applied to other benchmarks, and outperforms the state-of-the-art methods.  The authors also show that Mark is able to learn reusable knowledge that can be re-used across different tasks, and that it is more robust to forgetfulness.","This paper addresses the problem of Catastrophic Forgetting (CF) in artificial neural networks. The authors propose a method called MetA Reusable Knowledge (MARK) to address the issue of forgetting of old information. The proposed method is based on the idea of weight reusability, i.e. the reusing of weights in a network that have been trained on the same task. To achieve this, Mark uses shared weights from a common Knowledge Base (KB) and trainable masks. A metalearning approach to the KB is used to find the KB relevant weights, and the model is trained to re-use these weights. Mark is evaluated on the 20-Split-MiniImageNet dataset, and is shown to outperform existing methods in terms of average accuracy, forgetting, and overwriting. The paper also shows that Mark can be applied to other benchmarks, and outperforms the state-of-the-art methods.  The authors also show that Mark is able to learn reusable knowledge that can be re-used across different tasks, and that it is more robust to forgetfulness."
18828,SP:722c52467e384058f8fdffa254d0e8db47440a64,"exact solvers USED-FOR Mixed Integer Programming ( MIP ). Primal heuristics USED-FOR exact solvers. MIP heuristics PART-OF solver. hard - coded rules USED-FOR solvers. rules USED-FOR problem. rules USED-FOR heuristics. data - driven framework USED-FOR scheduling heuristics. scheduling heuristics PART-OF exact MIP solver. data - driven framework USED-FOR exact MIP solver. algorithm USED-FOR schedule. Task are real - world applications, and learning task. Method are primal heuristics, problem - specific schedule of heuristics, and academic MIP solver. Metric is average primal integral. ","This paper studies the problem of finding exact solvers for Mixed Integer Programming (MIP) using primal heuristics. In real-world applications, there is a large gap between the amount of work required to train primal solvers and the number of samples required to solve the problem. This paper proposes a data-driven framework to learn the exact solver for MIP using a set of hard-coded rules. The solver consists of two parts: (1) a solver that combines the MIP heuristic with the primal heuristic, and (2) a new problem-specific schedule for the primal solver. The main contribution of the paper is that the solver is built on top of the hard-coding rules for solving the problem, and that the rules for the problem are used to train the heuristic and the solvers. The paper also proposes a new algorithm for learning the new schedule. The algorithm is based on the idea that the optimal solution to a learning task should be the one that minimizes the average primal integral of the solution to the new learning task. The authors show that the exact MIP solver can be learned using the proposed algorithm, and they show that their algorithm is able to learn a scheduling heuristic that maximizes the expected return of the new problem. They also show that this algorithm can be applied to any problem with a problem- specific schedule.    The paper is well-written and well-motivated, and the idea of learning a problem specific schedule is interesting. However, there are a few issues that prevent the paper from being a clear contribution to the field. The problem is not well-grounded, and there are also a few questions that need to be answered. ","This paper studies the problem of finding exact solvers for Mixed Integer Programming (MIP) using primal heuristics. In real-world applications, there is a large gap between the amount of work required to train primal solvers and the number of samples required to solve the problem. This paper proposes a data-driven framework to learn the exact solver for MIP using a set of hard-coded rules. The solver consists of two parts: (1) a solver that combines the MIP heuristic with the primal heuristic, and (2) a new problem-specific schedule for the primal solver. The main contribution of the paper is that the solver is built on top of the hard-coding rules for solving the problem, and that the rules for the problem are used to train the heuristic and the solvers. The paper also proposes a new algorithm for learning the new schedule. The algorithm is based on the idea that the optimal solution to a learning task should be the one that minimizes the average primal integral of the solution to the new learning task. The authors show that the exact MIP solver can be learned using the proposed algorithm, and they show that their algorithm is able to learn a scheduling heuristic that maximizes the expected return of the new problem. They also show that this algorithm can be applied to any problem with a problem- specific schedule.    The paper is well-written and well-motivated, and the idea of learning a problem specific schedule is interesting. However, there are a few issues that prevent the paper from being a clear contribution to the field. The problem is not well-grounded, and there are also a few questions that need to be answered. "
18892,SP:5a21f0a49731dcb1d68deb06a75138e8e9d514d5,"it COMPARE real - world applications. real - world applications COMPARE it. self - driving cars CONJUNCTION robotics. robotics CONJUNCTION self - driving cars. real - world applications PART-OF reinforcement learning. robotics HYPONYM-OF reinforcement learning. self - driving cars HYPONYM-OF reinforcement learning. robotics HYPONYM-OF real - world applications. self - driving cars HYPONYM-OF real - world applications. sublinear regret EVALUATE-FOR algorithm. unknown parametric model USED-FOR trajectory labels. Task are reinforcement learning ( RL ), RL practice, and learning. OtherScientificTerm are binary feedback, and reward signal. Generic is this. ","This paper considers the problem of reinforcement learning (RL) in the setting where there is binary feedback, and the goal is to learn a policy that maximally maximizes the expected return of the agent. The authors consider an RL practice where the agent is given a reward signal, and it is trained in a similar way to real-world applications in reinforcement learning such as self-driving cars and robotics. The algorithm is shown to have a sublinear regret when the reward signal is binary and the agent has access to trajectory labels from an unknown parametric model. The paper also shows that this can be extended to the case where the learning is done in an unsupervised way.","This paper considers the problem of reinforcement learning (RL) in the setting where there is binary feedback, and the goal is to learn a policy that maximally maximizes the expected return of the agent. The authors consider an RL practice where the agent is given a reward signal, and it is trained in a similar way to real-world applications in reinforcement learning such as self-driving cars and robotics. The algorithm is shown to have a sublinear regret when the reward signal is binary and the agent has access to trajectory labels from an unknown parametric model. The paper also shows that this can be extended to the case where the learning is done in an unsupervised way."
18956,SP:e66bd9582058ba0f6091bb1042ce2ecfdaae1515,"node embedding CONJUNCTION graph pooling methods. graph pooling methods CONJUNCTION node embedding. Graph neural networks USED-FOR representing graph - structured data. edges PART-OF graph. edges PART-OF graph. edges USED-FOR discrimination. graph reconstruction and generation CONJUNCTION graph classification tasks. graph classification tasks CONJUNCTION graph reconstruction and generation. graph classification tasks HYPONYM-OF tasks. graph reconstruction and generation HYPONYM-OF tasks. nodes PART-OF hypergraph. edges PART-OF graph. Dual Hypergraph Transformation ( DHT ) USED-FOR edge representation learning framework. message - passing techniques USED-FOR node representations. message - passing techniques USED-FOR edges. dual hypergraph construction USED-FOR message - passing techniques. hypergraphs USED-FOR edge representations. method COMPARE graph representation learning methods. graph representation learning methods COMPARE method. edge representation learning method USED-FOR graph representation and generation. graph datasets USED-FOR graph representation and generation. hypergraphs USED-FOR edge representation learning method. graph datasets EVALUATE-FOR hypergraphs. graph datasets EVALUATE-FOR edge representation learning method. lossless compression of the nodes CONJUNCTION removal of irrelevant edges. removal of irrelevant edges CONJUNCTION lossless compression of the nodes. edge representation learning and pooling method COMPARE graph pooling methods. graph pooling methods COMPARE edge representation learning and pooling method. graph pooling methods USED-FOR graph classification. edge representation learning and pooling method USED-FOR graph classification. Material is graph - structured data. Generic is they. OtherScientificTerm is connectivity. Method are graph representation learning, holistic graph - level edge representations, and edge representation learning. ","Graph neural networks have been widely used for representing graph-structured data, but they have not been applied to the problem of graph representation learning. In this paper, the authors propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT) that learns holistic graph-level edge representations. Specifically, the edges in a graph are represented as a hypergraph, where nodes in the hypergraph are connected to each other via connectivity. The edges in the graph are used for discrimination, and message-passing techniques are used to learn node representations based on the dual hypergraph construction. Experiments are conducted on three tasks: graph reconstruction and generation, graph classification tasks, and graph representation and generation on three graph datasets. The edge representations learned using hypergraphs are shown to outperform the existing graph embedding and graph pooling methods, and the proposed method is shown to be competitive with the state-of-the-art edge representation and pooling method on three different graph datasets, and is also shown to perform well on graph classification. The authors also show that the edge representation learned by the proposed edge representation learns on three existing graph datasets outperforms the state of the art in terms of lossless compression of the nodes and removal of irrelevant edges.   ","Graph neural networks have been widely used for representing graph-structured data, but they have not been applied to the problem of graph representation learning. In this paper, the authors propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT) that learns holistic graph-level edge representations. Specifically, the edges in a graph are represented as a hypergraph, where nodes in the hypergraph are connected to each other via connectivity. The edges in the graph are used for discrimination, and message-passing techniques are used to learn node representations based on the dual hypergraph construction. Experiments are conducted on three tasks: graph reconstruction and generation, graph classification tasks, and graph representation and generation on three graph datasets. The edge representations learned using hypergraphs are shown to outperform the existing graph embedding and graph pooling methods, and the proposed method is shown to be competitive with the state-of-the-art edge representation and pooling method on three different graph datasets, and is also shown to perform well on graph classification. The authors also show that the edge representation learned by the proposed edge representation learns on three existing graph datasets outperforms the state of the art in terms of lossless compression of the nodes and removal of irrelevant edges.   "
19020,SP:e398873e29b05176e1d52dc6f86a59a4f405e6fd,"Mutual information ( MI ) maximization USED-FOR learning representations of data. representations USED-FOR learning. representations USED-FOR reinforcement learning ( RL ). representations USED-FOR RL. MI objectives USED-FOR representations. samples of high - dimensional observations USED-FOR MI. state representation USED-FOR optimal policy. objectives USED-FOR insufficient representations. visual observations FEATURE-OF simulated game environment. OtherScientificTerm are irrelevant and redundant information, MI based objectives, and structure of the MDP. Task is control. Generic is methods. ","This paper studies the problem of learning representations of data using Mutual information (MI) maximization. The authors show that existing representations for reinforcement learning (RL) can be learned using MI objectives that are insensitive to irrelevant and redundant information. They also show that MI objectives for learning representations that are sensitive to irrelevant information can be used to learn representations for RL that are more robust to the structure of the MDP. Finally, they show that using samples of high-dimensional observations from MI is sufficient to learn an optimal policy that maximizes the mutual information between the state representation of the agent and the environment.    The paper is well-written, well-motivated, and well-structured. The paper provides a detailed analysis of existing MI based objectives. They show that insufficient representations are learned using these objectives, and that the optimal policy is learned by learning a state representation that is invariant to the irrelevant information.  They also provide experiments on a simulated game environment with visual observations, where the agent is given a set of visual observations and the goal is to learn a control policy that minimizes the optimal state representation. The results show that the proposed methods outperform existing methods. ","This paper studies the problem of learning representations of data using Mutual information (MI) maximization. The authors show that existing representations for reinforcement learning (RL) can be learned using MI objectives that are insensitive to irrelevant and redundant information. They also show that MI objectives for learning representations that are sensitive to irrelevant information can be used to learn representations for RL that are more robust to the structure of the MDP. Finally, they show that using samples of high-dimensional observations from MI is sufficient to learn an optimal policy that maximizes the mutual information between the state representation of the agent and the environment.    The paper is well-written, well-motivated, and well-structured. The paper provides a detailed analysis of existing MI based objectives. They show that insufficient representations are learned using these objectives, and that the optimal policy is learned by learning a state representation that is invariant to the irrelevant information.  They also provide experiments on a simulated game environment with visual observations, where the agent is given a set of visual observations and the goal is to learn a control policy that minimizes the optimal state representation. The results show that the proposed methods outperform existing methods. "
19084,SP:50181f740910195d3a50dd7d7f8cbb1c476d730b,"steerable convolution USED-FOR 3D semantic analysis. SS - Conv USED-FOR steerable convolution. sparse tensors USED-FOR steerable convolution. pipeline USED-FOR precise estimation of object poses. SS - Conv USED-FOR pipeline. Feature - Steering module USED-FOR pose refinement. SE(3)-equivariance USED-FOR Feature - Steering module. instance - level 6D pose estimation CONJUNCTION category - level 6D pose and size estimation. category - level 6D pose and size estimation CONJUNCTION instance - level 6D pose estimation. category - level 6D pose and size estimation CONJUNCTION categorylevel 6D pose tracking. categorylevel 6D pose tracking CONJUNCTION category - level 6D pose and size estimation. categorylevel 6D pose tracking HYPONYM-OF 3D object semantic analysis. instance - level 6D pose estimation HYPONYM-OF 3D object semantic analysis. category - level 6D pose and size estimation HYPONYM-OF 3D object semantic analysis. pipeline COMPARE methods. methods COMPARE pipeline. metrics EVALUATE-FOR tasks. tasks EVALUATE-FOR pipeline. tasks EVALUATE-FOR methods. metrics EVALUATE-FOR pipeline. metrics EVALUATE-FOR methods. SS - Conv USED-FOR pipeline. SS - Conv COMPARE convolutions. convolutions COMPARE SS - Conv. accuracy CONJUNCTION efficiency. efficiency CONJUNCTION accuracy. efficiency EVALUATE-FOR SS - Conv. accuracy EVALUATE-FOR SS - Conv. efficiency EVALUATE-FOR convolutions. accuracy EVALUATE-FOR convolutions. Method are SE(3)-equivariant deep feature learning, and Sparse Steerable Convolution ( SS - Conv ). Material is dense, volumetric data. Task is processing of 3D data. Generic are designs, and code. ","This paper proposes a steerable convolution for 3D semantic analysis based on sparse tensors. The authors propose to use SE(3)-equivariant deep feature learning to steer the processing of 3D data. The proposed pipeline, called Sparse Steerable Convolution (SS-Conv), is a pipeline for precise estimation of object poses. The Feature-Steering module is built on top of previous work that leverages SE(2) and (3) to achieve pose refinement. Experiments are conducted on 3D object semantic analysis (instance-level 6D pose estimation, category-level pose and size estimation, and categorylevel 6d pose tracking). The authors show that the proposed pipeline outperforms existing methods on all three metrics for the three tasks. In addition, the authors also show that SS-conv outperforms other convolutions in terms of accuracy and efficiency.    The authors also provide a detailed analysis of the proposed designs. The code is well-written and easy to follow.","This paper proposes a steerable convolution for 3D semantic analysis based on sparse tensors. The authors propose to use SE(3)-equivariant deep feature learning to steer the processing of 3D data. The proposed pipeline, called Sparse Steerable Convolution (SS-Conv), is a pipeline for precise estimation of object poses. The Feature-Steering module is built on top of previous work that leverages SE(2) and (3) to achieve pose refinement. Experiments are conducted on 3D object semantic analysis (instance-level 6D pose estimation, category-level pose and size estimation, and categorylevel 6d pose tracking). The authors show that the proposed pipeline outperforms existing methods on all three metrics for the three tasks. In addition, the authors also show that SS-conv outperforms other convolutions in terms of accuracy and efficiency.    The authors also provide a detailed analysis of the proposed designs. The code is well-written and easy to follow."
19148,SP:d746bfb200577c980d92727bb0b1a3c23e7bfdc5,"Attention USED-FOR vision transformers. informative tokens USED-FOR image recognition. dynamic token sparsification framework USED-FOR redundant tokens. lightweight prediction module USED-FOR importance score. features USED-FOR importance score. module USED-FOR redundant tokens. module PART-OF layers. layers USED-FOR redundant tokens. attention masking strategy USED-FOR prediction module. hierarchically pruning USED-FOR method. accuracy EVALUATE-FOR vision transformers. FLOPs EVALUATE-FOR method. accuracy EVALUATE-FOR method. throughput EVALUATE-FOR method. DynamicViT models COMPARE CNNs. CNNs COMPARE DynamicViT models. CNNs CONJUNCTION vision transformers. vision transformers CONJUNCTION CNNs. DynamicViT models COMPARE vision transformers. vision transformers COMPARE DynamicViT models. dynamic token sparsification framework USED-FOR DynamicViT models. ImageNet EVALUATE-FOR CNNs. ImageNet EVALUATE-FOR vision transformers. complexity / accuracy trade - offs EVALUATE-FOR DynamicViT models. Method are self - attention, and raoyongming. OtherScientificTerm is unstructured sparse tokens. Generic is framework. ","This paper proposes a dynamic token sparsification framework to remove redundant tokens from self-attention in vision transformers. The authors argue that redundant tokens are important for informative tokens for image recognition, but unstructured sparse tokens are also important. To this end, the authors propose a lightweight prediction module that generates an importance score based on the features of each token. This module is then incorporated into other layers to remove the redundant tokens. The proposed method is based on hierarchically pruning, and the authors also propose an attention masking strategy to make the prediction module more interpretable. Experiments on ImageNet show that the proposed method can reduce FLOPs and improve throughput while maintaining the accuracy of Vision transformers in terms of complexity/accuracy trade-offs. The DynamicViT models trained with the proposed DynamicTokenSparsification (DTSP) framework outperform standard CNNs and vision transformerers in ImageNet.    The authors also provide a theoretical analysis of the proposed framework. ","This paper proposes a dynamic token sparsification framework to remove redundant tokens from self-attention in vision transformers. The authors argue that redundant tokens are important for informative tokens for image recognition, but unstructured sparse tokens are also important. To this end, the authors propose a lightweight prediction module that generates an importance score based on the features of each token. This module is then incorporated into other layers to remove the redundant tokens. The proposed method is based on hierarchically pruning, and the authors also propose an attention masking strategy to make the prediction module more interpretable. Experiments on ImageNet show that the proposed method can reduce FLOPs and improve throughput while maintaining the accuracy of Vision transformers in terms of complexity/accuracy trade-offs. The DynamicViT models trained with the proposed DynamicTokenSparsification (DTSP) framework outperform standard CNNs and vision transformerers in ImageNet.    The authors also provide a theoretical analysis of the proposed framework. "
19212,SP:d0b6cde42b1cba5e6e3c7c5131426fd84adbd3d7,"cross - validation methods CONJUNCTION conformal prediction. conformal prediction CONJUNCTION cross - validation methods. holdout methods CONJUNCTION cross - validation methods. cross - validation methods CONJUNCTION holdout methods. inference USED-FOR regression function. methods USED-FOR predictive inference. distribution - free guarantees USED-FOR predictive inference. methods USED-FOR distribution - free guarantees. inference USED-FOR inference. inference USED-FOR conditional mean. inference HYPONYM-OF regression function. conformal prediction HYPONYM-OF methods. holdout methods HYPONYM-OF methods. cross - validation methods HYPONYM-OF methods. non - vanishing width FEATURE-OF confidence interval. inference USED-FOR E [ Y |X ]. finite setting CONJUNCTION continuous setting. continuous setting CONJUNCTION finite setting. Task is data analysis problems. OtherScientificTerm are distributional assumptions, inference guarantees, sample size, and vanishing - width confidence intervals. ","This paper studies the problem of data analysis problems in which the data is drawn from a distribution with distributional assumptions. The authors propose three methods for obtaining distribution-free guarantees for predictive inference: holdout methods, cross-validation methods, and conformal prediction. They show that inference for a regression function (i.e., inference for the conditional mean) with inference for E [Y |X] and E [X] can be obtained with inference guarantees that are non-vanishing in terms of the number of samples and the sample size. They also show that the confidence interval of the confidence intervals has a non-disappearing width.    The main contribution of this paper is that the authors provide a theoretical analysis of the vanishing-width confidence intervals obtained by inference in the finite setting and in the continuous setting. In particular, the authors show that under certain assumptions on the distributional assumption, the inference guarantees obtained by the proposed methods (e.g., cross-Validation methods and cross-validation methods) can be extended to the case of predictive inference. ","This paper studies the problem of data analysis problems in which the data is drawn from a distribution with distributional assumptions. The authors propose three methods for obtaining distribution-free guarantees for predictive inference: holdout methods, cross-validation methods, and conformal prediction. They show that inference for a regression function (i.e., inference for the conditional mean) with inference for E [Y |X] and E [X] can be obtained with inference guarantees that are non-vanishing in terms of the number of samples and the sample size. They also show that the confidence interval of the confidence intervals has a non-disappearing width.    The main contribution of this paper is that the authors provide a theoretical analysis of the vanishing-width confidence intervals obtained by inference in the finite setting and in the continuous setting. In particular, the authors show that under certain assumptions on the distributional assumption, the inference guarantees obtained by the proposed methods (e.g., cross-Validation methods and cross-validation methods) can be extended to the case of predictive inference. "
19276,SP:123952325765c040c3078fc7dca2b6d370e55590,bias mitigation methods USED-FOR DNN models. learning debiased encoders USED-FOR bias mitigation methods. instance - level annotations USED-FOR sensitive attributes. fairness sensitive information PART-OF encoder. discrimination EVALUATE-FOR DNN models. task - specific classification head PART-OF DNN models. Representation Neutralization for Fairness ( RNF ) HYPONYM-OF mitigation technique. neutralized representations USED-FOR classification head. ground - truth label CONJUNCTION sensitive attributes. sensitive attributes CONJUNCTION ground - truth label. classification head PART-OF DNN model. neutralized representations USED-FOR DNN model. RNF USED-FOR classification head. fairness sensitive information PART-OF encoder representations. bias - amplified model USED-FOR proxy annotations. proxy annotations USED-FOR sensitive attributes. bias - amplified model USED-FOR low - resource settings. benchmark datasets EVALUATE-FOR RNF framework. RNF framework USED-FOR DNN models. benchmark datasets EVALUATE-FOR discrimination of DNN models. task - specific performance EVALUATE-FOR discrimination of DNN models. task - specific performance EVALUATE-FOR RNF framework. Method is biased representations. Task is fairness. OtherScientificTerm is sensitive attribute annotations. ,"This paper studies the problem of learning debiased encoders for bias mitigation methods for DNN models. The authors propose a new mitigation technique called Representation Neutralization for Fairness (RNF), which aims to mitigate the impact of biased representations on fairness. Specifically, the authors propose to remove the fairness sensitive information from the encoder and use instance-level annotations to identify sensitive attributes from the ground-truth label and sensitive attributes for sensitive attribute annotations. They show that this discrimination improves the discrimination performance of DNNs with a single task-specific classification head. They also show that the neutralized representations of the classification head of a DNN model trained with RNF can be used to improve the performance of a classification head trained with a bias-amplified version of RNF. Finally, they demonstrate that the bias-implified model can also be used for proxy annotations for sensitive attributes using proxy annotations from the biased-im amplified model in low-resource settings. The proposed RNF framework is evaluated on several benchmark datasets and shows that the proposed method is able to achieve better discrimination performance for different tasks. ","This paper studies the problem of learning debiased encoders for bias mitigation methods for DNN models. The authors propose a new mitigation technique called Representation Neutralization for Fairness (RNF), which aims to mitigate the impact of biased representations on fairness. Specifically, the authors propose to remove the fairness sensitive information from the encoder and use instance-level annotations to identify sensitive attributes from the ground-truth label and sensitive attributes for sensitive attribute annotations. They show that this discrimination improves the discrimination performance of DNNs with a single task-specific classification head. They also show that the neutralized representations of the classification head of a DNN model trained with RNF can be used to improve the performance of a classification head trained with a bias-amplified version of RNF. Finally, they demonstrate that the bias-implified model can also be used for proxy annotations for sensitive attributes using proxy annotations from the biased-im amplified model in low-resource settings. The proposed RNF framework is evaluated on several benchmark datasets and shows that the proposed method is able to achieve better discrimination performance for different tasks. "
19340,SP:210eb2c811f966bb1ac53932cacabbad9bb608fe,translations CONJUNCTION rotations. rotations CONJUNCTION translations. rotations FEATURE-OF learning models. translations FEATURE-OF learning models. learning models USED-FOR image analysis. Convolutional Neural Networks ( CNN ) USED-FOR image analysis. convolutions USED-FOR They. physics FEATURE-OF Bessel functions. Bessel functions USED-FOR convolutional layer. Task is medical imaging. Method is Bessel - CNNs ( B - CNNs ). OtherScientificTerm is rotation angles. ,"This paper proposes a new class of convolutional neural networks, called Bessel-CNNs, for medical imaging. They are based on Bessel functions, which are a generalization of convolutions. The authors show that the Bessel function is invariant to rotations and can be used as a regularizer for convolutions, which can be applied to any convolution layer of a Bessel network. They also show that Bessels are invariant in terms of rotation angles.   ","This paper proposes a new class of convolutional neural networks, called Bessel-CNNs, for medical imaging. They are based on Bessel functions, which are a generalization of convolutions. The authors show that the Bessel function is invariant to rotations and can be used as a regularizer for convolutions, which can be applied to any convolution layer of a Bessel network. They also show that Bessels are invariant in terms of rotation angles.   "
19404,SP:ee51ecbd476d5b65903c942a62be89ff5d91698b,large - scale solver USED-FOR kernel ridge regression. ParK HYPONYM-OF large - scale solver. ParK HYPONYM-OF kernel ridge regression. random projections CONJUNCTION iterative optimization. iterative optimization CONJUNCTION random projections. partitioning CONJUNCTION random projections. random projections CONJUNCTION partitioning. partitioning CONJUNCTION iterative optimization. iterative optimization CONJUNCTION partitioning. partitioning PART-OF approach. iterative optimization PART-OF approach. space and time complexity EVALUATE-FOR approach. random projections PART-OF approach. statistical accuracy EVALUATE-FOR approach. local effective dimension CONJUNCTION bias. bias CONJUNCTION local effective dimension. orthogonality FEATURE-OF local estimators. feature space COMPARE input space. input space COMPARE feature space. feature space FEATURE-OF partitions. statistical - computational tradeoff EVALUATE-FOR model. large - scale datasets EVALUATE-FOR method. ,"This paper proposes a large-scale solver for kernel ridge regression, called ParK, which is a general approach that combines partitioning, random projections, iterative optimization, and partitioning with space and time complexity. The authors show that the proposed approach can achieve state-of-the-art statistical accuracy while maintaining a high statistical accuracy in terms of both local effective dimension and bias. The paper also shows that the orthogonality of the local estimators is preserved, and that the feature space of the partitions is orthogonal to the input space. The proposed model achieves a statistical-computational tradeoff between the number of partitions and the number (number of iterations) of iterations. The method is tested on several large scale datasets, and is shown to perform well.","This paper proposes a large-scale solver for kernel ridge regression, called ParK, which is a general approach that combines partitioning, random projections, iterative optimization, and partitioning with space and time complexity. The authors show that the proposed approach can achieve state-of-the-art statistical accuracy while maintaining a high statistical accuracy in terms of both local effective dimension and bias. The paper also shows that the orthogonality of the local estimators is preserved, and that the feature space of the partitions is orthogonal to the input space. The proposed model achieves a statistical-computational tradeoff between the number of partitions and the number (number of iterations) of iterations. The method is tested on several large scale datasets, and is shown to perform well."
19468,SP:1f096d6fabd5b1fde43d06c552d46d87cd35cb4a,"reinforcement learning settings USED-FOR Neural agents. discrete tokens USED-FOR Neural agents. one - hot vectors USED-FOR discrete communication tokens. zero - shot understanding HYPONYM-OF communication. natural language processing USED-FOR word embedding techniques. discrete tokens USED-FOR neural agent architectures. continuous space USED-FOR discrete tokens. technique USED-FOR communication. technique COMPARE one - hot tokens. one - hot tokens COMPARE technique. decision theoretic framework EVALUATE-FOR technique. Generic are techniques, and method. OtherScientificTerm are human communication, unlabeled emergent agent communication, and one - hot communication. ","This paper studies the problem of learning discrete tokens for Neural agents in reinforcement learning settings. Neural agents typically learn discrete tokens as discrete tokens in the form of one-hot vectors, which are discrete communication tokens that are generated from human communication. The authors propose to use natural language processing to learn word embedding techniques that can be used to encode discrete tokens into neural agent architectures. They show that the discrete tokens can be encoded in a continuous space, which is a natural extension of existing techniques. They also show that this method can be applied to the case of unlabeled emergent agent communication, which can be seen as a form of communication that is useful for zero-shot understanding. They demonstrate that this technique is able to learn communication in a way that is more efficient than using one- hot tokens, and they show that their technique is also able to be used in a decision theoretic framework.   ","This paper studies the problem of learning discrete tokens for Neural agents in reinforcement learning settings. Neural agents typically learn discrete tokens as discrete tokens in the form of one-hot vectors, which are discrete communication tokens that are generated from human communication. The authors propose to use natural language processing to learn word embedding techniques that can be used to encode discrete tokens into neural agent architectures. They show that the discrete tokens can be encoded in a continuous space, which is a natural extension of existing techniques. They also show that this method can be applied to the case of unlabeled emergent agent communication, which can be seen as a form of communication that is useful for zero-shot understanding. They demonstrate that this technique is able to learn communication in a way that is more efficient than using one- hot tokens, and they show that their technique is also able to be used in a decision theoretic framework.   "
19532,SP:8630ccc627534f9033bced04e2137a897ffef701,they COMPARE convolutional networks. convolutional networks COMPARE they. Transformers USED-FOR computer vision. generalization COMPARE convolutional networks. convolutional networks COMPARE generalization. model capacity FEATURE-OF Transformers. depthwise Convolution CONJUNCTION self - Attention. self - Attention CONJUNCTION depthwise Convolution. convolution layers CONJUNCTION attention layers. attention layers CONJUNCTION convolution layers. CoAtNets HYPONYM-OF hybrid models. capacity CONJUNCTION efficiency. efficiency CONJUNCTION capacity. generalization CONJUNCTION capacity. capacity CONJUNCTION generalization. coat ” nets HYPONYM-OF CoAtNets. relative attention USED-FOR hybrid models. relative attention USED-FOR depthwise Convolution. relative attention USED-FOR self - Attention. CoAtNets COMPARE CoAtNet. CoAtNet COMPARE CoAtNets. resource constraints FEATURE-OF CoAtNets. JFT-3B USED-FOR CoAtNet. top-1 accuracy EVALUATE-FOR CoAtNet. top-1 accuracy EVALUATE-FOR it. ImageNet EVALUATE-FOR it. OtherScientificTerm is inductive bias. Generic is architectures. Metric is ImageNet top-1 accuracy. Material is ImageNet-21 K. ,"This paper studies the generalization performance of Transformers in computer vision, and shows that they outperform standard convolutional networks in terms of model capacity, generalization, and efficiency. The authors propose two hybrid models, called CoAtNets, which are “coat” nets that combine depthwise Convolution and self-Attention, and show that they achieve better generalization and efficiency compared to standard ConvNets. They also show that the inductive bias of these architectures is that the convolution layers and the attention layers are related, and that the relative attention is used to train the hybrid models. The paper also shows that the depthwise convolution can be trained with relative attention, and the self-attention can also be trained using relative attention. Finally, the paper shows that CoNets with “coat” networks (i.e. “capes”) achieve better capacity, efficiency, and generalization compared to the state-of-the-art, and it achieves top-1 accuracy on ImageNet-21K, and achieves a similar level of performance as CoAtNet with JFT-3B under resource constraints.   ","This paper studies the generalization performance of Transformers in computer vision, and shows that they outperform standard convolutional networks in terms of model capacity, generalization, and efficiency. The authors propose two hybrid models, called CoAtNets, which are “coat” nets that combine depthwise Convolution and self-Attention, and show that they achieve better generalization and efficiency compared to standard ConvNets. They also show that the inductive bias of these architectures is that the convolution layers and the attention layers are related, and that the relative attention is used to train the hybrid models. The paper also shows that the depthwise convolution can be trained with relative attention, and the self-attention can also be trained using relative attention. Finally, the paper shows that CoNets with “coat” networks (i.e. “capes”) achieve better capacity, efficiency, and generalization compared to the state-of-the-art, and it achieves top-1 accuracy on ImageNet-21K, and achieves a similar level of performance as CoAtNet with JFT-3B under resource constraints.   "
19596,SP:d3ecbeeffa5ab365743ba8653c6739f24742ee31,second - order oracle bound USED-FOR expected risk. expected risk FEATURE-OF weighted majority vote. second - order oracle bound USED-FOR weighted majority vote. one - sided Chebyshev ’s ) HYPONYM-OF parametric form of the ChebyshevCantelli inequality. parametric form of the ChebyshevCantelli inequality USED-FOR bound. form USED-FOR optimization challenge. Chebyshev - Cantelli inequality CONJUNCTION C - bounds. C - bounds CONJUNCTION Chebyshev - Cantelli inequality. optimization challenge USED-FOR prior oracle bounds. Chebyshev - Cantelli inequality USED-FOR prior oracle bounds. it USED-FOR oracle bound. second order Markov ’s inequality USED-FOR it. second order Markov ’s inequality USED-FOR oracle bound. PAC - Bayesian bounding CONJUNCTION Bennett ’s inequality. Bennett ’s inequality CONJUNCTION PAC - Bayesian bounding. PAC - Bayes - Bennett HYPONYM-OF concentration of measure inequality. PAC - Bayesian bounding PART-OF it. Bennett ’s inequality PART-OF it. it USED-FOR empirical estimation of the oracle bound. PAC - Bayes - Bennett inequality COMPARE PAC - Bayes - Bernstein inequality. PAC - Bayes - Bernstein inequality COMPARE PAC - Bayes - Bennett inequality. ChebyshevCantelli inequality CONJUNCTION PAC - Bayes - Bennett inequality. PAC - Bayes - Bennett inequality CONJUNCTION ChebyshevCantelli inequality. parametric form USED-FOR concentration of measure. PAC - Bayes - Bennett inequality USED-FOR concentration of measure. parametric form CONJUNCTION PAC - Bayes - Bennett inequality. PAC - Bayes - Bennett inequality CONJUNCTION parametric form. parametric form FEATURE-OF ChebyshevCantelli inequality. Task is minimization. Generic is bounds. ,"This paper proposes a second-order oracle bound for the expected risk of a weighted majority vote in the presence of a large majority of voters. The bound is based on a parametric form of the ChebyshevCantelli inequality (one-sided Chebysherv’s) which is a generalization of the prior oracle bounds. The authors show that this form can be used to solve the optimization challenge of prior prior work, and that the prior work can be seen as an optimization challenge to prior work on the prior.    The prior work builds upon prior work that uses the previous work on prior work (Chen et al. 2017) to derive prior bounds for the prior on the concentration of measure inequality (PAC-Bayes-Bennett and PAC-Bernstein).   In this work, the authors extend the prior to the case of minimization and show that it is equivalent to the prior for the oracle upper bound of the second order Markov's inequality.  The authors also show that prior work also uses the prior and prior bounds in this setting.  In particular, the prior works in this paper use the prior Chebyshes inequality and prior C-bounds to obtain prior bounds that are equivalent to prior results in the case where the prior is non-asymptotic and non-convex.  Finally, the paper shows that it can be combined with prior work in the form of a PAC-Bayesian bounding and the prior, which combines the prior PAC-BENNETT inequality with the prior (and prior work).  The paper also shows that the proposed method can be applied to the empirical estimation of the oracles and that it converges to an empirical bound based on the previous bound.  This paper is well motivated and well-motivated, and the paper is clearly written. The paper is also well-written and well motivated. ","This paper proposes a second-order oracle bound for the expected risk of a weighted majority vote in the presence of a large majority of voters. The bound is based on a parametric form of the ChebyshevCantelli inequality (one-sided Chebysherv’s) which is a generalization of the prior oracle bounds. The authors show that this form can be used to solve the optimization challenge of prior prior work, and that the prior work can be seen as an optimization challenge to prior work on the prior.    The prior work builds upon prior work that uses the previous work on prior work (Chen et al. 2017) to derive prior bounds for the prior on the concentration of measure inequality (PAC-Bayes-Bennett and PAC-Bernstein).   In this work, the authors extend the prior to the case of minimization and show that it is equivalent to the prior for the oracle upper bound of the second order Markov's inequality.  The authors also show that prior work also uses the prior and prior bounds in this setting.  In particular, the prior works in this paper use the prior Chebyshes inequality and prior C-bounds to obtain prior bounds that are equivalent to prior results in the case where the prior is non-asymptotic and non-convex.  Finally, the paper shows that it can be combined with prior work in the form of a PAC-Bayesian bounding and the prior, which combines the prior PAC-BENNETT inequality with the prior (and prior work).  The paper also shows that the proposed method can be applied to the empirical estimation of the oracles and that it converges to an empirical bound based on the previous bound.  This paper is well motivated and well-motivated, and the paper is clearly written. The paper is also well-written and well motivated. "
19660,SP:5bac542a6532d43cf100e085398b4a4783719814,"audio - visual video parsing task USED-FOR audio or visual event categories. method USED-FOR audio or visual events. common and diverse event semantics USED-FOR audio or visual events. common and diverse event semantics USED-FOR method. method USED-FOR event co - occurrence. method COMPARE methods. methods COMPARE method. weakly - supervised audio - visual video parsing EVALUATE-FOR method. weakly - supervised audio - visual video parsing EVALUATE-FOR methods. OtherScientificTerm are audio and visual events, cross - modality co - occurrence, supervisory signals, and video - level annotations. Method is parsing model. ",This paper tackles the audio-visual video parsing task for audio or visual event categories. The authors propose a new method that uses common and diverse event semantics to distinguish between audio and visual events. The idea is to use cross-modality co-occurrence as a metric to measure the amount of supervisory signals that are sent to the parsing model. The proposed method is evaluated on weakly-supervised audio-video video parsing and shows that the proposed method outperforms existing methods in the task. The paper also provides video-level annotations.,This paper tackles the audio-visual video parsing task for audio or visual event categories. The authors propose a new method that uses common and diverse event semantics to distinguish between audio and visual events. The idea is to use cross-modality co-occurrence as a metric to measure the amount of supervisory signals that are sent to the parsing model. The proposed method is evaluated on weakly-supervised audio-video video parsing and shows that the proposed method outperforms existing methods in the task. The paper also provides video-level annotations.
19724,SP:8fd6a03c1794afa524328d45f4232eacf6f86693,"federated learning ( FL ) USED-FOR global model. quantized and personalized FL algorithm USED-FOR collective ( personalized model compression ) training. knowledge distillation ( KD ) USED-FOR collective ( personalized model compression ) training. quantization parameters CONJUNCTION model dimensions / structures. model dimensions / structures CONJUNCTION quantization parameters. model dimensions / structures FEATURE-OF compressed personalized models. quantization parameters FEATURE-OF compressed personalized models. algorithm USED-FOR quantized models. relaxed optimization problem USED-FOR algorithm. knowledge distillation loss USED-FOR local client objectives. global model USED-FOR knowledge distillation loss. model dimension EVALUATE-FOR compressed model. knowledge distillation loss USED-FOR compressed personalization framework. alternating proximal gradient update USED-FOR compressed personalization problem. FedAvg CONJUNCTION local training of clients. local training of clients CONJUNCTION FedAvg. QuPeD COMPARE personalized FL methods. personalized FL methods COMPARE QuPeD. personalized FL methods CONJUNCTION FedAvg. FedAvg CONJUNCTION personalized FL methods. QuPeD COMPARE FedAvg. FedAvg COMPARE QuPeD. QuPeD COMPARE local training of clients. local training of clients COMPARE QuPeD. local training of clients HYPONYM-OF personalized FL methods. Method are FL algorithms, and ( federated ) learning process. Material is heterogeneous data. Task is personalization. OtherScientificTerm is quantization values. ","This paper studies the problem of federated learning (FL) to compress a global model in the presence of heterogeneous data. The authors propose a quantized and personalized FL algorithm for collective (personalized model compression) training based on knowledge distillation (KD) and show that the proposed algorithm is able to compress both quantized models and personalized models with different quantization parameters and model dimensions/structures. The paper also shows that the compressed personalization problem can be decomposed into an alternating proximal gradient update, where the algorithm compresses the (federated) learning process and compresses personalization. The proposed algorithm, QuPeD, uses a relaxed optimization problem to compute the quantization values for each client, and then computes the local client objectives based on the knowledgedistillation loss on the global model. The compressed model dimension of the compressed model is then used as a metric to measure the model dimension for the local clients, and the compression loss is used to optimize the knowledge distilation loss for the compression of the global client objectives. Experiments are conducted to show the effectiveness of the proposed compressed personalisation framework in the context of the existing personalized FL algorithms. The results are shown to outperform the state-of-the-art personalized FL methods (FedAvg and FedAvg with local training of clients) as well as the state of the art personalization methods (QuPeD).   ","This paper studies the problem of federated learning (FL) to compress a global model in the presence of heterogeneous data. The authors propose a quantized and personalized FL algorithm for collective (personalized model compression) training based on knowledge distillation (KD) and show that the proposed algorithm is able to compress both quantized models and personalized models with different quantization parameters and model dimensions/structures. The paper also shows that the compressed personalization problem can be decomposed into an alternating proximal gradient update, where the algorithm compresses the (federated) learning process and compresses personalization. The proposed algorithm, QuPeD, uses a relaxed optimization problem to compute the quantization values for each client, and then computes the local client objectives based on the knowledgedistillation loss on the global model. The compressed model dimension of the compressed model is then used as a metric to measure the model dimension for the local clients, and the compression loss is used to optimize the knowledge distilation loss for the compression of the global client objectives. Experiments are conducted to show the effectiveness of the proposed compressed personalisation framework in the context of the existing personalized FL algorithms. The results are shown to outperform the state-of-the-art personalized FL methods (FedAvg and FedAvg with local training of clients) as well as the state of the art personalization methods (QuPeD).   "
19788,SP:fca8b4f1e765cf1724a37f0ae9a7dac1cb79c8b1,"Constrained clustering PART-OF machine learning. partially labeled data USED-FOR prior information. prior information USED-FOR it. partially labeled data USED-FOR it. framework USED-FOR constrained clustering. deep generative models USED-FOR framework. stochastic gradient variational inference USED-FOR framework. domain knowledge USED-FOR model ( DC - GMM ). probabilistic relations FEATURE-OF domain knowledge. DC - GMM COMPARE deep constrained clustering methods. deep constrained clustering methods COMPARE DC - GMM. robustness EVALUATE-FOR deep constrained clustering methods. data sets EVALUATE-FOR DC - GMM. data sets EVALUATE-FOR deep constrained clustering methods. robustness EVALUATE-FOR DC - GMM. real - world applications EVALUATE-FOR approach. Generic are model, and constraints. OtherScientificTerm are prior clustering preferences, and pairwise constraints. Task is clustering process. ","Constrained clustering is an important problem in machine learning, and it is often used to leverage prior information from partially labeled data. This paper proposes a new framework for constrained clustering based on deep generative models. The framework is based on stochastic gradient variational inference, and the model (DC-GMM) uses domain knowledge from probabilistic relations between the prior clustering preferences and the constraints. The authors show that the proposed DC-GMM outperforms state-of-the-art deep constrained clusters and robustness to perturbations in the clustering process. The proposed approach is evaluated on several real-world applications, and DC-GM is shown to be more robust to changes in the pairwise constraints. ","Constrained clustering is an important problem in machine learning, and it is often used to leverage prior information from partially labeled data. This paper proposes a new framework for constrained clustering based on deep generative models. The framework is based on stochastic gradient variational inference, and the model (DC-GMM) uses domain knowledge from probabilistic relations between the prior clustering preferences and the constraints. The authors show that the proposed DC-GMM outperforms state-of-the-art deep constrained clusters and robustness to perturbations in the clustering process. The proposed approach is evaluated on several real-world applications, and DC-GM is shown to be more robust to changes in the pairwise constraints. "
19852,SP:84379c0c881b7390ecc22fb398edfaf66d1af1ff,Neural Tangent Kernel ( NTK ) USED-FOR infinitely - wide neural networks. least squares loss USED-FOR infinitely - wide neural networks. gradient descent USED-FOR least squares loss. gradient descent USED-FOR infinitely - wide neural networks. NTK regression COMPARE finitely - wide neural networks. finitely - wide neural networks COMPARE NTK regression. small - scale datasets USED-FOR finitely - wide neural networks. kernel methods USED-FOR large - scale learning tasks. computational complexity EVALUATE-FOR kernel methods. near input - sparsity time approximation algorithm USED-FOR NTK. near input - sparsity time approximation algorithm USED-FOR learning. polynomial expansions of arc - cosine kernels USED-FOR near input - sparsity time approximation algorithm. NTK USED-FOR learning. spectral approximation guarantee USED-FOR NTK matrix. random features CONJUNCTION sketching algorithm. sketching algorithm CONJUNCTION random features. random features PART-OF arc - cosine kernels. leverage score sampling USED-FOR random features. accuracy EVALUATE-FOR CNTK. linear regressor COMPARE CNTK. CNTK COMPARE linear regressor. accuracy EVALUATE-FOR linear regressor. speedup EVALUATE-FOR linear regressor. CNTK features USED-FOR linear regressor. speedup EVALUATE-FOR CNTK. large - scale regression and classification tasks EVALUATE-FOR methods. CIFAR-10 dataset EVALUATE-FOR CNTK. Method is convolutional counterpart of NTK ( CNTK ). OtherScientificTerm is linear runtime. ,"This paper studies the Neural Tangent Kernel (NTK) for infinitely-wide neural networks trained with gradient descent on the least squares loss. The authors propose a convolutional counterpart of NTK (CNTK), which has a linear runtime of $O(1/\sqrt{n})$. The authors show that the NTK regression outperforms finitely-widest neural networks on small-scale datasets. They also show that kernel methods for large-scale learning tasks have the same computational complexity as NTK, but with a spectral approximation guarantee for the NTk matrix.  The authors also propose a near input-sparsity time approximation algorithm for NTK based on polynomial expansions of arc-cosine kernels, where the random features of the arc-sinine kernels are approximated by leverage score sampling, and a sketching algorithm is used to combine random features from different kernels. The proposed methods are evaluated on both large scale regression and classification tasks, and CNTK is shown to outperform NTK on the CIFAR-10 dataset. In addition, a linear regressor based on the linear function of the CNTk features is also shown to achieve speedup over the standard linear function.   ","This paper studies the Neural Tangent Kernel (NTK) for infinitely-wide neural networks trained with gradient descent on the least squares loss. The authors propose a convolutional counterpart of NTK (CNTK), which has a linear runtime of $O(1/\sqrt{n})$. The authors show that the NTK regression outperforms finitely-widest neural networks on small-scale datasets. They also show that kernel methods for large-scale learning tasks have the same computational complexity as NTK, but with a spectral approximation guarantee for the NTk matrix.  The authors also propose a near input-sparsity time approximation algorithm for NTK based on polynomial expansions of arc-cosine kernels, where the random features of the arc-sinine kernels are approximated by leverage score sampling, and a sketching algorithm is used to combine random features from different kernels. The proposed methods are evaluated on both large scale regression and classification tasks, and CNTK is shown to outperform NTK on the CIFAR-10 dataset. In addition, a linear regressor based on the linear function of the CNTk features is also shown to achieve speedup over the standard linear function.   "
19916,SP:fa2668083ff3bb592c29a4c6822ae96ff54d0dbe,"framework USED-FOR multi - person 3D motion trajectory prediction. local - range encoder CONJUNCTION global - range encoder. global - range encoder CONJUNCTION local - range encoder. global - range encoder USED-FOR social interactions. local - range encoder USED-FOR individual motion. local - range encoder PART-OF Multi - Range Transformers model. global - range encoder PART-OF Multi - Range Transformers model. Transformer decoder USED-FOR prediction. model COMPARE methods. methods COMPARE model. long - term 3D motion prediction EVALUATE-FOR methods. long - term 3D motion prediction EVALUATE-FOR model. OtherScientificTerm are human pose trajectory, and local and global - range encoder features. ","This paper proposes a framework for multi-person 3D motion trajectory prediction. The proposed Multi-Range Transformers model consists of a local-range encoder for individual motion and a global-range decoder for social interactions. The human pose trajectory is used as input, and the prediction is made using a Transformer decoder. The model is evaluated on long-term 3D object detection, and is shown to outperform existing methods in terms of both the quality of the human pose and of the individual motion prediction. ","This paper proposes a framework for multi-person 3D motion trajectory prediction. The proposed Multi-Range Transformers model consists of a local-range encoder for individual motion and a global-range decoder for social interactions. The human pose trajectory is used as input, and the prediction is made using a Transformer decoder. The model is evaluated on long-term 3D object detection, and is shown to outperform existing methods in terms of both the quality of the human pose and of the individual motion prediction. "
19997,SP:0a0e07af37c8fe8580639b1df62d27b6f63f8dee,"reinforcement learning USED-FOR long - horizon planning problems. programs USED-FOR reinforcement learning. programs USED-FOR settings. strategy USED-FOR program. program synthesis USED-FOR guiding programs. generative model USED-FOR It. It USED-FOR program. model USED-FOR program. approach COMPARE non - program - guided approaches. non - program - guided approaches COMPARE approach. benchmarks EVALUATE-FOR non - program - guided approaches. 2D Minecraft - inspired environment HYPONYM-OF benchmarks. benchmarks EVALUATE-FOR approach. program - guided reinforcement learning USED-FOR approach. Generic is approaches. Method are guiding program, model predictive program synthesis ( MPPS ), and handcrafted programs. Task is programming task. ","This paper proposes a new approach for learning a program-guided RL agent that can be used to solve long-horizon planning problems. The proposed approach is based on the idea of predictive program synthesis (MPPS), which is a generative model that learns to generate programs that can guide the agent to solve a programming task. The paper shows that the proposed approach outperforms non-program-guided approaches on a number of benchmarks, including a 2D Minecraft-inspired environment. ","This paper proposes a new approach for learning a program-guided RL agent that can be used to solve long-horizon planning problems. The proposed approach is based on the idea of predictive program synthesis (MPPS), which is a generative model that learns to generate programs that can guide the agent to solve a programming task. The paper shows that the proposed approach outperforms non-program-guided approaches on a number of benchmarks, including a 2D Minecraft-inspired environment. "
20078,SP:5bb42b178b0d27da271bfa60e633fdac718638c4,"causal imitation learning USED-FOR Imitation learning. sequential settings USED-FOR causal imitation learning. graphical criterion USED-FOR causal imitation. algorithm USED-FOR imitability. Task are naïve imitation, and single - stage decision - making. OtherScientificTerm are sensors, imitator, demonstrator ’s behavior ( DO ), and demonstrator. Generic is theory. ","This paper studies the problem of causal imitation learning in sequential settings. Imitation learning is an important problem in the context of naïve imitation, where the goal is to learn to imitate a demonstrator’s behavior (DO) given a set of sensors. In this setting, the imitator is trained in a single-stage decision-making. The authors propose a graphical criterion for causal imitation, which they call “imitability”, that is, the ability of the demonstrator to imitate the behavior of an imitators. They also propose an algorithm for learning the imitability. The theory is well-motivated and the theory is clearly stated. ","This paper studies the problem of causal imitation learning in sequential settings. Imitation learning is an important problem in the context of naïve imitation, where the goal is to learn to imitate a demonstrator’s behavior (DO) given a set of sensors. In this setting, the imitator is trained in a single-stage decision-making. The authors propose a graphical criterion for causal imitation, which they call “imitability”, that is, the ability of the demonstrator to imitate the behavior of an imitators. They also propose an algorithm for learning the imitability. The theory is well-motivated and the theory is clearly stated. "
20159,SP:85bd81f0c5b6ccbc421ebbaf6f5c72164bc70b7f,transition losses USED-FOR object - structured representation. object - structured representation COMPARE pixels. pixels COMPARE object - structured representation. transition losses COMPARE pixels. pixels COMPARE transition losses. transition losses USED-FOR model. object persistence CONJUNCTION object identity. object identity CONJUNCTION object persistence. alignment module USED-FOR model. object identity HYPONYM-OF transition models. object persistence HYPONYM-OF transition models. objectlevel loss CONJUNCTION object alignment. object alignment CONJUNCTION objectlevel loss. object occlusion CONJUNCTION re - appearance. re - appearance CONJUNCTION object occlusion. model COMPARE baseline. baseline COMPARE model. it USED-FOR object occlusion. it USED-FOR re - appearance. partially observable environments FEATURE-OF re - appearance. partially observable environments FEATURE-OF object occlusion. OtherScientificTerm is slot - wise object memory. ,This paper proposes to use transition losses to learn an object-structured representation instead of pixels. The proposed model is based on two transition losses: object persistence and object identity. The model is trained with an alignment module. The transition models are two types of transition models: the objectlevel loss and the object alignment. The paper also proposes a slot-wise object memory. The experiments show that the proposed model outperforms a baseline and it is able to learn object occlusion and re-appearance in partially observable environments. ,This paper proposes to use transition losses to learn an object-structured representation instead of pixels. The proposed model is based on two transition losses: object persistence and object identity. The model is trained with an alignment module. The transition models are two types of transition models: the objectlevel loss and the object alignment. The paper also proposes a slot-wise object memory. The experiments show that the proposed model outperforms a baseline and it is able to learn object occlusion and re-appearance in partially observable environments. 
20240,SP:f32eddbb5c33a8422c075579ff08aa9833338d44,"classification CONJUNCTION regression. regression CONJUNCTION classification. regression CONJUNCTION off - policy policy learning. off - policy policy learning CONJUNCTION regression. classification CONJUNCTION off - policy policy learning. off - policy policy learning CONJUNCTION classification. Empirical risk minimization ( ERM ) PART-OF machine learning. classification HYPONYM-OF machine learning. adaptively collected data USED-FOR generic importance sampling weighted ERM algorithm. maximal inequality USED-FOR rates. exploration rate FEATURE-OF rates. fast rates USED-FOR regression. convexity of squared - error loss USED-FOR fast rates. regret guarantees USED-FOR policy learning. OtherScientificTerm are modelagnostic guarantees, hypothesis class, importance sampling structure, and exploration. Method is contextual bandit algorithm. Metric is fast convergence rates. Material is bandit - collected data. Generic is theory. ","This paper studies the problem of empirical risk minimization (ERM) in machine learning, specifically classification, regression, and off-policy policy learning. The authors propose a generic importance sampling weighted ERM algorithm based on adaptively collected data, and provide modelagnostic guarantees. They also propose a contextual bandit algorithm that achieves fast convergence rates. The main contribution of the paper is that the authors prove fast rates for classification based on the convexity of squared-error loss, and for regression based on maximal inequality of the rates with respect to the exploration rate.  The authors also provide regret guarantees for policy learning based on their theory.    The main idea is to learn a hypothesis class from bandit-constrained data that is invariant to the importance sampling structure, and then use this hypothesis class for exploration. ","This paper studies the problem of empirical risk minimization (ERM) in machine learning, specifically classification, regression, and off-policy policy learning. The authors propose a generic importance sampling weighted ERM algorithm based on adaptively collected data, and provide modelagnostic guarantees. They also propose a contextual bandit algorithm that achieves fast convergence rates. The main contribution of the paper is that the authors prove fast rates for classification based on the convexity of squared-error loss, and for regression based on maximal inequality of the rates with respect to the exploration rate.  The authors also provide regret guarantees for policy learning based on their theory.    The main idea is to learn a hypothesis class from bandit-constrained data that is invariant to the importance sampling structure, and then use this hypothesis class for exploration. "
20321,SP:f549a0c231b71bae0acbed6e3afb41890ee89cd9,weighted regression model USED-FOR machine learning tasks. predictive power EVALUATE-FOR models. low sample sizes CONJUNCTION covariate perturbations. covariate perturbations CONJUNCTION low sample sizes. mitigation strategy USED-FOR problems. doubly non - negative matrix USED-FOR sample weights. log - determinant divergence CONJUNCTION Bures - Wasserstein distance. Bures - Wasserstein distance CONJUNCTION log - determinant divergence. uncertainty set FEATURE-OF weighting matrix. Bures - Wasserstein distance USED-FOR weighting matrix. log - determinant divergence USED-FOR weighting matrix. first - order methods USED-FOR adversarially reweighted estimate. Task is kernel - reweighted regression. Method is reweighting strategy. ,"This paper proposes a reweighting strategy for kernel-reweighted regression, where the goal is to improve the predictive power of models that are re-weighted in the presence of covariate perturbations. The authors propose a mitigation strategy to tackle the problems of low sample sizes, covariate adversarial perturbation, and covariate noise in the weighting of a weighted regression model for machine learning tasks. They propose a doubly non-negative matrix to reweight the sample weights, and use the log-determinant divergence and the Bures-Wasserstein distance to reweigh the weighted matrix on the uncertainty set. They also propose an adversarially reweighted estimate based on first-order methods. ","This paper proposes a reweighting strategy for kernel-reweighted regression, where the goal is to improve the predictive power of models that are re-weighted in the presence of covariate perturbations. The authors propose a mitigation strategy to tackle the problems of low sample sizes, covariate adversarial perturbation, and covariate noise in the weighting of a weighted regression model for machine learning tasks. They propose a doubly non-negative matrix to reweight the sample weights, and use the log-determinant divergence and the Bures-Wasserstein distance to reweigh the weighted matrix on the uncertainty set. They also propose an adversarially reweighted estimate based on first-order methods. "
20402,SP:fe12e13602925b9400fd596a987755beb10aa3d1,"discrete latent variables USED-FOR models. continuous relaxation USED-FOR low - variance reparameterization gradients. binary random variables USED-FOR it. importance sampling CONJUNCTION statistical couplings. statistical couplings CONJUNCTION importance sampling. importance sampling USED-FOR estimator. statistical couplings USED-FOR estimator. sequences of binary variables CONJUNCTION Rao - Blackwellization. Rao - Blackwellization CONJUNCTION sequences of binary variables. Rao - Blackwellization USED-FOR reparameterizing categorical variables. sequences of binary variables USED-FOR reparameterizing categorical variables. reparameterizing categorical variables USED-FOR gradient estimators. Rao - Blackwellization USED-FOR gradient estimators. estimators COMPARE REINFORCE. REINFORCE COMPARE estimators. leave - one - out - baseline estimator USED-FOR estimators. leave - one - out - baseline estimator USED-FOR REINFORCE. Method are unbiased gradient estimators, performant estimator, continuous relaxations, and categorical gradient estimators. Material is categorical setting. OtherScientificTerm is stick - breaking coupling. ","This paper studies the problem of unbiased gradient estimators for discrete latent variables in models. In particular, the authors consider the categorical setting, where it is assumed that the model is trained on binary random variables. The authors propose a continuous relaxation for low-variance reparameterization gradients, and show that it is a performant estimator. They also show that this estimator can be improved by importance sampling and statistical couplings.   The authors show that continuous relaxations for categorical variables can be used to improve the performance of categorical gradient estimations. They show that, for a set of sequences of binary variables and Rao-Blackwellization, the gradient estimator of a pair of pairs of pairs can be approximated by reparameters of the two sets of variables.  They also demonstrate that their estimators outperform REINFORCE, which uses a leave-one-out-baseline estimator, in terms of stick-breaking coupling. ","This paper studies the problem of unbiased gradient estimators for discrete latent variables in models. In particular, the authors consider the categorical setting, where it is assumed that the model is trained on binary random variables. The authors propose a continuous relaxation for low-variance reparameterization gradients, and show that it is a performant estimator. They also show that this estimator can be improved by importance sampling and statistical couplings.   The authors show that continuous relaxations for categorical variables can be used to improve the performance of categorical gradient estimations. They show that, for a set of sequences of binary variables and Rao-Blackwellization, the gradient estimator of a pair of pairs of pairs can be approximated by reparameters of the two sets of variables.  They also demonstrate that their estimators outperform REINFORCE, which uses a leave-one-out-baseline estimator, in terms of stick-breaking coupling. "
20483,SP:e16fdf963ec2f9c0d79fa404e47e7862a5d6e922,"predictors USED-FOR top architectures. search path USED-FOR high - performance sub - space. weaker predictors USED-FOR search path. strong predictor USED-FOR architecture space. predictor USED-FOR well - performed architectures. coarse - to - fine iteration USED-FOR ranking of sampling space. NAS - Bench-101 CONJUNCTION NAS - Bench-201. NAS - Bench-201 CONJUNCTION NAS - Bench-101. WeakNAS USED-FOR top - performance architectures. ImageNet MobileNet Search Space EVALUATE-FOR SOTA. SOTA EVALUATE-FOR WeakNAS. ImageNet MobileNet Search Space EVALUATE-FOR WeakNAS. Method are Neural Architecture Search ( NAS ), predictor - based NAS approaches, proxy accuracy predictor, and weak predictor. OtherScientificTerm are architecture - performance pairs, and weak predictors. Generic are architecture, and framework. ","This paper proposes a new approach to Neural Architecture Search (NAS) based on predictor-based NAS approaches. The idea is to train two predictors for top architectures, one for architecture-performance pairs, and one for the architecture space. The search path for the high-performance sub-space is based on the weaker predictors. The strong predictor is used to guide the search path to the best architecture space, while the proxy accuracy predictor is the best predictor for the well-performed architectures. The ranking of sampling space is done via coarse-to-fine iteration. Experiments on NAS-Bench-101 and NAS- Bench-201 show that the proposed WeakNAS outperforms SOTA on the ImageNet MobileNet Search Space. The paper also shows that the weak predictors can be used as a proxy for the top-performance architectures. ","This paper proposes a new approach to Neural Architecture Search (NAS) based on predictor-based NAS approaches. The idea is to train two predictors for top architectures, one for architecture-performance pairs, and one for the architecture space. The search path for the high-performance sub-space is based on the weaker predictors. The strong predictor is used to guide the search path to the best architecture space, while the proxy accuracy predictor is the best predictor for the well-performed architectures. The ranking of sampling space is done via coarse-to-fine iteration. Experiments on NAS-Bench-101 and NAS- Bench-201 show that the proposed WeakNAS outperforms SOTA on the ImageNet MobileNet Search Space. The paper also shows that the weak predictors can be used as a proxy for the top-performance architectures. "
20564,SP:8f74abb04037ba2e59dcf8320dc555b149f68ed8,"latent codes PART-OF globally consistent coordinate system. Entropic Desired Dynamics USED-FOR Intrinsic ConTrol ( EDDICT ). tractable learning CONJUNCTION interpretable latent space. interpretable latent space CONJUNCTION tractable learning. EDDICT ’s globally consistent codes USED-FOR it. prior methods COMPARE EDDICT ’s globally consistent codes. EDDICT ’s globally consistent codes COMPARE prior methods. state coverage CONJUNCTION unsupervised. unsupervised CONJUNCTION state coverage. hard exploration games EVALUATE-FOR unsupervised. Montezuma ’s Revenge HYPONYM-OF hard exploration games. OtherScientificTerm are local objective, and fixed additive latent dynamics. ","This paper proposes Intrinsic ConTrol (EDDICT) based on Entropic Desired Dynamics to learn a globally consistent coordinate system with latent codes that are invariant to changes in the local objective. EDDICT’s globally consistent codes allow it to learn tractable learning and interpretable latent space. The paper shows that prior methods do not learn EDDCT’S globally consistent code, and that it can learn a fixed additive latent dynamics. Experiments on state coverage and unsupervised performance on hard exploration games (e.g. Montezuma's Revenge) are conducted.   ","This paper proposes Intrinsic ConTrol (EDDICT) based on Entropic Desired Dynamics to learn a globally consistent coordinate system with latent codes that are invariant to changes in the local objective. EDDICT’s globally consistent codes allow it to learn tractable learning and interpretable latent space. The paper shows that prior methods do not learn EDDCT’S globally consistent code, and that it can learn a fixed additive latent dynamics. Experiments on state coverage and unsupervised performance on hard exploration games (e.g. Montezuma's Revenge) are conducted.   "
20645,SP:c731a78c3e7f98ccd0253b51a0d42bf8deeb71f9,"reinforcement learning ( RL ) USED-FOR drug design. reward scoring function USED-FOR RL. molecular docking program USED-FOR RL. molecular docking program HYPONYM-OF physical simulation. molecular docking program HYPONYM-OF reward scoring function. physical simulation USED-FOR protein - small molecule binding affinity. models USED-FOR chemically realistic and pharmacochemically acceptable molecules. local optima CONJUNCTION smooth surfaces. smooth surfaces CONJUNCTION local optima. docking score optimization HYPONYM-OF exploration problem. RL framework USED-FOR pharmacochemically acceptable molecules. docking scores FEATURE-OF pharmacochemically acceptable molecules. fragment - based generation method CONJUNCTION error - prioritized experience replay ( PER ). error - prioritized experience replay ( PER ) CONJUNCTION fragment - based generation method. Explorative Experience replay USED-FOR Drug design ( FREED ). Explorative Experience replay USED-FOR Fragment - based generative RL. de novo and scaffold - based schemes EVALUATE-FOR model. model COMPARE methods. methods COMPARE model. method USED-FOR model. predictive error - PER ( FREED(PE ) ) USED-FOR model. predictive error - PER ( FREED(PE ) ) HYPONYM-OF method. OtherScientificTerm are molecular structure, realistic and qualified chemical space, and drugs. ","This paper proposes a reinforcement learning (RL) for drug design using a reward scoring function, the molecular docking program, which is a physical simulation (i.e., molecular docking) of a molecule’s binding affinity to a protein-small molecule binding affinity in a molecular structure. The authors propose two models for designing chemically realistic and pharmacochemically acceptable molecules, and show that their models are able to generate molecules that are both biologically plausible and biologically reproducible. They also show that the RL framework can be used to design molecules that have good docking scores, and that the exploration problem (docking score optimization) can be reduced to a local optima and smooth surfaces. Fragment-based generative RL based on Explorative Experience replay for Drug design (FREED) is proposed, which combines a fragment-based generation method with error-prioritized experience replay (PER). The proposed model is evaluated on both de novo and scaffold-based schemes, and the proposed method, predictive error-PER (PE), is shown to outperform existing methods. ","This paper proposes a reinforcement learning (RL) for drug design using a reward scoring function, the molecular docking program, which is a physical simulation (i.e., molecular docking) of a molecule’s binding affinity to a protein-small molecule binding affinity in a molecular structure. The authors propose two models for designing chemically realistic and pharmacochemically acceptable molecules, and show that their models are able to generate molecules that are both biologically plausible and biologically reproducible. They also show that the RL framework can be used to design molecules that have good docking scores, and that the exploration problem (docking score optimization) can be reduced to a local optima and smooth surfaces. Fragment-based generative RL based on Explorative Experience replay for Drug design (FREED) is proposed, which combines a fragment-based generation method with error-prioritized experience replay (PER). The proposed model is evaluated on both de novo and scaffold-based schemes, and the proposed method, predictive error-PER (PE), is shown to outperform existing methods. "
20726,SP:b938bca513e7de1231212064caf8877a78d8b612,"complexity EVALUATE-FOR directed acyclic graphical models. observational data USED-FOR directed acyclic graphical models. local Markov boundary search procedure USED-FOR ancestral sets. ancestral sets PART-OF graphical model. local Markov boundary search procedure USED-FOR approach. forward greedy search algorithm USED-FOR Markov boundary. backward pruning phase PART-OF forward greedy search algorithm. forward greedy search algorithm USED-FOR graph ensembles. identifiability condition FEATURE-OF graph. finite - sample guarantees USED-FOR recovering Markov boundaries. results USED-FOR polytrees. polynomial time FEATURE-OF polytrees. minimal assumptions USED-FOR structure of directed graphical models. approach USED-FOR discrete or continuous distributions. OtherScientificTerm are distributional assumptions, nodes, and Markov boundaries. Metric is sample complexity. Generic is algorithm. ","This paper studies the complexity of directed acyclic graphical models on observational data. The approach is based on a local Markov boundary search procedure to recover ancestral sets of the graphical model. The forward greedy search algorithm is used to recover the ancestral sets, and the backward pruning phase of the proposed approach is applied to graph ensembles. The authors provide finite-sample guarantees for recovering Markov boundaries under distributional assumptions. They also provide results for recovering polytrees in polynomial time. The sample complexity of the algorithm is shown to be O(1/\sqrt{n}) for discrete or continuous distributions, and O(O(n^2) for continuous distributions.   The authors also provide an identifiability condition on the graph, and show that under minimal assumptions on the structure of directed graphical models, their approach can be applied to discrete/continuous distributions, where the nodes are connected to each other and the graph is connected to a set of nodes connected to other nodes. ","This paper studies the complexity of directed acyclic graphical models on observational data. The approach is based on a local Markov boundary search procedure to recover ancestral sets of the graphical model. The forward greedy search algorithm is used to recover the ancestral sets, and the backward pruning phase of the proposed approach is applied to graph ensembles. The authors provide finite-sample guarantees for recovering Markov boundaries under distributional assumptions. They also provide results for recovering polytrees in polynomial time. The sample complexity of the algorithm is shown to be O(1/\sqrt{n}) for discrete or continuous distributions, and O(O(n^2) for continuous distributions.   The authors also provide an identifiability condition on the graph, and show that under minimal assumptions on the structure of directed graphical models, their approach can be applied to discrete/continuous distributions, where the nodes are connected to each other and the graph is connected to a set of nodes connected to other nodes. "
20807,SP:af08109d4c45dc9401efb0e63c22167e9da28adb,"( "", ) DP algorithm USED-FOR privately learnable class. public randomness USED-FOR global stability. Task is learning with differential privacy ( DP ). OtherScientificTerm are privacy protection, probabilistic representation dimension, and nearly - matching lower bound. Method are "" -DP algorithms, local model, and correlated sampling strategy. ","This paper studies the problem of learning with differential privacy (DP) in the setting where there is no privacy protection. The authors propose a new DP algorithm for learning a privately learnable class, which they call ""-DP algorithms"". They prove a nearly-matching lower bound for the probabilistic representation dimension of the local model. They also prove that the global stability is guaranteed by the use of public randomness. Finally, they propose a correlated sampling strategy to improve the performance of the algorithm.","This paper studies the problem of learning with differential privacy (DP) in the setting where there is no privacy protection. The authors propose a new DP algorithm for learning a privately learnable class, which they call ""-DP algorithms"". They prove a nearly-matching lower bound for the probabilistic representation dimension of the local model. They also prove that the global stability is guaranteed by the use of public randomness. Finally, they propose a correlated sampling strategy to improve the performance of the algorithm."
20888,SP:da4f21d107a7f442c4d3e3ec13bdb44b041e07cf,"Estimating the per - state expected cumulative rewards PART-OF reinforcement learning approaches. latent Markov decision process USED-FOR transition and reward models. gradient descent USED-FOR global optima. gradient descent USED-FOR linear parametrization. convergence rates USED-FOR cases. stochastic gradient descent ( SGD ) COMPARE explicit counterpart. explicit counterpart COMPARE stochastic gradient descent ( SGD ). implicit representation COMPARE explicit counterpart. explicit counterpart COMPARE implicit representation. implicit representation USED-FOR stochastic gradient descent ( SGD ). OtherScientificTerm are per - state expected cumulative rewards, and value predictions. Method are deep neural - network function - approximation methods, value iteration networks, and implicit representations of value functions. Generic is approach. ","Estimating the per-state expected cumulative rewards in reinforcement learning approaches is an important problem. This paper proposes to use deep neural-network function-approximation methods to address this problem. The authors propose to use a latent Markov decision process to model the transition and reward models, and use value iteration networks to learn the implicit representations of value functions. They show that gradient descent converges to global optima for the linear parametrization of the value predictions. They also provide convergence rates for two cases, where the implicit representation of stochastic gradient descent (SGD) is more powerful than its explicit counterpart. ","Estimating the per-state expected cumulative rewards in reinforcement learning approaches is an important problem. This paper proposes to use deep neural-network function-approximation methods to address this problem. The authors propose to use a latent Markov decision process to model the transition and reward models, and use value iteration networks to learn the implicit representations of value functions. They show that gradient descent converges to global optima for the linear parametrization of the value predictions. They also provide convergence rates for two cases, where the implicit representation of stochastic gradient descent (SGD) is more powerful than its explicit counterpart. "
20988,SP:992aa07d4f815d1c81f967374590eece933833b1,text sources USED-FOR Knowledge Graphs ( KGs ). embeddings USED-FOR inferring new facts. KG refinement task USED-FOR KGs. embeddings USED-FOR KGs. techniques USED-FOR KG refinement. inference rules USED-FOR techniques. ontological information USED-FOR embeddings. ontological information CONJUNCTION inferences rules. inferences rules CONJUNCTION ontological information. IterefinE HYPONYM-OF KG refinement framework. inferences rules USED-FOR one. ontological information USED-FOR one. ComplEx HYPONYM-OF KG embeddings. KG embeddings USED-FOR IterefinE. ontological information USED-FOR IterefinE. type - supervised embeddings USED-FOR KG. KG benchmarks EVALUATE-FOR embeddings. embeddings USED-FOR KG. PSL - KGI USED-FOR KG. overall weighted F1 score EVALUATE-FOR embeddings. Task is KG - based question answering. OtherScientificTerm is ontologies. Method is IterefinE framework. ,"This paper proposes a new approach to KG-based question answering. The authors propose a new KG refinement task, called IterefinE, which is based on the idea that knowledge graph embeddings (KGs) are useful for inferring new facts from text sources. The paper proposes two techniques to improve the quality of the KGs produced by the proposed techniques in the context of knowledge graph refinement. The first one uses the ontological information in the KG embedding and inferences rules, while the second one uses a combination of the two.   The paper shows that the proposed IterefineE framework is able to improve on the performance of existing methods on KG based question answering tasks.  The authors also show that the ItereformE framework can be applied to the task of question answering in a more general KG setting, where the ontologies are more general.  Experiments are conducted on a number of KG benchmarks, and the authors show that their approach outperforms existing methods in terms of the overall weighted F1 score. They also show the effectiveness of their approach on the PSL-KGI task, where they show that using the proposed approach, they are able to obtain better embedding performance than existing methods. ","This paper proposes a new approach to KG-based question answering. The authors propose a new KG refinement task, called IterefinE, which is based on the idea that knowledge graph embeddings (KGs) are useful for inferring new facts from text sources. The paper proposes two techniques to improve the quality of the KGs produced by the proposed techniques in the context of knowledge graph refinement. The first one uses the ontological information in the KG embedding and inferences rules, while the second one uses a combination of the two.   The paper shows that the proposed IterefineE framework is able to improve on the performance of existing methods on KG based question answering tasks.  The authors also show that the ItereformE framework can be applied to the task of question answering in a more general KG setting, where the ontologies are more general.  Experiments are conducted on a number of KG benchmarks, and the authors show that their approach outperforms existing methods in terms of the overall weighted F1 score. They also show the effectiveness of their approach on the PSL-KGI task, where they show that using the proposed approach, they are able to obtain better embedding performance than existing methods. "
20997,SP:676fc4a3041af22e8f20ccba7daa2a0b1f5d6af5,"binary predictions USED-FOR KBC quality. evaluation paradigm USED-FOR model selection criteria. real - world entities PART-OF KB. model COMPARE KB. KB COMPARE model. benchmark EVALUATE-FOR KB embeddings models. completion task EVALUATE-FOR ranking task. prediction separability FEATURE-OF KB embedding models. thresholding PART-OF TransE. classification F1 score EVALUATE-FOR TransE. Method is Knowledge base completion ( KBC ) methods. Material are knowledge base ( KB ), and FB14k - QAQ. Generic are method, and models. OtherScientificTerm are likelihood ranking, evaluation data structure, and KB queries. Task are ranking setting, and ranking - based and classification - based evaluation. ","Knowledge base completion (KBC) methods are used to evaluate the quality of a knowledge base (KB) based on a likelihood ranking of all entities in the knowledge base. This paper proposes a new evaluation paradigm for model selection criteria, which is based on binary predictions of the KBC quality. The proposed method, called FB14k-QAQ, is evaluated on a new benchmark for KB embeddings models, where real-world entities are included in the KB, and a ranking task is used as well as a completion task. The authors show that a model trained on the new benchmark performs better than a baseline model trained only on the original KB. They also show that models trained on this new benchmark are able to achieve better prediction separability compared to existing KB embedding models. They further show that the proposed method TransE, which adds thresholding to the classification F1 score of the model, is able to outperform the baseline model on the ranking setting.    The authors also provide a detailed analysis of the evaluation data structure, showing that the ranking-based and classification-based evaluation are not the same, and that there is a trade-off between the ranking performance of a model and the performance of the final model. The paper also shows that the performance on the benchmark is highly correlated with the number of KB queries, which suggests that the models are not well-suited for the task. ","Knowledge base completion (KBC) methods are used to evaluate the quality of a knowledge base (KB) based on a likelihood ranking of all entities in the knowledge base. This paper proposes a new evaluation paradigm for model selection criteria, which is based on binary predictions of the KBC quality. The proposed method, called FB14k-QAQ, is evaluated on a new benchmark for KB embeddings models, where real-world entities are included in the KB, and a ranking task is used as well as a completion task. The authors show that a model trained on the new benchmark performs better than a baseline model trained only on the original KB. They also show that models trained on this new benchmark are able to achieve better prediction separability compared to existing KB embedding models. They further show that the proposed method TransE, which adds thresholding to the classification F1 score of the model, is able to outperform the baseline model on the ranking setting.    The authors also provide a detailed analysis of the evaluation data structure, showing that the ranking-based and classification-based evaluation are not the same, and that there is a trade-off between the ranking performance of a model and the performance of the final model. The paper also shows that the performance on the benchmark is highly correlated with the number of KB queries, which suggests that the models are not well-suited for the task. "
21006,SP:83fe0a496a79bcf97ccba1c6d34b7d11e7d5c330,dialog system models USED-FOR tasks. human annotations USED-FOR dialog system models. language priors USED-FOR down - stream NLP tasks. BERT CONJUNCTION GPT-2. GPT-2 CONJUNCTION BERT. GPT-2 HYPONYM-OF large pre - trained language models. BERT HYPONYM-OF large pre - trained language models. pre - trained language models USED-FOR dialog response generation. Alternating Roles Dialog Model ( ARDM ) HYPONYM-OF framework. large pretrained language model USED-FOR ARDM. belief states CONJUNCTION dialog acts. dialog acts CONJUNCTION belief states. It USED-FOR conversations. supervision USED-FOR It. human annotations USED-FOR It. belief states HYPONYM-OF human annotations. dialog acts HYPONYM-OF human annotations. ARDM COMPARE state - of - the - art methods. state - of - the - art methods COMPARE ARDM. CamRest676 CONJUNCTION MultiWOZ. MultiWOZ CONJUNCTION CamRest676. task - oriented dialog datasets EVALUATE-FOR state - of - the - art methods. task - oriented dialog datasets EVALUATE-FOR ARDM. MultiWOZ HYPONYM-OF task - oriented dialog datasets. CamRest676 HYPONYM-OF task - oriented dialog datasets. ARDM USED-FOR non - collaborative tasks. persuasion HYPONYM-OF non - collaborative tasks. ARDM USED-FOR human - like responses. ARDM USED-FOR persuasion tasks. ,"This paper proposes an extension of the Alternating Roles Dialog Model (ARDM) framework to dialog system models for tasks that require human annotations. The authors propose a framework called Alternating roles Dialog Models (ARDMs) that uses a large pre-trained language model (e.g. BERT, GPT-2) to generate dialog response generation from human-annotated language priors for down-stream NLP tasks. ARDM is based on a large pretrained language model. It generates dialog responses from human annotations (belief states, dialog acts, etc.). It is able to generate conversations with human annotations, and can be used as supervision for downstream tasks. The paper shows that ARDM outperforms state-of-the-art methods on two task-oriented dialog datasets (CamRest676 and MultiWOZ). ARDM can also generate human-like responses for non-collaborative tasks (persuasion and persuasion).","This paper proposes an extension of the Alternating Roles Dialog Model (ARDM) framework to dialog system models for tasks that require human annotations. The authors propose a framework called Alternating roles Dialog Models (ARDMs) that uses a large pre-trained language model (e.g. BERT, GPT-2) to generate dialog response generation from human-annotated language priors for down-stream NLP tasks. ARDM is based on a large pretrained language model. It generates dialog responses from human annotations (belief states, dialog acts, etc.). It is able to generate conversations with human annotations, and can be used as supervision for downstream tasks. The paper shows that ARDM outperforms state-of-the-art methods on two task-oriented dialog datasets (CamRest676 and MultiWOZ). ARDM can also generate human-like responses for non-collaborative tasks (persuasion and persuasion)."
21015,SP:b11c06b7c4ef1aa43c59f808a679425e302d158e,deep neural network USED-FOR classification. softmax values FEATURE-OF network. implied loss FEATURE-OF uncertainty measure. confidence measures USED-FOR Top k. networks USED-FOR method. binning values USED-FOR confidence measures. Generic is values. ,"This paper considers the problem of classification in a deep neural network, where the softmax values of the network are known. The authors propose a new uncertainty measure, the implied loss, which is a measure of the uncertainty of the classifier. They also propose two confidence measures for Top k, which are based on binning values. The proposed method is tested on two networks, and the results show that the proposed values are more accurate than existing methods.","This paper considers the problem of classification in a deep neural network, where the softmax values of the network are known. The authors propose a new uncertainty measure, the implied loss, which is a measure of the uncertainty of the classifier. They also propose two confidence measures for Top k, which are based on binning values. The proposed method is tested on two networks, and the results show that the proposed values are more accurate than existing methods."
21024,SP:ab9666e15f2a0113d96cb4b47b1cbb30fa1f7982,"architecture CONJUNCTION hyperparameters. hyperparameters CONJUNCTION architecture. generalization FEATURE-OF neural networks. wide neural networks USED-FOR gradient descent. spectrum FEATURE-OF NNGP kernel. kernel USED-FOR gradient descent. kernel USED-FOR Gaussian Processes. Neural Network Gaussian Process ( NNGP ) kernel HYPONYM-OF kernel. Neural Tangent Kernel ( NTK ) HYPONYM-OF kernel. NTK COMPARE NNGP kernel. NNGP kernel COMPARE NTK. spectrum COMPARE NNGP kernel. NNGP kernel COMPARE spectrum. spectrum FEATURE-OF NTK. Fully Connected Networks ( FCNs ) CONJUNCTION Convolutional Neural Networks ( CNNs ). Convolutional Neural Networks ( CNNs ) CONJUNCTION Fully Connected Networks ( FCNs ). Convolutional Neural Networks ( CNNs ) HYPONYM-OF architectures. Fully Connected Networks ( FCNs ) HYPONYM-OF architectures. CNNs COMPARE FCNs. FCNs COMPARE CNNs. learning dynamics FEATURE-OF CNNs. average pooling FEATURE-OF CNNs. pooling FEATURE-OF CNNs. Task is deep learning. OtherScientificTerm are wide network limit, large depth limit, and hyperparameter space. Method are random networks, and NNGP. Metric are trainability, and training accuracy. Material is real datasets. ","This paper studies the generalization performance of neural networks with different architecture and hyperparameters. The authors show that wide neural networks have a wide network limit, and that gradient descent on wide networks is more likely to converge to a large depth limit. They also show that the spectrum of the NNGP kernel (Neural Network Gaussian Process (NNGP) kernel), a kernel similar to the Neural Tangent Kernel (NTK) kernel, is the limit of the spectrum for gradient descent. They show that NTK has the same spectrum as the NGGP kernel, but the spectrum is much wider than that of the NTK.    The authors further show that random networks are not the only ones with a wide limit, as wide networks also have a large limit. This is true for other architectures such as Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs).   Finally, they show that CNNs have a similar learning dynamics as FCNs, and CNNs with average pooling are similar to FCNs in terms of pooling, but CNNs also have similar pooling in the hyperparameter space.  They also demonstrate that training accuracy on real datasets is improved when the width of the network is larger. ","This paper studies the generalization performance of neural networks with different architecture and hyperparameters. The authors show that wide neural networks have a wide network limit, and that gradient descent on wide networks is more likely to converge to a large depth limit. They also show that the spectrum of the NNGP kernel (Neural Network Gaussian Process (NNGP) kernel), a kernel similar to the Neural Tangent Kernel (NTK) kernel, is the limit of the spectrum for gradient descent. They show that NTK has the same spectrum as the NGGP kernel, but the spectrum is much wider than that of the NTK.    The authors further show that random networks are not the only ones with a wide limit, as wide networks also have a large limit. This is true for other architectures such as Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs).   Finally, they show that CNNs have a similar learning dynamics as FCNs, and CNNs with average pooling are similar to FCNs in terms of pooling, but CNNs also have similar pooling in the hyperparameter space.  They also demonstrate that training accuracy on real datasets is improved when the width of the network is larger. "
21033,SP:d3470c35aae48bf92439a55fdb98ccf07100e567,"computational methods USED-FOR protein folding. geometric invariance CONJUNCTION computational efficiency. computational efficiency CONJUNCTION geometric invariance. graph - based method USED-FOR protein models. GRAPHQA HYPONYM-OF graph - based method. GRAPHQA USED-FOR protein models. geometric invariance HYPONYM-OF favorable properties. representation learning HYPONYM-OF favorable properties. state - ofthe - art EVALUATE-FOR hand - engineered and representation - learning approaches. Material is Proteins. Task is biological processes. OtherScientificTerm are 3D structure, protein ’s structure, and sequential and 3D structure. Method is GRAPHQA components. ","This paper proposes a graph-based method called GRAPHQA, which aims to learn protein models that are able to fold a protein into a 3D structure. Proteins have been shown to fold in biological processes, and computational methods for protein folding have been proposed to learn a protein’s structure. The authors argue that there are a number of favorable properties (geometric invariance, computational efficiency, and representation learning) that can be exploited to improve the performance of protein models learned by GRAPH QA. The paper shows that the state-of-the-art hand-engineered and representation-learning approaches outperform the current state of the art in terms of performance on both sequential and 3D structures.    The paper also shows that there is a trade-off between the quality of the learned representation and the performance, and the authors show that the performance can be improved by removing the need to learn the sequential component of the representation.  The authors also show how to remove the need for a sequential component from the representation learning, and how to decompose the representation into the sequential and the 3D part of the protein.  Finally, the authors demonstrate that the proposed method is more computationally efficient than existing methods. They also show that their method can be combined with GRAPHQLA components. ","This paper proposes a graph-based method called GRAPHQA, which aims to learn protein models that are able to fold a protein into a 3D structure. Proteins have been shown to fold in biological processes, and computational methods for protein folding have been proposed to learn a protein’s structure. The authors argue that there are a number of favorable properties (geometric invariance, computational efficiency, and representation learning) that can be exploited to improve the performance of protein models learned by GRAPH QA. The paper shows that the state-of-the-art hand-engineered and representation-learning approaches outperform the current state of the art in terms of performance on both sequential and 3D structures.    The paper also shows that there is a trade-off between the quality of the learned representation and the performance, and the authors show that the performance can be improved by removing the need to learn the sequential component of the representation.  The authors also show how to remove the need for a sequential component from the representation learning, and how to decompose the representation into the sequential and the 3D part of the protein.  Finally, the authors demonstrate that the proposed method is more computationally efficient than existing methods. They also show that their method can be combined with GRAPHQLA components. "
21042,SP:5188280131b58a35d3deda126a0754aea8fa6e58,"loss function FEATURE-OF neural network. geometry of the functional space CONJUNCTION parameterization of this space. parameterization of this space CONJUNCTION geometry of the functional space. network ’s weights USED-FOR parameterization of this space. pure critical points COMPARE spurious critical points. spurious critical points COMPARE pure critical points. loss function FEATURE-OF linear neural networks. determinantal variety HYPONYM-OF functional space. linear maps HYPONYM-OF determinantal variety. bounded rank FEATURE-OF linear maps. functional space USED-FOR network. loss functions CONJUNCTION parameterizations. parameterizations CONJUNCTION loss functions. loss functions FEATURE-OF linear networks. architectures USED-FOR linear maps. loss landscape FEATURE-OF linear networks. determinantal variety FEATURE-OF functional space. Generic is space. OtherScientificTerm are parameterization, determinantal varieties, smooth convex losses, filling architectures, quadratic loss, non - filling architectures, and architecture. Task is landscape of linear networks. ","This paper studies the loss function of a neural network in a space where the geometry of the functional space and the parameterization of this space depends on the network’s weights. The authors show that pure critical points are more likely to exist than spurious critical points for linear neural networks with the same loss function. They also show that the landscape of linear networks is a determinantal variety of a functional space, i.e., the determinant of the function of the network in this space. They show that this space can be seen as the space in which the function is of bounded rank.    The authors further show that linear networks with different loss functions and different parameterizations share a similar loss landscape. In particular, they show that for smooth convex losses, the landscape is dominated by linear maps with bounded rank, and for linear maps of a certain kind (linear maps of the form linear maps over a finite set of architectures) the landscape becomes more and more complex as the number of layers increases.  They also find that for certain types of linear maps, there exist determinantals of the landscape that are more complex than the ones in the literature.  Finally, the authors provide a number of experiments that show that filling architectures with a quadratic loss are more stable than non-filling architectures, and that for a certain class of networks, there exists a set of networks that lie in this functional space that lie on pure critical point (or in a set that lies on a set on pure point).  ","This paper studies the loss function of a neural network in a space where the geometry of the functional space and the parameterization of this space depends on the network’s weights. The authors show that pure critical points are more likely to exist than spurious critical points for linear neural networks with the same loss function. They also show that the landscape of linear networks is a determinantal variety of a functional space, i.e., the determinant of the function of the network in this space. They show that this space can be seen as the space in which the function is of bounded rank.    The authors further show that linear networks with different loss functions and different parameterizations share a similar loss landscape. In particular, they show that for smooth convex losses, the landscape is dominated by linear maps with bounded rank, and for linear maps of a certain kind (linear maps of the form linear maps over a finite set of architectures) the landscape becomes more and more complex as the number of layers increases.  They also find that for certain types of linear maps, there exist determinantals of the landscape that are more complex than the ones in the literature.  Finally, the authors provide a number of experiments that show that filling architectures with a quadratic loss are more stable than non-filling architectures, and that for a certain class of networks, there exists a set of networks that lie in this functional space that lie on pure critical point (or in a set that lies on a set on pure point).  "
21051,SP:ee71597ceab23eb4db1d6608f15f80ad51f7ff6d,"Inductive and unsupervised graph learning USED-FOR predictive or information retrieval tasks. graph similarity evaluation USED-FOR learning processes. reconstruction error based loss functions USED-FOR learning processes. embedding of the subgraph vector distribution USED-FOR output vector representation. output vector representation USED-FOR graph. SEED framework USED-FOR subgraphs. reconstruction errors FEATURE-OF subgraphs. SEED CONJUNCTION graph isomorphism. graph isomorphism CONJUNCTION SEED. SEED framework COMPARE competitive baseline methods. competitive baseline methods COMPARE SEED framework. public benchmark datasets EVALUATE-FOR SEED framework. OtherScientificTerm are label information, subgraph samples, subgraph vectors, and subgraph vector distribution. Method is graph learning. Material is graph structured objects. ","This paper considers the problem of inductive and unsupervised graph learning for predictive or information retrieval tasks. In particular, the authors focus on the setting where there is no label information and the learning processes are based on graph similarity evaluation. The authors propose a SEED framework to learn subgraphs with reconstruction error based loss functions, where the subgraph samples are sampled from a set of subgraph vectors, and the output vector representation of the graph is learned from the embedding of a subgraph vector distribution.  The authors show that the reconstruction errors of the subGraphs are related to the number of reconstruction errors in the original graph, which is a common problem in graph learning. They also show that SEED and graph isomorphism can be used as a way to connect graph structured objects to each other. Experiments are conducted on several public benchmark datasets and the authors demonstrate that the proposed SEED approach outperforms competitive baseline methods. ","This paper considers the problem of inductive and unsupervised graph learning for predictive or information retrieval tasks. In particular, the authors focus on the setting where there is no label information and the learning processes are based on graph similarity evaluation. The authors propose a SEED framework to learn subgraphs with reconstruction error based loss functions, where the subgraph samples are sampled from a set of subgraph vectors, and the output vector representation of the graph is learned from the embedding of a subgraph vector distribution.  The authors show that the reconstruction errors of the subGraphs are related to the number of reconstruction errors in the original graph, which is a common problem in graph learning. They also show that SEED and graph isomorphism can be used as a way to connect graph structured objects to each other. Experiments are conducted on several public benchmark datasets and the authors demonstrate that the proposed SEED approach outperforms competitive baseline methods. "
21060,SP:d9406fdf0a180a5efc6f15ba8739524665f0f9d2,Counterfactual regret minimization ( CFR ) methods USED-FOR twoplayer zero - sum extensive games. imperfect information FEATURE-OF twoplayer zero - sum extensive games. vanilla CFR USED-FOR large - scale games. game tree USED-FOR vanilla CFR. Lazy - CFR HYPONYM-OF CFR algorithm. lazy update strategy USED-FOR CFR algorithm. Lazy - CFR COMPARE vanilla CFR. vanilla CFR COMPARE Lazy - CFR. regret EVALUATE-FOR Lazy - CFR. regret COMPARE regret. regret COMPARE regret. Lazy - CFR COMPARE regret. regret COMPARE Lazy - CFR. regret EVALUATE-FOR vanilla CFR. regret EVALUATE-FOR vanilla CFR. Lazy - CFR COMPARE CFR. CFR COMPARE Lazy - CFR. ,"This paper studies counterfactual regret minimization (CFRM) methods for the problem of playing twoplayer zero-sum extensive games with imperfect information. The authors propose a new CFR algorithm called Lazy-FRM, which is a variant of vanilla CFR for large-scale games with a game tree. The main idea is to use a lazy update strategy to improve the performance of the CFR algorithm. The regret of Lazy -FRM is shown to be much lower than the regret of the vanilla CFR, and the regret is also lower than that of the original CFR. ","This paper studies counterfactual regret minimization (CFRM) methods for the problem of playing twoplayer zero-sum extensive games with imperfect information. The authors propose a new CFR algorithm called Lazy-FRM, which is a variant of vanilla CFR for large-scale games with a game tree. The main idea is to use a lazy update strategy to improve the performance of the CFR algorithm. The regret of Lazy -FRM is shown to be much lower than the regret of the vanilla CFR, and the regret is also lower than that of the original CFR. "
21069,SP:023aa3dca1cf7992b22993a7088e8a74c92bb47e,"Unsupervised Domain Adaptation ( UDA ) methods USED-FOR transferable features. explicit feature distribution modeling USED-FOR UDA. Distribution Matching Prototypical Network ( DMPN ) USED-FOR deep features. Gaussian mixture distributions USED-FOR Distribution Matching Prototypical Network ( DMPN ). Gaussian mixture distributions USED-FOR deep features. domain discrepancy losses PART-OF DMPN. probabilistic interpretations FEATURE-OF domain discrepancy losses. one USED-FOR pseudo negative log likelihood. classification loss CONJUNCTION domain discrepancy losses. domain discrepancy losses CONJUNCTION classification loss. labeled source data CONJUNCTION domain discrepancy losses. domain discrepancy losses CONJUNCTION labeled source data. classification loss USED-FOR DMPN. labeled source data USED-FOR classification loss. DMPN USED-FOR discriminative and domain invariant features. domain discrepancy losses USED-FOR DMPN. Digits Image transfer task EVALUATE-FOR state - of - the - art approaches. Digits Image transfer task EVALUATE-FOR approach. mean accuracy EVALUATE-FOR DMPN. VisDA 2017 dataset EVALUATE-FOR DMPN. hyper - parameter sensitivity analysis EVALUATE-FOR approach. hyper - parameter changes FEATURE-OF approach. OtherScientificTerm are feature distribution discrepancy, feature distributions, Gaussian component means, and source feature distribution. Generic is methods. Task is UDA tasks. ",This paper proposes a new unsupervised domain adaptation (UDA) method based on the Distribution Matching Prototypical Network (DMPN) framework. DMPN is an extension of the previous work that uses a Gaussian mixture distribution to model the feature distribution discrepancy between the source and target distributions. The authors show that the proposed method is able to learn discriminative and domain-invariant features that are transferable across different domains. They also show that their method is more robust to hyperparameter changes than previous methods.,This paper proposes a new unsupervised domain adaptation (UDA) method based on the Distribution Matching Prototypical Network (DMPN) framework. DMPN is an extension of the previous work that uses a Gaussian mixture distribution to model the feature distribution discrepancy between the source and target distributions. The authors show that the proposed method is able to learn discriminative and domain-invariant features that are transferable across different domains. They also show that their method is more robust to hyperparameter changes than previous methods.
21078,SP:40be996e8bb86e887077b762b87c7c34a786ac98,"deep generative models USED-FOR tasks. Continuous Normalizing Flows ( CNFs ) HYPONYM-OF deep generative models. conditional image generation CONJUNCTION downstream predictive tasks. downstream predictive tasks CONJUNCTION conditional image generation. CNFs USED-FOR conditional image generation. CNFs USED-FOR downstream predictive tasks. model USED-FOR highdimensional latent code. class - specific supervised code CONJUNCTION unsupervised code. unsupervised code CONJUNCTION class - specific supervised code. InfoCNF HYPONYM-OF conditional CNF. unsupervised code PART-OF conditional CNF. class - specific supervised code PART-OF conditional CNF. gating networks USED-FOR error tolerances. gating networks USED-FOR ordinary differential equation ( ODE ) solvers. partitioning strategy USED-FOR InfoCNF. error tolerances FEATURE-OF ordinary differential equation ( ODE ) solvers. InfoCNF USED-FOR error tolerances. gating networks USED-FOR InfoCNF. test accuracy EVALUATE-FOR baseline. InfoCNF COMPARE baseline. baseline COMPARE InfoCNF. NFEs FEATURE-OF CIFAR10. likelihood scores EVALUATE-FOR InfoCNF. CIFAR10 EVALUATE-FOR InfoCNF. test accuracy EVALUATE-FOR InfoCNF. partitioning strategy USED-FOR extrapolation. partitioning strategy USED-FOR InfoCNF. time - series data USED-FOR InfoCNF. time - series data EVALUATE-FOR partitioning strategy. Task is exact likelihood estimation. OtherScientificTerm are latent space, and labeled information. ","This paper proposes a new class of deep generative models, called Continuous Normalizing Flows (CNFs), for tasks that require exact likelihood estimation. CNFs are commonly used for both conditional image generation and downstream predictive tasks. In this paper, the authors propose a conditional CNF, called InfoCNF, that combines class-specific supervised code and unsupervised code in the latent space. The proposed model learns a highdimensional latent code, which is then partitioned into two parts: (1) a set of classes, and (2) a subset of classes that share the same labeled information. The gating networks are used to learn error tolerances for ordinary differential equation (ODE) solvers, and the partitioning strategy is used to improve the performance of InfoC NF on time-series data. The authors show that the proposed method improves the test accuracy of the baseline by a large margin, and improves the likelihood scores on CIFAR10 with high NFEs. They also show that a partitioned strategy is also effective for extrapolation. ","This paper proposes a new class of deep generative models, called Continuous Normalizing Flows (CNFs), for tasks that require exact likelihood estimation. CNFs are commonly used for both conditional image generation and downstream predictive tasks. In this paper, the authors propose a conditional CNF, called InfoCNF, that combines class-specific supervised code and unsupervised code in the latent space. The proposed model learns a highdimensional latent code, which is then partitioned into two parts: (1) a set of classes, and (2) a subset of classes that share the same labeled information. The gating networks are used to learn error tolerances for ordinary differential equation (ODE) solvers, and the partitioning strategy is used to improve the performance of InfoC NF on time-series data. The authors show that the proposed method improves the test accuracy of the baseline by a large margin, and improves the likelihood scores on CIFAR10 with high NFEs. They also show that a partitioned strategy is also effective for extrapolation. "
21087,SP:97764e3393216106ff2ac3f550845acf4636119f,"nonlinear functions USED-FOR approximation of the value function. Temporal - Difference ( TD ) learning algorithm USED-FOR nonlinear functions. lazy training regime FEATURE-OF algorithm. non - lazy TD learning USED-FOR models. Generic are problem, regime, model, and convergence. OtherScientificTerm are approximating function, learning process, scaling, and exponential convergence. Method are lazy training, neural networks, and underand over - parametrized frameworks. ","This paper considers the problem of learning nonlinear functions for the approximation of the value function in the context of the Temporal-Difference (TD) learning algorithm. The problem is well-motivated and well-studied. In this regime, the approximating function is nonlinear and the learning process is non-asymptotically linear. The authors show that under the lazy training regime of the algorithm, the algorithm converges to the optimal solution of the problem. They also show that non-lazy TD learning can be used to train models that are non-over-parametrized.    The main contribution of this paper is that the authors provide a theoretical analysis of the convergence of non-linear functions in the regime of lazy training. They show that the scaling of neural networks in this regime is exponential in the number of training samples, and that the model is able to converge to a solution that minimizes the difference between the true solution and the solution obtained by the nonlinear function.  The authors also provide empirical evidence that the non-Lazy learning is more effective than non-slackled TD learning in terms of convergence.  Finally, the authors show empirically that their results are consistent with the theoretical analysis and show that neural networks trained with non-discriminative learning outperform underand over-parameterized frameworks. ","This paper considers the problem of learning nonlinear functions for the approximation of the value function in the context of the Temporal-Difference (TD) learning algorithm. The problem is well-motivated and well-studied. In this regime, the approximating function is nonlinear and the learning process is non-asymptotically linear. The authors show that under the lazy training regime of the algorithm, the algorithm converges to the optimal solution of the problem. They also show that non-lazy TD learning can be used to train models that are non-over-parametrized.    The main contribution of this paper is that the authors provide a theoretical analysis of the convergence of non-linear functions in the regime of lazy training. They show that the scaling of neural networks in this regime is exponential in the number of training samples, and that the model is able to converge to a solution that minimizes the difference between the true solution and the solution obtained by the nonlinear function.  The authors also provide empirical evidence that the non-Lazy learning is more effective than non-slackled TD learning in terms of convergence.  Finally, the authors show empirically that their results are consistent with the theoretical analysis and show that neural networks trained with non-discriminative learning outperform underand over-parameterized frameworks. "
21096,SP:c518e4030f12b0f59ad1d7c0fc0ebd313c68ef95,"reinforcement learning problem USED-FOR hypothesis verification. agents USED-FOR problem. action sequence CONJUNCTION post - condition. post - condition CONJUNCTION action sequence. pre - condition CONJUNCTION action sequence. action sequence CONJUNCTION pre - condition. Generic are agent, and they. ","This paper proposes a reinforcement learning problem for hypothesis verification, where the agent is given a pre-condition, an action sequence, and a post-condition. The problem is formulated as an optimization problem, and agents are trained to solve this problem. The agent is trained to verify the hypothesis of the pre-conditions and the action sequence. The authors show that they are able to achieve good performance in this setting.  ","This paper proposes a reinforcement learning problem for hypothesis verification, where the agent is given a pre-condition, an action sequence, and a post-condition. The problem is formulated as an optimization problem, and agents are trained to solve this problem. The agent is trained to verify the hypothesis of the pre-conditions and the action sequence. The authors show that they are able to achieve good performance in this setting.  "
21105,SP:6fa2f842b1bc993ed8024a3ce13dbd91529c61be,"neural networks USED-FOR approximate reasoning. fixed dimensional latent space USED-FOR approximate reasoning. latent space FEATURE-OF approximate reasoning. formula space CONJUNCTION latent space. latent space CONJUNCTION formula space. embeddings COMPARE predicted embeddings. predicted embeddings COMPARE embeddings. formula space FEATURE-OF rewrite steps. latent space FEATURE-OF rewrite steps. graph neural networks USED-FOR rewrite - success of statements. mathematical disciplines PART-OF corpus of mathematical formulas. OtherScientificTerm are transformations, semantic features, vector space, rewrite rule, and predicted latent representations. Generic are reasoning, and they. ","This paper studies the use of neural networks for approximate reasoning in a fixed dimensional latent space. The authors consider the problem of rewriting a sequence of mathematical formulas, where the goal is to find a set of transformations that are compatible with the current state of the art in the field. In this setting, the authors propose to use graph neural networks to model the rewrite-success of statements, and show that the embeddings of the rewrite steps in the formula space and in the latent space are very similar, and that the semantic features in the vector space are similar to those in the embedding space of the original statement. The paper also shows that the rewrite rule can be interpreted as a rewrite rule, and the authors show that this reasoning can be applied to a number of mathematical disciplines in a corpus of mathematicalulas.  ","This paper studies the use of neural networks for approximate reasoning in a fixed dimensional latent space. The authors consider the problem of rewriting a sequence of mathematical formulas, where the goal is to find a set of transformations that are compatible with the current state of the art in the field. In this setting, the authors propose to use graph neural networks to model the rewrite-success of statements, and show that the embeddings of the rewrite steps in the formula space and in the latent space are very similar, and that the semantic features in the vector space are similar to those in the embedding space of the original statement. The paper also shows that the rewrite rule can be interpreted as a rewrite rule, and the authors show that this reasoning can be applied to a number of mathematical disciplines in a corpus of mathematicalulas.  "
21114,SP:a77ab500a5e7d4ea8430871d1e603941e92974fd,"multi - view geometry USED-FOR methods. approach USED-FOR depth. images CONJUNCTION sparse depth measurements. sparse depth measurements CONJUNCTION images. images USED-FOR approach. sparse depth measurements USED-FOR depth. images USED-FOR depth. global - local network architecture USED-FOR inductive bias. model USED-FOR monocular dense depth estimation. sparse ground truth USED-FOR model. sparse ground truth USED-FOR monocular dense depth estimation. global parameters USED-FOR metric agent motion. network USED-FOR global parameters. Method are Natural intelligent agents, artificial systems, and natural agents. OtherScientificTerm are equations of projective geometry, visual and haptic feedback, and sparse supervision. ","Natural intelligent agents are a class of artificial systems that are able to learn to solve equations of projective geometry (e.g. geometry of a scene) using visual and haptic feedback. However, existing methods require multi-view geometry, which can be challenging for natural agents. This paper proposes an approach to learn depth from images and sparse depth measurements. The key idea is to use a global-local network architecture to mitigate the inductive bias of natural agents and to learn a model for monocular dense depth estimation based on sparse ground truth. The global parameters of the network are used to model the metric agent motion, and the network is trained with sparse supervision. ","Natural intelligent agents are a class of artificial systems that are able to learn to solve equations of projective geometry (e.g. geometry of a scene) using visual and haptic feedback. However, existing methods require multi-view geometry, which can be challenging for natural agents. This paper proposes an approach to learn depth from images and sparse depth measurements. The key idea is to use a global-local network architecture to mitigate the inductive bias of natural agents and to learn a model for monocular dense depth estimation based on sparse ground truth. The global parameters of the network are used to model the metric agent motion, and the network is trained with sparse supervision. "
21123,SP:2afba5e24478da4e9d493887c7cf00e288cc0deb,word pieces USED-FOR machine learning tasks. word pieces USED-FOR natural language models. natural language models USED-FOR machine learning tasks. machine learning tasks USED-FOR opaque ids. hash functions USED-FOR hash tokens. multi - layer Transformer USED-FOR Bloom filter digests. multi - layer Transformer USED-FOR models. accuracy EVALUATE-FOR models. They COMPARE models. models COMPARE They. computational budget FEATURE-OF sampled softmax. sampled softmax USED-FOR models. multi - layer Transformer USED-FOR Bloom filter digests. method USED-FOR problems. this USED-FOR problems. this USED-FOR method. large vocabulary size FEATURE-OF problems. Method is Bloom filter. OtherScientificTerm is hashing. ,"This paper studies the problem of learning word pieces for machine learning tasks with opaque ids. The authors propose to use natural language models with word pieces as word filters to tackle this problem. The idea is to use a Bloom filter, which is a variant of the Bloom filter proposed in [1]. The Bloom filter is a special case of hashing, where hash functions are used to encode the hash tokens.  The authors show that models trained with a multi-layer Transformer on Bloom filter digests can achieve higher accuracy than models trained using a single layer Transformer. They also show that such models can be trained with sampled softmax within the same computational budget.  Finally, the authors demonstrate that this method can be applied to other problems with large vocabulary size.  ","This paper studies the problem of learning word pieces for machine learning tasks with opaque ids. The authors propose to use natural language models with word pieces as word filters to tackle this problem. The idea is to use a Bloom filter, which is a variant of the Bloom filter proposed in [1]. The Bloom filter is a special case of hashing, where hash functions are used to encode the hash tokens.  The authors show that models trained with a multi-layer Transformer on Bloom filter digests can achieve higher accuracy than models trained using a single layer Transformer. They also show that such models can be trained with sampled softmax within the same computational budget.  Finally, the authors demonstrate that this method can be applied to other problems with large vocabulary size.  "
21132,SP:745dd86d7f7bba79a02d27922003b764b620f83e,"grouping policy USED-FOR small part proposals. grouping policy USED-FOR learningbased agglomerative clustering framework. local context USED-FOR part - level features. largescale fine - grained 3D part dataset EVALUATE-FOR method. method USED-FOR knowledge of parts. PartNet HYPONYM-OF largescale fine - grained 3D part dataset. shape segmentation baselines COMPARE approach. approach COMPARE shape segmentation baselines. Task are discovering 3D parts, and contextual bandit problem. Generic is prior. Method is data - driven shape segmentation approaches. ","This paper tackles the problem of discovering 3D parts. The authors propose a learningbased agglomerative clustering framework based on a grouping policy for small part proposals. The prior is a prior over a set of small parts, and the part-level features are learned in a local context. The proposed method is evaluated on a largescale fine-grained 3D part dataset called PartNet. Compared to existing shape segmentation baselines, the proposed approach is able to learn the knowledge of parts in a more general way. The paper also proposes a contextual bandit problem, which is a generalization of existing data-driven Shape Segmentation approaches. ","This paper tackles the problem of discovering 3D parts. The authors propose a learningbased agglomerative clustering framework based on a grouping policy for small part proposals. The prior is a prior over a set of small parts, and the part-level features are learned in a local context. The proposed method is evaluated on a largescale fine-grained 3D part dataset called PartNet. Compared to existing shape segmentation baselines, the proposed approach is able to learn the knowledge of parts in a more general way. The paper also proposes a contextual bandit problem, which is a generalization of existing data-driven Shape Segmentation approaches. "
21141,SP:868fc6df740b04963442d5abcfe2f4845585cfc8,"input / output datasets USED-FOR they. gender - related characteristics CONJUNCTION hair color. hair color CONJUNCTION gender - related characteristics. generative adversarial network ( GAN ) USED-FOR images of black - haired men. edit USED-FOR transformation. latent space FEATURE-OF transformation. autoencoder USED-FOR transformed data. editing transformation USED-FOR transformed data. transformation USED-FOR complex and non - linear transformations. latent trained space USED-FOR transformation. data domains CONJUNCTION modalities. modalities CONJUNCTION data domains. modalities CONJUNCTION applications. applications CONJUNCTION modalities. technique USED-FOR data domains. applications PART-OF biology. image transformations USED-FOR it. removal of batch artifacts HYPONYM-OF biology. removal of batch artifacts HYPONYM-OF applications. Method are generative neural networks, discriminator, generative models, and neuron editing. OtherScientificTerm are blond - haired men, source distribution, target distribution, manifold, distribution shifts, neuron ’s activations, unwanted noise, and drug treatments. Material is images of black - haired women. ","This paper proposes a new way of training generative neural networks that can edit input/output datasets so that they are more robust to distribution shifts (e.g. gender-related characteristics, hair color, etc.). The authors propose to train a generative adversarial network (GAN) to edit images of black-haired men trained on images of blond-haired men, and then train a discriminator to distinguish between the source distribution and the target distribution. The discriminator is trained in a similar way to the way that generative models are trained.   The key difference is that the discriminator does not need to know the source and target distributions of the input and output data, but instead it only needs to be able to distinguish the source from the target on the manifold. The authors show that this edit can be used to produce a transformation in the latent space, and that this transformation can be applied to complex and non-linear transformations.  The authors also show that an autoencoder can be trained on the transformed data generated by the editing transformation. The transformation is then applied to the latent trained space. The technique is applied to a variety of data domains, modalities, and applications in biology, including removal of batch artifacts (i.e. the removal of unwanted noise from the input) and drug treatments. The main contribution of the paper is that it is able to learn image transformations that are robust to different distribution shifts in the input space, which is an important property of neuron’s activations. The paper also shows that the neuron editing can be performed in a way that is more robust than a single edit.","This paper proposes a new way of training generative neural networks that can edit input/output datasets so that they are more robust to distribution shifts (e.g. gender-related characteristics, hair color, etc.). The authors propose to train a generative adversarial network (GAN) to edit images of black-haired men trained on images of blond-haired men, and then train a discriminator to distinguish between the source distribution and the target distribution. The discriminator is trained in a similar way to the way that generative models are trained.   The key difference is that the discriminator does not need to know the source and target distributions of the input and output data, but instead it only needs to be able to distinguish the source from the target on the manifold. The authors show that this edit can be used to produce a transformation in the latent space, and that this transformation can be applied to complex and non-linear transformations.  The authors also show that an autoencoder can be trained on the transformed data generated by the editing transformation. The transformation is then applied to the latent trained space. The technique is applied to a variety of data domains, modalities, and applications in biology, including removal of batch artifacts (i.e. the removal of unwanted noise from the input) and drug treatments. The main contribution of the paper is that it is able to learn image transformations that are robust to different distribution shifts in the input space, which is an important property of neuron’s activations. The paper also shows that the neuron editing can be performed in a way that is more robust than a single edit."
21150,SP:6dee6932e64fe47bb44dd42fc242fa9d89b8d89c,"training algorithms CONJUNCTION model architectures. model architectures CONJUNCTION training algorithms. reinforcement learning CONJUNCTION image semantic segmentation. image semantic segmentation CONJUNCTION reinforcement learning. few - shot image classification CONJUNCTION reinforcement learning. reinforcement learning CONJUNCTION few - shot image classification. model architectures USED-FOR few - shot domain. meta - learning approaches USED-FOR few - shot image classification. meta - learning approaches USED-FOR reinforcement learning. training algorithms USED-FOR few - shot domain. neural network representations USED-FOR meta - learning approaches. learning systems USED-FOR few - shot to many - shot settings. first - order meta - learning of initializations USED-FOR deep neural networks. first - order meta - learning of initializations USED-FOR dense, structured predictions. FOMAML CONJUNCTION Reptile. Reptile CONJUNCTION FOMAML. neural network architecture USED-FOR fast learning. generalization error EVALUATE-FOR meta - learning algorithms. small benchmark dataset EVALUATE-FOR meta - learning systems. meta - learning systems USED-FOR fewand many - shot settings. EfficientLab HYPONYM-OF neural network architecture. FP - k HYPONYM-OF small benchmark dataset. meta - learned initializations USED-FOR image segmentation. meta - learned initializations USED-FOR canonical few - shot learning problems. meta - learned initializations COMPARE random and ImageNet - trained initializations. random and ImageNet - trained initializations COMPARE meta - learned initializations. update routine USED-FOR tasks. FSS-1000 dataset EVALUATE-FOR network. Method are ensembling many models, relation networks, and MAML - type algorithms. OtherScientificTerm is initialization. Generic are task, and model. ","This paper proposes a meta-learning approach for few-shot learning, where the goal is to learn a model that generalizes well to unseen tasks. The authors propose to use a meta learning approach to learn the initializations of the model, and then use the learned initializations to improve the generalization performance of the meta-learner. They show that this approach improves the performance of meta-learners on a few shot classification and reinforcement learning tasks.  ","This paper proposes a meta-learning approach for few-shot learning, where the goal is to learn a model that generalizes well to unseen tasks. The authors propose to use a meta learning approach to learn the initializations of the model, and then use the learned initializations to improve the generalization performance of the meta-learner. They show that this approach improves the performance of meta-learners on a few shot classification and reinforcement learning tasks.  "
21159,SP:ec6f390f6d45fb79c33ae5d9c8a24cadb96fbd60,"unlabelled data USED-FOR few - shot learning. Prototypical Random Walk Networks(PRWN ) HYPONYM-OF SS - FSL approach. Prototypical Networks ( PN ) USED-FOR SS - FSL approach. Prototypical Networks ( PN ) USED-FOR Prototypical Random Walk Networks(PRWN ). random walk semi - supervised loss USED-FOR representations. random walk semi - supervised loss USED-FOR network. network USED-FOR representations. graph - based approaches USED-FOR few - shot learning. prototypical random walk notion USED-FOR compact and well - separated class representations. model COMPARE art. art COMPARE model. model COMPARE fully supervised prototypical networks. fully supervised prototypical networks COMPARE model. 1 - shot mini - Imagenet case EVALUATE-FOR it. accuracy EVALUATE-FOR it. robustness FEATURE-OF labelled / unlabelled class distribution mismatch. discriminative power test EVALUATE-FOR baseline. Task are human intelligence, transductive setting, and mini - Imagenet 5 - shot classification task. Method is AI models. OtherScientificTerm are graph - NN parameters, distractors, and unlabeled data. Material are collective test set, mini - Imagenet, and Omniglot. ","This paper proposes Prototypical Random Walk Networks(PRWN), an extension of the SS-FSL approach based on Prototypial Networks (PN) to few-shot learning with unlabeled data. The authors propose a novel SS-FSL approach that builds on the idea of prototypical random walk (PRW) in the context of graph-NNs. PRWN uses a random walk semi-supervised loss to learn representations for each class using a network that is trained on a collective test set of all classes in a transductive setting.  The authors show that the proposed model outperforms the state-of-the-art art in the 1-shot mini-Imagenet case, and it outperforms it in the 5-shot setting on Omniglot. They also show that their model is more robust to distractors than fully supervised prototypical networks.    In general, the paper is well-written and well-motivated. The idea of using graph-based approaches as a baseline for few-task learning is interesting. However, there are several issues with the paper:  1. The paper does not provide sufficient analysis of the impact of distractors on the performance of AI models.  2. The proposed model is not well-suited for the mini-imagenet setting, as the authors do not provide a discriminative power test for their baseline.  3. They do not compare their model to the state of the art in terms of robustness against distractors.  4. They show that they are able to learn compact and well separated class representations that are more robust in the sense that human intelligence is.  5. Finally, the authors provide an analysis of their model and show that it is more sensitive to the distractors (i.e., distractors that are not present in the training set).   The paper also shows that their proposed model performs better in the case where there is no labelled/unlabeled class distribution mismatch, which is a common problem in the real world. ","This paper proposes Prototypical Random Walk Networks(PRWN), an extension of the SS-FSL approach based on Prototypial Networks (PN) to few-shot learning with unlabeled data. The authors propose a novel SS-FSL approach that builds on the idea of prototypical random walk (PRW) in the context of graph-NNs. PRWN uses a random walk semi-supervised loss to learn representations for each class using a network that is trained on a collective test set of all classes in a transductive setting.  The authors show that the proposed model outperforms the state-of-the-art art in the 1-shot mini-Imagenet case, and it outperforms it in the 5-shot setting on Omniglot. They also show that their model is more robust to distractors than fully supervised prototypical networks.    In general, the paper is well-written and well-motivated. The idea of using graph-based approaches as a baseline for few-task learning is interesting. However, there are several issues with the paper:  1. The paper does not provide sufficient analysis of the impact of distractors on the performance of AI models.  2. The proposed model is not well-suited for the mini-imagenet setting, as the authors do not provide a discriminative power test for their baseline.  3. They do not compare their model to the state of the art in terms of robustness against distractors.  4. They show that they are able to learn compact and well separated class representations that are more robust in the sense that human intelligence is.  5. Finally, the authors provide an analysis of their model and show that it is more sensitive to the distractors (i.e., distractors that are not present in the training set).   The paper also shows that their proposed model performs better in the case where there is no labelled/unlabeled class distribution mismatch, which is a common problem in the real world. "
21168,SP:d12e687bd2ee9fa60554312e644bb0a6487974f1,"machine learning USED-FOR remote sensing. labeled data USED-FOR machine learning. deep convolutional neural networks HYPONYM-OF models. selfsupervised learning approaches USED-FOR remote sensing domain. multi - sensor, multi - channel information USED-FOR remote sensing applications. Contrastive Sensor Fusion USED-FOR representations. Contrastive Sensor Fusion HYPONYM-OF self - supervised training objective. model USED-FOR representation. encoder USED-FOR representations. dataset USED-FOR encoder. unlabeled coterminous image triplets PART-OF dataset. remote sensing classification task EVALUATE-FOR representations. Material are unlabeled data, and coterminous data. Method is fused multi - sensor representations. Generic is method. ","This paper proposes a new self-supervised learning framework for multi-sensor, multi-channel remote sensing. The proposed method is based on the Contrastive Sensitive Fusion (CSF) framework, which is an extension of Contrastive Convolutional Neural Networks (CCNN) to the remote sensing domain. The authors show that CSF is able to learn representations that are more robust to label noise, which can be used to improve the performance of the model in remote sensing applications. ","This paper proposes a new self-supervised learning framework for multi-sensor, multi-channel remote sensing. The proposed method is based on the Contrastive Sensitive Fusion (CSF) framework, which is an extension of Contrastive Convolutional Neural Networks (CCNN) to the remote sensing domain. The authors show that CSF is able to learn representations that are more robust to label noise, which can be used to improve the performance of the model in remote sensing applications. "
21177,SP:4d8e054f07006b4f896721b5c24da805727d2c22,"fine - tuning HYPONYM-OF retraining technique. fine - tuning COMPARE retraining techniques. retraining techniques COMPARE fine - tuning. Learning rate rewinding USED-FOR unpruned weights. learning rate schedule USED-FOR weight rewinding. Learning rate rewinding COMPARE weight rewinding. weight rewinding COMPARE Learning rate rewinding. learning rate schedule USED-FOR Learning rate rewinding. rewinding techniques COMPARE fine - tuning. fine - tuning COMPARE rewinding techniques. accuracy CONJUNCTION compression ratios. compression ratios CONJUNCTION accuracy. rewinding techniques USED-FOR network - agnostic pruning algorithm. compression ratios EVALUATE-FOR network - agnostic pruning algorithm. accuracy EVALUATE-FOR network - agnostic pruning algorithm. Method are neural network pruning algorithms, and Weight rewinding. Generic is network. OtherScientificTerm are learning rate, and training schedule. ","This paper studies the problem of fine-tuning neural network pruning algorithms. In particular, the authors propose a new retraining technique called ""weight rewinding"", which is an extension of the classic retaining technique known as ""fine-tunning"" (Zhang et al., 2017). Weight rewiring is a simple way to prune a network, where the weights are re-weighted according to the learning rate, and the training schedule is re-set to a pre-specified learning rate.    The authors show that fine-tuning outperforms other retraining techniques in terms of accuracy and compression ratios. They also show that learning rate re-winding can be applied to unpruned weights as well. Learning rate reweighing can be done with a different learning rate schedule from the one used for weight rewowing.  The paper also shows that the performance of a network-agnostic pruning algorithm based on rewitching techniques is comparable to that of the original fine -tuning. ","This paper studies the problem of fine-tuning neural network pruning algorithms. In particular, the authors propose a new retraining technique called ""weight rewinding"", which is an extension of the classic retaining technique known as ""fine-tunning"" (Zhang et al., 2017). Weight rewiring is a simple way to prune a network, where the weights are re-weighted according to the learning rate, and the training schedule is re-set to a pre-specified learning rate.    The authors show that fine-tuning outperforms other retraining techniques in terms of accuracy and compression ratios. They also show that learning rate re-winding can be applied to unpruned weights as well. Learning rate reweighing can be done with a different learning rate schedule from the one used for weight rewowing.  The paper also shows that the performance of a network-agnostic pruning algorithm based on rewitching techniques is comparable to that of the original fine -tuning. "
21186,SP:3bb1c79f9482e09828eda45fbb2e654f37219365,normalized ) output margin CONJUNCTION generalization. generalization CONJUNCTION normalized ) output margin. output margin FEATURE-OF generalization. all - layer margin HYPONYM-OF margin. generalization EVALUATE-FOR deep models. theoretically inspired training algorithm USED-FOR all - layer margin. neural net USED-FOR adversarially robust setting. robust test error FEATURE-OF deep networks. Jacobian and hidden layer norms USED-FOR neural nets. Method is linear classifiers. Generic is algorithm. ,"This paper studies the relationship between the (normalized) output margin and generalization of deep models. The authors show that the output margin of linear classifiers is a measure of generalization, and that the margin, called the all-layer margin, is a theoretically inspired training algorithm. They also show that neural nets with Jacobian and hidden layer norms are robust to adversarially robust setting, and show that deep networks with robust test error can be trained with a neural net with the proposed algorithm. ","This paper studies the relationship between the (normalized) output margin and generalization of deep models. The authors show that the output margin of linear classifiers is a measure of generalization, and that the margin, called the all-layer margin, is a theoretically inspired training algorithm. They also show that neural nets with Jacobian and hidden layer norms are robust to adversarially robust setting, and show that deep networks with robust test error can be trained with a neural net with the proposed algorithm. "
21195,SP:3d44f27468087280e85dfb1fc7291db05179fe6d,"ungrounded dialogues CONJUNCTION unstructured documents. unstructured documents CONJUNCTION ungrounded dialogues. unstructured documents USED-FOR model. ungrounded dialogues USED-FOR model. limited training examples USED-FOR small parameters. benchmarks EVALUATE-FOR model. out - of - domain knowledge EVALUATE-FOR model. Task are intelligent conversational agent, and knowledge - grounded dialogue generation. Material is knowledge - grounded dialogues. Method are response generation model, disentangled response decoder, and generation model. ","This paper tackles the problem of learning an intelligent conversational agent that is capable of generating knowledge-grounded dialogues. The authors propose a novel response generation model that learns a disentangled response decoder and a generation model. The model is trained on ungrounded dialogue, unstructured dialogues, and unsupervised documents. The proposed model is evaluated on three benchmarks, where it is shown to be able to generate dialogue with out-of-domain knowledge with limited training examples for small parameters.   ","This paper tackles the problem of learning an intelligent conversational agent that is capable of generating knowledge-grounded dialogues. The authors propose a novel response generation model that learns a disentangled response decoder and a generation model. The model is trained on ungrounded dialogue, unstructured dialogues, and unsupervised documents. The proposed model is evaluated on three benchmarks, where it is shown to be able to generate dialogue with out-of-domain knowledge with limited training examples for small parameters.   "
21204,SP:9b555f7fe743f5effdbdc8701ed519ce3159c4b0,"parallel corpus USED-FOR neural machine translation models ( NMT ). non - parallel bilingual data USED-FOR decoding. non - parallel bilingual data USED-FOR training. training CONJUNCTION decoding. decoding CONJUNCTION training. non - parallel bilingual data USED-FOR Existing approaches. source to target translation model CONJUNCTION target to source translation model. target to source translation model CONJUNCTION source to target translation model. target to source translation model CONJUNCTION language models. language models CONJUNCTION target to source translation model. mirror - generative NMT ( MGNMT ) HYPONYM-OF single unified architecture. source to target translation model PART-OF single unified architecture. target to source translation model PART-OF single unified architecture. language models PART-OF single unified architecture. translation models CONJUNCTION language models. language models CONJUNCTION translation models. latent semantic space FEATURE-OF language models. non - parallel data USED-FOR translation directions. translation models CONJUNCTION language models. language models CONJUNCTION translation models. language models USED-FOR decoding. translation models USED-FOR decoding. MGNMT COMPARE approaches. approaches COMPARE MGNMT. Material are non - parallel corpora, and resource - rich and low - resource situations. ","This paper proposes a new parallel corpus for training neural machine translation models (NMT) on non-parallel bilingual data. Existing approaches have been shown to benefit from the use of non-plausible bilingual data for training and decoding. The authors propose a single unified architecture called mirror-generative NMT (MGNMT), which consists of a source to target translation model, a target to source translation model and language models. The translation directions are generated from non-differentiable translation models in the latent semantic space, and the translation models are trained on the translation directions generated by translation models.  The authors show that MGNMT outperforms existing approaches in both resource-rich and low-resource situations. They also show that translation models trained on translation models, language models trained for decoding on the nonparallel data, as well as language models that are trained with translation directions from the source language. ","This paper proposes a new parallel corpus for training neural machine translation models (NMT) on non-parallel bilingual data. Existing approaches have been shown to benefit from the use of non-plausible bilingual data for training and decoding. The authors propose a single unified architecture called mirror-generative NMT (MGNMT), which consists of a source to target translation model, a target to source translation model and language models. The translation directions are generated from non-differentiable translation models in the latent semantic space, and the translation models are trained on the translation directions generated by translation models.  The authors show that MGNMT outperforms existing approaches in both resource-rich and low-resource situations. They also show that translation models trained on translation models, language models trained for decoding on the nonparallel data, as well as language models that are trained with translation directions from the source language. "
21213,SP:d7a530a0ec4112095a58cef4cda9646f8ca6449d,maximum entropy reinforcement learning algorithms USED-FOR Deep Reinforcement Learning ( DRL ). benchmarks EVALUATE-FOR sample efficiency. entropy term USED-FOR maximum entropy algorithms. entropy term USED-FOR bounded nature of the action spaces. entropy term PART-OF Soft Actor Critic ( SAC ). entropy term USED-FOR Mujoco benchmark. streamlined algorithms USED-FOR SAC. entropy maximization USED-FOR streamlined algorithms. non - uniform sampling method USED-FOR transitions. transitions PART-OF replay buffer. streamlined algorithm COMPARE SAC. SAC COMPARE streamlined algorithm. continuous control tasks EVALUATE-FOR streamlined algorithm. non - uniform sampling scheme USED-FOR streamlined algorithm. OtherScientificTerm is maximum entropy objective. Task is training. ,"This paper studies the sample efficiency of maximum entropy reinforcement learning algorithms for Deep Reinforcement Learning (DRL). The authors propose a new maximum entropy objective for the Mujoco benchmark. They show that the entropy term in the standard maximum entropy algorithms (e.g., Soft Actor Critic (SAC) and SAC with the same entropy term for the bounded nature of the action spaces) can be used to improve sample efficiency on these benchmarks. They also show that this entropy term can be incorporated into the standard SAC in order to improve the performance of SAC.  The authors also propose two streamlined algorithms for SAC based on entropy maximization. The first streamlined algorithm uses a non-uniform sampling scheme, where the transitions in the replay buffer are sampled from a different distribution. The second streamlined algorithm is based on a non - uniform sampling method, where transitions are sampled based on the number of times they have been visited during training. The authors demonstrate that the proposed streamlined algorithm outperforms SAC on continuous control tasks. ","This paper studies the sample efficiency of maximum entropy reinforcement learning algorithms for Deep Reinforcement Learning (DRL). The authors propose a new maximum entropy objective for the Mujoco benchmark. They show that the entropy term in the standard maximum entropy algorithms (e.g., Soft Actor Critic (SAC) and SAC with the same entropy term for the bounded nature of the action spaces) can be used to improve sample efficiency on these benchmarks. They also show that this entropy term can be incorporated into the standard SAC in order to improve the performance of SAC.  The authors also propose two streamlined algorithms for SAC based on entropy maximization. The first streamlined algorithm uses a non-uniform sampling scheme, where the transitions in the replay buffer are sampled from a different distribution. The second streamlined algorithm is based on a non - uniform sampling method, where transitions are sampled based on the number of times they have been visited during training. The authors demonstrate that the proposed streamlined algorithm outperforms SAC on continuous control tasks. "
21222,SP:545e8da553fcb47d84eaa044d8a4947d3cd3230e,"machine learning models USED-FOR adversarial attacks. industrial copyright detection tools USED-FOR web. industrial copyright detection tools USED-FOR adversarial attacks. neural net USED-FOR system. gradient methods USED-FOR system. AudioTag copyright detector CONJUNCTION YouTube ’s Content ID system. YouTube ’s Content ID system CONJUNCTION AudioTag copyright detector. Adversarial music USED-FOR industrial systems. YouTube ’s Content ID system HYPONYM-OF industrial systems. AudioTag copyright detector HYPONYM-OF industrial systems. Method are classifier, copyright detection systems, neural network based systems, and music identification method. Generic is they. OtherScientificTerm is attacks. Material is adversarial examples. ","This paper studies the problem of adversarial attacks on machine learning models. The authors show that existing industrial copyright detection tools for the web are vulnerable to adversarial music, and that they can be easily exploited to fool existing copyright detection systems. Adversarial music can be used to fool industrial systems such as the AudioTag copyright detector and YouTube’s Content ID system.   The authors propose a new music identification method that can be applied to existing neural network based systems. The system uses a neural net to identify the source of the adversarial examples, and then uses gradient methods to fool the system. They show that the system can be trained to fool a classifier. ","This paper studies the problem of adversarial attacks on machine learning models. The authors show that existing industrial copyright detection tools for the web are vulnerable to adversarial music, and that they can be easily exploited to fool existing copyright detection systems. Adversarial music can be used to fool industrial systems such as the AudioTag copyright detector and YouTube’s Content ID system.   The authors propose a new music identification method that can be applied to existing neural network based systems. The system uses a neural net to identify the source of the adversarial examples, and then uses gradient methods to fool the system. They show that the system can be trained to fool a classifier. "
21231,SP:b511822850da3bf1079a36ed6f5ad4db80fbc424,"visual explanation USED-FOR deep metric learning. model COMPARE classification. classification COMPARE model. framework USED-FOR metric learning applications. framework USED-FOR model. cross - view pattern discovery CONJUNCTION interactive retrieval. interactive retrieval CONJUNCTION cross - view pattern discovery. interactive retrieval HYPONYM-OF applications. cross - view pattern discovery HYPONYM-OF applications. Method are learning representation, and metric learning. OtherScientificTerm are overall activation map, and point - to - point activation intensity. ","This paper proposes a new framework for deep metric learning with visual explanation. The authors propose a learning representation that is based on the overall activation map of each point in the image, and the point-to-point activation intensity. They show that the proposed framework can be applied to several metric learning applications, including cross-view pattern discovery and interactive retrieval, and show that their model outperforms classification. ","This paper proposes a new framework for deep metric learning with visual explanation. The authors propose a learning representation that is based on the overall activation map of each point in the image, and the point-to-point activation intensity. They show that the proposed framework can be applied to several metric learning applications, including cross-view pattern discovery and interactive retrieval, and show that their model outperforms classification. "
21240,SP:67bf71219fe6bedec5f5525200e734638e4a6ca2,"learning control USED-FOR online lifelong learning scenario. they USED-FOR failure modes. computational resources USED-FOR model - based planning methods. model - based planning CONJUNCTION model - free learning. model - free learning CONJUNCTION model - based planning. planner CONJUNCTION model - free components. model - free components CONJUNCTION planner. Method are model - free policy learning methods, Adaptive Online Planning ( AOP ), AOP, continual learning agent, and reinforcement learning methods. OtherScientificTerm are compact networks, performance degradation, dynamics, constrained computation limits, and unpredictable changes in the world. Generic are setting, and algorithm. Task is planning. ","This paper studies the problem of learning control in an online lifelong learning scenario, where the goal is to learn compact networks that do not suffer from performance degradation. The authors propose Adaptive Online Planning (AOP), which is an extension of model-free policy learning methods. AOP learns a continual learning agent that is able to adaptively adapt to new environments and environments that are more challenging to learn from. The setting is very similar to the one in which the authors propose the setting of learning-to-learn (L2L), but the authors argue that they are able to avoid some of the failure modes of L2L due to the fact that they do not require any computational resources that are typically required in model-based planning methods.  The authors show that AOP can be applied to a variety of settings, including a setting where there is no planning, a setting in which there is a planner, and a setting with constrained computation limits. They show that in this setting, AOP is more robust to the dynamics of the environment, and that it can learn to adapt to a new environment in a way that is more resilient to unpredictable changes in the world. They also show that the algorithm can be used to learn a planner that can be combined with any of a number of existing models, and can be seen as a generalization of existing reinforcement learning methods, which can be considered as a way to combine the benefits of model based planning with the benefits from model free learning.   ","This paper studies the problem of learning control in an online lifelong learning scenario, where the goal is to learn compact networks that do not suffer from performance degradation. The authors propose Adaptive Online Planning (AOP), which is an extension of model-free policy learning methods. AOP learns a continual learning agent that is able to adaptively adapt to new environments and environments that are more challenging to learn from. The setting is very similar to the one in which the authors propose the setting of learning-to-learn (L2L), but the authors argue that they are able to avoid some of the failure modes of L2L due to the fact that they do not require any computational resources that are typically required in model-based planning methods.  The authors show that AOP can be applied to a variety of settings, including a setting where there is no planning, a setting in which there is a planner, and a setting with constrained computation limits. They show that in this setting, AOP is more robust to the dynamics of the environment, and that it can learn to adapt to a new environment in a way that is more resilient to unpredictable changes in the world. They also show that the algorithm can be used to learn a planner that can be combined with any of a number of existing models, and can be seen as a generalization of existing reinforcement learning methods, which can be considered as a way to combine the benefits of model based planning with the benefits from model free learning.   "
21249,SP:11159cb878a436a5d4fc6edb4132f2cc3c1b3f72,"Visual attention mechanisms USED-FOR image captioning models. sparsemax CONJUNCTION Total - Variation Sparse Attention ( TVMAX ). Total - Variation Sparse Attention ( TVMAX ) CONJUNCTION sparsemax. sparsity - promoting transformations USED-FOR softmax attention mechanism. Total - Variation Sparse Attention ( TVMAX ) HYPONYM-OF sparsity - promoting transformations. sparsemax HYPONYM-OF sparsity - promoting transformations. sparsemax USED-FOR sparse attention weights. interpretability EVALUATE-FOR TVMAX transformation. humanrated caption quality CONJUNCTION attention relevance. attention relevance CONJUNCTION humanrated caption quality. TVMAX COMPARE attention mechanisms. attention mechanisms COMPARE TVMAX. attention relevance EVALUATE-FOR attention mechanisms. attention relevance EVALUATE-FOR TVMAX. humanrated caption quality EVALUATE-FOR attention mechanisms. humanrated caption quality EVALUATE-FOR TVMAX. OtherScientificTerm are image structure, relevant features, and sparsity. Material is Microsoft COCO and Flickr30k datasets. Method is softmax. ",Visual attention mechanisms for image captioning models have been a topic of interest for many years. This paper proposes two sparsity-promoting transformations to the softmax attention mechanism: sparsemax and Total-Variation Sparse Attention (TVMAX). The idea is that sparsemax encourages the sparse attention weights to be close to the relevant features in the image structure. The authors show that the TVMAX transformation improves interpretability and human-rated caption quality on the Microsoft COCO and Flickr30k datasets. They also show that TVMAX outperforms other attention mechanisms in terms of humanrated captions quality and attention relevance.   ,Visual attention mechanisms for image captioning models have been a topic of interest for many years. This paper proposes two sparsity-promoting transformations to the softmax attention mechanism: sparsemax and Total-Variation Sparse Attention (TVMAX). The idea is that sparsemax encourages the sparse attention weights to be close to the relevant features in the image structure. The authors show that the TVMAX transformation improves interpretability and human-rated caption quality on the Microsoft COCO and Flickr30k datasets. They also show that TVMAX outperforms other attention mechanisms in terms of humanrated captions quality and attention relevance.   
21258,SP:fb0c3ce3db6ad674ddc615bdc6203cdcbe42c804,Neural networks USED-FOR structured data. Neural networks USED-FOR graphs. graphs HYPONYM-OF structured data. Predicting the evolution of dynamic graphs PART-OF graph mining. model USED-FOR evolution of dynamic graphs. graph neural network USED-FOR temporal evolution patterns of dynamic graphs. graph neural network CONJUNCTION recurrent architecture. recurrent architecture CONJUNCTION graph neural network. recurrent architecture USED-FOR temporal evolution patterns of dynamic graphs. generative model USED-FOR graph instance. graph instance USED-FOR topology. common network evolving dynamics FEATURE-OF artificial datasets. real - world datasets EVALUATE-FOR model. artificial datasets EVALUATE-FOR model. OtherScientificTerm is static graphs. Material is real - world networks. Generic is task. ,"This paper proposes a new model for predicting the evolution of dynamic graphs in graph mining. Neural networks are commonly used to model structured data (e.g. graphs). This paper proposes to use a graph neural network and a recurrent architecture to predict the temporal evolution patterns in dynamic graphs. The key idea is to learn a generative model that predicts the topology of a graph instance, and then use this graph instance to learn the underlying topology. The proposed model is evaluated on two artificial datasets with common network evolving dynamics, and on two real-world datasets with static graphs. It is shown that the proposed model outperforms the real world networks on both of these datasets. ","This paper proposes a new model for predicting the evolution of dynamic graphs in graph mining. Neural networks are commonly used to model structured data (e.g. graphs). This paper proposes to use a graph neural network and a recurrent architecture to predict the temporal evolution patterns in dynamic graphs. The key idea is to learn a generative model that predicts the topology of a graph instance, and then use this graph instance to learn the underlying topology. The proposed model is evaluated on two artificial datasets with common network evolving dynamics, and on two real-world datasets with static graphs. It is shown that the proposed model outperforms the real world networks on both of these datasets. "
21267,SP:ff722957a1765c0568426ed88dd910a6b74054ef,"incomplete datasets USED-FOR machine learning applications. missing data imputation techniques USED-FOR filling missing values. method USED-FOR imputing missing features. method USED-FOR distribution of target assignments. incomplete data USED-FOR distribution of target assignments. generator network USED-FOR imputations. generator network USED-FOR imputations. predictor network USED-FOR classification uncertainties. generator network USED-FOR predictor network. imputed samples USED-FOR predictor network. CIFAR-10 image dataset CONJUNCTION real - world tabular classification datasets. real - world tabular classification datasets CONJUNCTION CIFAR-10 image dataset. real - world tabular classification datasets EVALUATE-FOR method. CIFAR-10 image dataset EVALUATE-FOR method. method USED-FOR generating imputations. class uncertainties FEATURE-OF classification task. OtherScientificTerm are missing values, distribution of missing values, missing features, and missingness rates. Method is discriminator network. ",This paper proposes a new method for imputing missing features from incomplete datasets for machine learning applications. The authors extend previous missing data imputation techniques for filling missing values to the case where the distribution of missing values is unknown. The proposed method is able to generate a distribution of target assignments from incomplete data. The imputations are made by using a generator network to generate imputations from the incomplete data and a discriminator network to discriminate between the imputed features and the true missingness rates. The generator network is then used to train a predictor network that uses imputed samples to model classification uncertainties. The method is evaluated on the CIFAR-10 image dataset and real-world tabular classification datasets. The results show that the proposed method can achieve state-of-the-art performance for generating imputations with class uncertainties for a classification task.,This paper proposes a new method for imputing missing features from incomplete datasets for machine learning applications. The authors extend previous missing data imputation techniques for filling missing values to the case where the distribution of missing values is unknown. The proposed method is able to generate a distribution of target assignments from incomplete data. The imputations are made by using a generator network to generate imputations from the incomplete data and a discriminator network to discriminate between the imputed features and the true missingness rates. The generator network is then used to train a predictor network that uses imputed samples to model classification uncertainties. The method is evaluated on the CIFAR-10 image dataset and real-world tabular classification datasets. The results show that the proposed method can achieve state-of-the-art performance for generating imputations with class uncertainties for a classification task.
21276,SP:c051b0fe779d9e4131016970b7ba469b596f3009,"Off - policy estimation USED-FOR long - horizon problems. healthcare CONJUNCTION robotics. robotics CONJUNCTION healthcare. Off - policy estimation USED-FOR real - life applications. robotics HYPONYM-OF real - life applications. healthcare HYPONYM-OF real - life applications. curse of horizon FEATURE-OF importance - sampling - based methods. stationary distribution FEATURE-OF known behavior policy. estimator USED-FOR importance ratios of stationary distributions. Reproducing Kernel Hilbert Spaces ( RKHSs ) USED-FOR estimator. asymptotic consistency CONJUNCTION finite - sample generalization. finite - sample generalization CONJUNCTION asymptotic consistency. Method is high - fidelity simulators. Task is on - policy evaluation. Generic are approach, it, problem, and operator. Material is off - policy data. ","This paper considers the problem of off-policy estimation for long-horizon problems, where high-fidelity simulators are available but not available for on-policy evaluation. In real-life applications such as healthcare and robotics, there is a curse of horizon that existing importance-sampling-based methods suffer from. This paper proposes a new approach to address this problem. The proposed estimator is based on Reproducing Kernel Hilbert Hilbert Spaces (RKHSs) and is able to estimate the importance ratios of stationary distributions of a known behavior policy with respect to a stationary distribution of the data. The authors show that their estimator achieves asymptotic consistency and finite-sample generalization, and that it can be applied to any problem where the operator is a linear function of the number of times that the operator has been used in the past.","This paper considers the problem of off-policy estimation for long-horizon problems, where high-fidelity simulators are available but not available for on-policy evaluation. In real-life applications such as healthcare and robotics, there is a curse of horizon that existing importance-sampling-based methods suffer from. This paper proposes a new approach to address this problem. The proposed estimator is based on Reproducing Kernel Hilbert Hilbert Spaces (RKHSs) and is able to estimate the importance ratios of stationary distributions of a known behavior policy with respect to a stationary distribution of the data. The authors show that their estimator achieves asymptotic consistency and finite-sample generalization, and that it can be applied to any problem where the operator is a linear function of the number of times that the operator has been used in the past."
21285,SP:065c900843011a71b70ed35357a2f71fe83872a7,"probabilistic framework USED-FOR dataset. Mixture Model ( MM ) HYPONYM-OF probabilistic framework. modes PART-OF dataset. Gaussian distribution FEATURE-OF modes. paintings dataset CONJUNCTION fashion images. fashion images CONJUNCTION paintings dataset. unlabelled modes PART-OF large datasets. fashion images HYPONYM-OF large datasets. paintings dataset HYPONYM-OF large datasets. plausible method USED-FOR probabilities. Generative Adversarial Network ( GAN ) framework USED-FOR plausible method. GAN USED-FOR distribution. GAN USED-FOR classification network. techniques USED-FOR unsupervised dataset. smooth linear interpolation USED-FOR outdistribution ” data. Method are Gaussian MM, GMM, and GMM paradigm. OtherScientificTerm are conditional likelihood, distribution index, Euclidean distances, responsibility distribution, latent representation of x, and dataset segments. Generic is responsibility. ","This paper proposes a new probabilistic framework called the Mixture Model (MM) to model a dataset with a Gaussian distribution. The dataset consists of a set of modes, each of which corresponds to a different conditional likelihood. The authors propose a new GMM paradigm, where each mode is associated with a distribution index, and the distribution index is a function of the number of modes in the dataset.    The authors show that under Gaussian MM, the distribution of the modes in a dataset can be approximated by a GAN, where the modes are assumed to be drawn from the same distribution.  The paper also shows that under Euclidean distances, a plausible method to estimate the probabilities of the probabilities can be derived from the Generative Adversarial Network (GAN) framework.  In particular, the paper shows that for large datasets with unlabelled modes (e.g., the paintings dataset and fashion images), the probability of the distributions in a GMM can be estimated from the latent representation of x, and that the distribution in the latent space of the GAN can be used to train a classification network.  Finally, the authors propose two techniques to sample from the unsupervised dataset. First, they use smooth linear interpolation to sample samples from the “outdistribution” data, which is defined as samples from a distribution that is independent of the dataset segments. Second, they apply GAN to learn the distribution over the samples, and use the learned distribution as a basis for the task of assigning a “responsibility” to each sample. ","This paper proposes a new probabilistic framework called the Mixture Model (MM) to model a dataset with a Gaussian distribution. The dataset consists of a set of modes, each of which corresponds to a different conditional likelihood. The authors propose a new GMM paradigm, where each mode is associated with a distribution index, and the distribution index is a function of the number of modes in the dataset.    The authors show that under Gaussian MM, the distribution of the modes in a dataset can be approximated by a GAN, where the modes are assumed to be drawn from the same distribution.  The paper also shows that under Euclidean distances, a plausible method to estimate the probabilities of the probabilities can be derived from the Generative Adversarial Network (GAN) framework.  In particular, the paper shows that for large datasets with unlabelled modes (e.g., the paintings dataset and fashion images), the probability of the distributions in a GMM can be estimated from the latent representation of x, and that the distribution in the latent space of the GAN can be used to train a classification network.  Finally, the authors propose two techniques to sample from the unsupervised dataset. First, they use smooth linear interpolation to sample samples from the “outdistribution” data, which is defined as samples from a distribution that is independent of the dataset segments. Second, they apply GAN to learn the distribution over the samples, and use the learned distribution as a basis for the task of assigning a “responsibility” to each sample. "
21294,SP:2da1608209058d214f8671062cc9eb0833ba4831,method USED-FOR large capacity neural networks. accuracy CONJUNCTION dynamic computational cost. dynamic computational cost CONJUNCTION accuracy. accuracy EVALUATE-FOR method. fine - grained - level FEATURE-OF deep - learning architecture. residual block architecture USED-FOR convolutional channels. fine - grained manner FEATURE-OF residual block architecture. fine - grained manner FEATURE-OF convolutional channels. marginal aggregate posteriors of features PART-OF neural network. pre - specified prior distribution FEATURE-OF marginal aggregate posteriors of features. technique USED-FOR gates. CIFAR-10 and ImageNet datasets USED-FOR image classification. Cityscapes USED-FOR semantic segmentation. image classification CONJUNCTION Cityscapes. Cityscapes CONJUNCTION image classification. average computational cost COMPARE architecture. architecture COMPARE average computational cost. method USED-FOR architectures. method COMPARE architecture. architecture COMPARE method. ImageNet EVALUATE-FOR ResNet34 gated networks. accuracy EVALUATE-FOR ResNet18 model. top-1 accuracy EVALUATE-FOR ResNet34 gated networks. complexity EVALUATE-FOR ResNet18 model. features CONJUNCTION features. features CONJUNCTION features. features USED-FOR networks. OtherScientificTerm is convolutional maps. Generic is network. Method is batch - shaping. ,"This paper proposes a new method for training large capacity neural networks with a focus on accuracy and dynamic computational cost. The authors propose a deep-learning architecture at the fine-grained-level, where the convolutional maps of the network are aggregated to form the final output of the deep network. This is achieved by using a residual block architecture in a fine- grained manner, which is similar to the residual block that is used in the original deep learning architecture. The marginal aggregate posteriors of features in a neural network are obtained from a pre-specified prior distribution. The technique is applied to the gates of the gates, and the authors show that the technique can be applied to both image classification on CIFAR-10 and ImageNet datasets, and semantic segmentation on Cityscapes. The proposed method is shown to outperform the average computational cost of the original architecture as well as the architecture of a ResNet34 gated networks in terms of accuracy and top-1 accuracy on ImageNet. The complexity of the ResNet18 model is also shown to be much smaller than the original one.  The authors also show that their method can be extended to other architectures, and that the networks trained with the same number of features can be trained with different number of gates.  Finally, the authors provide a theoretical analysis of batch-shaping, which shows that the network is able to learn a good trade-off between the quality of the features and the number of parameters. ","This paper proposes a new method for training large capacity neural networks with a focus on accuracy and dynamic computational cost. The authors propose a deep-learning architecture at the fine-grained-level, where the convolutional maps of the network are aggregated to form the final output of the deep network. This is achieved by using a residual block architecture in a fine- grained manner, which is similar to the residual block that is used in the original deep learning architecture. The marginal aggregate posteriors of features in a neural network are obtained from a pre-specified prior distribution. The technique is applied to the gates of the gates, and the authors show that the technique can be applied to both image classification on CIFAR-10 and ImageNet datasets, and semantic segmentation on Cityscapes. The proposed method is shown to outperform the average computational cost of the original architecture as well as the architecture of a ResNet34 gated networks in terms of accuracy and top-1 accuracy on ImageNet. The complexity of the ResNet18 model is also shown to be much smaller than the original one.  The authors also show that their method can be extended to other architectures, and that the networks trained with the same number of features can be trained with different number of gates.  Finally, the authors provide a theoretical analysis of batch-shaping, which shows that the network is able to learn a good trade-off between the quality of the features and the number of parameters. "
21303,SP:f90e9f0eb53f92601bdfa3f7bf86f71d037aad30,"probabilistic importance inference approach USED-FOR pruning DNNs. approach COMPARE techniques. techniques COMPARE approach. lossless compression rates EVALUATE-FOR techniques. lossless compression rates EVALUATE-FOR approach. Method are Deep neural networks ( DNNs ), DNNs, DNN, and nonparemetric scoring test. OtherScientificTerm are energy and computational resources, and DNN ’s outputs. ","Deep neural networks (DNNs) have been shown to be efficient in terms of energy and computational resources. However, pruning DNNs can be expensive due to the large number of parameters required to prune a DNN. This paper proposes a probabilistic importance inference approach to pruning the parameters of DNN, which is based on a nonparemetric scoring test. The paper shows that the proposed approach outperforms existing techniques in lossless compression rates. The main contribution of the paper is that the DNN’s outputs can be pruned in a way that does not require any additional parameters.","Deep neural networks (DNNs) have been shown to be efficient in terms of energy and computational resources. However, pruning DNNs can be expensive due to the large number of parameters required to prune a DNN. This paper proposes a probabilistic importance inference approach to pruning the parameters of DNN, which is based on a nonparemetric scoring test. The paper shows that the proposed approach outperforms existing techniques in lossless compression rates. The main contribution of the paper is that the DNN’s outputs can be pruned in a way that does not require any additional parameters."
21312,SP:64cbbb6a2f6847ef71cd5a23ba3e4cc5c815a56e,"approaches USED-FOR hierarchical reinforcement learning. approaches USED-FOR sub - goal structure. method USED-FOR iteratively compressing action trajectories. iteratively compressing action trajectories USED-FOR nested behavioral hierarchies. method USED-FOR nested behavioral hierarchies. action primitives USED-FOR deeper hierarchies. approach USED-FOR learning. transfer USED-FOR approach. Generic is perspective. OtherScientificTerm are compact code of action trajectories, and non - trivial hierarchical structure. ","This paper presents a new perspective on hierarchical reinforcement learning. The authors show that existing approaches for hierarchical reinforcement learn can be suboptimal when sub-goal structure is not well defined. They propose a method for iteratively compressing action trajectories to learn nested behavioral hierarchies. The key idea is to use action primitives to encode deeper hierarchies, which are then used to learn a compact code of actions and sub-goals. They show that this approach can be used to speed up learning by learning a non-trivial hierarchical structure. They also show that the proposed approach is transfer-agnostic and can be applied to a variety of tasks.","This paper presents a new perspective on hierarchical reinforcement learning. The authors show that existing approaches for hierarchical reinforcement learn can be suboptimal when sub-goal structure is not well defined. They propose a method for iteratively compressing action trajectories to learn nested behavioral hierarchies. The key idea is to use action primitives to encode deeper hierarchies, which are then used to learn a compact code of actions and sub-goals. They show that this approach can be used to speed up learning by learning a non-trivial hierarchical structure. They also show that the proposed approach is transfer-agnostic and can be applied to a variety of tasks."
21321,SP:e1ccfb3a684aef8a0fb36194eb16af1667811e81,"generative models USED-FOR complex data. Autoencoders HYPONYM-OF generative models. images HYPONYM-OF complex data. variational autoencoder ( VAE ) HYPONYM-OF models. unimodal Gaussian decoders USED-FOR models. Hierarchical Bayes Autoencoder ( HBAE ) HYPONYM-OF probabilistic generative model. energybased model ( EBM ) USED-FOR multimodal decoder. multimodal decoder PART-OF HBAE. variational inference USED-FOR VAE. variational inference USED-FOR HBAE. conditional generator USED-FOR EBM distribution. adversarial approximation USED-FOR decoder. conditional generator USED-FOR stochastic reconstruction. code USED-FOR stochastic reconstruction. sampling steps PART-OF HBAE. HBAE USED-FOR sets. latent code USED-FOR HBAE. decoder USED-FOR realistic unconditional samples. single image and set cases EVALUATE-FOR decoder. model USED-FOR complex image sets. Set - HBAE USED-FOR complex image sets. Set - HBAE HYPONYM-OF model. OtherScientificTerm are semantic variations, and latent codes. Method is unimodal Gaussian distribution. ","Autoencoders are one of the most popular generative models for complex data (e.g. images). However, existing models such as the variational autoencoder (VAE) are unimodal Gaussian decoders. This paper proposes a new probabilistic generative model called the Hierarchical Bayes Autoencoder (HBAE). HBAE consists of a multimodal decoder that uses an energybased model (EBM) and a variational inference to learn a latent code. The decoder is trained using an adversarial approximation to the EBM distribution, and the conditional generator is used to learn the EMC of the EMI of the decoder, which is then used for stochastic reconstruction from the code.   The paper shows that the proposed model, called Set-HBAEs, is able to learn complex image sets with semantic variations, where the sets are generated from a set of latent codes. The authors also show that the model can learn sets with multiple sampling steps, and that a decoder can be trained to generate realistic unconditional samples in the single image and set cases. ","Autoencoders are one of the most popular generative models for complex data (e.g. images). However, existing models such as the variational autoencoder (VAE) are unimodal Gaussian decoders. This paper proposes a new probabilistic generative model called the Hierarchical Bayes Autoencoder (HBAE). HBAE consists of a multimodal decoder that uses an energybased model (EBM) and a variational inference to learn a latent code. The decoder is trained using an adversarial approximation to the EBM distribution, and the conditional generator is used to learn the EMC of the EMI of the decoder, which is then used for stochastic reconstruction from the code.   The paper shows that the proposed model, called Set-HBAEs, is able to learn complex image sets with semantic variations, where the sets are generated from a set of latent codes. The authors also show that the model can learn sets with multiple sampling steps, and that a decoder can be trained to generate realistic unconditional samples in the single image and set cases. "
21330,SP:1130a391afa30d1e0fddadedd2a3aaa70a4cb751,"deep off - policy TD algorithms CONJUNCTION feature normalization techniques. feature normalization techniques CONJUNCTION deep off - policy TD algorithms. normalization USED-FOR target networks. normalization USED-FOR optimization stability. mixture of onand off - policy transitions USED-FOR normalization. batch normalization USED-FOR It. DDPG CONJUNCTION TD3. TD3 CONJUNCTION DDPG. cross - normalization USED-FOR TD3. cross - normalization USED-FOR DDPG. MuJoCo benchmark tasks EVALUATE-FOR cross - normalization. Method are reinforcement learning ( RL ) algorithms, normalization techniques, and off - policy learning. ","This paper studies the problem of normalization in reinforcement learning (RL) algorithms. In particular, the authors investigate the relationship between deep off-policy TD algorithms and feature normalization techniques. They show that normalization for target networks can improve optimization stability when normalization is applied to a mixture of on-policy transitions. It can be seen as a variant of batch normalization. The authors also show that cross-normalization improves the performance of DDPG and TD3 on the MuJoCo benchmark tasks.    The main contribution of the paper is that the authors provide a theoretical analysis of the effect of normalisation techniques on the stability of off-propagation learning.","This paper studies the problem of normalization in reinforcement learning (RL) algorithms. In particular, the authors investigate the relationship between deep off-policy TD algorithms and feature normalization techniques. They show that normalization for target networks can improve optimization stability when normalization is applied to a mixture of on-policy transitions. It can be seen as a variant of batch normalization. The authors also show that cross-normalization improves the performance of DDPG and TD3 on the MuJoCo benchmark tasks.    The main contribution of the paper is that the authors provide a theoretical analysis of the effect of normalisation techniques on the stability of off-propagation learning."
21339,SP:f9cafaa5131176290fa069e6d24046c079cd9eea,"bias CONJUNCTION confounding effects. confounding effects CONJUNCTION bias. spurious associations of confounding variables HYPONYM-OF challenges. residualization CONJUNCTION stratification. stratification CONJUNCTION residualization. precomputed features USED-FOR confounding variables. techniques USED-FOR precomputed features. stratification HYPONYM-OF techniques. residualization HYPONYM-OF techniques. techniques USED-FOR statistical methods. techniques USED-FOR end - to - end deep learning methods. method USED-FOR discriminative features. adversarial training strategy USED-FOR discriminative features. adversarial training strategy USED-FOR method. synthetic data CONJUNCTION medical images. medical images CONJUNCTION synthetic data. method USED-FOR synthetic data. medical images EVALUATE-FOR method. Task are machine learning applications, and face recognition systems. Material is medical studies. Generic are datasets, and models. OtherScientificTerm are biases, confounder(s ), and bias or confounder variables. Method is adversarial loss function. ","This paper tackles the problem of bias and confounding effects in machine learning applications. The authors consider two challenges: (1) spurious associations of confounding variables, and (2) biases in medical studies. They propose two techniques to improve precomputed features for confounding variables: residualization and stratification. These techniques are commonly used in statistical methods, but not in end-to-end deep learning methods.   The authors propose a method to improve discriminative features by using an adversarial training strategy to train a discriminator on datasets where the confounder(s) are known. The method is tested on synthetic data and medical images, and is shown to improve the performance of face recognition systems. The adversarial loss function is used to train the discriminator, and it is shown that the method is more robust to bias or confounding variables than existing models. ","This paper tackles the problem of bias and confounding effects in machine learning applications. The authors consider two challenges: (1) spurious associations of confounding variables, and (2) biases in medical studies. They propose two techniques to improve precomputed features for confounding variables: residualization and stratification. These techniques are commonly used in statistical methods, but not in end-to-end deep learning methods.   The authors propose a method to improve discriminative features by using an adversarial training strategy to train a discriminator on datasets where the confounder(s) are known. The method is tested on synthetic data and medical images, and is shown to improve the performance of face recognition systems. The adversarial loss function is used to train the discriminator, and it is shown that the method is more robust to bias or confounding variables than existing models. "
21348,SP:783049ff463edd1283c058c6106a3e1f9a033df4,Transformer USED-FOR Character - level language modeling. limited resources USED-FOR character - level language models. computational resources USED-FOR Transformer - based models. lightweight model USED-FOR calculation paths. GroupTransformer HYPONYM-OF lightweight model. grouped embedding operators USED-FOR lightweight model. grouped embedding operators USED-FOR calculation paths. inter - group linear operators USED-FOR Group - Transformer. enwik8 CONJUNCTION text8. text8 CONJUNCTION enwik8. LSTM - based models COMPARE Transformer - based models. Transformer - based models COMPARE LSTM - based models. benchmark tasks EVALUATE-FOR GroupTransformer. text8 EVALUATE-FOR GroupTransformer. enwik8 EVALUATE-FOR GroupTransformer. enwik8 HYPONYM-OF benchmark tasks. text8 HYPONYM-OF benchmark tasks. OtherScientificTerm is limitation of recursive operation. Method is group strategy. Task is qualitative analysis. ,"Character-level language modeling with a Transformer is a challenging problem due to limited resources and the limitation of recursive operation. In this paper, the authors propose a lightweight model called GroupTransformer, which uses grouped embedding operators to reduce the computational resources of existing Transformer-based models. The Group-Transformer uses inter-group linear operators, and the authors show that the proposed lightweight model is able to efficiently compute the calculation paths for different groups of groups. The authors also provide a qualitative analysis of the proposed group strategy. Experiments on two benchmark tasks, enwik8 and text8, show that Grouptransformer outperforms existing LSTM-based and Transformer -based models in terms of performance. ","Character-level language modeling with a Transformer is a challenging problem due to limited resources and the limitation of recursive operation. In this paper, the authors propose a lightweight model called GroupTransformer, which uses grouped embedding operators to reduce the computational resources of existing Transformer-based models. The Group-Transformer uses inter-group linear operators, and the authors show that the proposed lightweight model is able to efficiently compute the calculation paths for different groups of groups. The authors also provide a qualitative analysis of the proposed group strategy. Experiments on two benchmark tasks, enwik8 and text8, show that Grouptransformer outperforms existing LSTM-based and Transformer -based models in terms of performance. "
21357,SP:946c26d371297c88d0ac246257104099b4585edc,hierarchical - latent - variable structures FEATURE-OF Probabilistic models. approach USED-FOR models. Variational Autoencoders USED-FOR approach. Variational Autoencoders USED-FOR models. inference and optimisation schemes USED-FOR approaches. non - likelihood - based framework USED-FOR generative models. bespoke models CONJUNCTION inference networks. inference networks CONJUNCTION bespoke models. approach USED-FOR models. deep - latent hierarchies USED-FOR models. Optimal Transport USED-FOR approach. Optimal Transport USED-FOR models. it COMPARE Wasserstein Autoencoder. Wasserstein Autoencoder COMPARE it. method USED-FOR generative model. deep - latent hierarchy USED-FOR generative model. Maximum Mean Discrepancy divergence FEATURE-OF Wasserstein Autoencoder. ," Probabilistic models with hierarchical-latent-variable structures can be seen as an extension of Variational Autoencoders. This paper proposes a new approach to train models with deep- latent hierarchies based on Optimal Transport. The authors show that this non-likelihood-based framework can be used to train generative models with bespoke models and inference networks. They also show that both inference and optimisation schemes can be applied to improve the performance of existing approaches. The method is shown to be able to learn a generative model with a deep-latency hierarchy, and it outperforms the Wasserstein Autoencoder with Maximum Mean Discrepancy divergence. "," Probabilistic models with hierarchical-latent-variable structures can be seen as an extension of Variational Autoencoders. This paper proposes a new approach to train models with deep- latent hierarchies based on Optimal Transport. The authors show that this non-likelihood-based framework can be used to train generative models with bespoke models and inference networks. They also show that both inference and optimisation schemes can be applied to improve the performance of existing approaches. The method is shown to be able to learn a generative model with a deep-latency hierarchy, and it outperforms the Wasserstein Autoencoder with Maximum Mean Discrepancy divergence. "
21366,SP:309b47441d227ffa33f96f9f16f2addc607e5bb0,"latent variable models CONJUNCTION adversarial training. adversarial training CONJUNCTION latent variable models. video - specific neural network architectures CONJUNCTION latent variable models. latent variable models CONJUNCTION video - specific neural network architectures. adversarial training CONJUNCTION methods. methods CONJUNCTION adversarial training. methods PART-OF video generation models. they USED-FOR continuations. realism FEATURE-OF continuations. benchmark datasets EVALUATE-FOR autoregressive video generation models. three - dimensional self - attention mechanism USED-FOR autoregressive video generation models. camera movement CONJUNCTION complex object interactions. complex object interactions CONJUNCTION camera movement. complex object interactions CONJUNCTION human movement. human movement CONJUNCTION complex object interactions. Kinetics HYPONYM-OF large scale action recognition dataset. Kinetics HYPONYM-OF phenomena. YouTube videos FEATURE-OF large scale action recognition dataset. Kinetics USED-FOR models. human movement HYPONYM-OF phenomena. camera movement HYPONYM-OF phenomena. complex object interactions HYPONYM-OF phenomena. Metric are statistical complexity, complexity, and fidelity. OtherScientificTerm are inherent stochasticity, and narrow domains. Task is generating natural video. Material is natural video. Generic is approaches. ","This paper studies the problem of generating natural video. The authors consider the statistical complexity of video-specific neural network architectures, latent variable models, adversarial training, and other methods in video generation models. They show that the inherent stochasticity of natural video is a limitation of existing approaches, and propose a three-dimensional self-attention mechanism to improve the performance of autoregressive video generation methods on standard benchmark datasets. They also show that they are able to generate continuations that achieve high realism and fidelity, and that they do so in narrow domains. They evaluate their models on Kinetics, a large scale action recognition dataset based on YouTube videos, which contains phenomena such as camera movement, complex object interactions, and human movement. ","This paper studies the problem of generating natural video. The authors consider the statistical complexity of video-specific neural network architectures, latent variable models, adversarial training, and other methods in video generation models. They show that the inherent stochasticity of natural video is a limitation of existing approaches, and propose a three-dimensional self-attention mechanism to improve the performance of autoregressive video generation methods on standard benchmark datasets. They also show that they are able to generate continuations that achieve high realism and fidelity, and that they do so in narrow domains. They evaluate their models on Kinetics, a large scale action recognition dataset based on YouTube videos, which contains phenomena such as camera movement, complex object interactions, and human movement. "
21375,SP:ad8fcdbc47a50dd2bf58aba2bc6cfe199e84dd4d,"International Classification of Diseases ( ICD ) HYPONYM-OF classification codes. noisy clinical document inputs CONJUNCTION long - tailed label distribution. long - tailed label distribution CONJUNCTION noisy clinical document inputs. Automatic ICD coding HYPONYM-OF multi - label text classification task. frequent and zeroshot codes USED-FOR fine - grained classification. long - tailed label distribution FEATURE-OF multi - label text classification task. noisy clinical document inputs FEATURE-OF multi - label text classification task. latent feature generation framework USED-FOR generalized zero - shot ICD coding. codes USED-FOR prediction. ICD code hierarchical structure CONJUNCTION cycle architecture. cycle architecture CONJUNCTION ICD code hierarchical structure. cycle architecture USED-FOR keywords. framework USED-FOR semantically meaningful features. semantically meaningful features USED-FOR zero - shot codes. cycle architecture USED-FOR framework. ICD code hierarchical structure USED-FOR framework. adversarial generative model USED-FOR generalized zero - shot learning. generalized zero - shot learning USED-FOR multi - label text classification. public MIMIC - III dataset EVALUATE-FOR methods. methods USED-FOR zero - shot codes. AUC score EVALUATE-FOR methods. F1 score EVALUATE-FOR methods. OtherScientificTerm are labeled data, and seen codes. Generic is approach. ","This paper proposes a framework for zero-shot learning of International Classification of Diseases (ICD) codes, which are the most commonly used classification codes in the medical community. Automatic ICD coding is a popular multi-label text classification task with noisy clinical document inputs and long-tailed label distribution. The authors propose a latent feature generation framework to learn generalized zero-Shot ICD codes from labeled data. They use frequent and zeroshot codes for fine-grained classification, and use frequent codes for classification with seen codes. They propose a framework that learns semantically meaningful features to generate zero-shots, and uses the ICD code hierarchical structure and a cycle architecture to generate keywords. The proposed approach is evaluated on the public MIMIC-III dataset, and shows that the proposed methods are able to learn zero- shot codes with high AUC score and low F1 score. They also propose an adversarial generative model to further improve the performance of the proposed framework in the context of multi-labels text classification.   ","This paper proposes a framework for zero-shot learning of International Classification of Diseases (ICD) codes, which are the most commonly used classification codes in the medical community. Automatic ICD coding is a popular multi-label text classification task with noisy clinical document inputs and long-tailed label distribution. The authors propose a latent feature generation framework to learn generalized zero-Shot ICD codes from labeled data. They use frequent and zeroshot codes for fine-grained classification, and use frequent codes for classification with seen codes. They propose a framework that learns semantically meaningful features to generate zero-shots, and uses the ICD code hierarchical structure and a cycle architecture to generate keywords. The proposed approach is evaluated on the public MIMIC-III dataset, and shows that the proposed methods are able to learn zero- shot codes with high AUC score and low F1 score. They also propose an adversarial generative model to further improve the performance of the proposed framework in the context of multi-labels text classification.   "
21384,SP:3ce82ae297e5759ab957babe9927062e7a71b0ba,self - supervised representation learning USED-FOR reinforcement learning ( RL ). self - supervised representation learning USED-FOR sample efficiency. sample efficiency EVALUATE-FOR reinforcement learning ( RL ). forward prediction objective USED-FOR embeddings of states and action sequences. embeddings USED-FOR policy learning. embeddings USED-FOR structure of the environment ’s dynamics. action embeddings USED-FOR model - free RL. sample efficiency EVALUATE-FOR model - free RL. low - dimensional states USED-FOR model - free RL. sample efficiency EVALUATE-FOR action embeddings. state and action embeddings USED-FOR learning of high - quality policies. goal - conditioned continuous control USED-FOR learning of high - quality policies. pixel observations USED-FOR learning of high - quality policies. ,"This paper proposes to use self-supervised representation learning to improve sample efficiency in reinforcement learning (RL) by using the forward prediction objective to learn embeddings of states and action sequences. The authors show that the embedding of the states and actions are useful for policy learning, and that the learned representations can capture the structure of the environment’s dynamics. They also show that learning action embedding improves sample efficiency of model-free RL in low-dimensional states. Finally, the authors demonstrate that learning of high-quality policies from pixel observations using state and action embeddens is possible using goal-conditioned continuous control. ","This paper proposes to use self-supervised representation learning to improve sample efficiency in reinforcement learning (RL) by using the forward prediction objective to learn embeddings of states and action sequences. The authors show that the embedding of the states and actions are useful for policy learning, and that the learned representations can capture the structure of the environment’s dynamics. They also show that learning action embedding improves sample efficiency of model-free RL in low-dimensional states. Finally, the authors demonstrate that learning of high-quality policies from pixel observations using state and action embeddens is possible using goal-conditioned continuous control. "
21393,SP:11ce1616e721340eea9e80dad7460c77355ac7d1,meta - learning USED-FOR tasks. hand - crafted structure design USED-FOR task - specific meta - learning methods. knowledge bases USED-FOR knowledge organization. structure knowledge USED-FOR meta - learner. framework USED-FOR task heterogeneity. model interpretability EVALUATE-FOR framework. meta - knowledge graph USED-FOR framework. meta - knowledge graph USED-FOR task heterogeneity. 2D toy regression CONJUNCTION few - shot image classification. few - shot image classification CONJUNCTION 2D toy regression. ARML COMPARE baselines. baselines COMPARE ARML. Generic is ones. Method is globally shared meta - learning methods. OtherScientificTerm is cross - task relations. ,"This paper proposes a meta-learning framework for tasks where the task-specific structure of the knowledge base is not shared across all tasks. The authors propose to use a hand-crafted structure design to learn a set of tasks that can be shared across tasks. They show that this framework is able to capture task heterogeneity and improve model interpretability. They also show that the knowledge organization in the knowledge bases can be learned through the meta-learner using the structure knowledge.    The authors show that ARML outperforms other baselines on tasks such as 2D toy regression and few-shot image classification. The paper also shows that the framework can be used to capture the task heterogeneity through the use of meta-knowledge graph.  The paper shows that there is no clear advantage of the proposed framework over existing ones. The main contribution of the paper is to propose a framework that can capture the cross-task relations between different tasks. This is an interesting direction to explore in the context of the literature. The idea is interesting and the paper has some interesting contributions. However, there is a lack of comparison with other globally shared meta - learning methods and there is not a lot of experimental evidence to show the effectiveness of the framework.","This paper proposes a meta-learning framework for tasks where the task-specific structure of the knowledge base is not shared across all tasks. The authors propose to use a hand-crafted structure design to learn a set of tasks that can be shared across tasks. They show that this framework is able to capture task heterogeneity and improve model interpretability. They also show that the knowledge organization in the knowledge bases can be learned through the meta-learner using the structure knowledge.    The authors show that ARML outperforms other baselines on tasks such as 2D toy regression and few-shot image classification. The paper also shows that the framework can be used to capture the task heterogeneity through the use of meta-knowledge graph.  The paper shows that there is no clear advantage of the proposed framework over existing ones. The main contribution of the paper is to propose a framework that can capture the cross-task relations between different tasks. This is an interesting direction to explore in the context of the literature. The idea is interesting and the paper has some interesting contributions. However, there is a lack of comparison with other globally shared meta - learning methods and there is not a lot of experimental evidence to show the effectiveness of the framework."
21402,SP:37c209cd1c628b5c2f2b282fbeaf4bbf437c7670,"model architecture CONJUNCTION fine - tuning. fine - tuning CONJUNCTION model architecture. attribute - specific data USED-FOR fine - tuning. Plug and Play Language Model ( PPLM ) USED-FOR controllable language generation. pretrained LM CONJUNCTION attribute classifiers. attribute classifiers CONJUNCTION pretrained LM. attribute classifiers USED-FOR text generation. pretrained LM PART-OF Plug and Play Language Model ( PPLM ). attribute models HYPONYM-OF classifiers. attribute models COMPARE LM. LM COMPARE attribute models. attribute alignment CONJUNCTION fluency. fluency CONJUNCTION attribute alignment. automated and human annotated evaluations EVALUATE-FOR attribute alignment. automated and human annotated evaluations EVALUATE-FOR fluency. differentiable attribute models USED-FOR text generation. Material are huge text corpora, and Model samples. Method are retraining, attribute model, and PPLMs. Task are Sampling, and generation. OtherScientificTerm are gradients, and hidden activations. ","This paper proposes a Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM and attribute classifiers (i.e., attribute models) for text generation and fine-tuning on attribute-specific data. The idea is to sample from a huge text corpora and then fine-tune the model architecture and fine -tuning based on the generated text. Sampling is done in an unsupervised way: no retraining is performed on the entire corpus, but only on a subset of it. Model samples are used to train an attribute model, and the attribute model is used to guide the generation. The authors show that the attribute models outperform the LM on both automated and human annotated evaluations of attribute alignment and fluency. They also show that differentiable attribute models can be used to improve the quality of text generation.    The authors also propose a new way of training PPLMs. The key idea is that the gradients of the attribute classifier are used as hidden activations, and that the output of the classifier is used as a proxy for the true input of the PPLM.","This paper proposes a Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM and attribute classifiers (i.e., attribute models) for text generation and fine-tuning on attribute-specific data. The idea is to sample from a huge text corpora and then fine-tune the model architecture and fine -tuning based on the generated text. Sampling is done in an unsupervised way: no retraining is performed on the entire corpus, but only on a subset of it. Model samples are used to train an attribute model, and the attribute model is used to guide the generation. The authors show that the attribute models outperform the LM on both automated and human annotated evaluations of attribute alignment and fluency. They also show that differentiable attribute models can be used to improve the quality of text generation.    The authors also propose a new way of training PPLMs. The key idea is that the gradients of the attribute classifier are used as hidden activations, and that the output of the classifier is used as a proxy for the true input of the PPLM."
21411,SP:12d0980bfea2de880905a0b87b40856969bb1c58,"deep neural networks USED-FOR machine learning tasks. unlabeled data USED-FOR learning robust representations. unsupervised and self - supervised learning approaches USED-FOR visual data. domain knowledge USED-FOR unsupervised and self - supervised learning approaches. gradient domain FEATURE-OF clean data. clean data USED-FOR noisy input data. denoising autoencoder USED-FOR data representations. visual benchmarks EVALUATE-FOR representations. approach USED-FOR representations. representations USED-FOR vision tasks. Material is supervised data. Method is unsupervised learning framework. Generic is agent. OtherScientificTerm are data structures, and single - scale corruption. ","This paper studies the problem of learning robust representations on unlabeled data from deep neural networks for machine learning tasks. The authors propose an unsupervised learning framework where the goal is to learn representations that are robust to noisy input data from clean data in the gradient domain. They show that both un supervised and self-supervised approaches to learning visual data with domain knowledge can benefit from domain knowledge. They propose to learn a denoising autoencoder to learn the data representations from the clean data, and then use the learned representations to train an agent that is robust to single-scale corruption. They evaluate their representations on several visual benchmarks and show that their approach is able to learn robust representations for vision tasks.","This paper studies the problem of learning robust representations on unlabeled data from deep neural networks for machine learning tasks. The authors propose an unsupervised learning framework where the goal is to learn representations that are robust to noisy input data from clean data in the gradient domain. They show that both un supervised and self-supervised approaches to learning visual data with domain knowledge can benefit from domain knowledge. They propose to learn a denoising autoencoder to learn the data representations from the clean data, and then use the learned representations to train an agent that is robust to single-scale corruption. They evaluate their representations on several visual benchmarks and show that their approach is able to learn robust representations for vision tasks."
21420,SP:12afc1b259e51a31cbeb72366d2b93fbee1aafaa,"Neural networks USED-FOR Natural Language Processing. under - sensitivity FEATURE-OF natural language inference. technique USED-FOR formal verification of this specification. technique USED-FOR models. interval bound propagation ( IBP ) approach USED-FOR technique. training methods USED-FOR under - sensitivity. SNLI and MNLI datasets EVALUATE-FOR IBP training. verified accuracy EVALUATE-FOR IBP training. Generic are they, specification, method, model, and metrics. Method are decomposable attention mechanism, and training. OtherScientificTerm is under - sensitivity problem. Material is SNLI test set. ","This paper studies the problem of under-sensitivity in neural networks for Natural Language Processing. The authors propose a new technique for formal verification of this specification, which they call the decomposable attention mechanism. The technique is based on the interval bound propagation (IBP) approach, which is a technique for training models that satisfy this specification.  The authors show empirically that IBP training on the SNLI and MNLI datasets improves the verified accuracy of the proposed method. They also show that the training methods that are designed to alleviate the under-sensitiveness of natural language inference also improve the training performance.  Finally, the authors show that their proposed method can be applied to any model that is trained on a large number of training examples.    The main contribution of this paper is that the authors have proposed a new metric to measure the under sensitivity problem, which can be used to evaluate the performance of a model on a set of different metrics. The paper also shows that the proposed metric is more sensitive to the number of examples in the training set than the total training set. ","This paper studies the problem of under-sensitivity in neural networks for Natural Language Processing. The authors propose a new technique for formal verification of this specification, which they call the decomposable attention mechanism. The technique is based on the interval bound propagation (IBP) approach, which is a technique for training models that satisfy this specification.  The authors show empirically that IBP training on the SNLI and MNLI datasets improves the verified accuracy of the proposed method. They also show that the training methods that are designed to alleviate the under-sensitiveness of natural language inference also improve the training performance.  Finally, the authors show that their proposed method can be applied to any model that is trained on a large number of training examples.    The main contribution of this paper is that the authors have proposed a new metric to measure the under sensitivity problem, which can be used to evaluate the performance of a model on a set of different metrics. The paper also shows that the proposed metric is more sensitive to the number of examples in the training set than the total training set. "
21429,SP:14257af9fe83522c6e5b5d6b0d68945b944e30fb,"replay memory USED-FOR network updates. soft divergence FEATURE-OF structure. data graph USED-FOR transitions. Q - values FEATURE-OF Markov Decision Process ( MDP ). favorable structure FEATURE-OF subgraph. transition PART-OF MDP. transition PART-OF continuous Q - learning problem. Q - value FEATURE-OF transition. Q - value USED-FOR continuous Q - learning problem. Q - value FEATURE-OF transition. lower bounds USED-FOR method. lower bounds USED-FOR TD learning. TD learning USED-FOR method. soft divergence FEATURE-OF method. sample efficiency EVALUATE-FOR method. replay memory capacity FEATURE-OF algorithm. OtherScientificTerm are state and action spaces, QGRAPH, hyperparameters, and QGRAPHs. ","This paper considers the problem of learning the Q-values of a Markov Decision Process (MDP) with state and action spaces. The authors propose QGRAPH, where transitions are represented as a data graph, and the transition in a continuous Q-learning problem is a Q-value of a transition in an MDP. The transition is defined as a subgraph that has a favorable structure (i.e., a soft divergence between the structure of the subgraph and that of the current state and the current action) and a replay memory for network updates.  The authors provide lower bounds on the soft divergence of their method, which is based on TD learning, and show that their method achieves a sample efficiency of $O(1/\sqrt{T})$ when the transition is a continuous MDP with favorable structure. They also provide a lower bound on the replay memory capacity of their algorithm, which shows that their algorithm achieves a trade-off between sample efficiency and replay memory.  Finally, the authors provide a theoretical analysis of the algorithm, showing that the algorithm converges to the optimal solution with high probability, and that the hyperparameters are well-behaved. The algorithm is shown to be computationally tractable, and can be applied to a wide range of Q-valued MDPs.","This paper considers the problem of learning the Q-values of a Markov Decision Process (MDP) with state and action spaces. The authors propose QGRAPH, where transitions are represented as a data graph, and the transition in a continuous Q-learning problem is a Q-value of a transition in an MDP. The transition is defined as a subgraph that has a favorable structure (i.e., a soft divergence between the structure of the subgraph and that of the current state and the current action) and a replay memory for network updates.  The authors provide lower bounds on the soft divergence of their method, which is based on TD learning, and show that their method achieves a sample efficiency of $O(1/\sqrt{T})$ when the transition is a continuous MDP with favorable structure. They also provide a lower bound on the replay memory capacity of their algorithm, which shows that their algorithm achieves a trade-off between sample efficiency and replay memory.  Finally, the authors provide a theoretical analysis of the algorithm, showing that the algorithm converges to the optimal solution with high probability, and that the hyperparameters are well-behaved. The algorithm is shown to be computationally tractable, and can be applied to a wide range of Q-valued MDPs."
21438,SP:c92c97e47d8b218dfd009bbf61f5b3547b395f91,approach USED-FOR problem. domain - invariant embeddings USED-FOR approach. domain - invariant embeddings USED-FOR problem. embedding complexity FEATURE-OF generalization. theoretical framework USED-FOR multilayer neural networks. strategy COMPARE layer - dependent complexity tradeoff. layer - dependent complexity tradeoff COMPARE strategy. Task is Unsupervised domain adaptation. Generic is complexity. ,This paper studies the problem of Unsupervised domain adaptation. The authors propose an approach to this problem based on domain-invariant embeddings. They show that the embedding complexity of the target domain is the limiting factor of the generalization performance. They also provide a theoretical framework for multilayer neural networks. The proposed strategy is shown to be more efficient than the layer-dependent complexity tradeoff.,This paper studies the problem of Unsupervised domain adaptation. The authors propose an approach to this problem based on domain-invariant embeddings. They show that the embedding complexity of the target domain is the limiting factor of the generalization performance. They also provide a theoretical framework for multilayer neural networks. The proposed strategy is shown to be more efficient than the layer-dependent complexity tradeoff.
21447,SP:f3f3c6fbae757836551b3f1ee54a7d1e040132b8,"framework USED-FOR algorithm - dependent generalization error bounds. PAC - Bayesian theory CONJUNCTION algorithmic stability. algorithmic stability CONJUNCTION PAC - Bayesian theory. PAC - Bayesian theory USED-FOR framework. mini - batch and acceleration CONJUNCTION Entropy - SGD. Entropy - SGD CONJUNCTION mini - batch and acceleration. momentum CONJUNCTION mini - batch and acceleration. mini - batch and acceleration CONJUNCTION momentum. Bayes - Stability method USED-FOR data - dependent generalization bounds. data - dependent generalization bounds USED-FOR stochastic gradient Langevin dynamics ( SGLD ). momentum HYPONYM-OF noisy gradient methods. Entropy - SGD HYPONYM-OF noisy gradient methods. mini - batch and acceleration HYPONYM-OF noisy gradient methods. data - dependent bounds USED-FOR randomly labelled data. randomly labelled data COMPARE normal data. normal data COMPARE randomly labelled data. bounded loss CONJUNCTION ` 2 regularization term. ` 2 regularization term CONJUNCTION bounded loss. bounded loss PART-OF total loss. ` 2 regularization term PART-OF total loss. Log - Sobolev inequality USED-FOR parameter distribution. generalization bounds USED-FOR continuous Langevin dynamic. Log - Sobolev inequality USED-FOR generalization bounds. Metric is Generalization error. OtherScientificTerm are out - of - sample error, tight generalization error bounds, generalization error bounds, and noise level. Task is statistical learning theory. Method is Bayes - Stability. Generic is bounds. ","This paper proposes a new framework for algorithm-dependent generalization error bounds based on PAC-Bayesian theory and algorithmic stability. Generalization error is defined as the difference between the out-of-sample error and the true error of the algorithm. The authors propose a Bayes-Stability method to derive data-dependent bounds for stochastic gradient Langevin dynamics (SGLD) for noisy gradient methods such as momentum, mini-batch and acceleration, and Entropy-SGD.    The authors show that under certain assumptions on the statistical learning theory, tight generalisation error bounds can be obtained for any continuous Langevin dynamic. They also show that the data-dependence of generalization bounds for randomly labelled data is tighter than for random labelled data with the same noise level.  The main contribution of the paper is that the authors derive generalization bound based on the Log-Sobolev inequality of the parameter distribution of the Langevin dynamical system, and show that for any total loss consisting of a bounded loss and a `2 regularization term, the bound is tight.  In addition, the authors provide a theoretical analysis of the generalization of the bounds. ","This paper proposes a new framework for algorithm-dependent generalization error bounds based on PAC-Bayesian theory and algorithmic stability. Generalization error is defined as the difference between the out-of-sample error and the true error of the algorithm. The authors propose a Bayes-Stability method to derive data-dependent bounds for stochastic gradient Langevin dynamics (SGLD) for noisy gradient methods such as momentum, mini-batch and acceleration, and Entropy-SGD.    The authors show that under certain assumptions on the statistical learning theory, tight generalisation error bounds can be obtained for any continuous Langevin dynamic. They also show that the data-dependence of generalization bounds for randomly labelled data is tighter than for random labelled data with the same noise level.  The main contribution of the paper is that the authors derive generalization bound based on the Log-Sobolev inequality of the parameter distribution of the Langevin dynamical system, and show that for any total loss consisting of a bounded loss and a `2 regularization term, the bound is tight.  In addition, the authors provide a theoretical analysis of the generalization of the bounds. "
21456,SP:a82fcd1d3196ddf078cfe8f4bc6f445d9d2bdc11,"spatial memory CONJUNCTION goal - directed spatial navigation. goal - directed spatial navigation CONJUNCTION spatial memory. hippocampus USED-FOR goal - directed spatial navigation. hippocampus USED-FOR spatial memory. hippocampal CA1 neurons USED-FOR continual learning. populationlevel activity FEATURE-OF hippocampal CA1 neurons. populationlevel activity USED-FOR continual learning. continual learning USED-FOR spatial navigation strategies. navigational strategies CONJUNCTION reward location. reward location CONJUNCTION navigational strategies. hippocampal neurons USED-FOR task variables. decisions CONJUNCTION navigational strategies. navigational strategies CONJUNCTION decisions. firing activity USED-FOR dPCA. decisions HYPONYM-OF task variables. reward location HYPONYM-OF task variables. navigational strategies HYPONYM-OF task variables. dPCA USED-FOR components. hippocampal features COMPARE reinforcement learning algorithms. reinforcement learning algorithms COMPARE hippocampal features. deep reinforcement learning model COMPARE animal learning. animal learning COMPARE deep reinforcement learning model. hippocampus USED-FOR reinforced spatial continual learning. biological and machine learning USED-FOR spatial continual learning. Task are continual learning of navigational strategies, and allocentric and egocentric spatial tasks. Method is Demixed Principal Component Analysis ( dPCA ). OtherScientificTerm is task switching. ","This paper studies the continual learning of navigational strategies in the hippocampus. The authors show that the hippocampus plays a key role in spatial memory and goal-directed spatial navigation, and that the populationlevel activity of hippocampal CA1 neurons in continual learning is correlated with the population level activity of the hippocampus for spatial navigation strategies. They propose a new method, called Demixed Principal Component Analysis (dPCA), to identify which components of the neurons are important for continual learning. They use dPCA to measure the firing activity of a set of task variables (decisions, navigation strategies, reward location, etc.). The authors demonstrate that the hippocampal neurons are highly correlated with these task variables. They also show that a deep reinforcement learning model trained on the hippocampus is more robust to task switching than other reinforcement learning algorithms.   The authors also demonstrate that in both allocentric and egocentric spatial tasks, the hippocampus can be used as a good resource for continual continual learning, and show that it can be trained in a similar way as animal learning. Finally, they show that using biological and machine learning, spatial continual learning can be learned using the hippocampus as a resource for reinforcement learning. ","This paper studies the continual learning of navigational strategies in the hippocampus. The authors show that the hippocampus plays a key role in spatial memory and goal-directed spatial navigation, and that the populationlevel activity of hippocampal CA1 neurons in continual learning is correlated with the population level activity of the hippocampus for spatial navigation strategies. They propose a new method, called Demixed Principal Component Analysis (dPCA), to identify which components of the neurons are important for continual learning. They use dPCA to measure the firing activity of a set of task variables (decisions, navigation strategies, reward location, etc.). The authors demonstrate that the hippocampal neurons are highly correlated with these task variables. They also show that a deep reinforcement learning model trained on the hippocampus is more robust to task switching than other reinforcement learning algorithms.   The authors also demonstrate that in both allocentric and egocentric spatial tasks, the hippocampus can be used as a good resource for continual continual learning, and show that it can be trained in a similar way as animal learning. Finally, they show that using biological and machine learning, spatial continual learning can be learned using the hippocampus as a resource for reinforcement learning. "
21465,SP:51acf1f8108683dce543a1fb4a61fbd593f9b4cc,"Monte Carlo Tree Search ( MCTS ) USED-FOR discrete environments. it USED-FOR continuous domains. Go HYPONYM-OF discrete environments. tree search based policy optimization method USED-FOR continuous environments. TPO HYPONYM-OF tree search based policy optimization method. hybrid approach USED-FOR policy optimization. hybrid approach USED-FOR TPO. continuous action space FEATURE-OF MCTS tree. off - policy MCTS trajectories USED-FOR policy gradient. pre - trained policy USED-FOR bootstrapping tree search. branching factor CONJUNCTION simulation count. simulation count CONJUNCTION branching factor. policy bootstrapping USED-FOR continuous MCTS. branching factor USED-FOR continuous MCTS. simulation count USED-FOR continuous MCTS. PPO USED-FOR baseline policy optimization algorithm. TPO USED-FOR policy. Humanoid HYPONYM-OF complex environments. Method are limiting tree search branching factor, and MCTS training. OtherScientificTerm are tree search branching factor, policy distribution, and loss function. Generic are approach, and baseline algorithm. Metric is MCTS branching factor. ","This paper proposes a new tree search based policy optimization method called TPO, which extends Monte Carlo Tree Search (MCTS) to discrete environments (e.g. Go) and extends it to continuous domains. The authors propose a hybrid approach to policy optimization based on TPO. The main idea is to use the off-policy MCTS trajectories to guide the policy gradient from a pre-trained policy to a bootstrapping tree search, where the limiting tree search branching factor is a function of the branching factor of the policy distribution and the number of times the policy has been visited by the current policy. This approach is shown to be effective in a variety of settings, and the authors show that TPO can be used to bootstrap a policy in a continuous action space. They also show that the policy bootstrapped in continuous MCTT can be trained using policy bootstraping, and that the optimal branching factor and the simulation count of the continuous policy are the limiting factors of the optimal policy.  The authors also provide a baseline algorithm based on PPO, and show that this baseline algorithm is robust to the limiting factor of branching factor. The paper also shows that in complex environments such as Humanoid, the policy trained with TPO is able to learn a policy that is more robust to limiting factors.    The main contribution of this paper is that the authors provide a theoretical analysis of the limiting branching factor, which shows that the limit of the limit can be controlled by the branching factors of a policy distribution, and also depends on the loss function. This analysis is also shown to hold for complex environments. ","This paper proposes a new tree search based policy optimization method called TPO, which extends Monte Carlo Tree Search (MCTS) to discrete environments (e.g. Go) and extends it to continuous domains. The authors propose a hybrid approach to policy optimization based on TPO. The main idea is to use the off-policy MCTS trajectories to guide the policy gradient from a pre-trained policy to a bootstrapping tree search, where the limiting tree search branching factor is a function of the branching factor of the policy distribution and the number of times the policy has been visited by the current policy. This approach is shown to be effective in a variety of settings, and the authors show that TPO can be used to bootstrap a policy in a continuous action space. They also show that the policy bootstrapped in continuous MCTT can be trained using policy bootstraping, and that the optimal branching factor and the simulation count of the continuous policy are the limiting factors of the optimal policy.  The authors also provide a baseline algorithm based on PPO, and show that this baseline algorithm is robust to the limiting factor of branching factor. The paper also shows that in complex environments such as Humanoid, the policy trained with TPO is able to learn a policy that is more robust to limiting factors.    The main contribution of this paper is that the authors provide a theoretical analysis of the limiting branching factor, which shows that the limit of the limit can be controlled by the branching factors of a policy distribution, and also depends on the loss function. This analysis is also shown to hold for complex environments. "
21474,SP:1ce3bc4d31712886f7dcada5b5ae67c3c376819a,accuracy EVALUATE-FOR network. sparse subnetworks PART-OF neural networks. supervision USED-FOR generating process. winning tickets COMPARE winning tickets. winning tickets COMPARE winning tickets. ImageNet dataset EVALUATE-FOR winning tickets. ImageNet classification task EVALUATE-FOR winning tickets. ImageNet classification task EVALUATE-FOR winning tickets. Material is lottery ticket hypothesis. Method is neural network optimization. OtherScientificTerm is initializations. ,"This paper investigates the lottery ticket hypothesis, which claims that sparse subnetworks in neural networks can lead to higher accuracy in training the network but lower accuracy in test time. The authors argue that this is due to neural network optimization, where the initializations of the network are sparse. They show that winning tickets on the ImageNet dataset outperform winning tickets without any supervision in the generating process. They also show that on ImageNet classification task, winning tickets outperform other winning tickets. ","This paper investigates the lottery ticket hypothesis, which claims that sparse subnetworks in neural networks can lead to higher accuracy in training the network but lower accuracy in test time. The authors argue that this is due to neural network optimization, where the initializations of the network are sparse. They show that winning tickets on the ImageNet dataset outperform winning tickets without any supervision in the generating process. They also show that on ImageNet classification task, winning tickets outperform other winning tickets. "
21483,SP:dbcebe5b73486885d9f4478b258047c02f8481a2,"excessive prediction undersensitivity HYPONYM-OF complementary problem. spurious surface patterns USED-FOR models. data augmentation CONJUNCTION adversarial training. adversarial training CONJUNCTION data augmentation. adversarial training USED-FOR defence strategies. data augmentation USED-FOR defence strategies. undersensitivity attacks FEATURE-OF model. held out evaluation data EVALUATE-FOR undersensitivity attacks. held out evaluation data EVALUATE-FOR model. they COMPARE model. model COMPARE they. train / evaluation distribution mismatch FEATURE-OF biased data setting. biased data setting EVALUATE-FOR model. predictive cues USED-FOR they. biased data setting EVALUATE-FOR adversarially robust models. F1 EVALUATE-FOR they. F1 EVALUATE-FOR model. Method are Neural reading comprehension models, noisy adversarial attack, and NewsQA models. OtherScientificTerm are adversarially selected input, semantically invariant text perturbations, and semantic variations of comprehension questions. Generic is attack. Material is adversarially generated questions. ","This paper studies the problem of ""excessive prediction undersensitivity"" in Neural reading comprehension models. This is a complementary problem to the complementary problem of excessive prediction under-prediction, i.e., overfitting to adversarially selected input. The authors propose a noisy adversarial attack on NewsQA models, which is motivated by the observation that these models are sensitive to spurious surface patterns in the training data. They show that this attack is semantically invariant to the semantic invariant text perturbations. They also show that under this attack, the semantic variations of comprehension questions are semantically similar to each other. They then propose two defence strategies based on data augmentation and adversarial training to mitigate this problem. Finally, they demonstrate that under undersensitivity attacks on the model on held out evaluation data, they achieve a F1 that is on par with the best model trained on the train/evaluation distribution mismatch in the biased data setting, and that they outperform a model trained with no predictive cues. ","This paper studies the problem of ""excessive prediction undersensitivity"" in Neural reading comprehension models. This is a complementary problem to the complementary problem of excessive prediction under-prediction, i.e., overfitting to adversarially selected input. The authors propose a noisy adversarial attack on NewsQA models, which is motivated by the observation that these models are sensitive to spurious surface patterns in the training data. They show that this attack is semantically invariant to the semantic invariant text perturbations. They also show that under this attack, the semantic variations of comprehension questions are semantically similar to each other. They then propose two defence strategies based on data augmentation and adversarial training to mitigate this problem. Finally, they demonstrate that under undersensitivity attacks on the model on held out evaluation data, they achieve a F1 that is on par with the best model trained on the train/evaluation distribution mismatch in the biased data setting, and that they outperform a model trained with no predictive cues. "
21492,SP:5da870060778de460c1abe91562d6f3e707efef4,"reinforcement learning ( RL ) agents USED-FOR real - world tasks. reinforcement learning ( RL ) agents USED-FOR safety. approaches USED-FOR problem. safety penalty PART-OF reward function. complex domains EVALUATE-FOR approaches. model - based approach USED-FOR safety. imaginative module HYPONYM-OF directed graph. it CONJUNCTION RL algorithm. RL algorithm CONJUNCTION it. gridworld environments CONJUNCTION self - driving car simulator. self - driving car simulator CONJUNCTION gridworld environments. approach COMPARE baseline. baseline COMPARE approach. self - driving car simulator EVALUATE-FOR approach. gridworld environments EVALUATE-FOR approach. self - driving car simulator EVALUATE-FOR proposal. gridworld environments EVALUATE-FOR proposal. OtherScientificTerm are bad incentives, unsafe scenarios, transition dynamics of the environment, baseline state, and discrete action space. Generic are they, graph, method, and task. ","This paper proposes a model-based approach to improve the safety of reinforcement learning (RL) agents for real-world tasks where there are bad incentives or unsafe scenarios. Previous approaches to this problem have been evaluated on complex domains, but they do not consider the transition dynamics of the environment. In this paper, the authors propose to add a safety penalty to the reward function. The authors propose a directed graph, called the ""imagination module"", which is an extension of existing approaches. The idea is that the safety penalty is a function of the transition from a baseline state to the current state in a discrete action space, and the authors show that this graph can be used as a way to model the transition between states. The proposed method is evaluated on a gridworld environments and a self-driving car simulator, where the proposed approach is shown to outperform a baseline. The paper also shows that it can be combined with any RL algorithm, and that the proposed method can be applied to any task. ","This paper proposes a model-based approach to improve the safety of reinforcement learning (RL) agents for real-world tasks where there are bad incentives or unsafe scenarios. Previous approaches to this problem have been evaluated on complex domains, but they do not consider the transition dynamics of the environment. In this paper, the authors propose to add a safety penalty to the reward function. The authors propose a directed graph, called the ""imagination module"", which is an extension of existing approaches. The idea is that the safety penalty is a function of the transition from a baseline state to the current state in a discrete action space, and the authors show that this graph can be used as a way to model the transition between states. The proposed method is evaluated on a gridworld environments and a self-driving car simulator, where the proposed approach is shown to outperform a baseline. The paper also shows that it can be combined with any RL algorithm, and that the proposed method can be applied to any task. "
21501,SP:c2796f28fb067138303df8d424d646f4ada31558,"numerical error FEATURE-OF finite differences. deep learning models USED-FOR physics - governing observations. unstructured grid USED-FOR physics - governing observations. neighboring information USED-FOR finite differences. physics equations USED-FOR finite differences. PA - DGN USED-FOR dynamical relations. synthetic data CONJUNCTION real - world climate observations. real - world climate observations CONJUNCTION synthetic data. PA - DGN USED-FOR approximation of directional derivatives. PA - DGN USED-FOR prediction of graph signals. approximation of directional derivatives CONJUNCTION prediction of graph signals. prediction of graph signals CONJUNCTION approximation of directional derivatives. real - world climate observations USED-FOR prediction of graph signals. synthetic data USED-FOR prediction of graph signals. weather stations USED-FOR real - world climate observations. Task is dynamics of physical systems. OtherScientificTerm are discretization error, spatial and temporal differences, and sequential observations. Material is sparse data. Generic is architecture. ","This paper studies the problem of learning dynamics of physical systems from sparse data. The authors propose to use deep learning models to learn physics-regulating observations on an unstructured grid, where the discretization error is proportional to the number of spatial and temporal differences between the sequential observations. They show that the finite differences are related to the numerical error of finite differences induced by neighboring information, and that the physics equations for these finite differences can be approximated using physics equations. They then propose PA-DGN to learn the dynamical relations between the spatial, temporal, and dynamical components of the system. PA-GND is applied to the approximation of directional derivatives, the prediction of graph signals from synthetic data and real-world climate observations from weather stations. The architecture is well-motivated and the experiments are well-designed.","This paper studies the problem of learning dynamics of physical systems from sparse data. The authors propose to use deep learning models to learn physics-regulating observations on an unstructured grid, where the discretization error is proportional to the number of spatial and temporal differences between the sequential observations. They show that the finite differences are related to the numerical error of finite differences induced by neighboring information, and that the physics equations for these finite differences can be approximated using physics equations. They then propose PA-DGN to learn the dynamical relations between the spatial, temporal, and dynamical components of the system. PA-GND is applied to the approximation of directional derivatives, the prediction of graph signals from synthetic data and real-world climate observations from weather stations. The architecture is well-motivated and the experiments are well-designed."
21510,SP:db8ed4f4fc3967f5dd4d208d5d029730eb99e840,"nonsmooth regularization CONJUNCTION constraints. constraints CONJUNCTION nonsmooth regularization. nonsmooth regularization FEATURE-OF structured neural networks ( NN ). constraints FEATURE-OF structured neural networks ( NN ). interval constraints HYPONYM-OF constraints. ` 1 - norm HYPONYM-OF nonsmooth regularization. constrained nonsmooth nonconvex optimization problem USED-FOR training. ProxSGD USED-FOR sparse or binary neural networks. regularization function CONJUNCTION constraint set. constraint set CONJUNCTION regularization function. regularization function USED-FOR ProxSGD. constraint set USED-FOR ProxSGD. OtherScientificTerm are learning rates, and stationary point. Method is ProxSGD algorithm. ","This paper studies the problem of training structured neural networks (NN) with nonsmooth regularization and other constraints (e.g., interval constraints). The authors propose a novel algorithm ProxSGD, which is based on the notion of `1-norm` (i.e., the ` 1-norm is the sum of all the regularizations of the weights of the NN). They show that the training is equivalent to solving a nonconvex optimization problem, where the learning rates are fixed and the constraints are non-uniform. They also provide a theoretical analysis that shows that the stationary point of the ProxSgd algorithm is a stationary point, which implies that the learning rate is non-unsmooth. Finally, they show that Prox SGD can be used to train sparse or binary neural networks with the same regularization function and constraint set. ","This paper studies the problem of training structured neural networks (NN) with nonsmooth regularization and other constraints (e.g., interval constraints). The authors propose a novel algorithm ProxSGD, which is based on the notion of `1-norm` (i.e., the ` 1-norm is the sum of all the regularizations of the weights of the NN). They show that the training is equivalent to solving a nonconvex optimization problem, where the learning rates are fixed and the constraints are non-uniform. They also provide a theoretical analysis that shows that the stationary point of the ProxSgd algorithm is a stationary point, which implies that the learning rate is non-unsmooth. Finally, they show that Prox SGD can be used to train sparse or binary neural networks with the same regularization function and constraint set. "
21519,SP:2ca1f4da9faee79768764cda5d09d949cc942acc,"encoder USED-FOR compression algorithms. approximate methods USED-FOR encoder. approximate methods USED-FOR algorithms. framework USED-FOR lossy image compression. non - deterministic compression codec USED-FOR framework. expected code length CONJUNCTION relative entropy. relative entropy CONJUNCTION expected code length. gradient - based optimizers USED-FOR end - to - end differentiable compression framework. it USED-FOR lossy image compression. rate - distortion curves COMPARE state - of - the - art. state - of - the - art COMPARE rate - distortion curves. low bitrates FEATURE-OF rate - distortion curves. it USED-FOR method. Kodak dataset EVALUATE-FOR rate - distortion curves. Probabilistic Ladder Networks ( PLNs ) USED-FOR it. low bitrates EVALUATE-FOR state - of - the - art. CLIC 2018 dataset EVALUATE-FOR Probabilistic Ladder Networks ( PLNs ). Material is image. OtherScientificTerm are discrete code, quantization step, continuous space, and encoding distribution. Method is decoder. Generic is process. ","This paper proposes a new framework for lossy image compression based on a non-deterministic compression codec. The idea is to encode an image into a discrete code and then use approximate methods to train an encoder to learn compression algorithms. The encoder and decoder are trained end-to-end in a continuous space, where the quantization step is in continuous space and the encoding distribution is in discrete space. The expected code length and relative entropy of the encoder are used to guide the algorithms, and the decoder is trained to be differentiable. The authors show that gradient-based optimizers can be used to learn an end- to-end differentiable compression framework. The paper shows that the rate-distortion curves on the Kodak dataset have better rate-drift curves with low bitrates compared to the state-of-the-art, and it can be applied to any method based on Probabilistic Ladder Networks (PLNs) on the CLIC 2018 dataset. ","This paper proposes a new framework for lossy image compression based on a non-deterministic compression codec. The idea is to encode an image into a discrete code and then use approximate methods to train an encoder to learn compression algorithms. The encoder and decoder are trained end-to-end in a continuous space, where the quantization step is in continuous space and the encoding distribution is in discrete space. The expected code length and relative entropy of the encoder are used to guide the algorithms, and the decoder is trained to be differentiable. The authors show that gradient-based optimizers can be used to learn an end- to-end differentiable compression framework. The paper shows that the rate-distortion curves on the Kodak dataset have better rate-drift curves with low bitrates compared to the state-of-the-art, and it can be applied to any method based on Probabilistic Ladder Networks (PLNs) on the CLIC 2018 dataset. "
21528,SP:788fd2b6956dd69bf7752d39ea21883947128c8a,"compressed JPG ( C - JPG ) image USED-FOR SR. SR HYPONYM-OF image processing operations. components CONJUNCTION cycle loss. cycle loss CONJUNCTION components. components PART-OF SR structure. cycle loss PART-OF SR structure. high - qualified SR images USED-FOR prevalent C - JPG images. hybrid loss function USED-FOR SR generation. cycle loss PART-OF SR solver. cycle loss USED-FOR hybrid loss function. SR solver USED-FOR hybrid loss function. approach COMPARE state - of - the - art methods. state - of - the - art methods COMPARE approach. Task are Super Resolution ( SR ), and SR issue. Method are SR models, C - JPG, functional sub - model, and SR approaches. OtherScientificTerm is storage space. Material is C - JPG images. ","This paper tackles the problem of Super Resolution (SR) in the context of image processing operations, i.e., SR, where the goal is to generate a compressed JPG (C-JPG) image from a given SR image. The authors consider the SR issue in the setting where SR models are trained on a large number of high-qualified SR images, but the storage space is limited. They propose a functional sub-model, called Super-Resolution (SR-S), to address this SR issue. The SR structure is composed of two components: the components of the functional submodel and the cycle loss. The functional submodular part of SR-S is used to generate high-quality SR images from the prevalent C-jPG images. The hybrid loss function for SR generation is also proposed, which combines the SR solver with a cycle loss to further improve the performance of SR models. Experiments show that the proposed approach outperforms state-of-the-art methods in terms of the quality of the generated C -JPG images, as well as the number of steps required to generate the final image.","This paper tackles the problem of Super Resolution (SR) in the context of image processing operations, i.e., SR, where the goal is to generate a compressed JPG (C-JPG) image from a given SR image. The authors consider the SR issue in the setting where SR models are trained on a large number of high-qualified SR images, but the storage space is limited. They propose a functional sub-model, called Super-Resolution (SR-S), to address this SR issue. The SR structure is composed of two components: the components of the functional submodel and the cycle loss. The functional submodular part of SR-S is used to generate high-quality SR images from the prevalent C-jPG images. The hybrid loss function for SR generation is also proposed, which combines the SR solver with a cycle loss to further improve the performance of SR models. Experiments show that the proposed approach outperforms state-of-the-art methods in terms of the quality of the generated C -JPG images, as well as the number of steps required to generate the final image."
21537,SP:18dd92f2f55020be4f5a089b3b251327e47886f4,"fully convolutional network architecture USED-FOR surface of pass probabilities. professional soccer matches USED-FOR single - location labels. single - location labels USED-FOR fully convolutional network architecture. single - location labels USED-FOR surface of pass probabilities. feature hierarchy USED-FOR network. low - level inputs USED-FOR network. approach USED-FOR weakly supervised learning. approach USED-FOR spatiotemporal decision - making analysis. spatiotemporal decision - making analysis HYPONYM-OF sports. network USED-FOR pass - selection likelihood. deep learning architecture USED-FOR sports analytics. OtherScientificTerm are sampling levels, coarse and fine detail, single pixel correspondence, and predicted probability map. ","This paper proposes a fully convolutional network architecture for learning the surface of pass probabilities from single-location labels from professional soccer matches. The network is trained with a feature hierarchy, where low-level inputs are used to guide the network through the training process and high-level ones are used for sampling levels. The proposed approach is applied to weakly supervised learning and is shown to be effective for spatiotemporal decision-making analysis in the context of sports, where the goal is to select the best pass-selection likelihood from a set of players in a game. The paper also shows that the proposed deep learning architecture can be applied to a variety of applications in sports analytics, including the application of the proposed approach to the problem of learning a spatio-temporal decision making analysis in a setting where coarse and fine detail are available. The authors also show that the single pixel correspondence between the predicted probability map and the true probability map can be learned in this setting.","This paper proposes a fully convolutional network architecture for learning the surface of pass probabilities from single-location labels from professional soccer matches. The network is trained with a feature hierarchy, where low-level inputs are used to guide the network through the training process and high-level ones are used for sampling levels. The proposed approach is applied to weakly supervised learning and is shown to be effective for spatiotemporal decision-making analysis in the context of sports, where the goal is to select the best pass-selection likelihood from a set of players in a game. The paper also shows that the proposed deep learning architecture can be applied to a variety of applications in sports analytics, including the application of the proposed approach to the problem of learning a spatio-temporal decision making analysis in a setting where coarse and fine detail are available. The authors also show that the single pixel correspondence between the predicted probability map and the true probability map can be learned in this setting."
21546,SP:1ae31baf383fc520687b255d9cac14c3b040e253,"side information USED-FOR inductive matrix completion model. user ’s age CONJUNCTION movie ’s genre. movie ’s genre CONJUNCTION user ’s age. movie ’s genre HYPONYM-OF content ( side information ). user ’s age HYPONYM-OF content ( side information ). IGMC USED-FOR graph neural network ( GNN ). It COMPARE transductive baselines. transductive baselines COMPARE It. model USED-FOR Douban movie ratings. MovieLens dataset USED-FOR model. Long - range dependencies USED-FOR modeling recommender systems. side information USED-FOR inductive matrix completion models. OtherScientificTerm are ( rating ) matrix, low - dimensional latent embeddings, embeddings, rating matrix, subgraphs, and local graph patterns. Method are matrix completion methods, and transductive methods. Task is matrix completion. Generic is it. ","This paper proposes an inductive matrix completion model based on side information, i.e., user’s age, movie age, and movie genre, which is a combination of content (side information, e.g. movie rating) and a (rating) matrix. The authors propose a graph neural network (GNN) based on IGMC, which learns a low-dimensional latent embeddings of the input and its subgraphs, and uses them to learn a (ratio) completion matrix for each subgraph. It outperforms other transductive baselines on the MovieLens dataset, and the authors also show that their model can be applied to Douban movie ratings. Long-range dependencies are important for modeling recommender systems, and side information can be a useful tool for inductive Matrix completion models. However, existing matrix completion methods rely on embedding the embedding of the subgraph to the rating matrix, which can be computationally expensive. This paper proposes to use a graph GNN based on the IGMC to embed the sub-graphs to the matrix completion matrix, and shows that it is computationally efficient. The paper also shows that the proposed inductive methods are more robust to changes in local graph patterns.   ","This paper proposes an inductive matrix completion model based on side information, i.e., user’s age, movie age, and movie genre, which is a combination of content (side information, e.g. movie rating) and a (rating) matrix. The authors propose a graph neural network (GNN) based on IGMC, which learns a low-dimensional latent embeddings of the input and its subgraphs, and uses them to learn a (ratio) completion matrix for each subgraph. It outperforms other transductive baselines on the MovieLens dataset, and the authors also show that their model can be applied to Douban movie ratings. Long-range dependencies are important for modeling recommender systems, and side information can be a useful tool for inductive Matrix completion models. However, existing matrix completion methods rely on embedding the embedding of the subgraph to the rating matrix, which can be computationally expensive. This paper proposes to use a graph GNN based on the IGMC to embed the sub-graphs to the matrix completion matrix, and shows that it is computationally efficient. The paper also shows that the proposed inductive methods are more robust to changes in local graph patterns.   "
21555,SP:c5cb1b50e17a69e88d5ae28848e265215162da1e,smooth objective function USED-FOR unconstrained minimization. heavy ball momentum USED-FOR stochastic zeroth - order method. learning to continuous control tasks EVALUATE-FOR method. STP COMPARE policy gradient methods. policy gradient methods COMPARE STP. STP COMPARE derivative - free optimization algorithms. derivative - free optimization algorithms COMPARE STP. derivative - free optimization algorithms CONJUNCTION policy gradient methods. policy gradient methods CONJUNCTION derivative - free optimization algorithms. SMTP COMPARE STP. STP COMPARE SMTP. SMTP COMPARE methods. methods COMPARE SMTP. STP CONJUNCTION methods. methods CONJUNCTION STP. importance sampling USED-FOR SMTP. OtherScientificTerm is function evaluations. Metric is complexity. Method is SMTP_IS. ,"This paper proposes a stochastic zeroth-order method for unconstrained minimization of a smooth objective function. The authors propose to use heavy ball momentum, which is an extension of the popular Stochastic Zeroth Order Optimization (STO) algorithm. The proposed method is evaluated on learning to continuous control tasks, and compared to STP, derivative-free optimization algorithms, and policy gradient methods. SMTP is shown to be more efficient than STP and other methods, and the authors also show that SMTP with importance sampling is more robust to function evaluations. The complexity of SMTP_IS is also improved. ","This paper proposes a stochastic zeroth-order method for unconstrained minimization of a smooth objective function. The authors propose to use heavy ball momentum, which is an extension of the popular Stochastic Zeroth Order Optimization (STO) algorithm. The proposed method is evaluated on learning to continuous control tasks, and compared to STP, derivative-free optimization algorithms, and policy gradient methods. SMTP is shown to be more efficient than STP and other methods, and the authors also show that SMTP with importance sampling is more robust to function evaluations. The complexity of SMTP_IS is also improved. "
21564,SP:a216cfc29937eb398ea98cb1aea3481c9aed8240,"environmental stochasticity CONJUNCTION uncertainties. uncertainties CONJUNCTION environmental stochasticity. deep learning architecture USED-FOR multiagent coordination. multiagent coordination mechanisms USED-FOR deep learning architecture. Action Semantics Network ( ASN ) HYPONYM-OF network architecture. action semantics USED-FOR neural networks. neural networks USED-FOR ASN. action semantics USED-FOR ASN. ASN CONJUNCTION deep reinforcement learning ( DRL ) algorithms. deep reinforcement learning ( DRL ) algorithms CONJUNCTION ASN. StarCraft II micromanagement CONJUNCTION Neural MMO. Neural MMO CONJUNCTION StarCraft II micromanagement. DRL approaches COMPARE network architectures. network architectures COMPARE DRL approaches. Neural MMO EVALUATE-FOR ASN. StarCraft II micromanagement EVALUATE-FOR ASN. ASN COMPARE DRL approaches. DRL approaches COMPARE ASN. ASN COMPARE network architectures. network architectures COMPARE ASN. Task are multiagent systems ( MASs ), and system evolution. Material is MASs. OtherScientificTerm is co - learning agents. ","This paper proposes a new deep learning architecture for multiagent coordination mechanisms based on the Action Semantics Network (ASN), which is a network architecture inspired by the action semantics of neural networks. In multiagent systems (MASs) there is a strong correlation between environmental stochasticity and uncertainties, and the authors show that the ASN is able to capture the dynamics of system evolution in MASs. The authors also show that ASN can be combined with existing deep reinforcement learning (DRL) algorithms to improve the performance of co-learning agents. The ASN has been tested on StarCraft II micromanagement and Neural MMO, and is shown to outperform existing DRL approaches and existing network architectures.","This paper proposes a new deep learning architecture for multiagent coordination mechanisms based on the Action Semantics Network (ASN), which is a network architecture inspired by the action semantics of neural networks. In multiagent systems (MASs) there is a strong correlation between environmental stochasticity and uncertainties, and the authors show that the ASN is able to capture the dynamics of system evolution in MASs. The authors also show that ASN can be combined with existing deep reinforcement learning (DRL) algorithms to improve the performance of co-learning agents. The ASN has been tested on StarCraft II micromanagement and Neural MMO, and is shown to outperform existing DRL approaches and existing network architectures."
21573,SP:efaf3a440dc17e05177832083ffbc23760ed7c97,Value - based methods USED-FOR planning and deep reinforcement learning ( RL ). Q function HYPONYM-OF state - action value function. system dynamics USED-FOR global structures of the Q function. lowrank structure USED-FOR big data matrices. low - rank Q functions USED-FOR control and deep RL tasks. Matrix Estimation ( ME ) techniques USED-FOR framework. low - rank structure FEATURE-OF Q functions. low - rank structure USED-FOR framework. planning procedure USED-FOR classical control. scheme USED-FOR value - based RL techniques. scheme USED-FOR “ low - rank ” tasks. control tasks CONJUNCTION Atari games. Atari games CONJUNCTION control tasks. control tasks EVALUATE-FOR approach. Atari games EVALUATE-FOR approach. Task is planning and deep RL. Generic is structures. ,"Value-based methods for planning and deep reinforcement learning (RL) have been a popular topic of interest in recent years. This paper studies the problem of learning the global structures of the Q function, i.e., the state-action value function, which is a Q function that depends on the system dynamics. The authors propose a framework based on Matrix Estimation (ME) techniques to learn low-rank Q functions for both control and deep RL tasks. They show that the lowrank structure of big data matrices can be used to model the global structure of Q function. They also propose a scheme to learn a planning procedure for classical control that can be applied to “low-rank” tasks. The proposed approach is evaluated on a variety of control tasks and Atari games. ","Value-based methods for planning and deep reinforcement learning (RL) have been a popular topic of interest in recent years. This paper studies the problem of learning the global structures of the Q function, i.e., the state-action value function, which is a Q function that depends on the system dynamics. The authors propose a framework based on Matrix Estimation (ME) techniques to learn low-rank Q functions for both control and deep RL tasks. They show that the lowrank structure of big data matrices can be used to model the global structure of Q function. They also propose a scheme to learn a planning procedure for classical control that can be applied to “low-rank” tasks. The proposed approach is evaluated on a variety of control tasks and Atari games. "
21582,SP:430336893b247b7bd45687d78b0d0511a7369e87,"batch reinforcement learning USED-FOR sample - efficient learning. batch reinforcement learning USED-FOR Deep Reinforcement Learning ( DRL ). off - policy DRL algorithms USED-FOR batch DRL setting. action space FEATURE-OF maximizing Q functions. state - action pairs USED-FOR policy network. it USED-FOR policy network. state - action pairs USED-FOR it. imitation learning USED-FOR it. imitation learning USED-FOR policy network. Mujoco benchmark EVALUATE-FOR BAIL. Generic is algorithm. Method are Best - Action Imitation Learning ( BAIL ), and offpolicy DRL algorithms. OtherScientificTerm is Q functions. ","This paper proposes a new algorithm called Best-Action Imitation Learning (BAIL) for batch reinforcement learning for sample-efficient learning in Deep Reinforcement Learning (DRL). The algorithm is a generalization of off-policy DRL algorithms to the batch DRL setting, where the goal is to maximize the maximizing Q functions in the action space. The paper shows that it can be used to train a policy network using only state-action pairs, and it can also be used with imitation learning to learn a new policy network. Experiments on the Mujoco benchmark show that BAIL achieves state-of-the-art performance in terms of performance on the Q functions and on the number of samples. ","This paper proposes a new algorithm called Best-Action Imitation Learning (BAIL) for batch reinforcement learning for sample-efficient learning in Deep Reinforcement Learning (DRL). The algorithm is a generalization of off-policy DRL algorithms to the batch DRL setting, where the goal is to maximize the maximizing Q functions in the action space. The paper shows that it can be used to train a policy network using only state-action pairs, and it can also be used with imitation learning to learn a new policy network. Experiments on the Mujoco benchmark show that BAIL achieves state-of-the-art performance in terms of performance on the Q functions and on the number of samples. "
21591,SP:94078964876667e8a5d9ae7728d779d5b91a576e,"feature representations CONJUNCTION classifiers. classifiers CONJUNCTION feature representations. classifiers PART-OF deep extreme multi - label learning. feature representations PART-OF deep extreme multi - label learning. deep extreme classifiers USED-FOR short text documents. word embeddings USED-FOR DeepXML. negative sub - sampling techniques USED-FOR negative training data. accuracy EVALUATE-FOR DeepXML. residual connection USED-FOR them. Slice algorithm USED-FOR DeepXML architecture. Slice algorithm USED-FOR pretrained embeddings. pretrained embeddings USED-FOR DeepXML architecture. it COMPARE XML - CNN. XML - CNN COMPARE it. XML - CNN CONJUNCTION AttentionXML. AttentionXML CONJUNCTION XML - CNN. it COMPARE AttentionXML. AttentionXML COMPARE it. DeepXML COMPARE leading techniques. leading techniques COMPARE DeepXML. leading techniques USED-FOR search engine queries. search engine queries CONJUNCTION advertiser bid phrases. advertiser bid phrases CONJUNCTION search engine queries. DeepXML USED-FOR search engine queries. Method are DeepXML algorithm, and classifier. Generic is architecture. ","This paper proposes a new deep extreme multi-label learning algorithm, called DeepXML, for short text documents. Deep extreme classifiers are used to learn feature representations and classifiers for short texts. The authors propose to use word embeddings from the Slice algorithm to improve the performance of the Deep XML algorithm. Specifically, the authors use negative sub-sampling techniques to sample negative training data, and then use them to train a residual connection between the word embedding and the classifier.    The authors show that the proposed architecture is able to achieve state-of-the-art accuracy in terms of accuracy on search engine queries and advertiser bid phrases, and that it outperforms XML-CNN and AttentionXML. They also show that DeepXml architecture can be trained with pretrained embeddens from the previous work using the same amount of training data as well as with the same number of classes as the original training data. ","This paper proposes a new deep extreme multi-label learning algorithm, called DeepXML, for short text documents. Deep extreme classifiers are used to learn feature representations and classifiers for short texts. The authors propose to use word embeddings from the Slice algorithm to improve the performance of the Deep XML algorithm. Specifically, the authors use negative sub-sampling techniques to sample negative training data, and then use them to train a residual connection between the word embedding and the classifier.    The authors show that the proposed architecture is able to achieve state-of-the-art accuracy in terms of accuracy on search engine queries and advertiser bid phrases, and that it outperforms XML-CNN and AttentionXML. They also show that DeepXml architecture can be trained with pretrained embeddens from the previous work using the same amount of training data as well as with the same number of classes as the original training data. "
21600,SP:b1b1252d82fa1bea18309e0b0b894e0f28f48bc9,"binary vector representations ( hash codes ) USED-FOR Hashing - based collaborative filtering. Hamming distance USED-FOR recommendations. Hamming distance USED-FOR hashing - based collaborative filtering. user hash code USED-FOR mask. Boolean AND operation USED-FOR user hash code. Boolean AND operation USED-FOR mask. approach COMPARE baselines. baselines COMPARE approach. NDCG EVALUATE-FOR approach. runtime overhead EVALUATE-FOR Hamming distance. self - masking COMPARE Hamming distance. Hamming distance COMPARE self - masking. OtherScientificTerm are hash codes, and binary user - level importance weighting. Task is distance computation. Generic is it. ","This paper proposes a new hashing-based collaborative filtering based on binary vector representations (hash codes). The authors propose to use Hamming distance for recommendations based on a user hash code as a mask, and use a Boolean AND operation to compute the mask for each user. The proposed approach is evaluated on the NDCG, and compared with several baselines. The authors show that self-masking is more efficient than Hamming distances, and that Hamming is more computationally efficient than the binary user-level importance weighting. The paper also shows that the distance computation is computationally tractable. ","This paper proposes a new hashing-based collaborative filtering based on binary vector representations (hash codes). The authors propose to use Hamming distance for recommendations based on a user hash code as a mask, and use a Boolean AND operation to compute the mask for each user. The proposed approach is evaluated on the NDCG, and compared with several baselines. The authors show that self-masking is more efficient than Hamming distances, and that Hamming is more computationally efficient than the binary user-level importance weighting. The paper also shows that the distance computation is computationally tractable. "
21609,SP:80898d0f2b2c8dc3388fa9164e529eae36aa1b21,"Generative adversarial networks ( GANs ) USED-FOR images. mode collapse FEATURE-OF GAN ’s learned distribution. evaluation metrics EVALUATE-FOR image synthesis. low - level perceptual quality EVALUATE-FOR evaluation metrics. statistical tools USED-FOR mode collapse. mode collapse FEATURE-OF GANs. mode collapse FEATURE-OF GANs. toolset USED-FOR GANs. toolset USED-FOR mode collapse. OtherScientificTerm are GAN learned distribution, and model parameters. ","Generative adversarial networks (GANs) have been shown to generate images with low-level perceptual quality, but this paper investigates the phenomenon of mode collapse in the GAN’s learned distribution. The authors propose a new set of evaluation metrics for image synthesis that can be used to evaluate the mode collapse of a GAN learned distribution, and show that the evaluation metrics are sensitive to the low level perceptual quality of the generated images. They also propose a set of statistical tools to measure mode collapse for GANs using a new toolset. The paper also shows that mode collapse can be caused by the model parameters.","Generative adversarial networks (GANs) have been shown to generate images with low-level perceptual quality, but this paper investigates the phenomenon of mode collapse in the GAN’s learned distribution. The authors propose a new set of evaluation metrics for image synthesis that can be used to evaluate the mode collapse of a GAN learned distribution, and show that the evaluation metrics are sensitive to the low level perceptual quality of the generated images. They also propose a set of statistical tools to measure mode collapse for GANs using a new toolset. The paper also shows that mode collapse can be caused by the model parameters."
21618,SP:e5b5dda2f024cfda10526e744aa035e0165af58a,"over - parametrized neural networks CONJUNCTION linearized models. linearized models CONJUNCTION over - parametrized neural networks. Neural Tangent Kernels ( NTKs ) USED-FOR linearized models. neural networks COMPARE linearized models. linearized models COMPARE neural networks. Taylor expansion FEATURE-OF network. optimization landscape FEATURE-OF randomized two - layer networks. escaping - saddle algorithms USED-FOR optimization landscape. mild distributional assumptions FEATURE-OF dimension factor. it USED-FOR networks. higher - order terms PART-OF Taylor series. networks CONJUNCTION higher - order terms. higher - order terms CONJUNCTION networks. it USED-FOR randomization technique. Method are NTK theory, Taylor expansion of the network, quadratic models, and randomized networks. Generic are theory, and them. OtherScientificTerm are NTK regime, NTK, and sample complexity bounds. Material is quadratic case. ","This paper studies over-parametrized neural networks and linearized models with Neural Tangent Kernels (NTKs) under the NTK theory. The authors show that neural networks with NTKs are more robust to the Taylor expansion of the network compared to neural networks without neural Tangent Kernel (NKT). The theory shows that under mild distributional assumptions, the optimization landscape of randomized two-layer networks with escaping-saddle algorithms converges to the optimal solution of a Taylor expansion on the network.    The authors also show that under a similar setting, under the quadratic case, the optimal solutions of NTK regime can be found in the limit of a certain dimension factor, which they call the dimension factor.  The main contribution of this paper is that it provides a theoretical analysis of the sample complexity bounds for these networks and shows that it can be obtained for networks with higher-order terms in the Taylor series, and it also provides a randomization technique that can be used to improve the sample efficiency of the randomization.  In particular, it shows that the sample-efficiency bounds are tight for quadratically models, and that it is tight for the case where the dimension is larger than the dimension of the neural network. It also shows that this is also the case for randomized networks. ","This paper studies over-parametrized neural networks and linearized models with Neural Tangent Kernels (NTKs) under the NTK theory. The authors show that neural networks with NTKs are more robust to the Taylor expansion of the network compared to neural networks without neural Tangent Kernel (NKT). The theory shows that under mild distributional assumptions, the optimization landscape of randomized two-layer networks with escaping-saddle algorithms converges to the optimal solution of a Taylor expansion on the network.    The authors also show that under a similar setting, under the quadratic case, the optimal solutions of NTK regime can be found in the limit of a certain dimension factor, which they call the dimension factor.  The main contribution of this paper is that it provides a theoretical analysis of the sample complexity bounds for these networks and shows that it can be obtained for networks with higher-order terms in the Taylor series, and it also provides a randomization technique that can be used to improve the sample efficiency of the randomization.  In particular, it shows that the sample-efficiency bounds are tight for quadratically models, and that it is tight for the case where the dimension is larger than the dimension of the neural network. It also shows that this is also the case for randomized networks. "
21627,SP:cef7ea513eb3e42be4edf40e4ee1701a969bcbea,"graph data USED-FOR downstream tasks. Graph Neural Networks ( GNNs ) USED-FOR graph data. graph filter design PART-OF GNN models. filter USED-FOR graph data. graph properties USED-FOR graph filter. filters USED-FOR graph. assessment tool EVALUATE-FOR graph convolutional filters. graph convolutional filters USED-FOR graph. node classification EVALUATE-FOR graph convolutional filters. graphs USED-FOR graph convolutional filters. model USED-FOR data - specific filters. Adaptive Filter Graph Neural Network ( AFGNN ) HYPONYM-OF model. AFGNN USED-FOR graph. base filters PART-OF AFGNN. graph filter assessment USED-FOR AFGNN. synthetic and real - world benchmark datasets EVALUATE-FOR model. model USED-FOR filter. Method are optimal filter, and Graph Filter Discriminant Score ( GFD ). Task is semi - supervised node classification task. OtherScientificTerm is loss term. ","Graph Neural Networks (GNNs) are widely used for graph data for a variety of downstream tasks. However, the graph filter design in many GNN models is not well understood. This paper proposes a new assessment tool for evaluating the performance of graph convolutional filters on graphs. The paper proposes an adaptive filter design for graph filters that can be applied to any filter for any graph data. The authors propose a new model called Adaptive Filter Graph Neural Network (AFGNN), which is a model that adaptively selects data-specific filters for each filter for a given filter for the purpose of semi-supervised node classification task. The proposed AFGNN is based on the Graph Filter Discriminant Score (GFD) which measures the difference between the optimal filter and the one that is optimal for the given filter. The graph filter is constructed by considering the graph properties of the input graph and the number of filters applied to the graph, and the loss term is the sum of the difference of the average of the optimal and optimal filters across all the filters applied. The model is tested on both synthetic and real-world benchmark datasets and the authors show that AFGnn is able to learn a filter for each graph based on graph filter assessment, and that the model can learn the filter that is best suited for a particular filter for that filter. In addition, the authors also show that the base filters of AFGNet can be adapted to different filters for different graphs.  ","Graph Neural Networks (GNNs) are widely used for graph data for a variety of downstream tasks. However, the graph filter design in many GNN models is not well understood. This paper proposes a new assessment tool for evaluating the performance of graph convolutional filters on graphs. The paper proposes an adaptive filter design for graph filters that can be applied to any filter for any graph data. The authors propose a new model called Adaptive Filter Graph Neural Network (AFGNN), which is a model that adaptively selects data-specific filters for each filter for a given filter for the purpose of semi-supervised node classification task. The proposed AFGNN is based on the Graph Filter Discriminant Score (GFD) which measures the difference between the optimal filter and the one that is optimal for the given filter. The graph filter is constructed by considering the graph properties of the input graph and the number of filters applied to the graph, and the loss term is the sum of the difference of the average of the optimal and optimal filters across all the filters applied. The model is tested on both synthetic and real-world benchmark datasets and the authors show that AFGnn is able to learn a filter for each graph based on graph filter assessment, and that the model can learn the filter that is best suited for a particular filter for that filter. In addition, the authors also show that the base filters of AFGNet can be adapted to different filters for different graphs.  "
21636,SP:3c5ec9dbcf914c8901e4e35f3c2a7df4707422ab,"i.i.d. test set EVALUATE-FOR Overparameterized neural networks. Distributionally robust optimization ( DRO ) USED-FOR models. group DRO USED-FOR overparameterized neural networks. average training loss FEATURE-OF model. vanishing worst - case training loss FEATURE-OF model. natural language inference task CONJUNCTION image tasks. image tasks CONJUNCTION natural language inference task. early stopping HYPONYM-OF regularization. regularization USED-FOR group DRO models. regularization USED-FOR worst - group generalization. it USED-FOR average generalization. overparameterized regime FEATURE-OF worst - group generalization. stochastic optimization algorithm USED-FOR group DRO models. convergence guarantees FEATURE-OF stochastic optimization algorithm. OtherScientificTerm is spurious correlations. Metric are worst - case training loss, worst - group accuracies, and average accuracies. ","This paper studies overparameterized neural networks on the i.i.d. test set. The authors consider Distributionally robust optimization (DRO) for models trained with group DRO. They show that the average training loss of a model with vanishing worst-case training loss can be approximated by a group of samples with spurious correlations. They also show that worst-group generalization can be achieved with regularization (e.g. early stopping). They also provide convergence guarantees for a stochastic optimization algorithm that optimizes the worst-groups of a set of samples, and show that it can be used to regularize the average generalization in the over parameterized regime. Finally, they show that this regularization, called ""group DRO models with early stopping"", can be applied to improve the performance of the worst group in a natural language inference task and several image tasks. ","This paper studies overparameterized neural networks on the i.i.d. test set. The authors consider Distributionally robust optimization (DRO) for models trained with group DRO. They show that the average training loss of a model with vanishing worst-case training loss can be approximated by a group of samples with spurious correlations. They also show that worst-group generalization can be achieved with regularization (e.g. early stopping). They also provide convergence guarantees for a stochastic optimization algorithm that optimizes the worst-groups of a set of samples, and show that it can be used to regularize the average generalization in the over parameterized regime. Finally, they show that this regularization, called ""group DRO models with early stopping"", can be applied to improve the performance of the worst group in a natural language inference task and several image tasks. "
21645,SP:eb1ee2e0f7d8466a04b58508ecb3da7b667eecdf,"local explanation methods USED-FOR decision of black - box classifiers. ad hoc constraints FEATURE-OF classification loss. ad hoc constraints USED-FOR relevance scores. neural network USED-FOR distribution of relevance scores. it CONJUNCTION neural network. neural network CONJUNCTION it. it USED-FOR distribution of relevance scores. classification loss USED-FOR predictor. strategy USED-FOR discriminative scores. features USED-FOR discriminative scores. faithfulness CONJUNCTION explainability. explainability CONJUNCTION faithfulness. method COMPARE others. others COMPARE method. faithfulness EVALUATE-FOR others. explainability EVALUATE-FOR others. faithfulness EVALUATE-FOR method. explainability EVALUATE-FOR method. Generic is methods. Method are mask predictor, and distribution controllers. OtherScientificTerm is hyperparameters. ","This paper studies the problem of local explanation methods for the decision of black-box classifiers. In particular, the authors propose two methods: (1) learning a mask predictor, and (2) using it and a neural network to learn the distribution of relevance scores based on ad hoc constraints on the classification loss of the predictor. The authors propose a strategy to learn discriminative scores using features from the mask predictor. They show that their method outperforms others in terms of faithfulness and explainability. They also show that the hyperparameters of the distribution controllers are not too different from the ones used in previous work.","This paper studies the problem of local explanation methods for the decision of black-box classifiers. In particular, the authors propose two methods: (1) learning a mask predictor, and (2) using it and a neural network to learn the distribution of relevance scores based on ad hoc constraints on the classification loss of the predictor. The authors propose a strategy to learn discriminative scores using features from the mask predictor. They show that their method outperforms others in terms of faithfulness and explainability. They also show that the hyperparameters of the distribution controllers are not too different from the ones used in previous work."
21654,SP:32ea7cbc47cbdb1f703f4e07c31ce90abe083424,"deep network USED-FOR image reconstruction and classification problems. task - specific network USED-FOR domain specific problem. patches USED-FOR task - specific network. auto - encoder or classifier HYPONYM-OF task - specific network. slack variable USED-FOR top - K selection. method USED-FOR recurring structures. it COMPARE state - of - the - art. state - of - the - art COMPARE it. Task are detection of multiple object instances, and training optimization problem. OtherScientificTerm is supervision. Generic are network, and It. Method are non - differentiable top - K selection process, and multi - stage training. ","This paper tackles the problem of learning a deep network for image reconstruction and classification problems, where the detection of multiple object instances is difficult due to the lack of supervision. The authors propose to use a task-specific network (e.g., auto-encoder or classifier) that is trained to solve a domain specific problem using patches from a set of patches. The training optimization problem is formulated as a non-differentiable top-K selection process, where each patch is selected by a separate network. The top-k selection is based on a slack variable, which can be a function of the number of patches in the patch and the number (and the size) of the patches. It is shown that the proposed method is able to identify recurring structures in the training data, and that it outperforms the state-of-the-art by a large margin. The paper also shows that the method is more robust to multi-stage training, and can be applied to a variety of tasks.","This paper tackles the problem of learning a deep network for image reconstruction and classification problems, where the detection of multiple object instances is difficult due to the lack of supervision. The authors propose to use a task-specific network (e.g., auto-encoder or classifier) that is trained to solve a domain specific problem using patches from a set of patches. The training optimization problem is formulated as a non-differentiable top-K selection process, where each patch is selected by a separate network. The top-k selection is based on a slack variable, which can be a function of the number of patches in the patch and the number (and the size) of the patches. It is shown that the proposed method is able to identify recurring structures in the training data, and that it outperforms the state-of-the-art by a large margin. The paper also shows that the method is more robust to multi-stage training, and can be applied to a variety of tasks."
21663,SP:da1c5f6351d531482e90b86c3cceb52850c520de,assembly code USED-FOR state change. CPU FEATURE-OF state change. RAM FEATURE-OF state change. self - learning reinforcement learning USED-FOR large code space. AutoAssemblet HYPONYM-OF neural program synthesis algorithm. self - learning reinforcement learning USED-FOR AutoAssemblet. Policy networks CONJUNCTION value networks. value networks CONJUNCTION Policy networks. value networks USED-FOR Monte Carlo Tree Search. Policy networks USED-FOR synthesis. value networks USED-FOR synthesis. multi - entropy policy sampling technique USED-FOR online update correlations. AutoAssemblet USED-FOR basic programming tasks. success rates EVALUATE-FOR baselines. AutoAssemblet COMPARE baselines. baselines COMPARE AutoAssemblet. success rates EVALUATE-FOR AutoAssemblet. Method is Neural inductive program synthesis. Task is task generating instructions. ,"This paper proposes AutoAssemblet, a neural program synthesis algorithm based on self-learning reinforcement learning for large code space.   Neural inductive program synthesis is an important problem in many real-world applications, where the state change in assembly code can affect the performance of the CPU in terms of memory and RAM. Policy networks and value networks are used for Monte Carlo Tree Search, and synthesis is performed using Policy networks. The authors propose a multi-entropy policy sampling technique to reduce the online update correlations between task generating instructions and the current state of the art.  Experiments are conducted on a variety of tasks, and AutoAssemblest achieves better success rates than the baselines, and is able to generalize to more complex tasks. ","This paper proposes AutoAssemblet, a neural program synthesis algorithm based on self-learning reinforcement learning for large code space.   Neural inductive program synthesis is an important problem in many real-world applications, where the state change in assembly code can affect the performance of the CPU in terms of memory and RAM. Policy networks and value networks are used for Monte Carlo Tree Search, and synthesis is performed using Policy networks. The authors propose a multi-entropy policy sampling technique to reduce the online update correlations between task generating instructions and the current state of the art.  Experiments are conducted on a variety of tasks, and AutoAssemblest achieves better success rates than the baselines, and is able to generalize to more complex tasks. "
21672,SP:0d4687fc36c02e27d1b95d532a3947589f92b1da,"speed of training CONJUNCTION resource requirements. resource requirements CONJUNCTION speed of training. accuracy CONJUNCTION speed of training. speed of training CONJUNCTION accuracy. accuracy EVALUATE-FOR model architecture. model architecture USED-FOR gradient descent optimization. speed of training EVALUATE-FOR gradient descent optimization. speed of training EVALUATE-FOR model architecture. ODE ’s coefficient matrix H USED-FOR convergence rate. first - order ODE USED-FOR gradient descent. analysis technique USED-FOR H. analysis technique USED-FOR model architecture modifications. OtherScientificTerm are neural network architecture design space, network, and model architecture parameters. Generic is architecture. Metric is speed of convergence. ","This paper considers the problem of learning a neural network architecture design space, where the goal is to design a model architecture that achieves high accuracy, speed of training, and resource requirements. The authors consider gradient descent optimization under a first-order ODE, and study the convergence rate of the ODE’s coefficient matrix H, which depends on the architecture of the network and the number of parameters. They show that the speed of convergence can be improved by modifying the model architecture parameters. The paper also proposes a new analysis technique for computing H for model architecture modifications. ","This paper considers the problem of learning a neural network architecture design space, where the goal is to design a model architecture that achieves high accuracy, speed of training, and resource requirements. The authors consider gradient descent optimization under a first-order ODE, and study the convergence rate of the ODE’s coefficient matrix H, which depends on the architecture of the network and the number of parameters. They show that the speed of convergence can be improved by modifying the model architecture parameters. The paper also proposes a new analysis technique for computing H for model architecture modifications. "
21681,SP:3e3bc8f617df742a395e7d315ec3810a42071294,"overparametrized neural networks ( NNs ) CONJUNCTION kernel methods. kernel methods CONJUNCTION overparametrized neural networks ( NNs ). initialization USED-FOR overparametrized NNs. gradient descent USED-FOR overparametrized NNs. minimum complexity solution USED-FOR interpolating kernel method. squared loss USED-FOR fully - connected wide ReLU - NNs. test error EVALUATE-FOR wide NNs. minimum complexity interpolating kernel methods CONJUNCTION NNs. NNs CONJUNCTION minimum complexity interpolating kernel methods. generalization EVALUATE-FOR initialization scheme. Generic are first, and second. OtherScientificTerm are initialization variance, and generalization bounds. Method is initialization strategies. ","This paper studies the problem of initialization for overparametrized neural networks (NNs) and kernel methods. In particular, the authors consider the case of wide NNs and interpolating kernel methods, where the overparameterized NNs are trained with gradient descent. The authors show that the interpolating kernels are a minimum-complexity interpolation method, and they provide a minimum complexity solution to the problem. They also provide generalization bounds for fully-connected wide ReLU-NNs with a squared loss.    The authors also provide a generalization bound for wide NN with a test error of $O(1/\sqrt{n})$ for the case where the test error is $\mathcal{O}(\log n)$, where $n$ is the number of training samples and $n^{-1/n}$ the initialization variance.  Finally, they show that for a certain initialization scheme, the generalization performance of wide and interpolated NNs can be improved by a factor of $\log n$, which is a factor that depends on the initialization strategies. ","This paper studies the problem of initialization for overparametrized neural networks (NNs) and kernel methods. In particular, the authors consider the case of wide NNs and interpolating kernel methods, where the overparameterized NNs are trained with gradient descent. The authors show that the interpolating kernels are a minimum-complexity interpolation method, and they provide a minimum complexity solution to the problem. They also provide generalization bounds for fully-connected wide ReLU-NNs with a squared loss.    The authors also provide a generalization bound for wide NN with a test error of $O(1/\sqrt{n})$ for the case where the test error is $\mathcal{O}(\log n)$, where $n$ is the number of training samples and $n^{-1/n}$ the initialization variance.  Finally, they show that for a certain initialization scheme, the generalization performance of wide and interpolated NNs can be improved by a factor of $\log n$, which is a factor that depends on the initialization strategies. "
21690,SP:b15ea009a36a0a76728dfc103d668d6781a8a99a,"cars CONJUNCTION pedestrians. pedestrians CONJUNCTION cars. Detecting objects USED-FOR autonomous driving. 3D FEATURE-OF pedestrians. 3D FEATURE-OF cars. 3D FEATURE-OF Detecting objects. pedestrians HYPONYM-OF Detecting objects. cars HYPONYM-OF Detecting objects. LiDAR sensors USED-FOR accurate depth information. LiDAR sensors USED-FOR approaches. stereo images USED-FOR pseudo - LiDAR. stereo depth estimation USED-FOR pseudo - LiDAR framework. loss function USED-FOR depth estimation of faraway objects. stereo network architecture CONJUNCTION loss function. loss function CONJUNCTION stereo network architecture. depth estimates USED-FOR depthpropagation algorithm. depth estimation CONJUNCTION stereo - based 3D object detection. stereo - based 3D object detection CONJUNCTION depth estimation. KITTI object detection benchmark EVALUATE-FOR approach. approach USED-FOR depth estimation. detection accuracy USED-FOR faraway objects. approach USED-FOR stereo - based 3D object detection. approach COMPARE detection accuracy. detection accuracy COMPARE approach. OtherScientificTerm are insufficient information, and depth map. Task is 3D detection. ","Detecting objects in 3D for autonomous driving, i.e. cars, pedestrians, and vehicles, is a challenging problem due to insufficient information. Previous approaches have used LiDAR sensors to capture accurate depth information, but the depth map is not always available for 3D detection. This paper proposes a pseudo-LiDAR framework based on stereo depth estimation, which uses stereo images. The authors propose a new stereo network architecture, loss function, and depth estimation of faraway objects. The depthpropagation algorithm is based on the depth estimates obtained from the proposed depth estimates. Experiments on the KITTI object detection benchmark show that the proposed approach improves the depth estimation and stereo-based 3D object detection performance compared to the state-of-the-art detection accuracy. ","Detecting objects in 3D for autonomous driving, i.e. cars, pedestrians, and vehicles, is a challenging problem due to insufficient information. Previous approaches have used LiDAR sensors to capture accurate depth information, but the depth map is not always available for 3D detection. This paper proposes a pseudo-LiDAR framework based on stereo depth estimation, which uses stereo images. The authors propose a new stereo network architecture, loss function, and depth estimation of faraway objects. The depthpropagation algorithm is based on the depth estimates obtained from the proposed depth estimates. Experiments on the KITTI object detection benchmark show that the proposed approach improves the depth estimation and stereo-based 3D object detection performance compared to the state-of-the-art detection accuracy. "
21699,SP:983d84502264633f3385d426c1d4601a0744ea9a,"models USED-FOR sensitive domains. deep neural networks USED-FOR adversarial examples. defense USED-FOR attacks. detecting adversarial samples USED-FOR methods. adversarial example detection method USED-FOR norm - constrained white - box attacks. detector USED-FOR natural data. base detectors USED-FOR K class classification problem. generative approach USED-FOR detecting / classifying adversarial examples. classconditional data USED-FOR unnormalized density model. unnormalized density model USED-FOR base detector. classconditional data USED-FOR base detector. Method are detection mechanism, one - versus - the - rest classification, k - th detector, adversarial example detection / classification methods, and GAT - Generative - Adversarial - Training. OtherScientificTerm is adversarial example. ","This paper proposes a new adversarial example detection method for white-box adversarial attacks. The authors propose a generative adversarial training method to generate adversarial examples that can be used to improve the robustness of deep neural networks. The proposed method, GAT-Generative-Adversarial-Training (GAT-GAT), is based on the idea that adversarial samples can be generated from the training data, and the detection mechanism can be trained to distinguish between one-versus-the-rest classification and one-vs-all classification. The paper also proposes a defense against such attacks.  ","This paper proposes a new adversarial example detection method for white-box adversarial attacks. The authors propose a generative adversarial training method to generate adversarial examples that can be used to improve the robustness of deep neural networks. The proposed method, GAT-Generative-Adversarial-Training (GAT-GAT), is based on the idea that adversarial samples can be generated from the training data, and the detection mechanism can be trained to distinguish between one-versus-the-rest classification and one-vs-all classification. The paper also proposes a defense against such attacks.  "
21708,SP:461e9308d050bc3dc7b35233452668bb31f5d491,"Exploration PART-OF model - free reinforcement learning. sparse reward environments FEATURE-OF Exploration. intrinsic rewards USED-FOR exploration. MiniGrid FEATURE-OF procedurally - generated tasks. high - dimensional observations USED-FOR tasks. tasks EVALUATE-FOR method. procedurally - generated tasks EVALUATE-FOR method. approach COMPARE exploration methods. exploration methods COMPARE approach. exploration methods USED-FOR procedurally - generated MiniGrid environments. intrinsic reward FEATURE-OF agent. approaches COMPARE intrinsic reward. intrinsic reward COMPARE approaches. OtherScientificTerm are extrinsic rewards, and procedurally - generated environments. Method is learned state representation. Generic is it. ","Exploration in model-free reinforcement learning is an important component in sparse reward environments. Exploration is typically done in sparse environments with no extrinsic rewards. However, intrinsic rewards can be used to encourage exploration in environments with high-dimensional observations.  This paper proposes a method for exploring procedurally-generated tasks on MiniGrid, where the learned state representation is sparse.  The proposed method is evaluated on a number of tasks with high dimensional observations. The proposed approach is shown to outperform existing exploration methods in procedurally generated MiniGrid environments, and it is shown that the agent with an intrinsic reward is able to explore more sparse environments than existing approaches.  ","Exploration in model-free reinforcement learning is an important component in sparse reward environments. Exploration is typically done in sparse environments with no extrinsic rewards. However, intrinsic rewards can be used to encourage exploration in environments with high-dimensional observations.  This paper proposes a method for exploring procedurally-generated tasks on MiniGrid, where the learned state representation is sparse.  The proposed method is evaluated on a number of tasks with high dimensional observations. The proposed approach is shown to outperform existing exploration methods in procedurally generated MiniGrid environments, and it is shown that the agent with an intrinsic reward is able to explore more sparse environments than existing approaches.  "
21717,SP:c002c20b5e8696588e029c0f65e88860418826c4,"recall EVALUATE-FOR retrieval algorithm. scoring phase COMPARE retrieval phase. retrieval phase COMPARE scoring phase. cross - attention models USED-FOR BERT - style pre - training tasks. sparse handcrafted features USED-FOR models. pre - training tasks USED-FOR embedding - based Transformer model. Transformer models COMPARE BM-25. BM-25 COMPARE Transformer models. Transformer models COMPARE embedding models. embedding models COMPARE Transformer models. BM-25 CONJUNCTION embedding models. embedding models CONJUNCTION BM-25. paragraph - level pre - training tasks EVALUATE-FOR Transformer models. Body First Selection ( BFS ) CONJUNCTION Wiki Link Prediction ( WLP ). Wiki Link Prediction ( WLP ) CONJUNCTION Body First Selection ( BFS ). Inverse Cloze Task ( ICT ) CONJUNCTION Body First Selection ( BFS ). Body First Selection ( BFS ) CONJUNCTION Inverse Cloze Task ( ICT ). Inverse Cloze Task ( ICT ) HYPONYM-OF paragraph - level pre - training tasks. Wiki Link Prediction ( WLP ) HYPONYM-OF paragraph - level pre - training tasks. Body First Selection ( BFS ) HYPONYM-OF paragraph - level pre - training tasks. Task is large - scale query - document retrieval problem. Material is large document corpus. Generic is problem. OtherScientificTerm are solution space, and TF - IDF weights. Method are Information Retrieval ( IR ) methods, and embedding - based retrieval models. ","This paper tackles the large-scale query-document retrieval problem, where the goal is to retrieve documents from a large document corpus. The authors propose a retrieval algorithm that is based on the notion of ""recall"", i.e. the number of documents in the solution space that the retrieval algorithm is able to retrieve from a given document. The problem is formulated as the following: given a large corpus of documents, the retrieval problem is to find the most relevant document in the problem space.    The paper proposes to use cross-attention models for BERT-style pre-training tasks, where sparse handcrafted features are used to train the models.  The authors show that their retrieval algorithm outperforms existing Information Retrieval (IR) methods in terms of retrieval performance.  They also show that the score of the retrieval is more important in the scoring phase than in the retrieval phase, and that the TF-IDF weights are more sensitive to the amount of information in the documents.  Finally, they show that a pre-trained embedding-based Transformer model can be trained on a number of pre-train tasks that are similar to the ones used in previous work. They show that Transformer models trained with BM-25 and other embedding models outperform the previous state-of-the-art models on three paragraph-level pre -training tasks: Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP).  ","This paper tackles the large-scale query-document retrieval problem, where the goal is to retrieve documents from a large document corpus. The authors propose a retrieval algorithm that is based on the notion of ""recall"", i.e. the number of documents in the solution space that the retrieval algorithm is able to retrieve from a given document. The problem is formulated as the following: given a large corpus of documents, the retrieval problem is to find the most relevant document in the problem space.    The paper proposes to use cross-attention models for BERT-style pre-training tasks, where sparse handcrafted features are used to train the models.  The authors show that their retrieval algorithm outperforms existing Information Retrieval (IR) methods in terms of retrieval performance.  They also show that the score of the retrieval is more important in the scoring phase than in the retrieval phase, and that the TF-IDF weights are more sensitive to the amount of information in the documents.  Finally, they show that a pre-trained embedding-based Transformer model can be trained on a number of pre-train tasks that are similar to the ones used in previous work. They show that Transformer models trained with BM-25 and other embedding models outperform the previous state-of-the-art models on three paragraph-level pre -training tasks: Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP).  "
21726,SP:4e161e08a624f87633dfb49dfd46bd1665e15189,social graphs CONJUNCTION molecular structures. molecular structures CONJUNCTION social graphs. point clouds CONJUNCTION social graphs. social graphs CONJUNCTION point clouds. Graph neural networks USED-FOR applications. Graph neural networks USED-FOR learning relational representations. learning relational representations CONJUNCTION modeling data on irregular domains. modeling data on irregular domains CONJUNCTION learning relational representations. modeling data on irregular domains HYPONYM-OF applications. molecular structures HYPONYM-OF modeling data on irregular domains. point clouds HYPONYM-OF modeling data on irregular domains. social graphs HYPONYM-OF modeling data on irregular domains. learning relational representations HYPONYM-OF applications. graph convolution operator USED-FOR graph neural network architectures. graph convolution operations CONJUNCTION non - parameterized pooling or expansion layers. non - parameterized pooling or expansion layers CONJUNCTION graph convolution operations. non - parameterized pooling or expansion layers USED-FOR representational hierarchy. graph convolution operations USED-FOR representational hierarchy. parameterized strided and transpose convolution operations CONJUNCTION skip connections. skip connections CONJUNCTION parameterized strided and transpose convolution operations. convolutional network architectures CONJUNCTION parameterized strided and transpose convolution operations. parameterized strided and transpose convolution operations CONJUNCTION convolutional network architectures. bipartite graph convolution operation HYPONYM-OF parameterized transformation. framework USED-FOR multi - graph aggregation. framework COMPARE graph convolution and pooling. graph convolution and pooling COMPARE framework. framework USED-FOR flexible and adaptable network architectures. BiGraphNet HYPONYM-OF flexible and adaptable network architectures. memory requirements FEATURE-OF hierarchical networks. graph convolution USED-FOR hierarchical architectures. graph convolution CONJUNCTION single parametric bipartite graph convolution. single parametric bipartite graph convolution CONJUNCTION graph convolution. graph skip connections CONJUNCTION graph autoencoders. graph autoencoders CONJUNCTION graph skip connections. BiGraphNet formalism ( iii ) USED-FOR architectures. modeling flexibility USED-FOR architectures. BiGraphNet formalism ( iii ) USED-FOR modeling flexibility. graph autoen,"Graph neural networks have been widely used for several applications, including learning relational representations, modeling data on irregular domains (point clouds, social graphs, molecular structures, etc.), and modeling data from irregular domains. This paper proposes a new graph convolution operator that can be applied to existing graph neural network architectures. The authors show that the representational hierarchy can be learned from a set of graph convolutions operations and non-parameterized pooling or expansion layers, which can be used to learn a representational hierarchies. The paper also shows that the bipartitegraph convolution operation (a parameterized transformation of the graph) is a special case of the bipartisan convolution, which is a generalization of existing convolutional network architectures, including parameterized strided and transpose convolution operations, skip connections, and graph autoencoders. The proposed framework is able to learn multi-graph aggregation, and the authors demonstrate that the proposed framework can learn flexible and adaptable network architectures (e.g., BiGraphNet), which can learn hierarchical architectures with a single convolution and pooling, with different memory requirements, and with different number of layers. They also show that a BiGraphnet formalism (ii) allows to learn different architectures with different types of modeling flexibility, and can be combined with existing methods (i.e., graph convocation, single parametric bipartites, and skip connections).   ","Graph neural networks have been widely used for several applications, including learning relational representations, modeling data on irregular domains (point clouds, social graphs, molecular structures, etc.), and modeling data from irregular domains. This paper proposes a new graph convolution operator that can be applied to existing graph neural network architectures. The authors show that the representational hierarchy can be learned from a set of graph convolutions operations and non-parameterized pooling or expansion layers, which can be used to learn a representational hierarchies. The paper also shows that the bipartitegraph convolution operation (a parameterized transformation of the graph) is a special case of the bipartisan convolution, which is a generalization of existing convolutional network architectures, including parameterized strided and transpose convolution operations, skip connections, and graph autoencoders. The proposed framework is able to learn multi-graph aggregation, and the authors demonstrate that the proposed framework can learn flexible and adaptable network architectures (e.g., BiGraphNet), which can learn hierarchical architectures with a single convolution and pooling, with different memory requirements, and with different number of layers. They also show that a BiGraphnet formalism (ii) allows to learn different architectures with different types of modeling flexibility, and can be combined with existing methods (i.e., graph convocation, single parametric bipartites, and skip connections).   "
21735,SP:9b9b6ee9014e5538442ba76d6059ed01f59ec8fb,"metric function USED-FOR metric - based few - shot classification algorithms. metric function USED-FOR feature embeddings. metric - based methods USED-FOR few - shot classification. domain shifts FEATURE-OF few - shot classification. feature - wise transformation layers USED-FOR image features. affine transforms USED-FOR feature distributions. feature - wise transformation layers USED-FOR feature distributions. affine transforms USED-FOR feature - wise transformation layers. learning - to - learn approach USED-FOR hyper - parameters. hyper - parameters FEATURE-OF feature - wise transformation layers. learning - to - learn approach USED-FOR feature - wise transformation layers. learning - to - learn approach USED-FOR feature distributions. Cars CONJUNCTION Places. Places CONJUNCTION Cars. CUB CONJUNCTION Cars. Cars CONJUNCTION CUB. Places CONJUNCTION Plantae. Plantae CONJUNCTION Places. mini - ImageNet CONJUNCTION CUB. CUB CONJUNCTION mini - ImageNet. Plantae HYPONYM-OF few - shot classification datasets. mini - ImageNet HYPONYM-OF few - shot classification datasets. CUB HYPONYM-OF few - shot classification datasets. Places HYPONYM-OF few - shot classification datasets. Cars HYPONYM-OF few - shot classification datasets. feature - wise transformation layer USED-FOR metric - based models. feature - wise transformation layer USED-FOR few - shot classification. domain shift FEATURE-OF few - shot classification. Task are Few - shot classification, and domain generalization setting. Generic is methods. OtherScientificTerm is feature distribution. ","This paper proposes a new metric function for feature embeddings to improve the performance of metric-based few-shot classification algorithms. Few-shot learning has been a hot topic in recent years, and there is a large body of work on the problem of domain generalization. This paper focuses on the domain shift problem, and proposes a learning-to-learn (L2L) framework to improve performance of existing few shot learning methods.    The paper proposes to use feature-wise transformation layers to transform image features using affine transforms, and then use the learned feature distribution as a metric function. The paper shows that the proposed learning to learn approach is able to learn the hyper-parameters of the feature-wide transformation layers, which can be used to improve feature distributions.  Experiments are conducted on three standard few shot classification datasets: mini-ImageNet, CUB, Cars, Places, and Plantae. The results show that the use of feature-wise transformation layer improves performance of the metric- based models, and that the feature distribution of the learned features is more robust to domain shift. ","This paper proposes a new metric function for feature embeddings to improve the performance of metric-based few-shot classification algorithms. Few-shot learning has been a hot topic in recent years, and there is a large body of work on the problem of domain generalization. This paper focuses on the domain shift problem, and proposes a learning-to-learn (L2L) framework to improve performance of existing few shot learning methods.    The paper proposes to use feature-wise transformation layers to transform image features using affine transforms, and then use the learned feature distribution as a metric function. The paper shows that the proposed learning to learn approach is able to learn the hyper-parameters of the feature-wide transformation layers, which can be used to improve feature distributions.  Experiments are conducted on three standard few shot classification datasets: mini-ImageNet, CUB, Cars, Places, and Plantae. The results show that the use of feature-wise transformation layer improves performance of the metric- based models, and that the feature distribution of the learned features is more robust to domain shift. "
21744,SP:df46627cb984a56bba36d510bfc52e00751e9107,approach USED-FOR Lagrangian fluid simulation. convolutional network USED-FOR approach. moving particles USED-FOR fluids. networks USED-FOR moving particles. graph structure USED-FOR particles. N - D convolutions USED-FOR continuous domain. network architecture USED-FOR inverse problems. network architecture USED-FOR arbitrary collision geometries. continuous convolutions COMPARE prior formulations. prior formulations COMPARE continuous convolutions. accuracy CONJUNCTION speed. speed CONJUNCTION accuracy. speed EVALUATE-FOR prior formulations. speed EVALUATE-FOR continuous convolutions. accuracy EVALUATE-FOR continuous convolutions. accuracy EVALUATE-FOR prior formulations. Generic is approaches. Method is spatial convolutions. ,"This paper proposes a new approach for Lagrangian fluid simulation using a convolutional network. The approach is based on the observation that moving particles in fluids can be represented as moving particles with a graph structure, and the authors propose to use networks to model these moving particles. The authors use N-D convolutions to represent the continuous domain, which is a generalization of previous approaches. They also propose a network architecture that can handle arbitrary collision geometries and solve inverse problems. Experiments show that the proposed continuous convolutions outperform prior formulations in terms of accuracy and speed.  The authors also show that spatial convolutions can be used as well.","This paper proposes a new approach for Lagrangian fluid simulation using a convolutional network. The approach is based on the observation that moving particles in fluids can be represented as moving particles with a graph structure, and the authors propose to use networks to model these moving particles. The authors use N-D convolutions to represent the continuous domain, which is a generalization of previous approaches. They also propose a network architecture that can handle arbitrary collision geometries and solve inverse problems. Experiments show that the proposed continuous convolutions outperform prior formulations in terms of accuracy and speed.  The authors also show that spatial convolutions can be used as well."
21753,SP:3e17f333cf07183969c02bb66afdd3ccbf25bb19,"accuracy CONJUNCTION predictive uncertainty. predictive uncertainty CONJUNCTION accuracy. predictive uncertainty EVALUATE-FOR single neural networks. accuracy EVALUATE-FOR single neural networks. BatchEnsemble1 HYPONYM-OF ensemble method. Hadamard product USED-FOR weight matrix. ensembles COMPARE BatchEnsemble. BatchEnsemble COMPARE ensembles. BatchEnsemble COMPARE ensembles. ensembles COMPARE BatchEnsemble. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. speedup CONJUNCTION memory reduction. memory reduction CONJUNCTION speedup. CIFAR-10 EVALUATE-FOR BatchEnsemble. out - of - distribution tasks EVALUATE-FOR BatchEnsemble. BatchEnsemble USED-FOR lifelong learning. Split - CIFAR-100 EVALUATE-FOR BatchEnsemble. BatchEnsemble COMPARE progressive neural networks. progressive neural networks COMPARE BatchEnsemble. computational and memory costs EVALUATE-FOR BatchEnsemble. BatchEnsemble USED-FOR lifelong learning. Split - ImageNet USED-FOR lifelong learning. sequential learning tasks PART-OF lifelong learning. Method are Ensembles, and neural networks. Metric is ensemble ’s cost. OtherScientificTerm are mini - batch, and ensemble. ","This paper proposes a new ensemble method called BatchEnsemble1, which is a variant of Ensembles. The authors argue that the ensemble method can achieve better accuracy and predictive uncertainty than single neural networks, while having a fraction of the ensemble’s cost. The main idea is to use a mini-batch, where each batch is composed of a number of mini-batches, and the weight matrix is computed using the Hadamard product. Compared to ensembles, the authors show that Batchensemble achieves speedup and memory reduction on CIFAR-10 as well as on out-of-distribution tasks. They also show that the performance on lifelong learning on Split-ImageNet shows that the proposed Batch ensemble can be used for lifelong learning with sequential learning tasks, where the ensemble performs better than progressive neural networks in terms of computational and memory costs.  ","This paper proposes a new ensemble method called BatchEnsemble1, which is a variant of Ensembles. The authors argue that the ensemble method can achieve better accuracy and predictive uncertainty than single neural networks, while having a fraction of the ensemble’s cost. The main idea is to use a mini-batch, where each batch is composed of a number of mini-batches, and the weight matrix is computed using the Hadamard product. Compared to ensembles, the authors show that Batchensemble achieves speedup and memory reduction on CIFAR-10 as well as on out-of-distribution tasks. They also show that the performance on lifelong learning on Split-ImageNet shows that the proposed Batch ensemble can be used for lifelong learning with sequential learning tasks, where the ensemble performs better than progressive neural networks in terms of computational and memory costs.  "
21762,SP:a123a425ef3eb6188833d5a42e851bc3fa59df65,"neural network - based partial differential equations solver USED-FOR forward and inverse problems. mesh free and shape free FEATURE-OF solver. neural network USED-FOR solution. explicit smooth differentiable function USED-FOR solution. analytical form FEATURE-OF explicit smooth differentiable function. finite differences CONJUNCTION finite elements. finite elements CONJUNCTION finite differences. finite differences HYPONYM-OF numerical methods. finite elements HYPONYM-OF numerical methods. algorithm USED-FOR forward and inverse problems. Robust boundary conditions constraints CONJUNCTION regularizers. regularizers CONJUNCTION Robust boundary conditions constraints. Electrical Impedance Tomography ( EIT ) CONJUNCTION diffusion and wave equations. diffusion and wave equations CONJUNCTION Electrical Impedance Tomography ( EIT ). free shape 2D second order systems USED-FOR Electrical Impedance Tomography ( EIT ). method USED-FOR diffusion and wave equations. method USED-FOR Electrical Impedance Tomography ( EIT ). free shape 2D second order systems USED-FOR diffusion and wave equations. method USED-FOR free shape 2D second order systems. Method is unsupervised approach. Generic are network, framework, and methods. OtherScientificTerm are strong PDE solution, boundary conditions, derivatives of the desired function, and loss function. ","This paper proposes a neural network-based partial differential equations solver for solving forward and inverse problems. The proposed solver is mesh free and shape free, and the authors propose an unsupervised approach where the solution is learned by training a deep neural network. The solution is obtained by learning an explicit smooth differentiable function in an analytical form, which can be used as a strong PDE solution. Robust boundary conditions constraints and regularizers are used to ensure that the boundary conditions satisfy the desired boundary conditions. The authors also propose two numerical methods: finite differences and finite elements. The algorithm is able to solve both forward or inverse problems in a single pass, and can be applied to free shape 2D second order systems (e.g. for Electrical Impedance Tomography (EIT) and diffusion and wave equations).   The authors provide a theoretical analysis of the network and show that the proposed framework is a generalization of an existing framework. The paper also shows that the method can be extended to the case where the derivatives of the desired function are non-trivial and the loss function is non-convex.  The proposed method is applied to the problem of electrical impeding torsions and diffusion equations, and is shown to be able to learn free-shape 2D 2D systems, and to solve free shape two-order systems.   ","This paper proposes a neural network-based partial differential equations solver for solving forward and inverse problems. The proposed solver is mesh free and shape free, and the authors propose an unsupervised approach where the solution is learned by training a deep neural network. The solution is obtained by learning an explicit smooth differentiable function in an analytical form, which can be used as a strong PDE solution. Robust boundary conditions constraints and regularizers are used to ensure that the boundary conditions satisfy the desired boundary conditions. The authors also propose two numerical methods: finite differences and finite elements. The algorithm is able to solve both forward or inverse problems in a single pass, and can be applied to free shape 2D second order systems (e.g. for Electrical Impedance Tomography (EIT) and diffusion and wave equations).   The authors provide a theoretical analysis of the network and show that the proposed framework is a generalization of an existing framework. The paper also shows that the method can be extended to the case where the derivatives of the desired function are non-trivial and the loss function is non-convex.  The proposed method is applied to the problem of electrical impeding torsions and diffusion equations, and is shown to be able to learn free-shape 2D 2D systems, and to solve free shape two-order systems.   "
21771,SP:973d0ad0faadcf7298300f2758de9154205e7113,neural networks PART-OF deep learning. Binarized Neural Networks HYPONYM-OF networks. SAT solvers HYPONYM-OF logic - based reasoning tools. tools USED-FOR existential and probabilistic queries. tools USED-FOR explanation generation. existential and probabilistic queries FEATURE-OF network. they USED-FOR logic - based reasoners. training procedure USED-FOR network. network USED-FOR SAT solvers. BNN architecture CONJUNCTION training procedure. training procedure CONJUNCTION BNN architecture. approach COMPARE work. work COMPARE approach. work USED-FOR existential and probabilistic queries. approach USED-FOR existential and probabilistic queries. deep neural networks COMPARE work. work COMPARE deep neural networks. approach USED-FOR deep neural networks. deep neural networks USED-FOR existential and probabilistic queries. OtherScientificTerm is Boolean logic. Generic is methods. Method is BNNs. Metric is accuracy. ,"This paper introduces a new class of networks called Binarized Neural Networks (BNNs), which are neural networks that can be seen as a special case of deep learning. The authors show that they can be used to train logic-based reasoners (e.g., SAT solvers) and explainers (i.e., Boolean logic) and that these tools can be applied to existential and probabilistic queries for explanation generation. They show that a BNN architecture, training procedure, and the number of parameters of the network are sufficient to train a network that can solve existential and Probabilistic Questions. They also show that their approach outperforms previous work on existential and probing queries in terms of accuracy.   ","This paper introduces a new class of networks called Binarized Neural Networks (BNNs), which are neural networks that can be seen as a special case of deep learning. The authors show that they can be used to train logic-based reasoners (e.g., SAT solvers) and explainers (i.e., Boolean logic) and that these tools can be applied to existential and probabilistic queries for explanation generation. They show that a BNN architecture, training procedure, and the number of parameters of the network are sufficient to train a network that can solve existential and Probabilistic Questions. They also show that their approach outperforms previous work on existential and probing queries in terms of accuracy.   "
21780,SP:ca985e758f195bd04fb9f24b290a83974d6d308b,"expressive power FEATURE-OF graph neural networks. message - passing framework ( GNNmp ) FEATURE-OF graph neural networks. node attributes CONJUNCTION layer expressiveness. layer expressiveness CONJUNCTION node attributes. technique USED-FOR impossibility statements. approximation USED-FOR tasks. Method are GNNmp, and distributed computing. OtherScientificTerm is lower bounds. Material is graphs. Generic is problems. ",This paper studies the expressive power of graph neural networks in the message-passing framework (GNNmp) of the node attributes and layer expressiveness. The authors provide lower bounds on the expressiveness of GNNmp. They also provide a technique for computing impossibility statements for certain types of problems. They provide an approximation for a number of tasks that require distributed computing. ,This paper studies the expressive power of graph neural networks in the message-passing framework (GNNmp) of the node attributes and layer expressiveness. The authors provide lower bounds on the expressiveness of GNNmp. They also provide a technique for computing impossibility statements for certain types of problems. They provide an approximation for a number of tasks that require distributed computing. 
21789,SP:a98ae70a91850bbe624c307ba61d3daeb2494b82,"flow - based density models USED-FOR target distributions. complicated topologies FEATURE-OF target distributions. continuous bijections USED-FOR flow - based density models. stacked continuous mixtures of bijections PART-OF LGFs. flow - based methods USED-FOR LGF model. method COMPARE flow - based methods. flow - based methods COMPARE method. normalising flows COMPARE LGFs. LGFs COMPARE normalising flows. density estimation tasks EVALUATE-FOR LGFs. Method are localised generative flows ( LGFs ), and variational scheme. OtherScientificTerm are bijection, and log likelihoods. ","This paper introduces localised generative flows (LGFs), a new family of flow-based density models that use continuous bijections to model target distributions with complicated topologies. LGFs are stacked continuous mixtures of bijection, and the authors propose a variational scheme for learning the bijection. The authors show that the proposed method outperforms existing flow -based methods for training the LGF model. They also show that normalising flows outperform LGFs on density estimation tasks, and that the log likelihoods of LGFs can be improved.","This paper introduces localised generative flows (LGFs), a new family of flow-based density models that use continuous bijections to model target distributions with complicated topologies. LGFs are stacked continuous mixtures of bijection, and the authors propose a variational scheme for learning the bijection. The authors show that the proposed method outperforms existing flow -based methods for training the LGF model. They also show that normalising flows outperform LGFs on density estimation tasks, and that the log likelihoods of LGFs can be improved."
21798,SP:3adc341dece170f428195e4dccadfb5f5daddf2d,"environment re - splitting CONJUNCTION feature replacement. feature replacement CONJUNCTION environment re - splitting. environment re - splitting USED-FOR diagnosis experiments. feature replacement USED-FOR diagnosis experiments. language CONJUNCTION navigational graph. navigational graph CONJUNCTION language. ResNet features USED-FOR low - level visual appearance. low - level visual information FEATURE-OF semantic representations. features USED-FOR agent. baseline agent model CONJUNCTION training method. training method CONJUNCTION baseline agent model. R4R CONJUNCTION CVDN. CVDN CONJUNCTION R4R. R2R CONJUNCTION R4R. R4R CONJUNCTION R2R. R2R HYPONYM-OF datasets. OtherScientificTerm are naturallanguage instructions, step - by - step navigational instructions, and semantic features. Task is VLN. Method are neural agent models, and agent model. Generic is state - of - the - art models. ","This paper proposes a method to train an agent to follow naturallanguage instructions in an environment where the goal is to follow step-by-step navigational instructions. The authors propose to use a combination of environment re-splitting and feature replacement to perform diagnosis experiments. The main idea is to use language and a navigational graph as input to the neural agent models, and use ResNet features to model the low-level visual appearance of the environment. The semantic representations of the semantic representations are then used to capture the low level visual information. The agent is trained on a set of datasets, including R2R, R4R, and CVDN, where it is shown that VLN is able to learn to follow the instructions in the environment, and that the agent can be trained on the features learned by the baseline agent model and the training method. The paper also shows that the state-of-the-art models are able to generalize well to unseen environments. ","This paper proposes a method to train an agent to follow naturallanguage instructions in an environment where the goal is to follow step-by-step navigational instructions. The authors propose to use a combination of environment re-splitting and feature replacement to perform diagnosis experiments. The main idea is to use language and a navigational graph as input to the neural agent models, and use ResNet features to model the low-level visual appearance of the environment. The semantic representations of the semantic representations are then used to capture the low level visual information. The agent is trained on a set of datasets, including R2R, R4R, and CVDN, where it is shown that VLN is able to learn to follow the instructions in the environment, and that the agent can be trained on the features learned by the baseline agent model and the training method. The paper also shows that the state-of-the-art models are able to generalize well to unseen environments. "
21807,SP:298e0043e99f586d314fbd9d16fdc6ae885e1ebb,"implicit human feedback USED-FOR DRL algorithm. expert labeling CONJUNCTION demonstrations. demonstrations CONJUNCTION expert labeling. agent ’s learning USED-FOR RL tasks. implicit feedback USED-FOR agent ’s learning. system USED-FOR implicit human feedback. implicit human feedback USED-FOR state - action pairs. Atari - type environment FEATURE-OF state - action pairs. error - related event potentials HYPONYM-OF implicit human feedback. auxiliary reward function USED-FOR DRL algorithm. them USED-FOR DRL algorithm. DRL algorithm USED-FOR learning of the game. them USED-FOR auxiliary reward function. electroencephalogram ( EEG ) cap USED-FOR error - potentials. definition USED-FOR game. frameworks USED-FOR error - potential based feedback system. DRL USED-FOR error - potential based feedback system. implicit human feedback USED-FOR complex environments ( games ). synthetic and real user experiments EVALUATE-FOR approach. OtherScientificTerm are human feedback, non - expert humans, human ’s intrinsic reactions, event - related electric potentials, and Atari - games. Generic is paradigm. Method is RL agent. ","This paper proposes a system that uses implicit human feedback (i.e., error-related event potentials) to train a DRL algorithm in an Atari-type environment, where the agent’s learning for RL tasks is based on implicit feedback from non-expert humans. The system learns state-action pairs conditioned on the human feedback from the system, and then uses this system to train an RL agent that is able to generalize well to new environments.    The paper proposes to use the system as an auxiliary reward function for learning of the game, and uses the learned state and state action pairs from the human as an indicator of the state of the system. The paper also introduces a new definition of a game, which is defined as a game in which the agent is given the ability to learn from human feedback. In this paradigm, the RL agent is encouraged to learn a game that is similar to a game played by humans, but with the goal that the agent can generalize to a new environment.  In order to do so, the paper introduces two novelties: (1) the use of event-related electric potentials, and (2) using an electroencephalogram (EEG) cap to control the error-potentials and using them as an additional auxiliary reward to train the D RL algorithm for the learning of a new game.  Experiments on synthetic and real user experiments show that the proposed approach works well in a variety of complex environments (games) that require implicit human information, and that require expert labeling and demonstrations. In particular, it is shown that the RL algorithm trained with DRL can generalizes well to more complex environments, and generalizes better to new complex environments. It is also shown that this approach generalizes to new games, and is more robust to changes in the environment (e.g., changes in environment dynamics).  ","This paper proposes a system that uses implicit human feedback (i.e., error-related event potentials) to train a DRL algorithm in an Atari-type environment, where the agent’s learning for RL tasks is based on implicit feedback from non-expert humans. The system learns state-action pairs conditioned on the human feedback from the system, and then uses this system to train an RL agent that is able to generalize well to new environments.    The paper proposes to use the system as an auxiliary reward function for learning of the game, and uses the learned state and state action pairs from the human as an indicator of the state of the system. The paper also introduces a new definition of a game, which is defined as a game in which the agent is given the ability to learn from human feedback. In this paradigm, the RL agent is encouraged to learn a game that is similar to a game played by humans, but with the goal that the agent can generalize to a new environment.  In order to do so, the paper introduces two novelties: (1) the use of event-related electric potentials, and (2) using an electroencephalogram (EEG) cap to control the error-potentials and using them as an additional auxiliary reward to train the D RL algorithm for the learning of a new game.  Experiments on synthetic and real user experiments show that the proposed approach works well in a variety of complex environments (games) that require implicit human information, and that require expert labeling and demonstrations. In particular, it is shown that the RL algorithm trained with DRL can generalizes well to more complex environments, and generalizes better to new complex environments. It is also shown that this approach generalizes to new games, and is more robust to changes in the environment (e.g., changes in environment dynamics).  "
21816,SP:a8395f8b877e1eebaef9ff2e8b4e488d55a74ef4,"laconic classification USED-FOR diverse image classifiers. classifier USED-FOR approximate minimal - entropy positive image. classifier USED-FOR approximate minimal - entropy positive image. colour reduction CONJUNCTION resolution reduction. resolution reduction CONJUNCTION colour reduction. reductions USED-FOR classification. crop CONJUNCTION colour reduction. colour reduction CONJUNCTION crop. crop HYPONYM-OF reductions. resolution reduction HYPONYM-OF reductions. colour reduction HYPONYM-OF reductions. complementary frameworks USED-FOR minimal - entropy positive images. cropping CONJUNCTION reduced colour. reduced colour CONJUNCTION cropping. texture bias FEATURE-OF ILSVRC - trained models. minimal - entropy positive images USED-FOR human and machine classifiers. complementary frameworks USED-FOR human and machine classifiers. reduced resolution FEATURE-OF humans. machines CONJUNCTION reduced resolution. reduced resolution CONJUNCTION machines. cropping USED-FOR machines. reduced colour USED-FOR machines. Metric are entropy, and precision. Material is ILSVRC test - set. Method are machine classifiers, and machine models. ","This paper studies the problem of laconic classification for diverse image classifiers. The authors propose three reductions for classification: crop, colour reduction, and resolution reduction. They show that a classifier trained on the ILSVRC test-set can learn an approximate minimal-entropy positive image from a single classifier. They also show that complementary frameworks can be used to train both human and machine classifiers on minimal-Entropy positive images. The paper also shows that machines trained on cropping, reduced colour, and reduced resolution are more sensitive to cropping and reduced colour than human classifiers, and that there is a texture bias in the learned representations of machine classifying images. ","This paper studies the problem of laconic classification for diverse image classifiers. The authors propose three reductions for classification: crop, colour reduction, and resolution reduction. They show that a classifier trained on the ILSVRC test-set can learn an approximate minimal-entropy positive image from a single classifier. They also show that complementary frameworks can be used to train both human and machine classifiers on minimal-Entropy positive images. The paper also shows that machines trained on cropping, reduced colour, and reduced resolution are more sensitive to cropping and reduced colour than human classifiers, and that there is a texture bias in the learned representations of machine classifying images. "
21825,SP:81cec8f907d8fa0653b5bc08af1f59bfefd49619,"defenses USED-FOR Convolutional Neural Networks. instability assumption USED-FOR defense techniques. deterministic lossy compression algorithms CONJUNCTION randomized perturbations. randomized perturbations CONJUNCTION deterministic lossy compression algorithms. robustness EVALUATE-FOR randomized perturbations. deterministic lossy compression algorithms PART-OF defenses. randomized perturbations PART-OF defenses. Material is adversarial examples. OtherScientificTerm are small perturbations, and decision space. Method is perturbation defenses. "," adversarial examples can be generated by small perturbations in the decision space. The authors propose two new defenses for Convolutional Neural Networks, which are based on the instability assumption in existing defense techniques. Specifically, the authors propose deterministic lossy compression algorithms and randomized perturbation to improve the robustness of existing defenses. Experiments are conducted to validate the effectiveness of the proposed defenses.   "," adversarial examples can be generated by small perturbations in the decision space. The authors propose two new defenses for Convolutional Neural Networks, which are based on the instability assumption in existing defense techniques. Specifically, the authors propose deterministic lossy compression algorithms and randomized perturbation to improve the robustness of existing defenses. Experiments are conducted to validate the effectiveness of the proposed defenses.   "
21834,SP:a136b98e0ed478144ce9dd26e2b6d611543124e8,"view prediction HYPONYM-OF prediction tasks. view prediction USED-FOR 3D visual recognition. moving camera USED-FOR 2.5D ( color and depth ) video streams. 2.5D ( color and depth ) video streams USED-FOR neural 3D mapping networks. contrastive prediction losses COMPARE color regression loss. color regression loss COMPARE contrastive prediction losses. visual representations USED-FOR semi - supervised learning of 3D object detectors. model USED-FOR visual representations. visual representations USED-FOR unsupervised learning of 3D moving object detectors. model USED-FOR unsupervised learning of 3D moving object detectors. semi - supervised learning of 3D object detectors CONJUNCTION unsupervised learning of 3D moving object detectors. unsupervised learning of 3D moving object detectors CONJUNCTION semi - supervised learning of 3D object detectors. videos of dynamic scenes FEATURE-OF motion of the inferred 3D feature maps. scalable self - supervised task USED-FOR 3D object detection. view prediction USED-FOR 3D object detection. view prediction HYPONYM-OF scalable self - supervised task. Method is Predictive coding theories. Generic are task, and them. OtherScientificTerm are perception, retinas, and 3D feature maps. Material is complex photorealistic data. ","This paper proposes a new view prediction task for 3D visual recognition based on view prediction. Predictive coding theories have been applied to this task before, but this is the first paper to apply them to the case of view prediction, which is one of the most important prediction tasks in the field.    The paper proposes to use neural 3D mapping networks on 2.5D (color and depth) video streams from a moving camera, which are generated from a 2.2D (depth and color) moving camera.  The key idea is to use contrastive prediction losses instead of the standard color regression loss, which has been widely used in previous work. The proposed model is able to learn visual representations for semi-supervised learning of 3D object detectors, and for unsupervised training of a 3D moving object detectors. The paper also proposes a scalable self-supervision task called view prediction as a way to improve the performance of a view prediction for the task.  Experiments are performed on complex photorealistic data, where the motion of the inferred 3D feature maps is captured in videos of dynamic scenes, and retinas are used to map the scene to the 3D space. ","This paper proposes a new view prediction task for 3D visual recognition based on view prediction. Predictive coding theories have been applied to this task before, but this is the first paper to apply them to the case of view prediction, which is one of the most important prediction tasks in the field.    The paper proposes to use neural 3D mapping networks on 2.5D (color and depth) video streams from a moving camera, which are generated from a 2.2D (depth and color) moving camera.  The key idea is to use contrastive prediction losses instead of the standard color regression loss, which has been widely used in previous work. The proposed model is able to learn visual representations for semi-supervised learning of 3D object detectors, and for unsupervised training of a 3D moving object detectors. The paper also proposes a scalable self-supervision task called view prediction as a way to improve the performance of a view prediction for the task.  Experiments are performed on complex photorealistic data, where the motion of the inferred 3D feature maps is captured in videos of dynamic scenes, and retinas are used to map the scene to the 3D space. "
21843,SP:6fd61604a2eeb8a2cbbda6c40807cebef6d40f2f,"model USED-FOR applications. Optimal Transport ( OT ) framework USED-FOR UDT. low energy transformations FEATURE-OF mappings. methods USED-FOR mappings. theoretical guarantees EVALUATE-FOR methods. approach USED-FOR UDT. dynamic formulation of OT CONJUNCTION CycleGAN. CycleGAN CONJUNCTION dynamic formulation of OT. mapping USED-FOR domain translation. translation USED-FOR problems. image captioning CONJUNCTION natural language translation. natural language translation CONJUNCTION image captioning. neural network USED-FOR mapping. hidden layers PART-OF neural network. photographs CONJUNCTION paintings. paintings CONJUNCTION photographs. model USED-FOR task. dynamical formulation USED-FOR model. Optimal Transport theory USED-FOR UDT models. model COMPARE CycleGAN - like models. CycleGAN - like models COMPARE model. Task are Unsupervised Domain Translation ( UDT ), UDT problems, and UDT problem. OtherScientificTerm are implicit biases, implicit bias, map, unwanted pairings, objective function, and well - behaved mappings. Generic are approaches, and models. Method are CycleGAN model, and networks of minimal complexity. Metric is complexity. ","This paper studies the problem of Unsupervised Domain Translation (UDT), where the goal is to learn a mapping from a source domain to a target domain without implicit biases. The authors propose an Optimal Transport (OT) framework for UDT, where the implicit bias is that the map between the source domain and target domain has a low energy transformation. They show that existing approaches to learn such mappings have theoretical guarantees, and propose a model that can be applied to such applications.    The key idea of the proposed approach to UDT is to use a dynamic formulation of OT, similar to CycleGAN. The key difference between CycleGAN model and the authors' approach is that CycleGAN models are trained to learn the mapping between source and target domains, while the authors propose to train a neural network that learns the mapping from source to target using hidden layers in the neural network.  The authors show that this approach can be used for a variety of UDT problems, including image captioning, natural language translation, and domain translation. They also show that the mapping learned in domain translation can be transferred to other problems that require the translation to be performed on the target domain.  In addition, they show that their model can be adapted to a new task by learning a dynamical formulation of the mapping, and that the resulting model is more robust to unwanted pairings in the input data, and can be trained with networks of minimal complexity.  Finally, the authors demonstrate that the proposed model outperforms CycleGAN-like models trained using Optimal transport theory, and is able to learn well-behaved mappings to the target domains. The paper also shows that the model is robust to the presence of unwanted pairs of pairings (i.e. pairings that have low energy transformations) in the target and source domains, and the model can learn to learn to map the source to the source domains without unwanted pairs.  They also provide a theoretical analysis of the UDT problem, showing that the objective function can be learned in a way that ensures that the learned mapping is invariant to pairings of the source and the target. They further show that, under certain conditions, the model learns to learn mappings that are invariant under the mapping. ","This paper studies the problem of Unsupervised Domain Translation (UDT), where the goal is to learn a mapping from a source domain to a target domain without implicit biases. The authors propose an Optimal Transport (OT) framework for UDT, where the implicit bias is that the map between the source domain and target domain has a low energy transformation. They show that existing approaches to learn such mappings have theoretical guarantees, and propose a model that can be applied to such applications.    The key idea of the proposed approach to UDT is to use a dynamic formulation of OT, similar to CycleGAN. The key difference between CycleGAN model and the authors' approach is that CycleGAN models are trained to learn the mapping between source and target domains, while the authors propose to train a neural network that learns the mapping from source to target using hidden layers in the neural network.  The authors show that this approach can be used for a variety of UDT problems, including image captioning, natural language translation, and domain translation. They also show that the mapping learned in domain translation can be transferred to other problems that require the translation to be performed on the target domain.  In addition, they show that their model can be adapted to a new task by learning a dynamical formulation of the mapping, and that the resulting model is more robust to unwanted pairings in the input data, and can be trained with networks of minimal complexity.  Finally, the authors demonstrate that the proposed model outperforms CycleGAN-like models trained using Optimal transport theory, and is able to learn well-behaved mappings to the target domains. The paper also shows that the model is robust to the presence of unwanted pairs of pairings (i.e. pairings that have low energy transformations) in the target and source domains, and the model can learn to learn to map the source to the source domains without unwanted pairs.  They also provide a theoretical analysis of the UDT problem, showing that the objective function can be learned in a way that ensures that the learned mapping is invariant to pairings of the source and the target. They further show that, under certain conditions, the model learns to learn mappings that are invariant under the mapping. "
21852,SP:8bb3ce11ad773685f6e41d90db3e7a5481e5ba47,regularization method USED-FOR neural networks. RotationOut USED-FOR neural networks. RotationOut HYPONYM-OF regularization method. Dropout USED-FOR neuron / channel. Dropout COMPARE RotationOut. RotationOut COMPARE Dropout. convolutional layers CONJUNCTION recurrent layers. recurrent layers CONJUNCTION convolutional layers. RotationOut USED-FOR recurrent layers. RotationOut USED-FOR convolutional layers. RotationOut USED-FOR co - adaptation reduction. Dropout USED-FOR co - adaptation reduction. RotationOut CONJUNCTION Dropout. Dropout CONJUNCTION RotationOut. noise analysis method USED-FOR co - adaptation reduction. RotationOut / Dropout CONJUNCTION Batch Normalization. Batch Normalization CONJUNCTION RotationOut / Dropout. vision and language tasks EVALUATE-FOR method. RotationOut CONJUNCTION RotationOut. RotationOut CONJUNCTION RotationOut. Method is regularization. ,"This paper proposes a new regularization method called RotationOut to improve the performance of neural networks. Unlike Dropout, which regularizes each neuron/channel separately, RotationUp regularizes the whole network. The authors show that RotationThrough regularizes convolutional layers and recurrent layers, while Dropout only regularizes a single neuron or channel. They also show that the co-adaptation reduction can be performed with RotationOver and Dropout. The paper also shows that RotoOut/Dropout/RotationUp and Batch Normalization can be combined to improve performance on vision and language tasks. The main contribution of the paper is a noise analysis method to analyze the effect of RotationIn and RotoDropout regularization.   ","This paper proposes a new regularization method called RotationOut to improve the performance of neural networks. Unlike Dropout, which regularizes each neuron/channel separately, RotationUp regularizes the whole network. The authors show that RotationThrough regularizes convolutional layers and recurrent layers, while Dropout only regularizes a single neuron or channel. They also show that the co-adaptation reduction can be performed with RotationOver and Dropout. The paper also shows that RotoOut/Dropout/RotationUp and Batch Normalization can be combined to improve performance on vision and language tasks. The main contribution of the paper is a noise analysis method to analyze the effect of RotationIn and RotoDropout regularization.   "
21861,SP:37620ae8dc5683eb2843792e0aa4cbe6cba366f7,"method USED-FOR Universal Adversarial Perturbations ( UAP ). Universal Adversarial Perturbations ( UAP ) USED-FOR CNN. sequential optimization USED-FOR adversarial perturbation. dilate loss USED-FOR sequential optimization. dilate loss USED-FOR adversarial perturbation. Euclidean norm EVALUATE-FOR Dilate loss. method COMPARE data - free work. data - free work COMPARE method. fooling rate EVALUATE-FOR data - free work. fooling rate EVALUATE-FOR method. Method is Data - free approaches. Task are crafting adversaries, adversary generation, and crafting UAPs. OtherScientificTerm are nonlinearity, perturbation, ReLU activation function, and limited data cases. ","This paper proposes a novel method for crafting Universal Adversarial Perturbations (UAP) for CNN. Data-free approaches have been shown to be effective at crafting adversaries, but the problem of adversary generation is challenging due to nonlinearity of the perturbation. This paper proposes to use a ReLU activation function to generate UAPs.    The key idea is to use sequential optimization to generate adversarial perturbations using dilate loss, which is based on Euclidean norm.  Dilate loss is defined as a function of the number of samples and the size of the data set.  Experiments show that the proposed method has a better fooling rate than existing data-free work, especially in limited data cases. ","This paper proposes a novel method for crafting Universal Adversarial Perturbations (UAP) for CNN. Data-free approaches have been shown to be effective at crafting adversaries, but the problem of adversary generation is challenging due to nonlinearity of the perturbation. This paper proposes to use a ReLU activation function to generate UAPs.    The key idea is to use sequential optimization to generate adversarial perturbations using dilate loss, which is based on Euclidean norm.  Dilate loss is defined as a function of the number of samples and the size of the data set.  Experiments show that the proposed method has a better fooling rate than existing data-free work, especially in limited data cases. "
21870,SP:2fd7d5507a8727db743dc89379a6f021d31ed39a,"Neural Architecture Search ( NAS ) COMPARE hand - designed networks. hand - designed networks COMPARE Neural Architecture Search ( NAS ). Neural Architecture Search ( NAS ) USED-FOR artificial intelligence areas. architecture USED-FOR task. NAS CONJUNCTION fast adaptation of neural architectures. fast adaptation of neural architectures CONJUNCTION NAS. T - NAS HYPONYM-OF Transferable Neural Architecture Search method. meta - learning USED-FOR Transferable Neural Architecture Search method. architecture USED-FOR task. T - NAS USED-FOR meta - architecture. few - shot learning CONJUNCTION supervised learning. supervised learning CONJUNCTION few - shot learning. supervised learning EVALUATE-FOR T - NAS. few - shot learning EVALUATE-FOR T - NAS. Method are NAS methods, and neural architectures. Metric is searching cost. Generic is method. ","This paper studies the problem of Neural Architecture Search (NAS) in artificial intelligence areas, where the goal is to find the best architecture for a given task from a set of hand-designed networks. Previous NAS methods have been shown to be effective in this setting, but the searching cost can be prohibitively expensive. In this paper, the authors propose a new NAS method called T-NAS, which is a transferable neural architecture search method based on meta-learning. The idea of NAS and fast adaptation of neural architectures is interesting, and the proposed method is well motivated. The experiments on few-shot learning and supervised learning demonstrate the effectiveness of T- NAS in finding the best meta-architecture for a specific task.","This paper studies the problem of Neural Architecture Search (NAS) in artificial intelligence areas, where the goal is to find the best architecture for a given task from a set of hand-designed networks. Previous NAS methods have been shown to be effective in this setting, but the searching cost can be prohibitively expensive. In this paper, the authors propose a new NAS method called T-NAS, which is a transferable neural architecture search method based on meta-learning. The idea of NAS and fast adaptation of neural architectures is interesting, and the proposed method is well motivated. The experiments on few-shot learning and supervised learning demonstrate the effectiveness of T- NAS in finding the best meta-architecture for a specific task."
21879,SP:1314a79ba12474adb33ff31b3cb22bed25b94fb7,"variational information bottleneck ( VIB ) CONJUNCTION noise regularized learning. noise regularized learning CONJUNCTION variational information bottleneck ( VIB ). dropout CONJUNCTION Bayesian neural networks. Bayesian neural networks CONJUNCTION dropout. Bayesian neural networks CONJUNCTION variational information bottleneck ( VIB ). variational information bottleneck ( VIB ) CONJUNCTION Bayesian neural networks. noise regularized learning HYPONYM-OF paradigms. dropout HYPONYM-OF paradigms. Bayesian neural networks HYPONYM-OF paradigms. variational information bottleneck ( VIB ) HYPONYM-OF paradigms. network compression CONJUNCTION robustness. robustness CONJUNCTION network compression. generalization CONJUNCTION network compression. network compression CONJUNCTION generalization. adversarial attack CONJUNCTION label noise. label noise CONJUNCTION adversarial attack. robustness FEATURE-OF adversarial attack. activation uncertainty CONJUNCTION activation variability. activation variability CONJUNCTION activation uncertainty. pruning CONJUNCTION adversarial defense. adversarial defense CONJUNCTION pruning. SNNs COMPARE SE - SNN. SE - SNN COMPARE SNNs. adversarial defense CONJUNCTION learning with label noise. learning with label noise CONJUNCTION adversarial defense. network compression EVALUATE-FOR SE - SNN. adversarial defense USED-FOR network compression. pruning USED-FOR network compression. Method are Stochastic neural networks ( SNNs ), and neural network variants. Generic is networks. Task is discriminative learning. ","This paper proposes Stochastic neural networks (SNNs), a family of neural network variants that can be seen as a generalization of existing paradigms (e.g., dropout, Bayesian neural networks, variational information bottleneck (VIB), noise regularized learning, etc.). The authors show that these networks are robust to adversarial attack, robust to label noise, and generalize well. They also show that SNNs trained with pruning and adversarial defense outperform SE-SNN in terms of generalization, network compression, robustness, and learning with label noise. Finally, they show that pruning can be used to improve the performance of network compression through pruning, and that discriminative learning can also be used.","This paper proposes Stochastic neural networks (SNNs), a family of neural network variants that can be seen as a generalization of existing paradigms (e.g., dropout, Bayesian neural networks, variational information bottleneck (VIB), noise regularized learning, etc.). The authors show that these networks are robust to adversarial attack, robust to label noise, and generalize well. They also show that SNNs trained with pruning and adversarial defense outperform SE-SNN in terms of generalization, network compression, robustness, and learning with label noise. Finally, they show that pruning can be used to improve the performance of network compression through pruning, and that discriminative learning can also be used."
21888,SP:bd4935d4fcf33f60f22e0f2fd9f7dc8ddfab6d17,"inner loop USED-FOR reinforcement learning. curiosity mechanisms USED-FOR agent ’s reward signal. meta - learning USED-FOR generating curious behavior. reward signal USED-FOR reinforcement learning. reward signal USED-FOR inner loop. transferring neural network weights USED-FOR meta - RL methods. nearest - neighbor modules CONJUNCTION custom loss functions. custom loss functions CONJUNCTION nearest - neighbor modules. buffers CONJUNCTION nearest - neighbor modules. nearest - neighbor modules CONJUNCTION buffers. neural networks PART-OF rich language of programs. custom loss functions PART-OF rich language of programs. image inputs CONJUNCTION acrobot. acrobot CONJUNCTION image inputs. curiosity algorithms COMPARE human - designed published curiosity algorithms. human - designed published curiosity algorithms COMPARE curiosity algorithms. acrobot CONJUNCTION ant. ant CONJUNCTION acrobot. grid navigation CONJUNCTION acrobot. acrobot CONJUNCTION grid navigation. image inputs USED-FOR grid navigation. OtherScientificTerm are curiosity, and outer loop. Method are evolution, and meta - learn algorithms. Material is ML papers. Generic is approach. ","This paper proposes a meta-learning approach for learning curiosity mechanisms for reinforcement learning. The core idea is to train an agent’s reward signal using a set of curiosity mechanisms, and then use this reward signal to guide the inner loop of reinforcement learning by transferring neural network weights to the outer loop. The authors claim that this approach is useful for generating curious behavior that can be leveraged for meta-learning.   The authors propose a rich language of programs that includes neural networks, buffers, nearest-neighbor modules, and custom loss functions. They show experiments on grid navigation, grid navigation with image inputs, acrobot, and ant, and show that their curiosity algorithms outperform human-designed published curiosity algorithms. They also show that the evolution of these algorithms is similar to that of existing ML papers.  The paper is well-written and well-motivated, and the paper is clearly written.  However, there are a few issues with the paper:  1. The paper does not provide a clear definition of curiosity, which makes it difficult to understand the approach.  2. It is not clear how to compare the performance of different meta-learner algorithms.  3. There is a lack of discussion about the benefits and drawbacks of the approach, and it is unclear whether the approach can be applied to more complex problems.  4. The approach is not well-suited for transfer learning, as it does not seem to be able to transfer across different tasks. ","This paper proposes a meta-learning approach for learning curiosity mechanisms for reinforcement learning. The core idea is to train an agent’s reward signal using a set of curiosity mechanisms, and then use this reward signal to guide the inner loop of reinforcement learning by transferring neural network weights to the outer loop. The authors claim that this approach is useful for generating curious behavior that can be leveraged for meta-learning.   The authors propose a rich language of programs that includes neural networks, buffers, nearest-neighbor modules, and custom loss functions. They show experiments on grid navigation, grid navigation with image inputs, acrobot, and ant, and show that their curiosity algorithms outperform human-designed published curiosity algorithms. They also show that the evolution of these algorithms is similar to that of existing ML papers.  The paper is well-written and well-motivated, and the paper is clearly written.  However, there are a few issues with the paper:  1. The paper does not provide a clear definition of curiosity, which makes it difficult to understand the approach.  2. It is not clear how to compare the performance of different meta-learner algorithms.  3. There is a lack of discussion about the benefits and drawbacks of the approach, and it is unclear whether the approach can be applied to more complex problems.  4. The approach is not well-suited for transfer learning, as it does not seem to be able to transfer across different tasks. "
21897,SP:6dff0f3a84809ae0ba9f58f36303597f1ba6dcc5,"approach USED-FOR AnyC2C. approach USED-FOR code snippet. strict syntax of programming languages USED-FOR code snippet. tree – structural language modeling ( SLM ) USED-FOR code snippet. strict syntax of programming languages USED-FOR approach. neural model USED-FOR conditional probabilities. AST paths USED-FOR neural model. structural techniques COMPARE approach. approach COMPARE structural techniques. structural techniques USED-FOR expressions. structured approaches USED-FOR Java and C # code. model COMPARE seq2seq. seq2seq COMPARE model. model COMPARE structured approaches. structured approaches COMPARE model. model USED-FOR Java and C # code. seq2seq CONJUNCTION structured approaches. structured approaches CONJUNCTION seq2seq. Generic are problem, it, and task. OtherScientificTerm are structural information, and programming language. Method is SLM. ","This paper proposes a new approach for AnyC2C, an approach for learning a code snippet from the strict syntax of programming languages using tree–structural language modeling (SLM). The problem is interesting because it can be seen as a special case of the task of learning a sequence of expressions that encode structural information in a programming language. The authors propose a neural model that uses AST paths to learn the conditional probabilities of the expressions, which are then used to train a SLM. They show that their model outperforms seq2seq and other structured approaches for Java and C# code, and that their approach is more interpretable than other structural techniques for learning expressions.","This paper proposes a new approach for AnyC2C, an approach for learning a code snippet from the strict syntax of programming languages using tree–structural language modeling (SLM). The problem is interesting because it can be seen as a special case of the task of learning a sequence of expressions that encode structural information in a programming language. The authors propose a neural model that uses AST paths to learn the conditional probabilities of the expressions, which are then used to train a SLM. They show that their model outperforms seq2seq and other structured approaches for Java and C# code, and that their approach is more interpretable than other structural techniques for learning expressions."
21906,SP:7fc60d6fd1cfcc135c34f9664d172d3fd1c0ae0a,"gradient descent methods USED-FOR non - convex optimization problems. objective functions USED-FOR NNs. NN model space CONJUNCTION canonical space. canonical space CONJUNCTION NN model space. disparity matrix HYPONYM-OF pointwise linear transformation. pointwise linear transformation USED-FOR gradients. gradient descent methods USED-FOR global minimum of zero loss. full rank FEATURE-OF disparity matrices. learning of NNs COMPARE normal convex optimization. normal convex optimization COMPARE learning of NNs. gradient decent algorithms USED-FOR global minimum of zero loss. Method are large - scale neural networks ( NN ), large NNs, and over - parameterized NNs. OtherScientificTerm are canonical model space, full - rank condition, and singular disparity matrices. ","This paper studies gradient descent methods for non-convex optimization problems in large-scale neural networks (NN). The authors consider the case where the objective functions of NNs are non-vanishingly convex. They show that the gradients of a pointwise linear transformation (i.e., the disparity matrix) between the NN model space and the canonical space is non-zero, which is a result of the fact that the canonical model space is not a full-rank space. They also show that under this setting, large NNs can be seen as over-parameterized NNs. The authors then show that gradient descent algorithms can achieve a global minimum of zero loss using gradient decent algorithms. The main contribution of the paper is that the authors prove that under the full rank condition on the disparity matrices, the learning of such NNs is asymptotically equivalent to normal convex optimization. ","This paper studies gradient descent methods for non-convex optimization problems in large-scale neural networks (NN). The authors consider the case where the objective functions of NNs are non-vanishingly convex. They show that the gradients of a pointwise linear transformation (i.e., the disparity matrix) between the NN model space and the canonical space is non-zero, which is a result of the fact that the canonical model space is not a full-rank space. They also show that under this setting, large NNs can be seen as over-parameterized NNs. The authors then show that gradient descent algorithms can achieve a global minimum of zero loss using gradient decent algorithms. The main contribution of the paper is that the authors prove that under the full rank condition on the disparity matrices, the learning of such NNs is asymptotically equivalent to normal convex optimization. "
21915,SP:78a536138570fe9b5d88350e4b16d598a7db1fe0,"Large - scale ground truth data sets USED-FOR deep learning based segmentation models. interactive graph - based segmentation algorithms USED-FOR connectivity. instanceaware heuristic USED-FOR discrete Potts model. feature maps USED-FOR DCNN. feature maps USED-FOR algorithms. RGB USED-FOR algorithms. PASCAL VOC 2012 CONJUNCTION Cityscapes dataset. Cityscapes dataset CONJUNCTION PASCAL VOC 2012. Cityscapes dataset EVALUATE-FOR semantic ( and panoptic ) segmentation. PASCAL VOC 2012 EVALUATE-FOR semantic ( and panoptic ) segmentation. VOC validation set EVALUATE-FOR mIoU. mIoU EVALUATE-FOR interactive approach. VOC validation set EVALUATE-FOR interactive approach. They USED-FOR interactive annotation. They USED-FOR weakly supervised learning framework. OtherScientificTerm are global optimum, and scribbles. ","This paper proposes a novel method for learning deep learning based segmentation models from large-scale ground truth data sets. The authors propose interactive graph-based segmentation algorithms that encourage connectivity between different nodes in the graph. They propose a discrete Potts model based on an instanceaware heuristic, where each node is represented as a global optimum. The algorithms are trained on RGB and use feature maps from a DCNN. They are evaluated on semantic (and panoptic) segmentation on PASCAL VOC 2012 and the Cityscapes dataset. The interactive approach is also evaluated on the VOC validation set for mIoU. They also propose a weakly supervised learning framework, where they use a few scribbles to guide the learning process. They demonstrate that interactive annotation can be used to improve the performance. ","This paper proposes a novel method for learning deep learning based segmentation models from large-scale ground truth data sets. The authors propose interactive graph-based segmentation algorithms that encourage connectivity between different nodes in the graph. They propose a discrete Potts model based on an instanceaware heuristic, where each node is represented as a global optimum. The algorithms are trained on RGB and use feature maps from a DCNN. They are evaluated on semantic (and panoptic) segmentation on PASCAL VOC 2012 and the Cityscapes dataset. The interactive approach is also evaluated on the VOC validation set for mIoU. They also propose a weakly supervised learning framework, where they use a few scribbles to guide the learning process. They demonstrate that interactive annotation can be used to improve the performance. "
21924,SP:2eb90879ddbc39b6b5c05152784d6044d1940513,"salient features FEATURE-OF image. saliency tools USED-FOR adversarial examples. salient features USED-FOR defense. it COMPARE baseline. baseline COMPARE it. gradient - based saliency tools USED-FOR adversarial defense. model USED-FOR baseline. saliency map USED-FOR baseline. learnt saliency models USED-FOR saliency. computational cost EVALUATE-FOR learnt saliency models. saliency models USED-FOR real - time defense. learnt saliency model USED-FOR defense. adversarial images CONJUNCTION natural images. natural images CONJUNCTION adversarial images. CNN USED-FOR adversarial images. CNN USED-FOR natural images. CNN HYPONYM-OF defense. salient pixels USED-FOR CNN. CIFAR-10 CONJUNCTION ASSIRA. ASSIRA CONJUNCTION CIFAR-10. defense USED-FOR adversarial attacks. MNIST CONJUNCTION CIFAR-10. CIFAR-10 CONJUNCTION MNIST. MNIST EVALUATE-FOR defense. ASSIRA EVALUATE-FOR defense. CIFAR-10 EVALUATE-FOR defense. saliency map USED-FOR defense. C&W CONJUNCTION DeepFool. DeepFool CONJUNCTION C&W. weak defenses USED-FOR adversarial images. attacks USED-FOR adversarial images. DeepFool USED-FOR adversarial images. DeepFool HYPONYM-OF attacks. C&W HYPONYM-OF attacks. OtherScientificTerm are Adversarial perturbations, misclassification, and adversarial perturbations. ","This paper proposes to use saliency tools to detect adversarial examples based on salient features of an image. Adversarial perturbations to the saliency map of the image can lead to misclassification. The authors show that adversarial defense can be achieved using gradient-based saliency tool, and that it outperforms a baseline based on the same model. They also show that real-time defense with learnt saliency models can reduce the computational cost of saliency.    The paper also shows that a real defense against adversarial attacks can be performed with a learnt salient model. The defense is based on a CNN trained on salient pixels of the adversarial images and natural images, and the authors demonstrate that the learned saliency model can be used to improve the performance of the defense. They evaluate the defense on MNIST, CIFAR-10, and ASSIRA, and show that the proposed defense outperforms the baseline trained with the salency map. They further show that weak defenses can be applied to adversarial image generated by attacks such as C&W and DeepFool. ","This paper proposes to use saliency tools to detect adversarial examples based on salient features of an image. Adversarial perturbations to the saliency map of the image can lead to misclassification. The authors show that adversarial defense can be achieved using gradient-based saliency tool, and that it outperforms a baseline based on the same model. They also show that real-time defense with learnt saliency models can reduce the computational cost of saliency.    The paper also shows that a real defense against adversarial attacks can be performed with a learnt salient model. The defense is based on a CNN trained on salient pixels of the adversarial images and natural images, and the authors demonstrate that the learned saliency model can be used to improve the performance of the defense. They evaluate the defense on MNIST, CIFAR-10, and ASSIRA, and show that the proposed defense outperforms the baseline trained with the salency map. They further show that weak defenses can be applied to adversarial image generated by attacks such as C&W and DeepFool. "
21933,SP:fe5510d05ff091a5f133f2dbcd1b23d8d58d2c3e,"global adversarial robustness guarantees FEATURE-OF machine learning models. measurability FEATURE-OF local robustness properties. concentration inequalities USED-FOR global robustness. Fashion - MNIST CONJUNCTION CIFAR. CIFAR CONJUNCTION Fashion - MNIST. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. neural networks architectures CONJUNCTION training methods. training methods CONJUNCTION neural networks architectures. robustness / accuracy trade - off EVALUATE-FOR neural networks architectures. training methods USED-FOR MNIST. training methods USED-FOR Fashion - MNIST. robustness EVALUATE-FOR networks. accuracy EVALUATE-FOR networks. robustness CONJUNCTION accuracy. accuracy CONJUNCTION robustness. stochastic gradient descent CONJUNCTION iterative pruning techniques. iterative pruning techniques CONJUNCTION stochastic gradient descent. Bayesian settings FEATURE-OF them. iterative pruning techniques USED-FOR networks. stochastic gradient descent USED-FOR networks. Generic are model, and methods. OtherScientificTerm are adversarial attacks, and estimation error. ","This paper studies the global adversarial robustness guarantees of machine learning models. The authors consider the notion of local robustness properties, i.e., the measurability of a model to adversarial attacks, and derive concentration inequalities for global robustness. They show that the robustness/accuracy trade-off between neural networks architectures and training methods for MNIST, Fashion-MNIST, and CIFAR is highly correlated. They also show that networks trained with stochastic gradient descent and iterative pruning techniques can achieve robustness and accuracy trade-offs that are highly correlated with each other. They further show that these methods are Bayesian and that they can be combined with existing methods. Finally, they show that both of them work well in Bayesian settings and that the estimation error is low. ","This paper studies the global adversarial robustness guarantees of machine learning models. The authors consider the notion of local robustness properties, i.e., the measurability of a model to adversarial attacks, and derive concentration inequalities for global robustness. They show that the robustness/accuracy trade-off between neural networks architectures and training methods for MNIST, Fashion-MNIST, and CIFAR is highly correlated. They also show that networks trained with stochastic gradient descent and iterative pruning techniques can achieve robustness and accuracy trade-offs that are highly correlated with each other. They further show that these methods are Bayesian and that they can be combined with existing methods. Finally, they show that both of them work well in Bayesian settings and that the estimation error is low. "
21942,SP:8f5616a1480b68c04b496ed498d237d5a7e87794,"robustness FEATURE-OF environmental dynamics. learning algorithms USED-FOR robustness. robustness FEATURE-OF system dynamics. transition probability HYPONYM-OF system dynamics. Wasserstein distance USED-FOR disturbance. infinite - dimensional optimization problem CONJUNCTION finite - dimensional risk - aware problem. finite - dimensional risk - aware problem CONJUNCTION infinite - dimensional optimization problem. risk - aware optimal Bellman equation USED-FOR optimal robust policies. sensitivity analysis USED-FOR perturbations. Wasserstein Robust HYPONYM-OF robust learning algorithm. Cart - Pole environment EVALUATE-FOR algorithm. OtherScientificTerm are optimal policy, simulated environmental parameters, reference transition kernel, transition kernel disturbance, and state disturbance. ","This paper studies the problem of robustness to environmental dynamics in learning algorithms. In particular, the authors consider the robustness of system dynamics (e.g., transition probability) to perturbations in the environment. The authors propose a novel robust learning algorithm, called Wasserstein Robust, which is based on the observation that the optimal policy should be robust to environmental perturbation in the presence of a transition kernel disturbance. The disturbance is modeled as a Wassersteinsatz distance between the simulated environmental parameters and the reference transition kernel, and the authors prove that the Wasserststein distance is a function of the disturbance.  The authors then propose an infinite-dimensional optimization problem and a finite-dimensional risk-aware problem, and prove that optimal robust policies can be found by solving the risk-adversarial optimal Bellman equation. They also provide a sensitivity analysis of the perturbated state of the system, and propose a new algorithm, named WASSERSTEIN Robust. The proposed algorithm is tested on the Cart-Pole environment, and is shown to be robust against the transition kernel and state disturbance, as well as to be sensitive to state disturbance.","This paper studies the problem of robustness to environmental dynamics in learning algorithms. In particular, the authors consider the robustness of system dynamics (e.g., transition probability) to perturbations in the environment. The authors propose a novel robust learning algorithm, called Wasserstein Robust, which is based on the observation that the optimal policy should be robust to environmental perturbation in the presence of a transition kernel disturbance. The disturbance is modeled as a Wassersteinsatz distance between the simulated environmental parameters and the reference transition kernel, and the authors prove that the Wasserststein distance is a function of the disturbance.  The authors then propose an infinite-dimensional optimization problem and a finite-dimensional risk-aware problem, and prove that optimal robust policies can be found by solving the risk-adversarial optimal Bellman equation. They also provide a sensitivity analysis of the perturbated state of the system, and propose a new algorithm, named WASSERSTEIN Robust. The proposed algorithm is tested on the Cart-Pole environment, and is shown to be robust against the transition kernel and state disturbance, as well as to be sensitive to state disturbance."
21951,SP:d85963f5f0f6b20cf08f2a7c169ae33a45db7de2,"Nash equilibrium PART-OF multi - player games. deep learning based approaches USED-FOR pure strategy Nash equilibrium. method USED-FOR mixed strategy Nash equilibria. multi - player continuous games FEATURE-OF mixed strategy Nash equilibria. pure ones PART-OF method. pushforward measure technique USED-FOR mixed strategy. continuous spaces FEATURE-OF mixed strategy. joint strategy profile CONJUNCTION Nash equilibrium. Nash equilibrium CONJUNCTION joint strategy profile. gradient descent algorithm USED-FOR approach. convexity assumption FEATURE-OF payoff functions. approach USED-FOR stationary Nash equilibrium. blotto games CONJUNCTION GAMUT games. GAMUT games CONJUNCTION blotto games. method COMPARE works. works COMPARE method. quadratic games CONJUNCTION blotto games. blotto games CONJUNCTION quadratic games. works USED-FOR Nash equilibrium. method USED-FOR Nash equilibrium. approximating Nash equilibrium USED-FOR quadratic games. approximating Nash equilibrium CONJUNCTION blotto games. blotto games CONJUNCTION approximating Nash equilibrium. GAMUT games EVALUATE-FOR method. OtherScientificTerm are continuous strategy spaces, and pure strategy weakness. Task is generative adversarial networks. Generic is equilibrium. ","This paper proposes a method for finding mixed strategy Nash equilibria in multi-player continuous games, which is an important problem in the context of deep learning based approaches for finding a pure strategy Nash equilibrium for multi-agent games. The proposed method is based on the observation that the mixed strategy in continuous strategy spaces can be approximated by a pushforward measure technique, and the authors propose a method that combines the pure ones with the mixed ones. The mixed strategy is approximated in continuous spaces, and a convexity assumption on the payoff functions is made. The authors also propose a gradient descent algorithm for the proposed approach, and show that the approach converges to a stationary Nash equilibrium. The method is tested on quadratic games, blotto games, and GAMUT games, where the proposed method outperforms previous works in finding a Nash equilibrium, and is shown to be more robust to the pure strategy weakness in the case of generative adversarial networks.   ","This paper proposes a method for finding mixed strategy Nash equilibria in multi-player continuous games, which is an important problem in the context of deep learning based approaches for finding a pure strategy Nash equilibrium for multi-agent games. The proposed method is based on the observation that the mixed strategy in continuous strategy spaces can be approximated by a pushforward measure technique, and the authors propose a method that combines the pure ones with the mixed ones. The mixed strategy is approximated in continuous spaces, and a convexity assumption on the payoff functions is made. The authors also propose a gradient descent algorithm for the proposed approach, and show that the approach converges to a stationary Nash equilibrium. The method is tested on quadratic games, blotto games, and GAMUT games, where the proposed method outperforms previous works in finding a Nash equilibrium, and is shown to be more robust to the pure strategy weakness in the case of generative adversarial networks.   "
21960,SP:280d85cd8164a268f9d496ae5f17189c50f30dc1,"deep neural networks USED-FOR NLP tasks. labeled data USED-FOR data - hungry models. sufficient domain knowledge USED-FOR labeled data. supervision USED-FOR sufficient domain knowledge. supervision FEATURE-OF Natural language ( NL ) explanations. them USED-FOR augmenting model learning. modularized model USED-FOR semantics. linguistic variants FEATURE-OF NL explanations. Neural Execution Tree ( NExT ) framework1 USED-FOR text classification. NL explanations USED-FOR Neural Execution Tree ( NExT ) framework1. NExT USED-FOR actions. NL explanations USED-FOR executable logical forms. logical forms USED-FOR actions. semantic parsing USED-FOR NL explanations. semantic parsing USED-FOR executable logical forms. relation extraction CONJUNCTION sentiment analysis. sentiment analysis CONJUNCTION relation extraction. NLP tasks EVALUATE-FOR baseline methods. sentiment analysis HYPONYM-OF NLP tasks. relation extraction HYPONYM-OF NLP tasks. Task are data annotation, and multi - hop question answering. Metric is annotation time. Method is model learning. OtherScientificTerm is NL explanation. ","Natural language (NL) explanations can be used to augment deep neural networks for NLP tasks that require data annotation. However, the amount of labeled data for training data-hungry models can be prohibitively expensive, especially for tasks where sufficient domain knowledge is needed to obtain sufficient supervision for the labeled data. Natural language explanations (NL explanations) have been shown to be useful for augmenting model learning by augmenting them with additional supervision from NL explanations.   This paper proposes a Neural Execution Tree (NExT) framework1 for text classification that uses NL explanations from different linguistic variants to augment the model learning. The idea is to use a modularized model to learn the semantics of the NL explanations, and then use them to augment augmenting the training data. NExT is able to generate executable logical forms based on NL explanations using semantic parsing, which are then used to perform actions based on the logical forms. Experiments are conducted on a number of standard NLP benchmarks, including relation extraction, sentiment analysis, and multi-hop question answering, where NL explanation is used to improve the performance of the baseline methods. The results show that the proposed method can reduce the annotation time by a factor of at most 1.5. ","Natural language (NL) explanations can be used to augment deep neural networks for NLP tasks that require data annotation. However, the amount of labeled data for training data-hungry models can be prohibitively expensive, especially for tasks where sufficient domain knowledge is needed to obtain sufficient supervision for the labeled data. Natural language explanations (NL explanations) have been shown to be useful for augmenting model learning by augmenting them with additional supervision from NL explanations.   This paper proposes a Neural Execution Tree (NExT) framework1 for text classification that uses NL explanations from different linguistic variants to augment the model learning. The idea is to use a modularized model to learn the semantics of the NL explanations, and then use them to augment augmenting the training data. NExT is able to generate executable logical forms based on NL explanations using semantic parsing, which are then used to perform actions based on the logical forms. Experiments are conducted on a number of standard NLP benchmarks, including relation extraction, sentiment analysis, and multi-hop question answering, where NL explanation is used to improve the performance of the baseline methods. The results show that the proposed method can reduce the annotation time by a factor of at most 1.5. "
21969,SP:a9b5f7257dedd719cfe341fca275776734af1d98,"robustness FEATURE-OF misclassification. machine learning models USED-FOR Formal verification. robustness HYPONYM-OF properties. it USED-FOR complex specifications. it USED-FOR recurrent neural network architectures. recurrent neural network architectures CONJUNCTION complex specifications. complex specifications CONJUNCTION recurrent neural network architectures. specifications USED-FOR temporal properties. adversarial robustness FEATURE-OF complex specifications. it USED-FOR verified training. specifications HYPONYM-OF complex specifications. verified training method USED-FOR models. verified training method USED-FOR models. training USED-FOR models. OtherScientificTerm are perturbations of the input features, and desired specifications. Method are verification procedure, verifiably robust models, and language model. ","Formal verification of machine learning models is an important problem. Formal verification aims to verify two properties: (1) robustness to perturbations of the input features, and (2) misclassification in the presence of adversarial robustness. This paper proposes a new verification procedure, called verifiably robust models, which is based on the notion of desired specifications. Specifically, it can be applied to recurrent neural network architectures and complex specifications (i.e., specifications for temporal properties) and it allows for verified training of models that are robust to adversarial attacks. The paper also proposes a verified training method that can be used to train models that have been trained with adversarial training. Experiments show that the proposed training method is able to verify that models trained with this training are robust against adversarial examples.  ","Formal verification of machine learning models is an important problem. Formal verification aims to verify two properties: (1) robustness to perturbations of the input features, and (2) misclassification in the presence of adversarial robustness. This paper proposes a new verification procedure, called verifiably robust models, which is based on the notion of desired specifications. Specifically, it can be applied to recurrent neural network architectures and complex specifications (i.e., specifications for temporal properties) and it allows for verified training of models that are robust to adversarial attacks. The paper also proposes a verified training method that can be used to train models that have been trained with adversarial training. Experiments show that the proposed training method is able to verify that models trained with this training are robust against adversarial examples.  "
21978,SP:3903680e07b676409e3cf6a1044b67291fe38630,"learned state representations USED-FOR constant. technique COMPARE domain randomization. domain randomization COMPARE technique. generalization scores EVALUATE-FOR domain randomization. generalization scores EVALUATE-FOR technique. Task are reinforcement learning, and visual domain randomization problem. Generic is method. Method are visual domain randomization, and regularization method. OtherScientificTerm are policies, and randomization parameters. ","This paper studies the problem of domain randomization in reinforcement learning. The authors propose a novel method to regularize the randomness of the policy distribution of the learned state representations. The proposed method is based on the idea of visual domain randomisation, where the goal is to find a constant that maximizes the generalization performance of policies trained on the visual domain.  The authors show that the proposed regularization method can be applied to any randomization parameters. They also show that their technique achieves better generalization scores than the state-of-the-art method of standard randomization. ","This paper studies the problem of domain randomization in reinforcement learning. The authors propose a novel method to regularize the randomness of the policy distribution of the learned state representations. The proposed method is based on the idea of visual domain randomisation, where the goal is to find a constant that maximizes the generalization performance of policies trained on the visual domain.  The authors show that the proposed regularization method can be applied to any randomization parameters. They also show that their technique achieves better generalization scores than the state-of-the-art method of standard randomization. "
21987,SP:c79046dc56b9ee9c926f87386046422ea134ae8d,"deep learning USED-FOR Deep metric learning ( DML ). complicated losses CONJUNCTION hard example mining methods. hard example mining methods CONJUNCTION complicated losses. pairwise binary classification problem USED-FOR DML. framework USED-FOR model. distributionally robust optimization USED-FOR robust loss. uncertainty decision set FEATURE-OF dual variable. uncertainty decision set USED-FOR complicated losses. benchmark data sets EVALUATE-FOR method. method COMPARE state. state COMPARE method. Task is computer vision. Generic are problem, and variants. OtherScientificTerm is imbalanced data pairs. ","Deep metric learning (DML) is an important problem in computer vision. Deep metric learning is a special case of deep learning in which complicated losses and hard example mining methods are considered. DML is a pairwise binary classification problem, and the authors consider the problem of imbalanced data pairs. In this paper, the authors propose a new framework to learn a model that is robust to imbalanced pairs. The authors propose to use distributionally robust optimization to learn the robust loss. They also propose two variants of complicated losses based on the uncertainty decision set of the dual variable. Experiments on benchmark data sets show that the proposed method outperforms the state of the art. ","Deep metric learning (DML) is an important problem in computer vision. Deep metric learning is a special case of deep learning in which complicated losses and hard example mining methods are considered. DML is a pairwise binary classification problem, and the authors consider the problem of imbalanced data pairs. In this paper, the authors propose a new framework to learn a model that is robust to imbalanced pairs. The authors propose to use distributionally robust optimization to learn the robust loss. They also propose two variants of complicated losses based on the uncertainty decision set of the dual variable. Experiments on benchmark data sets show that the proposed method outperforms the state of the art. "
21996,SP:38420928e40ef80c0136ad607b9275f9ab1e0769,"local minimum PART-OF non - convex finite - sum minimization. inexact gradient and Hessian estimation USED-FOR trust region method. stochastic trust region ( STR ) algorithm USED-FOR (, √ ) -approximate local minimum. runtime complexity EVALUATE-FOR Hessian - free STR algorithms. Metric is convergence rate. Method are differential estimations, and Hessian estimator. OtherScientificTerm is stochastic Hessian oracle queries. Generic is algorithms. ","This paper considers the problem of finding the local minimum in non-convex finite-sum minimization. The authors propose a trust region method based on inexact gradient and Hessian estimation, and show a convergence rate of O(1/\sqrt{T}^T) and O(T^T). They also show that the (,√)-approximate local minimum can be found using the stochastic trust region (STR) algorithm. The main contribution of this paper is that the authors show that Hessian-free STR algorithms have a runtime complexity of O(\sqrt{\log T}) and O(\log T) times faster than existing algorithms.   The authors also provide a theoretical analysis that shows that the differential estimations of the Hessian of the local minimizer can be approximated by a simple Hessian estimator, and that the convergence rate depends only on the number of times that the Hessians are estimated.  Finally, the authors provide some numerical experiments to show the effectiveness of the proposed algorithms.","This paper considers the problem of finding the local minimum in non-convex finite-sum minimization. The authors propose a trust region method based on inexact gradient and Hessian estimation, and show a convergence rate of O(1/\sqrt{T}^T) and O(T^T). They also show that the (,√)-approximate local minimum can be found using the stochastic trust region (STR) algorithm. The main contribution of this paper is that the authors show that Hessian-free STR algorithms have a runtime complexity of O(\sqrt{\log T}) and O(\log T) times faster than existing algorithms.   The authors also provide a theoretical analysis that shows that the differential estimations of the Hessian of the local minimizer can be approximated by a simple Hessian estimator, and that the convergence rate depends only on the number of times that the Hessians are estimated.  Finally, the authors provide some numerical experiments to show the effectiveness of the proposed algorithms."
22005,SP:28a35b70b5e6915af28cacebc4ea50690c9534af,"batch normalization CONJUNCTION weight initialization. weight initialization CONJUNCTION batch normalization. linear programming USED-FOR Farkas layers. benchmark datasets EVALUATE-FOR network sizes. ReLU activation USED-FOR residual networks. Method are deep neural networks, and geometrically motivated method. Generic is method. Metric is training capacity. OtherScientificTerm is initialization. ","This paper proposes a geometrically motivated method to reduce the size of deep neural networks. The proposed method is motivated by the observation that batch normalization and weight initialization are the main causes of over-parameterization in training neural networks, and that Farkas layers can be decomposed into linear programming. The authors show that the ReLU activation of residual networks can be reduced to linear programming, which can be used to reduce network sizes on standard benchmark datasets. The paper also shows that the proposed method can reduce the training capacity by up to 1.5x while maintaining the same initialization.  ","This paper proposes a geometrically motivated method to reduce the size of deep neural networks. The proposed method is motivated by the observation that batch normalization and weight initialization are the main causes of over-parameterization in training neural networks, and that Farkas layers can be decomposed into linear programming. The authors show that the ReLU activation of residual networks can be reduced to linear programming, which can be used to reduce network sizes on standard benchmark datasets. The paper also shows that the proposed method can reduce the training capacity by up to 1.5x while maintaining the same initialization.  "
22014,SP:1d325b148e3efe407241c1f1cbe8d17400499741,"decision boundary PART-OF classifier. adversarial examples FEATURE-OF robustness certificate. minimum distance FEATURE-OF robustness certificate. robustness certificates USED-FOR deep classifiers. nonconvex optimization USED-FOR it. computationally - efficient robustness certificates USED-FOR deep classifiers. differentiable activation functions USED-FOR computationally - efficient robustness certificates. eigenvalues FEATURE-OF Hessian. Hessian FEATURE-OF network. l2 norm FEATURE-OF robustness certificate. convex optimization USED-FOR robustness certificate. curvature FEATURE-OF deep network. computationallyefficient differentiable upper bound USED-FOR deep network. curvature FEATURE-OF computationallyefficient differentiable upper bound. curvature bound USED-FOR regularization term. regularization term USED-FOR network. curvature bound USED-FOR network. adversarial examples FEATURE-OF certified robustness. Curvature - based Robustness Certificate ( CRC ) CONJUNCTION Curvature - based Robust Training ( CRT ). Curvature - based Robust Training ( CRT ) CONJUNCTION Curvature - based Robustness Certificate ( CRC ). CRT COMPARE adversarial training. adversarial training COMPARE CRT. CRC COMPARE CROWN ’s certificate. CROWN ’s certificate COMPARE CRC. certified accuracy EVALUATE-FOR adversarial training. CRC COMPARE CRT. CRT COMPARE CRC. CRC COMPARE adversarial training. adversarial training COMPARE CRC. regularizer USED-FOR CRC. regularizer USED-FOR CROWN ’s certificate. certified accuracy EVALUATE-FOR CRC. certified accuracy EVALUATE-FOR CRT. OtherScientificTerm are lower bound, and classification output. ","This paper proposes two computationally-efficient robustness certificates for deep classifiers based on differentiable activation functions. The idea is that the decision boundary of a classifier is a function of the eigenvalues of the Hessian of the parameters of the network, and the robustness certificate is defined as the minimum distance between the classifier’s decision boundary and the boundary of an adversarial examples of the same classifier. The authors prove a lower bound on the l2 norm of the robusts certificate, and show that it can be computed efficiently via nonconvex optimization. They also show that the curvature of a deep network can be used as a computationally efficient differentiable upper bound for the certified robustness of a network.    The authors also propose Curvature-based Robustness Certificate (CRC) as a regularization term for a network based on curvature, and CRT as an efficient regularizer for a trained deep network. They show that CRT achieves better certified accuracy on adversarial attacks compared to adversarial training, and that the proposed regularizer is more effective than CROWN’S certificate, which is based on the classification output. ","This paper proposes two computationally-efficient robustness certificates for deep classifiers based on differentiable activation functions. The idea is that the decision boundary of a classifier is a function of the eigenvalues of the Hessian of the parameters of the network, and the robustness certificate is defined as the minimum distance between the classifier’s decision boundary and the boundary of an adversarial examples of the same classifier. The authors prove a lower bound on the l2 norm of the robusts certificate, and show that it can be computed efficiently via nonconvex optimization. They also show that the curvature of a deep network can be used as a computationally efficient differentiable upper bound for the certified robustness of a network.    The authors also propose Curvature-based Robustness Certificate (CRC) as a regularization term for a network based on curvature, and CRT as an efficient regularizer for a trained deep network. They show that CRT achieves better certified accuracy on adversarial attacks compared to adversarial training, and that the proposed regularizer is more effective than CROWN’S certificate, which is based on the classification output. "
22023,SP:33f6f5aa0d4655e5d75fe612e0eff05e579d45c5,"method USED-FOR compressed sensing recovery. untrained deep generative models USED-FOR method. convolutional weights PART-OF network. Deep Image Prior ( DIP ) USED-FOR method. approach USED-FOR differentiable linear inverse problem. approaches COMPARE method. method COMPARE approaches. generative models USED-FOR approaches. prior information FEATURE-OF network weights. prior information PART-OF learned regularization technique. DIP optimization approach USED-FOR overparameterized single - layer networks. Method are unlearned methods, and early stopping. OtherScientificTerm are pre - training, and noisy measurements. Metric is reconstruction error. Task is fitting problem. ","This paper proposes a method for compressed sensing recovery from untrained deep generative models. The proposed method is based on Deep Image Prior (DIP) and uses a learned regularization technique to incorporate prior information into the network weights. The approach is formulated as a differentiable linear inverse problem, and the authors show that the reconstruction error can be reduced to zero when the convolutional weights of the network are learned during pre-training. Compared to existing approaches that use differentiable models, the proposed method achieves better results. The authors also propose a DIP optimization approach for overparameterized single-layer networks, which can be applied to unlearned methods. The paper also proposes an early stopping to prevent over-parametrized networks from overfitting to noisy measurements.   ","This paper proposes a method for compressed sensing recovery from untrained deep generative models. The proposed method is based on Deep Image Prior (DIP) and uses a learned regularization technique to incorporate prior information into the network weights. The approach is formulated as a differentiable linear inverse problem, and the authors show that the reconstruction error can be reduced to zero when the convolutional weights of the network are learned during pre-training. Compared to existing approaches that use differentiable models, the proposed method achieves better results. The authors also propose a DIP optimization approach for overparameterized single-layer networks, which can be applied to unlearned methods. The paper also proposes an early stopping to prevent over-parametrized networks from overfitting to noisy measurements.   "
22032,SP:23c0f621e6041003b59bf0532130760694cf6a4a,"reinforcement learning ( RL ) USED-FOR real - world problems. long time horizons FEATURE-OF action - reward correlation. hand - tuned network structure CONJUNCTION pre - defined subgoals. pre - defined subgoals CONJUNCTION hand - tuned network structure. hand - tuned network structure PART-OF hierarchies. pre - defined subgoals PART-OF hierarchies. HRL framework USED-FOR temporal abstraction. TAIC USED-FOR temporal abstraction. approach USED-FOR latent space. information - theoretic constraints USED-FOR approach. latent representations of action sequences USED-FOR temporal abstraction problem. latent variables CONJUNCTION state changes. state changes CONJUNCTION latent variables. algorithm USED-FOR abstraction of the long action sequences. abstraction USED-FOR tasks. convergence rate CONJUNCTION sample efficiency. sample efficiency CONJUNCTION convergence rate. sample efficiency EVALUATE-FOR RL algorithms. convergence rate EVALUATE-FOR RL algorithms. convergence rate EVALUATE-FOR technique. sample efficiency EVALUATE-FOR technique. Method are Hierarchical reinforcement learning ( HRL ) methods, and temporal abstractions. OtherScientificTerm are task - specific knowledge, and visualization of the latent space. Metric is mutual information. Material is benchmark learning problems. ","This paper proposes a hierarchical reinforcement learning (HRL) approach for long-horizon RL problems with long-range dependencies. The authors propose a new algorithm TAIC, which is based on the TAIC framework. TAIC is a hierarchical RL algorithm that learns a latent representation of the long-term history of a sequence of actions. The paper shows that the proposed algorithm achieves better sample efficiency and convergence rate compared to other HRL algorithms.  ","This paper proposes a hierarchical reinforcement learning (HRL) approach for long-horizon RL problems with long-range dependencies. The authors propose a new algorithm TAIC, which is based on the TAIC framework. TAIC is a hierarchical RL algorithm that learns a latent representation of the long-term history of a sequence of actions. The paper shows that the proposed algorithm achieves better sample efficiency and convergence rate compared to other HRL algorithms.  "
22041,SP:4e54c9196ba1eb2b6a0b0eee41e4a6f3a9de72dd,"Graph Convolutional Network ( GCN ) USED-FOR graph representation learning. small graphs USED-FOR shallow models. acceleration methods USED-FOR GCNs. larger graphs CONJUNCTION deeper layers. deeper layers CONJUNCTION larger graphs. GCN - like models USED-FOR deeper layers. GCN - like models USED-FOR larger graphs. local bi - directional influence ( correlation ) FEATURE-OF mini - batch of nodes. recursive propagation CONJUNCTION skip connection. skip connection CONJUNCTION recursive propagation. first - order and higher - order proximities FEATURE-OF single layer propagation process. first - order and higher - order proximities PART-OF model. large benchmark graphs EVALUATE-FOR model. Task is graph - based applications. Method are layer - wise sampling strategy, and self - attention mechanism. Metric is time complexity. OtherScientificTerm is sampled nodes. ","Graph Convolutional Network (GCN) is a popular technique for graph representation learning, especially in graph-based applications. However, the time complexity of shallow models on small graphs can be prohibitively slow due to the lack of local bi-directional influence (correlation) in the mini-batch of nodes. This paper proposes a layer-wise sampling strategy to speed up the training of GCNs by using acceleration methods. Specifically, GCN-like models are trained on larger graphs and deeper layers, and a self-attention mechanism is introduced to reduce the number of sampled nodes. The proposed model combines first-order and higher-order proximities in the single layer propagation process, which is achieved through recursive propagation and skip connection. The model is evaluated on large benchmark graphs and shows promising results. ","Graph Convolutional Network (GCN) is a popular technique for graph representation learning, especially in graph-based applications. However, the time complexity of shallow models on small graphs can be prohibitively slow due to the lack of local bi-directional influence (correlation) in the mini-batch of nodes. This paper proposes a layer-wise sampling strategy to speed up the training of GCNs by using acceleration methods. Specifically, GCN-like models are trained on larger graphs and deeper layers, and a self-attention mechanism is introduced to reduce the number of sampled nodes. The proposed model combines first-order and higher-order proximities in the single layer propagation process, which is achieved through recursive propagation and skip connection. The model is evaluated on large benchmark graphs and shows promising results. "
22050,SP:bb0af9c011ef982c34fcadb545f6b5771818e7fa,velocities CONJUNCTION interactions. interactions CONJUNCTION velocities. STOVE HYPONYM-OF state - space model. state - space model USED-FOR videos. image model CONJUNCTION dynamics model. dynamics model CONJUNCTION image model. dynamics model USED-FOR inference. image model USED-FOR It. dynamics model USED-FOR It. STOVE COMPARE unsupervised models. unsupervised models COMPARE STOVE. STOVE COMPARE supervised baselines. supervised baselines COMPARE STOVE. unsupervised models COMPARE supervised baselines. supervised baselines COMPARE unsupervised models. model USED-FOR model - based control. Method is physical system. Generic is models. Task is regularizing training. OtherScientificTerm is physical behavior. ,"This paper proposes STOVE, a state-space model for videos that is able to capture velocities and interactions in a physical system. It uses an image model and a dynamics model for inference. The authors claim that models trained with this model are more robust to changes in physical system dynamics, which can be useful for regularizing training. Experiments are conducted on a series of simulated and real-world robotic control tasks, where the model is trained for model-based control, and compared to supervised baselines. The results show that STOve outperforms other unsupervised models in most cases, and can capture physical behavior more accurately.","This paper proposes STOVE, a state-space model for videos that is able to capture velocities and interactions in a physical system. It uses an image model and a dynamics model for inference. The authors claim that models trained with this model are more robust to changes in physical system dynamics, which can be useful for regularizing training. Experiments are conducted on a series of simulated and real-world robotic control tasks, where the model is trained for model-based control, and compared to supervised baselines. The results show that STOve outperforms other unsupervised models in most cases, and can capture physical behavior more accurately."
22059,SP:e67b463bc0aec2345925d609fa521ea49df57fd9,"variational autoencoders ( VAE ) CONJUNCTION generative adversarial networks ( GAN ). generative adversarial networks ( GAN ) CONJUNCTION variational autoencoders ( VAE ). variational autoencoders ( VAE ) PART-OF autoencoding model. model USED-FOR λ - Jeffreys divergence. Gaussian CONJUNCTION Laplace. Laplace CONJUNCTION Gaussian. Laplace HYPONYM-OF explicit likelihood. Gaussian HYPONYM-OF explicit likelihood. approach USED-FOR VAE model. implicit likelihood USED-FOR approach. adversarially trained discriminator USED-FOR implicit likelihood. adversarially trained discriminator USED-FOR approach. implicit likelihood USED-FOR VAE model. mode - seeking CONJUNCTION mass - covering behaviour. mass - covering behaviour CONJUNCTION mode - seeking. mode - seeking FEATURE-OF model. CIFAR-10 and TinyImagent datasets EVALUATE-FOR model. mass - covering behaviour FEATURE-OF model. Method are GAN, VAE, and adversarial training. OtherScientificTerm are mode collapsing problem, model distribution, and VAE loss. Generic are it, It, and objective. ","This paper proposes an autoencoding model that combines the advantages of variational autoencoders (VAE) and generative adversarial networks (GAN). In particular, the authors argue that GANs are prone to the mode collapsing problem, where the model distribution of the training data converges to the same distribution as the target distribution. The authors propose a novel approach to address this issue by replacing the explicit likelihood (such as Gaussian or Laplace) in the VAE with an implicit likelihood, which is based on an adversarially trained discriminator. The model is trained to minimize the λ-Jeffreys divergence between the target and source distributions. It is shown that the proposed approach can be used to train a VAE model that achieves both mode-seeking and mass-covering behaviour on the CIFAR-10 and TinyImagent datasets.    The authors also propose a new objective for VAE training, which they call “adversarial training”, and show that it can be applied to any VAE loss. ","This paper proposes an autoencoding model that combines the advantages of variational autoencoders (VAE) and generative adversarial networks (GAN). In particular, the authors argue that GANs are prone to the mode collapsing problem, where the model distribution of the training data converges to the same distribution as the target distribution. The authors propose a novel approach to address this issue by replacing the explicit likelihood (such as Gaussian or Laplace) in the VAE with an implicit likelihood, which is based on an adversarially trained discriminator. The model is trained to minimize the λ-Jeffreys divergence between the target and source distributions. It is shown that the proposed approach can be used to train a VAE model that achieves both mode-seeking and mass-covering behaviour on the CIFAR-10 and TinyImagent datasets.    The authors also propose a new objective for VAE training, which they call “adversarial training”, and show that it can be applied to any VAE loss. "
22068,SP:87056d0147ddcaf5d78f6888b05161fbdbb3346c,"Adversarial attacks FEATURE-OF CNN classifiers. unreasonably linear extrapolation USED-FOR CNNs. attacks USED-FOR Bayes - Optimal classifier. Bayes - Optimal classifier USED-FOR class distributions. optimal classifier USED-FOR attacks. smooth decision surface FEATURE-OF classifier. datasets EVALUATE-FOR optimal classifier. datasets USED-FOR Bayes - Optimal classifier. adversarial examples USED-FOR it. large - margin methods USED-FOR classifier. machine learning USED-FOR adversarial vulnerability. Task is classification. OtherScientificTerm are geometry of high dimensions, data distribution, optimal decision boundary, and low dimensions. Material is digits. Method are CNN training, and suboptimal training methods. ","Adversarial attacks on CNN classifiers are known to be effective when the geometry of high dimensions (i.e., digits) of the data distribution is different from that of low dimensions (e.g., the optimal decision boundary of the classifier). This paper studies the problem of unreasonably linear extrapolation in CNNs. The authors propose a Bayes-Optimal classifier that is robust to attacks on the optimal classifier on two datasets. They show that under certain assumptions on the geometry and the class distributions, the optimal classesifier has a smooth decision surface. They also show that it can be trained on adversarial examples.    The authors show that adversarial vulnerability in machine learning is not due to CNN training, but rather to suboptimal training methods. In particular, they show that large-margin methods can be used to train a classifier which is more robust to adversarial attacks.","Adversarial attacks on CNN classifiers are known to be effective when the geometry of high dimensions (i.e., digits) of the data distribution is different from that of low dimensions (e.g., the optimal decision boundary of the classifier). This paper studies the problem of unreasonably linear extrapolation in CNNs. The authors propose a Bayes-Optimal classifier that is robust to attacks on the optimal classifier on two datasets. They show that under certain assumptions on the geometry and the class distributions, the optimal classesifier has a smooth decision surface. They also show that it can be trained on adversarial examples.    The authors show that adversarial vulnerability in machine learning is not due to CNN training, but rather to suboptimal training methods. In particular, they show that large-margin methods can be used to train a classifier which is more robust to adversarial attacks."
22077,SP:a7b3a35e6a79084bdfd1e4a963dfa081279cd8bb,"top-1 accuracy EVALUATE-FOR sparse and non - sparse models. Method are Neural network pruning techniques, network, pruning, abstract representations, and fine - grained classification. Metric are top1 test set accuracy, and image quality. OtherScientificTerm are pruning identified exemplars ( PIEs ), and sparsity. Material are PIE images, and hard - to - generalize - to images. ","This paper studies the effect of Neural network pruning techniques on the top-1 accuracy of sparse and non-sparse models. The authors show that the top1 test set accuracy can be improved by pruning identified exemplars (PIEs) from the network. They also show that pruning is beneficial to abstract representations, which is useful for fine-grained classification.   The authors also find that the sparsity of PIE images is correlated with the image quality, and that hard-to-generalize-to images are more likely to be pruned. ","This paper studies the effect of Neural network pruning techniques on the top-1 accuracy of sparse and non-sparse models. The authors show that the top1 test set accuracy can be improved by pruning identified exemplars (PIEs) from the network. They also show that pruning is beneficial to abstract representations, which is useful for fine-grained classification.   The authors also find that the sparsity of PIE images is correlated with the image quality, and that hard-to-generalize-to images are more likely to be pruned. "
22086,SP:4b17edaa7ec6201891433320d85f9a415656b763,"Interactive Fiction games HYPONYM-OF text - based simulations. natural language understanding CONJUNCTION partial observability. partial observability CONJUNCTION natural language understanding. reinforcement learning agents USED-FOR natural language understanding. partial observability CONJUNCTION action generation. action generation CONJUNCTION partial observability. combinatorially - large text - based action spaces FEATURE-OF action generation. template - based action space USED-FOR KG - A2C1. knowledge graph USED-FOR game state. knowledge graph USED-FOR natural language generation. KG - A2C COMPARE IF agents. IF agents COMPARE KG - A2C. IF games EVALUATE-FOR KG - A2C. OtherScientificTerm are natural language, dynamic knowledge graph, combinatorially large natural language actions, and action space size. Generic is They. ","This paper studies the problem of text-based simulations (Interactive Fiction games) in which natural language is used as a resource for reinforcement learning agents to improve natural language understanding, partial observability, and action generation in combinatorially-large text -based action spaces. The paper proposes KG-A2C1, which extends the template-based action space of [1] to a dynamic knowledge graph, where the knowledge graph represents the game state, and natural language generation is based on a knowledge graph of the current state of the game. The authors show that KG - A2C outperforms existing IF agents in a number of IF games, and is able to generate combinatorial large natural language actions in games where the action space size is very large. They also show that they are able to generalize to more complex games. ","This paper studies the problem of text-based simulations (Interactive Fiction games) in which natural language is used as a resource for reinforcement learning agents to improve natural language understanding, partial observability, and action generation in combinatorially-large text -based action spaces. The paper proposes KG-A2C1, which extends the template-based action space of [1] to a dynamic knowledge graph, where the knowledge graph represents the game state, and natural language generation is based on a knowledge graph of the current state of the game. The authors show that KG - A2C outperforms existing IF agents in a number of IF games, and is able to generate combinatorial large natural language actions in games where the action space size is very large. They also show that they are able to generalize to more complex games. "
22095,SP:b1784ecbb8f36eef9cae33d61ce60d80c2f9c38d,"language generation HYPONYM-OF sequence prediction problems. maximum likelihood estimation ( MLE ) USED-FOR sequence prediction problems. data - dependent Gaussian prior CONJUNCTION detailed training prediction. detailed training prediction CONJUNCTION data - dependent Gaussian prior. data - dependent Gaussian prior USED-FOR Kullback – Leibler divergence term. text summarization CONJUNCTION storytelling. storytelling CONJUNCTION text summarization. supervised and unsupervised machine translation CONJUNCTION text summarization. text summarization CONJUNCTION supervised and unsupervised machine translation. storytelling CONJUNCTION image captioning. image captioning CONJUNCTION storytelling. language generation tasks EVALUATE-FOR method. image captioning HYPONYM-OF language generation tasks. supervised and unsupervised machine translation HYPONYM-OF language generation tasks. text summarization HYPONYM-OF language generation tasks. storytelling HYPONYM-OF language generation tasks. Generic is it. Method is MLE. OtherScientificTerm are negative diversity ignorance, and prior topological order of tokens. Metric is MLE loss. ","This paper studies the problem of maximum likelihood estimation (MLE) in the context of sequence prediction problems such as language generation. The authors propose a novel Kullback–Leibler divergence term, which is based on a data-dependent Gaussian prior and detailed training prediction. They show that it can be used to mitigate the negative diversity ignorance, i.e., that the MLE loss is dominated by the prior topological order of tokens. The proposed method is evaluated on a variety of language generation tasks including supervised and unsupervised machine translation, text summarization, storytelling, and image captioning.","This paper studies the problem of maximum likelihood estimation (MLE) in the context of sequence prediction problems such as language generation. The authors propose a novel Kullback–Leibler divergence term, which is based on a data-dependent Gaussian prior and detailed training prediction. They show that it can be used to mitigate the negative diversity ignorance, i.e., that the MLE loss is dominated by the prior topological order of tokens. The proposed method is evaluated on a variety of language generation tasks including supervised and unsupervised machine translation, text summarization, storytelling, and image captioning."
22104,SP:7c29cb5a32b14e1392408dc5daba4cd35848bea9,"Temperature scaling USED-FOR DNN. Temperature scaling HYPONYM-OF calibration approach. cross - entropy loss CONJUNCTION focal loss. focal loss CONJUNCTION cross - entropy loss. focal loss USED-FOR models. temperature scaling CONJUNCTION focal loss. focal loss CONJUNCTION temperature scaling. confidence FEATURE-OF model. focal loss USED-FOR calibrated models. accuracy EVALUATE-FOR focal loss. network architectures USED-FOR approach. accuracy CONJUNCTION calibration. calibration CONJUNCTION accuracy. accuracy EVALUATE-FOR approach. calibration EVALUATE-FOR approach. Task are Miscalibration, downstream tasks, and miscalibration. Method is Deep Neural Networks ( DNNs ). Generic is networks. Material is NLP ( SST, 20 Newsgroup ) datasets. ","This paper proposes a new calibration approach, called Temperature scaling, to improve the calibration of a DNN. Miscalibration is a common problem in Deep Neural Networks (DNNs) and the authors propose to use temperature scaling to calibrate the DNN in order to improve its performance on downstream tasks.   The main idea is to use cross-entropy loss and focal loss for calibrating the models.  The authors show that temperature scaling, focal loss, and calibration can improve the accuracy and calibration of calibrated models. They also show that the confidence of a model trained with focal loss can be improved by temperature scaling.  Finally, the authors evaluate their approach on a variety of network architectures and show that their approach improves the accuracy on standard NLP (SST, 20 Newsgroup) datasets. The authors also demonstrate that the proposed approach can be used to improve both the accuracy as well as calibration. ","This paper proposes a new calibration approach, called Temperature scaling, to improve the calibration of a DNN. Miscalibration is a common problem in Deep Neural Networks (DNNs) and the authors propose to use temperature scaling to calibrate the DNN in order to improve its performance on downstream tasks.   The main idea is to use cross-entropy loss and focal loss for calibrating the models.  The authors show that temperature scaling, focal loss, and calibration can improve the accuracy and calibration of calibrated models. They also show that the confidence of a model trained with focal loss can be improved by temperature scaling.  Finally, the authors evaluate their approach on a variety of network architectures and show that their approach improves the accuracy on standard NLP (SST, 20 Newsgroup) datasets. The authors also demonstrate that the proposed approach can be used to improve both the accuracy as well as calibration. "
22113,SP:cd6b8417ec8bcb773c78cff677bb0a76d6b3f6f3,Lipschitz constant FEATURE-OF neural networks. LiPopt HYPONYM-OF polynomial optimization framework. sparse connectivity USED-FOR network. sparse connectivity USED-FOR complexity of computation. approach COMPARE baselines. baselines COMPARE approach. networks CONJUNCTION networks. networks CONJUNCTION networks. ` ∞-Lipschitz constant EVALUATE-FOR approach. random weights CONJUNCTION networks. networks CONJUNCTION random weights. random weights USED-FOR networks. MNIST USED-FOR networks. Task is optimization problems. Method is convolutional as well as pruned neural networks. ,"This paper studies the Lipschitz constant of neural networks. The authors propose LiPopt, a polynomial optimization framework that uses sparse connectivity in the network to reduce the complexity of computation. They show that the proposed approach outperforms the baselines in terms of `∞-Lipschatz constant. They also show that networks trained with random weights and networks trained on MNIST can be pruned to achieve the same `*∞***’.    The paper is well-written and well-motivated. The optimization problems are well-structured, and the paper is clearly written. The results are interesting and convincing. The paper also shows that convolutional as well as pruned neural networks can benefit from the proposed method. ","This paper studies the Lipschitz constant of neural networks. The authors propose LiPopt, a polynomial optimization framework that uses sparse connectivity in the network to reduce the complexity of computation. They show that the proposed approach outperforms the baselines in terms of `∞-Lipschatz constant. They also show that networks trained with random weights and networks trained on MNIST can be pruned to achieve the same `*∞***’.    The paper is well-written and well-motivated. The optimization problems are well-structured, and the paper is clearly written. The results are interesting and convincing. The paper also shows that convolutional as well as pruned neural networks can benefit from the proposed method. "
22122,SP:31c9dc0dd8806daddc9cb48c56ec819577fe46cd,self - supervised learning approach USED-FOR video features. video classification CONJUNCTION captioning and segmentation. captioning and segmentation CONJUNCTION video classification. tasks EVALUATE-FOR methods. self - supervised learning approach USED-FOR tasks. self - supervised learning approach COMPARE methods. methods COMPARE self - supervised learning approach. captioning and segmentation HYPONYM-OF tasks. video classification HYPONYM-OF tasks. BERT model USED-FOR text sequences. softmax loss CONJUNCTION noise contrastive estimation ( NCE ). noise contrastive estimation ( NCE ) CONJUNCTION softmax loss. sequences of real - valued feature vectors USED-FOR method. BERT model USED-FOR method. sequences of visual features CONJUNCTION sequences of words. sequences of words CONJUNCTION sequences of visual features. automatic speech recognition USED-FOR sequences of words. sequences of words USED-FOR representations. sequences of visual features USED-FOR representations. Method is cross - modal training. ,"This paper proposes a self-supervised learning approach to learn video features. The proposed self - supervised learning approach outperforms existing methods on three tasks: video classification, captioning and segmentation. The method uses sequences of real-valued feature vectors extracted from a BERT model trained on text sequences. The key idea is to combine the softmax loss with noise contrastive estimation (NCE) to improve the performance of cross-modal training. Experiments show that the proposed method can learn representations from sequences of visual features and sequences of words extracted from automatic speech recognition.","This paper proposes a self-supervised learning approach to learn video features. The proposed self - supervised learning approach outperforms existing methods on three tasks: video classification, captioning and segmentation. The method uses sequences of real-valued feature vectors extracted from a BERT model trained on text sequences. The key idea is to combine the softmax loss with noise contrastive estimation (NCE) to improve the performance of cross-modal training. Experiments show that the proposed method can learn representations from sequences of visual features and sequences of words extracted from automatic speech recognition."
22131,SP:0f24424d10f1201dd25e8c56354e10afc9b2b11c,"selection masks CONJUNCTION neural network. neural network CONJUNCTION selection masks. Task are inference phase, and data transfer. OtherScientificTerm is public storage server. Method is machine learning models. Generic are framework, model, and masks. ","This paper studies the problem of data transfer during the inference phase of machine learning models. The authors propose a new framework, which is based on the observation that the data is not always available in the public storage server, and that the model may not be able to transfer well when the data transfer is limited. To address this problem, the authors propose to use selection masks and a neural network to share information between the model and the server. The paper also proposes a way to share the masks between the server and the model.","This paper studies the problem of data transfer during the inference phase of machine learning models. The authors propose a new framework, which is based on the observation that the data is not always available in the public storage server, and that the model may not be able to transfer well when the data transfer is limited. To address this problem, the authors propose to use selection masks and a neural network to share information between the model and the server. The paper also proposes a way to share the masks between the server and the model."
22140,SP:aa4fcf5b2cae05c5c6a903c24e4992b56655dee2,"Deep neural networks USED-FOR classification tasks. methodology USED-FOR neural network. it USED-FOR outof - distribution ( OOD ) examples. Outlier Exposure ( OE ) technique USED-FOR loss function. loss function USED-FOR out - of - distribution detection. image and text classification tasks EVALUATE-FOR out - of - distribution detection. OE USED-FOR loss function. image and text classification tasks EVALUATE-FOR loss function. OE USED-FOR out - of - distribution detection. method CONJUNCTION Mahalanobis distance - based classifier. Mahalanobis distance - based classifier CONJUNCTION method. OOD detection task EVALUATE-FOR method. Task is artificial intelligence. Method are neural networks, and classification algorithms. OtherScientificTerm is novel class distributions. Metric is classification accuracy. ","This paper proposes a new method for out-of-distribution (OOD) detection. The method is based on the Outlier Exposure (OE) technique, which is a technique for detecting OOD examples in the training of deep neural networks for classification tasks. The authors propose a methodology to train a neural network so that it is able to detect out of distribution (ODD) examples. They propose a loss function based on OE to improve the performance of OOD detection on image and text classification tasks, and show that the proposed loss function is more robust to novel class distributions than existing classification algorithms. The proposed method is tested on a simple OLD detection task and is shown to outperform a Mahalanobis distance-based classifier in terms of classification accuracy. ","This paper proposes a new method for out-of-distribution (OOD) detection. The method is based on the Outlier Exposure (OE) technique, which is a technique for detecting OOD examples in the training of deep neural networks for classification tasks. The authors propose a methodology to train a neural network so that it is able to detect out of distribution (ODD) examples. They propose a loss function based on OE to improve the performance of OOD detection on image and text classification tasks, and show that the proposed loss function is more robust to novel class distributions than existing classification algorithms. The proposed method is tested on a simple OLD detection task and is shown to outperform a Mahalanobis distance-based classifier in terms of classification accuracy. "
22149,SP:89bc528ef801182365ac279e8963803afccb391d,end - to - end deep learning model USED-FOR RNA secondary structure prediction. E2Efold HYPONYM-OF end - to - end deep learning model. unrolled algorithm USED-FOR constrained programming. E2Efold USED-FOR RNA base - pairing matrix. unrolled algorithm USED-FOR deep architectures. deep architectures USED-FOR constraints. it COMPARE SOTA. SOTA COMPARE it. it COMPARE algorithms. algorithms COMPARE it. benchmark datasets EVALUATE-FOR E2Efold. E2Efold COMPARE it. it COMPARE E2Efold. SOTA USED-FOR pseudoknotted structures. E2Efold COMPARE SOTA. SOTA COMPARE E2Efold. inference time EVALUATE-FOR algorithms. ,"This paper proposes E2Efold, an end-to-end deep learning model for RNA secondary structure prediction. The authors propose an unrolled algorithm for constrained programming, which learns deep architectures to satisfy the constraints of the RNA base-pairing matrix. They show that E1Efold outperforms SOTA for pseudoknotted structures, and it outperforms other algorithms in terms of inference time on several benchmark datasets. ","This paper proposes E2Efold, an end-to-end deep learning model for RNA secondary structure prediction. The authors propose an unrolled algorithm for constrained programming, which learns deep architectures to satisfy the constraints of the RNA base-pairing matrix. They show that E1Efold outperforms SOTA for pseudoknotted structures, and it outperforms other algorithms in terms of inference time on several benchmark datasets. "
22158,SP:b68560cce8c64ebe0ca5e6534b3732c775d36452,"collective policies COMPARE individually trained policies. individually trained policies COMPARE collective policies. OtherScientificTerm are biases, virtual simulation, agents ’ simulations, biased representations, and internal simulations. Method is collective policy. Material is real - world environment. ","This paper studies the problem of learning policies that are robust to biases in a virtual simulation. The authors show that agents’ simulations are biased in the sense that they learn biased representations of the environment. They then propose a collective policy that is robust to these biases. They show that such collective policies are more robust than individually trained policies. They also show that in a real-world environment, the bias is more pronounced in internal simulations. ","This paper studies the problem of learning policies that are robust to biases in a virtual simulation. The authors show that agents’ simulations are biased in the sense that they learn biased representations of the environment. They then propose a collective policy that is robust to these biases. They show that such collective policies are more robust than individually trained policies. They also show that in a real-world environment, the bias is more pronounced in internal simulations. "
22167,SP:bd1dc08b4fd9a5cc78d26d7eb7f05dbb4a629ab1,"Generic responses HYPONYM-OF open - domain dialog generation. one - to - one task USED-FOR one - to - many task. dialog generation model USED-FOR semantic latent space. prompt USED-FOR features. model USED-FOR semantically related responses. regression task USED-FOR pair relationship. model COMPARE baselines. baselines COMPARE model. coherence EVALUATE-FOR baselines. model USED-FOR generic response problem. coherence EVALUATE-FOR model. OtherScientificTerm are latent space, and MLE loss. Method is autoencoder. ","This paper tackles the problem of open-domain dialog generation, i.e., Generic responses. The authors propose a dialog generation model that learns a semantic latent space, which is then used as a one-to-one task to solve the generic response problem. The model learns semantically related responses in the latent space. The features extracted from the prompt are used to train an autoencoder to generate the features of the response. A regression task is used to learn the pair relationship between the response and the prompt. The proposed model is compared with several baselines on the coherence of the generated responses, and is shown to outperform the baselines when the MLE loss is used.","This paper tackles the problem of open-domain dialog generation, i.e., Generic responses. The authors propose a dialog generation model that learns a semantic latent space, which is then used as a one-to-one task to solve the generic response problem. The model learns semantically related responses in the latent space. The features extracted from the prompt are used to train an autoencoder to generate the features of the response. A regression task is used to learn the pair relationship between the response and the prompt. The proposed model is compared with several baselines on the coherence of the generated responses, and is shown to outperform the baselines when the MLE loss is used."
22176,SP:ef0d5fd333ed60feb3946d24002e9a90642aea66,"it USED-FOR life - affecting decisions. Gaussian light and shadow ( GLAS ) HYPONYM-OF salient explanation method. feature perturbation USED-FOR deep models. GLAS USED-FOR coarseto - fine control. scalability of Gaussian mask USED-FOR GLAS. scalability of Gaussian mask USED-FOR coarseto - fine control. GLAS USED-FOR fine - grained classification. fine - grained classification dataset EVALUATE-FOR GLAS. high speed EVALUATE-FOR GLAS. ImageNet Large Scale Visual Recognition Challenge EVALUATE-FOR GLAS. OtherScientificTerm are discriminative features, salient explanation, and 224×224 image. Task is fine - grained classification task. Method are Gaussian mask, and recursive GLAS. ","This paper proposes Gaussian light and shadow (GLAS) which is a salient explanation method that uses feature perturbation to improve the performance of deep models. The key idea of GLAS is that the discriminative features of the input image can be used as salient explanation for a fine-grained classification task, and that it can be applied to life-affecting decisions. The authors show that GLAS improves coarseto-fine control through the scalability of Gaussian mask. They also show that the Gaussian masks are more robust to perturbations in the input images.  The authors evaluate GLAS on the fine-rigorous classification dataset on the ImageNet Large Scale Visual Recognition Challenge, and show that it improves the performance on the 224×224 image. GLAS also shows improved performance with high speed compared to the previous state-of-the-art method, recursive GLAS.   ","This paper proposes Gaussian light and shadow (GLAS) which is a salient explanation method that uses feature perturbation to improve the performance of deep models. The key idea of GLAS is that the discriminative features of the input image can be used as salient explanation for a fine-grained classification task, and that it can be applied to life-affecting decisions. The authors show that GLAS improves coarseto-fine control through the scalability of Gaussian mask. They also show that the Gaussian masks are more robust to perturbations in the input images.  The authors evaluate GLAS on the fine-rigorous classification dataset on the ImageNet Large Scale Visual Recognition Challenge, and show that it improves the performance on the 224×224 image. GLAS also shows improved performance with high speed compared to the previous state-of-the-art method, recursive GLAS.   "
22185,SP:d17ca20cc527c28ab7358cb5b14954e5fb56409f,"Convolution PART-OF Convolutional Neural Networks ( CNNs ). kernel USED-FOR Convolutional Neural Networks ( CNNs ). convolutional kernels USED-FOR redundant data. procedure USED-FOR pixel - wise and channel - wise correlations. computational cost EVALUATE-FOR convolution layer. deconvolution filters PART-OF network. center - surround structure FEATURE-OF biological neurons. center - surround structure HYPONYM-OF deconvolution filters. visual regions of the brain FEATURE-OF biological neurons. Filtering USED-FOR sparse representation. kernels USED-FOR sparse representation. kernels USED-FOR Filtering. network deconvolution operation USED-FOR neural network models. MNIST CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION MNIST. Fashion - MNIST CONJUNCTION Cityscapes. Cityscapes CONJUNCTION Fashion - MNIST. CIFAR-100 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-100. CIFAR-10 CONJUNCTION CIFAR-100. CIFAR-100 CONJUNCTION CIFAR-10. CIFAR-100 CONJUNCTION Fashion - MNIST. Fashion - MNIST CONJUNCTION CIFAR-100. Cityscapes CONJUNCTION ImageNet datasets. ImageNet datasets CONJUNCTION Cityscapes. CIFAR-10 CONJUNCTION MNIST. MNIST CONJUNCTION CIFAR-10. MNIST CONJUNCTION Cityscapes. Cityscapes CONJUNCTION MNIST. Fashion - MNIST CONJUNCTION ImageNet datasets. ImageNet datasets CONJUNCTION Fashion - MNIST. MNIST EVALUATE-FOR network deconvolution operation. Fashion - MNIST EVALUATE-FOR network deconvolution operation. CIFAR-100 EVALUATE-FOR network deconvolution operation. ImageNet datasets EVALUATE-FOR network deconvolution operation. CIFAR-10 EVALUATE-FOR network deconvolution operation. Material is real - world image data. Task is neural network training. Method are network deconvolution, Network deconvolution, neural networks, and batch normalization. Metric is faster convergence. ","This paper proposes a novel method to remove redundant data from convolutional kernels in convolution neural networks. The authors propose a method called ""deconvolutional deconvolution"" that removes redundant information from the convolution kernel. The method is based on the observation that the center-surround structure of biological neurons in the visual regions of the brain is similar to that of convolution filters in a neural network. The paper also shows that the method can be applied to any convolution kernels, and that it can be combined with existing techniques to reduce the computational cost of a convolution layer.","This paper proposes a novel method to remove redundant data from convolutional kernels in convolution neural networks. The authors propose a method called ""deconvolutional deconvolution"" that removes redundant information from the convolution kernel. The method is based on the observation that the center-surround structure of biological neurons in the visual regions of the brain is similar to that of convolution filters in a neural network. The paper also shows that the method can be applied to any convolution kernels, and that it can be combined with existing techniques to reduce the computational cost of a convolution layer."
22194,SP:e1b0de9a36bf8359df368b7a55a7f23e99d88db7,"smartphones HYPONYM-OF edge devices. neural network quantization methods USED-FOR GANs. CNN quantization methods USED-FOR GAN models. CNN quantization methods USED-FOR extreme low bits. quantization method USED-FOR GANs. QGAN HYPONYM-OF quantization method. EM algorithms USED-FOR quantization method. multi - precision algorithm USED-FOR quantization precision. multi - precision algorithm USED-FOR GANs. quantization precision FEATURE-OF GANs. CIFAR-10 CONJUNCTION CelebA. CelebA CONJUNCTION CIFAR-10. QGAN USED-FOR GANs. QGAN COMPARE models. models COMPARE QGAN. CIFAR-10 EVALUATE-FOR QGAN. CelebA EVALUATE-FOR QGAN. QGAN USED-FOR 1 - bit or 2 - bit representations. Method are generative adversarial neural networks ( GANs ), convolutional neural networks ( CNNs ), quantization algorithms, and generator and discriminator networks. Generic is them. OtherScientificTerm is image qualities requirements. ","This paper proposes a new quantization method for generative adversarial neural networks (GANs) based on neural network quantization methods for GANs. Previous work has shown that convolutional neural networks [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,21] can be quantized to 1-bit or 2-bit representations, but most of them require extreme low bits (e.g. 4 bits or less) which is not practical for edge devices such as smartphones. This paper shows that CNN quantization algorithms can be used to reduce the number of quantization bits required by GAN models.   [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14]  [15]   This paper proposes QGAN, which is a quantization algorithm based on EM algorithms [1]. The authors show that QGAN is a generalization of previous work [1], and that it can be applied to a wide range of GAN methods. The authors also propose a multi-precision algorithm to improve the quantization precision of the GAN.  [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [34] [35] [36] [37] [38] [39] [40] [41] [44] [45] [46] [47] [48] [49] [50] [51] [55] [56] [57] [58] [59] [60] [63] [64] [65]  The authors claim that the proposed QGAN outperforms existing models on CIFAR-10 and CelebA, and is competitive with them on image qualities requirements. ","This paper proposes a new quantization method for generative adversarial neural networks (GANs) based on neural network quantization methods for GANs. Previous work has shown that convolutional neural networks [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,21] can be quantized to 1-bit or 2-bit representations, but most of them require extreme low bits (e.g. 4 bits or less) which is not practical for edge devices such as smartphones. This paper shows that CNN quantization algorithms can be used to reduce the number of quantization bits required by GAN models.   [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14]  [15]   This paper proposes QGAN, which is a quantization algorithm based on EM algorithms [1]. The authors show that QGAN is a generalization of previous work [1], and that it can be applied to a wide range of GAN methods. The authors also propose a multi-precision algorithm to improve the quantization precision of the GAN.  [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [34] [35] [36] [37] [38] [39] [40] [41] [44] [45] [46] [47] [48] [49] [50] [51] [55] [56] [57] [58] [59] [60] [63] [64] [65]  The authors claim that the proposed QGAN outperforms existing models on CIFAR-10 and CelebA, and is competitive with them on image qualities requirements. "
22203,SP:58c4905f59f04a50b30d27c99521126a6455d38a,"Generative Adversarial Networks HYPONYM-OF nonconvex applications. bilinear and convex - strongly concave settings FEATURE-OF global last - iterate convergence rates. Simultaneous Gradient Descent / Ascent HYPONYM-OF natural algorithms. linear convergence FEATURE-OF HAMILTONIAN GRADIENT DESCENT ( HGD ) algorithm. “ sufficiently bilinear ” condition FEATURE-OF convex - concave problems. convergence rates FEATURE-OF stochastic HGD. Task are convex - concave min - max optimization, and training Generative Adversarial Networks. OtherScientificTerm are averageiterate convergence results, last - iterate convergence guarantees, last - iterate convergence, and convex - concave min - max settings. Method is Consensus Optimization algorithm. ","This paper studies the problem of convex-concave min-max optimization. In particular, the authors consider nonconvex applications such as Generative Adversarial Networks, where the goal is to obtain global last-iterate convergence rates in bilinear and convex - strongly concave settings.   The authors consider two natural algorithms, namely Simultaneous Gradient Descent/Ascent and Hamiltonian Gradient Decentraction (HGD).   They show that the HAMILTONIAN GRADIENT DESCENT (HDG) algorithm has a linear convergence in the convex case, which is a result of the “satisfying” “subsatisfied” condition on the number of steps in the Consensus Optimization algorithm.  They also show that for convex concave problems, the convergence rates of the stochastic HGD are linear.  The main contribution of this paper is that the authors provide the first generalization of the average iterate convergence results to the case where the last iterates of SGD and SGD-HGD are non-linear.  This is an interesting result, and it is an important contribution to the literature.  In addition, they also provide a theoretical analysis of the convergence rate of the last step of the SGD algorithm in the case that the last steps are nonlinear, and show that it converges to a stationary point.  Finally, they show that in the setting where the data is convex, the last-strongly concave, and non-convergentive, they provide last-th iterates for SGD.  For the case of training GANs and GAN, they give a theoretical explanation for the convergence of GAN-SGD to stationary points. They also provide an explanation for why training GNNs is a non-trivial problem, and provide a proof of convergence for the case when the data are convex.  As a result, they conclude that the convergence results are tight, and they also give a proof for the last iteration of the Hamiltonian gradient descent algorithm.","This paper studies the problem of convex-concave min-max optimization. In particular, the authors consider nonconvex applications such as Generative Adversarial Networks, where the goal is to obtain global last-iterate convergence rates in bilinear and convex - strongly concave settings.   The authors consider two natural algorithms, namely Simultaneous Gradient Descent/Ascent and Hamiltonian Gradient Decentraction (HGD).   They show that the HAMILTONIAN GRADIENT DESCENT (HDG) algorithm has a linear convergence in the convex case, which is a result of the “satisfying” “subsatisfied” condition on the number of steps in the Consensus Optimization algorithm.  They also show that for convex concave problems, the convergence rates of the stochastic HGD are linear.  The main contribution of this paper is that the authors provide the first generalization of the average iterate convergence results to the case where the last iterates of SGD and SGD-HGD are non-linear.  This is an interesting result, and it is an important contribution to the literature.  In addition, they also provide a theoretical analysis of the convergence rate of the last step of the SGD algorithm in the case that the last steps are nonlinear, and show that it converges to a stationary point.  Finally, they show that in the setting where the data is convex, the last-strongly concave, and non-convergentive, they provide last-th iterates for SGD.  For the case of training GANs and GAN, they give a theoretical explanation for the convergence of GAN-SGD to stationary points. They also provide an explanation for why training GNNs is a non-trivial problem, and provide a proof of convergence for the case when the data are convex.  As a result, they conclude that the convergence results are tight, and they also give a proof for the last iteration of the Hamiltonian gradient descent algorithm."
22212,SP:d8556b52272321a1415ac2d85bb12e88b51ee73a,"stability FEATURE-OF ResNet. stability FEATURE-OF ResNet. gradient descent USED-FOR global minima. over - parameterization requirement FEATURE-OF ResNet. ResNet COMPARE vanilla feedforward network. vanilla feedforward network COMPARE ResNet. normalization layer USED-FOR deep ResNet. normalization layer USED-FOR ResNet. Method is ResNet structure. OtherScientificTerm are ResNet block hl, ReLU activation, initialization, residual blocks, and global convergence. Metric is τ. Generic is forward process. ","This paper studies the stability of ResNet under the over-parameterization requirement. The authors consider the ResNet structure, where each ResNet block hl is a ReLU activation, and each block is a residual block. They show that gradient descent converges to global minima when the number of residual blocks is large enough. They also show that if the residual blocks are large enough, then the forward process converges in a similar way.    The authors also show the global convergence of a ResNet with a normalization layer, which is similar to the vanilla feedforward network. The main contribution of this paper is that the authors show that the stability in ResNet can be improved by adding a regularization layer to deep ResNet. ","This paper studies the stability of ResNet under the over-parameterization requirement. The authors consider the ResNet structure, where each ResNet block hl is a ReLU activation, and each block is a residual block. They show that gradient descent converges to global minima when the number of residual blocks is large enough. They also show that if the residual blocks are large enough, then the forward process converges in a similar way.    The authors also show the global convergence of a ResNet with a normalization layer, which is similar to the vanilla feedforward network. The main contribution of this paper is that the authors show that the stability in ResNet can be improved by adding a regularization layer to deep ResNet. "
22221,SP:cf70dc496825ece2f28fdf4f1a6f4316c69e0e48,Sparse neural networks COMPARE dense networks. dense networks COMPARE Sparse neural networks. they USED-FOR wall clock inference times. sparse networks USED-FOR inference. dense networks USED-FOR sparse networks. method USED-FOR sparse neural networks. fixed parameter count CONJUNCTION fixed computational cost. fixed computational cost CONJUNCTION fixed parameter count. accuracy EVALUATE-FOR dense - to - sparse training methods. fixed computational cost USED-FOR method. fixed parameter count USED-FOR method. parameter magnitudes CONJUNCTION infrequent gradient calculations. infrequent gradient calculations CONJUNCTION parameter magnitudes. method USED-FOR topology. topology FEATURE-OF network. parameter magnitudes USED-FOR topology. parameter magnitudes USED-FOR method. infrequent gradient calculations USED-FOR method. approach COMPARE prior techniques. prior techniques COMPARE approach. accuracy EVALUATE-FOR prior techniques. floating - point operations ( FLOPs ) USED-FOR approach. accuracy EVALUATE-FOR approach. MobileNet v1 CONJUNCTION MobileNet v2. MobileNet v2 CONJUNCTION MobileNet v1. ResNet-50 CONJUNCTION MobileNet v1. MobileNet v1 CONJUNCTION ResNet-50. WideResNets CONJUNCTION RNNs. RNNs CONJUNCTION WideResNets. ImageNet-2012 dataset CONJUNCTION WideResNets. WideResNets CONJUNCTION ImageNet-2012 dataset. WideResNets CONJUNCTION CIFAR-10 dataset. CIFAR-10 dataset CONJUNCTION WideResNets. MobileNet v2 CONJUNCTION WideResNets. WideResNets CONJUNCTION MobileNet v2. WikiText-103 dataset EVALUATE-FOR RNNs. ResNet-50 USED-FOR sparse training. ImageNet-2012 dataset EVALUATE-FOR MobileNet v2. Method is trainable sparse model. Task is optimization. OtherScientificTerm is local minima. ,"This paper proposes a method to train sparse neural networks with wall clock inference times. Sparse neural networks have been shown to be more efficient than dense networks, and they can be used to speed up wallclock inference times for inference. However, dense-to-sparse training methods tend to have a trade-off between accuracy and computational cost, and the authors propose a trainable sparse model. The proposed method uses a fixed parameter count and a fixed computational cost to train a sparse network. The method learns the topology of the network based on parameter magnitudes and infrequent gradient calculations. The authors show that the proposed approach can achieve better accuracy with fewer floating-point operations (FLOPs) compared to prior techniques. The paper also shows that sparse training can be performed on ResNet-50, MobileNet v1, Mobilenet v2, WideResNets, and RNNs on the ImageNet-2012 dataset, and on the CIFAR-10 dataset, as well as on the WikiText-103 dataset. In addition, the paper shows that the method is able to learn a topology based on the number of local minima, which is more suitable for optimization.  ","This paper proposes a method to train sparse neural networks with wall clock inference times. Sparse neural networks have been shown to be more efficient than dense networks, and they can be used to speed up wallclock inference times for inference. However, dense-to-sparse training methods tend to have a trade-off between accuracy and computational cost, and the authors propose a trainable sparse model. The proposed method uses a fixed parameter count and a fixed computational cost to train a sparse network. The method learns the topology of the network based on parameter magnitudes and infrequent gradient calculations. The authors show that the proposed approach can achieve better accuracy with fewer floating-point operations (FLOPs) compared to prior techniques. The paper also shows that sparse training can be performed on ResNet-50, MobileNet v1, Mobilenet v2, WideResNets, and RNNs on the ImageNet-2012 dataset, and on the CIFAR-10 dataset, as well as on the WikiText-103 dataset. In addition, the paper shows that the method is able to learn a topology based on the number of local minima, which is more suitable for optimization.  "
22230,SP:d2d2b892518d54d0e63e26a056f2298be3be2610,"deep generative models USED-FOR photo - realistic images. computer vision CONJUNCTION natural language processing. natural language processing CONJUNCTION computer vision. deep generative models USED-FOR visual or textual content embeddings. photo - realistic images CONJUNCTION visual or textual content embeddings. visual or textual content embeddings CONJUNCTION photo - realistic images. deep generative models USED-FOR natural language processing. deep generative models USED-FOR computer vision. translation CONJUNCTION zoom. zoom CONJUNCTION translation. zoom CONJUNCTION color variations. color variations CONJUNCTION zoom. color variations HYPONYM-OF transformations. translation HYPONYM-OF transformations. zoom HYPONYM-OF transformations. GANs CONJUNCTION variational auto - encoders. variational auto - encoders CONJUNCTION GANs. variational auto - encoders EVALUATE-FOR method. GANs EVALUATE-FOR method. approach CONJUNCTION BigGAN model. BigGAN model CONJUNCTION approach. Method are generative process, generative models, and generative model. OtherScientificTerm are latent space, and human annotations. ","This paper proposes to use deep generative models to generate photo-realistic images and visual or textual content embeddings, which can be used in both computer vision and natural language processing. The key idea is to train a generative process that can be applied to a variety of transformations, including translation, zoom, color variations, etc. The authors show that their approach is able to achieve state-of-the-art performance on both GANs and variational auto-encoders. They also show that the proposed approach can be combined with a BigGAN model, and that the generated images can be annotated with human annotations. ","This paper proposes to use deep generative models to generate photo-realistic images and visual or textual content embeddings, which can be used in both computer vision and natural language processing. The key idea is to train a generative process that can be applied to a variety of transformations, including translation, zoom, color variations, etc. The authors show that their approach is able to achieve state-of-the-art performance on both GANs and variational auto-encoders. They also show that the proposed approach can be combined with a BigGAN model, and that the generated images can be annotated with human annotations. "
22239,SP:1c63389e972d4652fac831e9d11609cd3c3c371a,"model USED-FOR unsupervised physical parameter estimation of systems. differential equations USED-FOR scene dynamics. video USED-FOR unsupervised physical parameter estimation of systems. object state supervision USED-FOR physical scene understanding methods. objects CONJUNCTION state and velocity representations. state and velocity representations CONJUNCTION objects. framework USED-FOR long term extrapolative video prediction. framework USED-FOR vision - based model - predictive control. long term extrapolative video prediction CONJUNCTION vision - based model - predictive control. vision - based model - predictive control CONJUNCTION long term extrapolative video prediction. approach COMPARE unsupervised methods. unsupervised methods COMPARE approach. unsupervised methods USED-FOR long - term future frame prediction of systems. approach USED-FOR long - term future frame prediction of systems. dynamics PART-OF model. interacting objects FEATURE-OF long - term future frame prediction of systems. inductive bias USED-FOR dynamics. vision - actuated model - based control USED-FOR pendulum system. goal - driven control CONJUNCTION physical reasoning. physical reasoning CONJUNCTION goal - driven control. controller ’s interpretability USED-FOR goal - driven control. physical reasoning USED-FOR zero - data adaptation. controller ’s interpretability USED-FOR physical reasoning. controller ’s interpretability USED-FOR zero - data adaptation. goal - driven control USED-FOR zero - data adaptation. OtherScientificTerm is labeled states. Method are differentiable physics, physics - as - inverse - graphics approach, and vision - physics integration. ","This paper proposes a new model for unsupervised physical parameter estimation of systems in video from video using only a few labeled states. The model is based on a physics-as-inverse-graphic approach, where the scene dynamics are modeled as differential equations, and the goal is to learn a differentiable physics that can be applied to any physical scene understanding methods that rely on object state supervision. The authors propose a framework for long term extrapolative video prediction, vision-based model-predictive control, and long-term future frame prediction of systems with interacting objects. The proposed approach is evaluated on a pendulum system with a vision-actuated model-based control and is shown to outperform existing un supervised methods for long-range future-frame prediction of such systems. In particular, the authors show that the dynamics in the model can be learned with an inductive bias and that the controller’s interpretability can be used for goal-driven control and physical reasoning for zero-data adaptation.   The authors also show that their approach is more interpretable than previous work that relies on vision-image-to-image translation, which is similar to the previous work, and that their model is able to capture the dynamics of the system in a single frame.  The main difference is that the authors propose to use a physics -as-intrinsic-rendering (PIG) approach, which uses a physics model of the state and velocity of the pendulum to learn the physics of the scene. This is a generalization of previous work (e.g. [1] from [2] that uses the physics-based approach to learn dynamics of a single pendulum, and [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15]  The paper is well-written and well-motivated. The idea of the approach is interesting and the results are convincing. However, there are a few issues in the paper:  (1) the authors do not discuss the relationship between their approach and prior work, (i.e. [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,19) and (12) the use of vision-physics integration is not discussed. ","This paper proposes a new model for unsupervised physical parameter estimation of systems in video from video using only a few labeled states. The model is based on a physics-as-inverse-graphic approach, where the scene dynamics are modeled as differential equations, and the goal is to learn a differentiable physics that can be applied to any physical scene understanding methods that rely on object state supervision. The authors propose a framework for long term extrapolative video prediction, vision-based model-predictive control, and long-term future frame prediction of systems with interacting objects. The proposed approach is evaluated on a pendulum system with a vision-actuated model-based control and is shown to outperform existing un supervised methods for long-range future-frame prediction of such systems. In particular, the authors show that the dynamics in the model can be learned with an inductive bias and that the controller’s interpretability can be used for goal-driven control and physical reasoning for zero-data adaptation.   The authors also show that their approach is more interpretable than previous work that relies on vision-image-to-image translation, which is similar to the previous work, and that their model is able to capture the dynamics of the system in a single frame.  The main difference is that the authors propose to use a physics -as-intrinsic-rendering (PIG) approach, which uses a physics model of the state and velocity of the pendulum to learn the physics of the scene. This is a generalization of previous work (e.g. [1] from [2] that uses the physics-based approach to learn dynamics of a single pendulum, and [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15]  The paper is well-written and well-motivated. The idea of the approach is interesting and the results are convincing. However, there are a few issues in the paper:  (1) the authors do not discuss the relationship between their approach and prior work, (i.e. [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,19) and (12) the use of vision-physics integration is not discussed. "
22248,SP:c6b8b682bf3087a65cb2379700b8a0183853c2af,"Graph Convolutional Networks ( GCN ) USED-FOR class relevance. Graph Convolutional Networks ( GCN ) USED-FOR structure of clean and noisy data. graph per class USED-FOR structure of clean and noisy data. GCN - inferred “ clean ” probability USED-FOR relevance measure. GCN USED-FOR binary classifier. weighted binary cross - entropy loss function USED-FOR GCN. weighted binary cross - entropy loss function USED-FOR binary classifier. few - shot learning problem EVALUATE-FOR method. classification accuracy EVALUATE-FOR few - shot classification. few - shot classification EVALUATE-FOR GCN - based cleaning process. classification accuracy EVALUATE-FOR GCN - based cleaning process. GCN - based method COMPARE transductive approach. transductive approach COMPARE GCN - based method. Method is classifier. OtherScientificTerm are clean from noisy examples, and relevance. Material is noisy data. ","Graph Convolutional Networks (GCN) are used to model class relevance, i.e., the ability of a classifier to distinguish clean from noisy examples. In this paper, the authors propose to learn the structure of clean and noisy data from a single graph per class. The relevance measure is based on the GCN-implicitly inferred “clean” probability, which is a measure of how well the classifier is able to distinguish the clean from the noisy data. The authors propose a novel GCN that uses a weighted binary cross-entropy loss function to train a binary classifier. The proposed method is evaluated on the few-shot learning problem, and the authors show that the proposed method improves the classification accuracy of a GCN -based cleaning process on the standard classification accuracy for few-task classification. The paper also shows that the performance of the proposed GCN is comparable to that of a transductive approach.  ","Graph Convolutional Networks (GCN) are used to model class relevance, i.e., the ability of a classifier to distinguish clean from noisy examples. In this paper, the authors propose to learn the structure of clean and noisy data from a single graph per class. The relevance measure is based on the GCN-implicitly inferred “clean” probability, which is a measure of how well the classifier is able to distinguish the clean from the noisy data. The authors propose a novel GCN that uses a weighted binary cross-entropy loss function to train a binary classifier. The proposed method is evaluated on the few-shot learning problem, and the authors show that the proposed method improves the classification accuracy of a GCN -based cleaning process on the standard classification accuracy for few-task classification. The paper also shows that the performance of the proposed GCN is comparable to that of a transductive approach.  "
22257,SP:dd9c9a5dccbba5dd15b03ca6b314a9e153e95548,"edge features CONJUNCTION message passing channels. message passing channels CONJUNCTION edge features. differentiable objective USED-FOR MI. variational approach USED-FOR differentiable objective. variational approach USED-FOR MI. objective USED-FOR model. MI - maximized models USED-FOR learning tasks. objective USED-FOR edge information. edge information FEATURE-OF model. molecular graphs USED-FOR regression. relation prediction in knowledge graphs HYPONYM-OF learning tasks. regression HYPONYM-OF learning tasks. Method is Graph Neural Networks ( GNNs ). OtherScientificTerm are representation vector, and parameterized transform matrix. Metric is Mutual Information ( MI ). Material is knowledge graphs. ","Graph Neural Networks (GNNs) learn a representation vector that maximizes the Mutual Information (MI) between edge features and message passing channels. The paper proposes a differentiable objective for maximizing the MI, which is based on a variational approach. The authors show that the proposed objective can be used to learn a model that maximises the edge information of a model, and that MI-maximized models can be applied to a variety of learning tasks, including regression on molecular graphs and relation prediction in knowledge graphs. ","Graph Neural Networks (GNNs) learn a representation vector that maximizes the Mutual Information (MI) between edge features and message passing channels. The paper proposes a differentiable objective for maximizing the MI, which is based on a variational approach. The authors show that the proposed objective can be used to learn a model that maximises the edge information of a model, and that MI-maximized models can be applied to a variety of learning tasks, including regression on molecular graphs and relation prediction in knowledge graphs. "
22266,SP:f1cf63d728da51b4f83eb50ef69e3788b3a5ed74,"models USED-FOR specifying visual transformations. Generative networks USED-FOR specifying visual transformations. Generative networks HYPONYM-OF models. verification methods USED-FOR generative networks. verifier USED-FOR generative networks. APPROXLINE HYPONYM-OF verifier. deterministic and probabilistic abstract interpretation USED-FOR APPROXLINE. Task is certification of generative models. Method is generative models. OtherScientificTerm are sufficient non - convexity, non - convexity, and network ’s latent space. ","This paper studies the certification of generative models, i.e., models that are capable of specifying visual transformations (e.g. Generative networks). The authors propose two verification methods to certify generative networks. The first verifier, APPROXLINE, is a verifier that verifies the non-convexity of the network’s latent space, and the second verifier is a deterministic and probabilistic abstract interpretation of the latent space. Both verifiers are based on the assumption that the network's latent space is non-consistent. ","This paper studies the certification of generative models, i.e., models that are capable of specifying visual transformations (e.g. Generative networks). The authors propose two verification methods to certify generative networks. The first verifier, APPROXLINE, is a verifier that verifies the non-convexity of the network’s latent space, and the second verifier is a deterministic and probabilistic abstract interpretation of the latent space. Both verifiers are based on the assumption that the network's latent space is non-consistent. "
22275,SP:2b0887dcf09249e8cee30d38163aeb9ef1e92b27,"deep architectures USED-FOR models. spectral graph convolutional operator USED-FOR graph neural networks ( GNNs ). GNNs USED-FOR suspended animation problem. GNNs USED-FOR suspended animation problem. nodes ’ raw features CONJUNCTION intermediate representations. intermediate representations CONJUNCTION nodes ’ raw features. intermediate representations PART-OF graph. intermediate representations USED-FOR model layers. GRESNET ( Graph Residual Network ) framework USED-FOR connected highways. norm preservation perspective USED-FOR graph residual terms. GCN CONJUNCTION GAT. GAT CONJUNCTION GCN. GAT CONJUNCTION LOOPYNET. LOOPYNET CONJUNCTION GAT. GRESNET framework USED-FOR GNNs. LOOPYNET HYPONYM-OF GNNs. GCN HYPONYM-OF GNNs. GAT HYPONYM-OF GNNs. Generic are problem, model, and learning settings. OtherScientificTerm are model depth, suspended animation limit, and node ’s representations. Material are graph data, and real - world benchmark datasets. Method is residual learning methods. ","This paper proposes a spectral graph convolutional operator for graph neural networks (GNNs) to solve the so-called ""suspended animation problem"", which is the problem that models trained with deep architectures are unable to perform well on graph data due to the lack of model depth. GNNs have been shown to solve this suspended animation problem in the past, but this paper proposes to extend the model to the case where the model depth is much smaller than the suspended animation limit. The authors propose a GRESNET (Graph Residual Network) framework to learn the connected highways between nodes’ raw features and intermediate representations in a graph, and then use these intermediate representations to train model layers.    The authors show that under certain learning settings, residual learning methods can be used to preserve the graph residual terms from a norm preservation perspective. They show that existing GNN’s such as GCN, GAT, and LOOPYNET can all be trained using the GRESnet framework. They also show that the learned residuals can be reused across different types of graph data and different learning settings. Finally, the authors provide some experiments on several real-world benchmark datasets to show the effectiveness of the proposed method. ","This paper proposes a spectral graph convolutional operator for graph neural networks (GNNs) to solve the so-called ""suspended animation problem"", which is the problem that models trained with deep architectures are unable to perform well on graph data due to the lack of model depth. GNNs have been shown to solve this suspended animation problem in the past, but this paper proposes to extend the model to the case where the model depth is much smaller than the suspended animation limit. The authors propose a GRESNET (Graph Residual Network) framework to learn the connected highways between nodes’ raw features and intermediate representations in a graph, and then use these intermediate representations to train model layers.    The authors show that under certain learning settings, residual learning methods can be used to preserve the graph residual terms from a norm preservation perspective. They show that existing GNN’s such as GCN, GAT, and LOOPYNET can all be trained using the GRESnet framework. They also show that the learned residuals can be reused across different types of graph data and different learning settings. Finally, the authors provide some experiments on several real-world benchmark datasets to show the effectiveness of the proposed method. "
22284,SP:dc436ade4d04072de35a90e5e4a1bfebfddb04e9,"face prior knowledge USED-FOR reconstruction process. limited scan data USED-FOR face prior knowledge. linear 3D morphable models ( 3DMM ) HYPONYM-OF face prior knowledge. limited scan data USED-FOR linear 3D morphable models ( 3DMM ). face prior knowledge USED-FOR ambiguity. expressions CONJUNCTION poses. poses CONJUNCTION expressions. poses CONJUNCTION lightings. lightings CONJUNCTION poses. expressions FEATURE-OF facial images. poses FEATURE-OF facial images. linear parametric models USED-FOR methods. convolutional neural networks ( CNN ) USED-FOR nonlinear parametric model. linear 3DMM USED-FOR dataset. dataset USED-FOR models. identity and expression representations PART-OF models. semi - supervised manner FEATURE-OF adversarial loss. unconstrained photo collections USED-FOR unlabeled face images. adversarial loss USED-FOR model. semi - supervised manner USED-FOR model. hybrid batches of unlabeled and labeled face images USED-FOR model. identity shape FEATURE-OF facial images. expression CONJUNCTION pose. pose CONJUNCTION expression. identity CONJUNCTION expression. expression CONJUNCTION identity. pose CONJUNCTION lighting representations. lighting representations CONJUNCTION pose. model USED-FOR facial editing applications. model USED-FOR expression. model USED-FOR identity. model USED-FOR lighting representations. expression transfer HYPONYM-OF facial editing applications. model COMPARE state - of - the - art methods. state - of - the - art methods COMPARE model. model USED-FOR expression. Task are Recovering 3D geometry shape, ill - posed problem, and reconstruction. OtherScientificTerm is center loss. ","This paper proposes a new method for face reconstruction based on linear 3D morphable models (3DMM). The key idea is to use the face prior knowledge of a 3D model to guide the reconstruction process. The authors propose a new dataset for this purpose, which is built on top of the existing 3D Morphable Model dataset. The paper also proposes a semi-supervised adversarial training method to improve the performance of the proposed method. ","This paper proposes a new method for face reconstruction based on linear 3D morphable models (3DMM). The key idea is to use the face prior knowledge of a 3D model to guide the reconstruction process. The authors propose a new dataset for this purpose, which is built on top of the existing 3D Morphable Model dataset. The paper also proposes a semi-supervised adversarial training method to improve the performance of the proposed method. "
22293,SP:f7bc06697b09e2d59ec06b2cbcf3c0828ece32ae,"transition kernel USED-FOR Model - based imitation learning methods. partial knowledge FEATURE-OF transition kernel. Reinforcement Learning ( RL ) USED-FOR imitation problems. unknown transition kernel CONJUNCTION synthetic kernel. synthetic kernel CONJUNCTION unknown transition kernel. synthetic kernel USED-FOR transition of state components. kernel USED-FOR state components. transition kernel USED-FOR transition of state components. multiplayer games FEATURE-OF imitation tasks. policy gradient algorithm CONJUNCTION model. model CONJUNCTION policy gradient algorithm. policy gradient algorithm COMPARE simulation - free alternative. simulation - free alternative COMPARE policy gradient algorithm. model COMPARE simulation - free alternative. simulation - free alternative COMPARE model. Task is policy evaluation. Method are eMDP, and transition model. Generic is components. OtherScientificTerm is sr. ","Model-based imitation learning methods rely on the transition kernel with partial knowledge. Reinforcement Learning (RL) has been shown to be useful for imitation problems, but policy evaluation is expensive. This paper proposes to use an eMDP to model the transition of state components between two states. The transition kernel of the transition model is known to be non-trivial. The authors propose to use the unknown transition kernel and a synthetic kernel to learn the transition between the two state components. Experiments on imitation tasks in multiplayer games show that the proposed policy gradient algorithm and the proposed model outperform a simulation-free alternative.","Model-based imitation learning methods rely on the transition kernel with partial knowledge. Reinforcement Learning (RL) has been shown to be useful for imitation problems, but policy evaluation is expensive. This paper proposes to use an eMDP to model the transition of state components between two states. The transition kernel of the transition model is known to be non-trivial. The authors propose to use the unknown transition kernel and a synthetic kernel to learn the transition between the two state components. Experiments on imitation tasks in multiplayer games show that the proposed policy gradient algorithm and the proposed model outperform a simulation-free alternative."
22302,SP:82cce92821e8168ab4a6fd67573b66c1d17673b8,"manually - designed reward function USED-FOR Learning useful skills. OpenAI Gym CONJUNCTION navigation task. navigation task CONJUNCTION OpenAI Gym. OpenAI Gym FEATURE-OF simulated robotic manipulation tasks. navigation task HYPONYM-OF simulated robotic manipulation tasks. Gazebo simulator USED-FOR navigation task. navigation task EVALUATE-FOR approach. simulated robotic manipulation tasks EVALUATE-FOR approach. intrinsic mutual information rewards USED-FOR method. mutual information discriminator USED-FOR learning. pre - trained policy USED-FOR learning. pre - trained policy CONJUNCTION mutual information discriminator. mutual information discriminator CONJUNCTION pre - trained policy. learning USED-FOR task rewards. mutual information discriminator USED-FOR task rewards. pre - trained policy USED-FOR task rewards. sparse rewards USED-FOR robotic manipulation tasks. Method are reinforcement learning, and selfsupervised Reinforcement Learning approach. OtherScientificTerm are external reward function, intrinsic objective, context states, robot states, states of interest, and mutual information. ","This paper proposes a self-supervised reinforcement learning approach to learn skills that are useful for reinforcement learning. Learning useful skills with a manually-designed reward function is an important problem in reinforcement learning, and the authors propose a selfsupervised Reinforcement Learning approach to address this problem. The proposed method is based on intrinsic mutual information rewards, where the intrinsic objective is to maximize the mutual information between the robot state and the external reward function. The authors demonstrate that the proposed method achieves state-of-the-art performance on two simulated robotic manipulation tasks from OpenAI Gym and a navigation task from the Gazebo simulator. The key idea is to learn a pre-trained policy and a mutual information discriminator to guide the learning of task rewards. The pre-training is done by sampling from a set of context states, where context states correspond to robot states of interest, and mutual information is used to train the discriminator. The paper also shows that learning with sparse rewards can be more effective than using sparse rewards for a variety of robotic manipulation environments.   ","This paper proposes a self-supervised reinforcement learning approach to learn skills that are useful for reinforcement learning. Learning useful skills with a manually-designed reward function is an important problem in reinforcement learning, and the authors propose a selfsupervised Reinforcement Learning approach to address this problem. The proposed method is based on intrinsic mutual information rewards, where the intrinsic objective is to maximize the mutual information between the robot state and the external reward function. The authors demonstrate that the proposed method achieves state-of-the-art performance on two simulated robotic manipulation tasks from OpenAI Gym and a navigation task from the Gazebo simulator. The key idea is to learn a pre-trained policy and a mutual information discriminator to guide the learning of task rewards. The pre-training is done by sampling from a set of context states, where context states correspond to robot states of interest, and mutual information is used to train the discriminator. The paper also shows that learning with sparse rewards can be more effective than using sparse rewards for a variety of robotic manipulation environments.   "
22311,SP:5db63d39cfd8132bec832ab64b8fbd403b3b8df0,"Neural network ( NN ) trojaning attack HYPONYM-OF attack. adversarial attacks COMPARE it. it COMPARE adversarial attacks. it USED-FOR malicious functionality. small datasets USED-FOR NN trojaning attacks. generality CONJUNCTION stealthiness. stealthiness CONJUNCTION generality. trojaning attack method USED-FOR large models. capability CONJUNCTION generality. generality CONJUNCTION capability. trojaning attack method COMPARE studies. studies COMPARE trojaning attack method. generality EVALUATE-FOR studies. capability EVALUATE-FOR studies. stealthiness EVALUATE-FOR studies. trojaning attack USED-FOR small domain. large - scale dataset USED-FOR trojaned model. biased behavior FEATURE-OF trojan. Method is NN models. OtherScientificTerm are weight parameters, fixed target classes, and malicious misclassification target. ","This paper proposes a new type of attack, called neural network (NN) trojaning attack, which aims to fool NN models to misclassify the weight parameters of the model. Unlike adversarial attacks, it does not require a fixed target classes, and it can be applied to any malicious functionality. Previous work has shown that NN trojans attacks on small datasets are effective. This paper shows that a trojaned model trained on a large-scale dataset with a large number of classes is able to fool a large model on a small domain. The authors show that the trojan has a biased behavior, and that the malicious misclassification target is not fixed. They also show that this trojaning attack method can be used to fool large models as well. The paper also shows that the proposed trojaned attack method outperforms previous studies in terms of capability, generality, and stealthiness. ","This paper proposes a new type of attack, called neural network (NN) trojaning attack, which aims to fool NN models to misclassify the weight parameters of the model. Unlike adversarial attacks, it does not require a fixed target classes, and it can be applied to any malicious functionality. Previous work has shown that NN trojans attacks on small datasets are effective. This paper shows that a trojaned model trained on a large-scale dataset with a large number of classes is able to fool a large model on a small domain. The authors show that the trojan has a biased behavior, and that the malicious misclassification target is not fixed. They also show that this trojaning attack method can be used to fool large models as well. The paper also shows that the proposed trojaned attack method outperforms previous studies in terms of capability, generality, and stealthiness. "
22320,SP:35ea626ee4dd1a7a368a660eb852192924966b7f,prediction tasks PART-OF drug 1 discovery. few - shot regression ( FSR ) problems USED-FOR prediction tasks. modelling of biological assays HYPONYM-OF few - shot regression ( FSR ) problems. reinforcement learning methods USED-FOR applications. few - shot classification USED-FOR applications. few - shot classification CONJUNCTION reinforcement learning methods. reinforcement learning methods CONJUNCTION few - shot classification. FSR methods USED-FOR tasks. real - world constraints FEATURE-OF tasks. deep kernel learning USED-FOR FSR 6 algorithm. kernel function CONJUNCTION differentiable kernel 8 algorithm. differentiable kernel 8 algorithm CONJUNCTION kernel function. deep network CONJUNCTION kernel function. kernel function CONJUNCTION deep network. deep network CONJUNCTION differentiable kernel 8 algorithm. differentiable kernel 8 algorithm CONJUNCTION deep network. deep network PART-OF algorithm. differentiable kernel 8 algorithm PART-OF algorithm. kernel function PART-OF algorithm. kernel USED-FOR task. algorithm USED-FOR kernel. kernel USED-FOR inference. task PART-OF inference. algorithm USED-FOR task. It COMPARE state - of - the - art algorithms. state - of - the - art algorithms COMPARE It. It USED-FOR complex task distributions. real - world benchmarks EVALUATE-FOR state - of - the - art algorithms. real - world benchmarks EVALUATE-FOR It. FSR algorithms USED-FOR noisy and uncertain environments. biological assays USED-FOR benchmarks. drug discovery HYPONYM-OF noisy and uncertain environments. Task is data generation. ,"This paper studies the few-shot regression (FSR) problems (e.g., modelling of biological assays) and prediction tasks in drug 1 discovery, which are popular prediction tasks with real-world constraints. The authors propose a new FSR 6 algorithm based on deep kernel learning, which is a generalization of existing FSR methods for a variety of tasks that are often considered in the literature. The paper also proposes a few applications of FSR and reinforcement learning methods to these applications. The algorithm consists of a deep network, a kernel function, and a differentiable kernel 8 algorithm. It outperforms state-of-the-art algorithms on a number of real-life benchmarks with noisy and uncertain environments (drug discovery, drug discovery, and data generation). It is also able to learn complex task distributions, and is able to generalize well to unseen tasks.    The algorithm is based on a kernel that is learned for each task, and the kernel is used in the inference of each task. The kernel is learned in the same way as in previous work, but the algorithm is differentiable, which means that the algorithm can be used to learn the kernel for any task.  The authors also propose an algorithm that combines the kernel function with the differentiable Kernel 8 algorithm, and show that this algorithm can generalize to unseen task distributions.  Experiments are conducted on a set of benchmarks based on biological assay and drug discovery. ","This paper studies the few-shot regression (FSR) problems (e.g., modelling of biological assays) and prediction tasks in drug 1 discovery, which are popular prediction tasks with real-world constraints. The authors propose a new FSR 6 algorithm based on deep kernel learning, which is a generalization of existing FSR methods for a variety of tasks that are often considered in the literature. The paper also proposes a few applications of FSR and reinforcement learning methods to these applications. The algorithm consists of a deep network, a kernel function, and a differentiable kernel 8 algorithm. It outperforms state-of-the-art algorithms on a number of real-life benchmarks with noisy and uncertain environments (drug discovery, drug discovery, and data generation). It is also able to learn complex task distributions, and is able to generalize well to unseen tasks.    The algorithm is based on a kernel that is learned for each task, and the kernel is used in the inference of each task. The kernel is learned in the same way as in previous work, but the algorithm is differentiable, which means that the algorithm can be used to learn the kernel for any task.  The authors also propose an algorithm that combines the kernel function with the differentiable Kernel 8 algorithm, and show that this algorithm can generalize to unseen task distributions.  Experiments are conducted on a set of benchmarks based on biological assay and drug discovery. "
22329,SP:91ca4c3ee07617356250bae9f4ef9799b3b134ff,"Neural networks USED-FOR reasoning tasks. Graph Neural Networks ( GNNs ) USED-FOR tasks. Graph Neural Networks ( GNNs ) HYPONYM-OF network structures. network structures USED-FOR tasks. expressive power FEATURE-OF they. framework USED-FOR reasoning tasks. computation structure COMPARE algorithmic structure. algorithmic structure COMPARE computation structure. algorithmic structure FEATURE-OF reasoning process. network USED-FOR reasoning tasks. sample complexity bound EVALUATE-FOR alignment. framework EVALUATE-FOR reasoning models. visual question answering CONJUNCTION shortest paths. shortest paths CONJUNCTION visual question answering. intuitive physics CONJUNCTION visual question answering. visual question answering CONJUNCTION intuitive physics. dynamic programming ( DP ) HYPONYM-OF algorithmic paradigm. dynamic programming ( DP ) USED-FOR reasoning tasks. algorithmic paradigm USED-FOR reasoning tasks. intuitive physics HYPONYM-OF reasoning tasks. shortest paths HYPONYM-OF reasoning tasks. visual question answering HYPONYM-OF reasoning tasks. GNNs USED-FOR tasks. GNNs COMPARE DP. DP COMPARE GNNs. reasoning tasks EVALUATE-FOR theory. Method is structured networks. OtherScientificTerm are network structure, and algorithmic alignment. ","This paper proposes a new framework for studying the expressive power of GNNs for reasoning tasks. The authors show that GNN-based models can be seen as a special case of dynamic programming (DP), which is a generalization of DP. The paper also shows that a GNN can be viewed as an extension of DP, and that it can be used to solve a number of reasoning tasks, including intuitive physics, visual question answering, and shortest paths. ","This paper proposes a new framework for studying the expressive power of GNNs for reasoning tasks. The authors show that GNN-based models can be seen as a special case of dynamic programming (DP), which is a generalization of DP. The paper also shows that a GNN can be viewed as an extension of DP, and that it can be used to solve a number of reasoning tasks, including intuitive physics, visual question answering, and shortest paths. "
22338,SP:a52aee8da5cf5acd2baf3c2a62cb679e13b18bd5,conditional consistency CONJUNCTION intra - conditioning diversity. intra - conditioning diversity CONJUNCTION conditional consistency. image quality CONJUNCTION conditional consistency. conditional consistency CONJUNCTION image quality. conditional consistency HYPONYM-OF metrics. metrics EVALUATE-FOR models. it USED-FOR properties. Fréchet distance FEATURE-OF joint distributions. metric USED-FOR it. metric USED-FOR properties. controllable synthetic dataset EVALUATE-FOR FJD. FJD COMPARE metrics. metrics COMPARE FJD. bounding boxes CONJUNCTION images. images CONJUNCTION bounding boxes. object masks CONJUNCTION bounding boxes. bounding boxes CONJUNCTION object masks. images CONJUNCTION text captions. text captions CONJUNCTION images. class labels CONJUNCTION object masks. object masks CONJUNCTION class labels. metric EVALUATE-FOR cGAN - based models. cGAN - based models USED-FOR conditioning modalities. text captions HYPONYM-OF conditioning modalities. images HYPONYM-OF conditioning modalities. class labels HYPONYM-OF conditioning modalities. bounding boxes HYPONYM-OF conditioning modalities. object masks HYPONYM-OF conditioning modalities. cGAN benchmarking CONJUNCTION model selection. model selection CONJUNCTION cGAN benchmarking. metric USED-FOR cGAN benchmarking. metric USED-FOR model selection. FJD USED-FOR metric. FJD USED-FOR cGAN benchmarking. Method is Conditional Generative Adversarial Networks ( cGANs ). Task is model benchmarking. Metric is Fréchet Joint Distance ( FJD ). OtherScientificTerm is conditioning. ,"This paper proposes a new benchmarking metric for Conditional Generative Adversarial Networks (cGANs) based on the Fréchet Joint Distance (FJD) between the joint distributions of the conditioned and unconditioned conditional distributions. The authors argue that existing metrics for evaluating models (e.g., image quality, conditional consistency, intra-conditioning diversity, etc.) do not take into account the joint distribution of the two conditional distributions, and propose a new metric that does so. The proposed metric is based on Frerchet distance between two joint distributions, which is a measure of the distance between the conditional consistency of a pair of data points and their joint distributions.  The authors show that the proposed FJD is more robust to conditioning modalities (i.e., class labels, object masks, bounding boxes, images, and text captions) than existing metrics, and that it can be used to measure the properties of the joint and non-joint distributions. They also show that FJD outperforms existing metrics on a controllable synthetic dataset.  Finally, the authors evaluate the proposed metric for cGAN-based models on a variety of conditioningmodalities (class labels, class masks, and images). The authors also demonstrate that the FJD improves the performance of cGAN benchmarking and model selection using the new metric.   ","This paper proposes a new benchmarking metric for Conditional Generative Adversarial Networks (cGANs) based on the Fréchet Joint Distance (FJD) between the joint distributions of the conditioned and unconditioned conditional distributions. The authors argue that existing metrics for evaluating models (e.g., image quality, conditional consistency, intra-conditioning diversity, etc.) do not take into account the joint distribution of the two conditional distributions, and propose a new metric that does so. The proposed metric is based on Frerchet distance between two joint distributions, which is a measure of the distance between the conditional consistency of a pair of data points and their joint distributions.  The authors show that the proposed FJD is more robust to conditioning modalities (i.e., class labels, object masks, bounding boxes, images, and text captions) than existing metrics, and that it can be used to measure the properties of the joint and non-joint distributions. They also show that FJD outperforms existing metrics on a controllable synthetic dataset.  Finally, the authors evaluate the proposed metric for cGAN-based models on a variety of conditioningmodalities (class labels, class masks, and images). The authors also demonstrate that the FJD improves the performance of cGAN benchmarking and model selection using the new metric.   "
22347,SP:fa822e8472efae17c7dfde8258057898383ecbbb,"decision states USED-FOR exploration. exploration USED-FOR downstream goal - driven tasks. partially observable environments FEATURE-OF downstream goal - driven tasks. Method is VIC framework. OtherScientificTerm are empowerment objective, and extrinsic rewards. Task is identification of decision states. ","This paper proposes the VIC framework, in which the empowerment objective is replaced by the identification of decision states that can be used to guide exploration in the context of exploration in downstream goal-driven tasks in partially observable environments. The key idea of the paper is to use extrinsic rewards to encourage exploration in an environment that is partially observable. The paper is well-written and well-motivated.   ","This paper proposes the VIC framework, in which the empowerment objective is replaced by the identification of decision states that can be used to guide exploration in the context of exploration in downstream goal-driven tasks in partially observable environments. The key idea of the paper is to use extrinsic rewards to encourage exploration in an environment that is partially observable. The paper is well-written and well-motivated.   "
22356,SP:a19a51df7e28a5d3380be4fba13842efbfe3efec,"architectures USED-FOR irregularly - sampled and asynchronous time series. real - world datasets FEATURE-OF irregularly - sampled and asynchronous time series. healthcare applications HYPONYM-OF real - world datasets. framework USED-FOR classifying irregularly sampled time series. data efficiency EVALUATE-FOR framework. unaligned measurements USED-FOR irregularly sampled time series. method COMPARE competitors. competitors COMPARE method. runtime EVALUATE-FOR it. healthcare time series datasets EVALUATE-FOR competitors. healthcare time series datasets EVALUATE-FOR method. Method are deep neural networks, and differentiable set function learning. Task is online monitoring scenarios. ","This paper proposes a new architecture for irregularly-sampled and asynchronous time series on real-world datasets (e.g. healthcare applications). The authors propose a new framework for classifying irregularly sampled time series with unaligned measurements, which is an important problem for deep neural networks. The framework is based on differentiable set function learning, and the authors show that the proposed framework improves data efficiency and data efficiency in online monitoring scenarios. The proposed method outperforms competitors on a number of healthcare time series datasets, and it also improves the runtime in some cases.","This paper proposes a new architecture for irregularly-sampled and asynchronous time series on real-world datasets (e.g. healthcare applications). The authors propose a new framework for classifying irregularly sampled time series with unaligned measurements, which is an important problem for deep neural networks. The framework is based on differentiable set function learning, and the authors show that the proposed framework improves data efficiency and data efficiency in online monitoring scenarios. The proposed method outperforms competitors on a number of healthcare time series datasets, and it also improves the runtime in some cases."
22365,SP:4ae89d64460b08749acc192004545c1fa8b7553b,Convolutional neural networks ( CNNs ) USED-FOR image recognition. inductive biases USED-FOR natural image priors. inductive biases FEATURE-OF CNNs. deep networks USED-FOR audio signals. inductive biases FEATURE-OF audio signals. inductive biases FEATURE-OF deep networks. network architectures USED-FOR audio processing. local neighborhoods USED-FOR convolutional kernels. harmonic series USED-FOR kernels. networks USED-FOR audio priors. networks USED-FOR unsupervised audio restoration. Harmonic Convolution USED-FOR networks. Harmonic Convolution USED-FOR they. they USED-FOR supervised musical source separation. Harmonic Convolution USED-FOR supervised musical source separation. generalization EVALUATE-FOR supervised musical source separation. generalization EVALUATE-FOR they. Generic is priors. OtherScientificTerm is harmonic structure. ,"This paper studies the inductive biases of convolutional neural networks (CNNs) for image recognition. The authors show that deep networks have similar audio signals that share the same inductive bias as natural image priors, but that their priors are more complex and more inductive.   The authors propose two network architectures for audio processing. The first is based on the harmonic structure of the input audio signal, and the second is built on the local neighborhoods of the convolution kernel of the harmonic series.  They show that these two networks are able to learn audio priors and that they can be trained with Harmonic Convolution. They also show that the networks can be used for unsupervised audio restoration. Finally, they show that they improve the generalization performance of supervised musical source separation with the help of the use of the proposed method, and demonstrate that they also improve generalization in the presence of noise. ","This paper studies the inductive biases of convolutional neural networks (CNNs) for image recognition. The authors show that deep networks have similar audio signals that share the same inductive bias as natural image priors, but that their priors are more complex and more inductive.   The authors propose two network architectures for audio processing. The first is based on the harmonic structure of the input audio signal, and the second is built on the local neighborhoods of the convolution kernel of the harmonic series.  They show that these two networks are able to learn audio priors and that they can be trained with Harmonic Convolution. They also show that the networks can be used for unsupervised audio restoration. Finally, they show that they improve the generalization performance of supervised musical source separation with the help of the use of the proposed method, and demonstrate that they also improve generalization in the presence of noise. "
22374,SP:c81a2b3fd1c56b9b18e4a358e3ff8b40aea5256a,"specialized hardware accelerators USED-FOR neural network training. GPUs USED-FOR neural network training. GPUs CONJUNCTION specialized hardware accelerators. specialized hardware accelerators CONJUNCTION GPUs. disk I / O CONJUNCTION data preprocessing. data preprocessing CONJUNCTION disk I / O. workloads EVALUATE-FOR data echoing algorithms. data echoing algorithm COMPARE baseline. baseline COMPARE data echoing algorithm. upstream computation USED-FOR data echoing algorithm. upstream computation USED-FOR baseline. wall - clock time FEATURE-OF ResNet-50. ImageNet FEATURE-OF ResNet-50. Task are training pipeline, and training. OtherScientificTerm are accelerators, echoing, and batch sizes. Method are data echoing, pipeline stages, Data echoing, and network. Metric is training time. ","This paper studies the problem of speeding up the training of neural network training on GPUs and specialized hardware accelerators on GPUs. The authors propose a new training pipeline, called data echoing, where the accelerators are shared across all the pipeline stages. Data echoing is used to reduce the amount of data that needs to be shared across the entire network during training. The paper shows that the benefits of data echoing are demonstrated on a number of different workloads, including disk I/O, data preprocessing, and batch sizes. In addition, the authors show that the data echoing algorithms outperform baseline algorithms on a variety of workloads. They also show that their data echoing algorithm outperforms a baseline that does not use any upstream computation. Finally, they show that ResNet-50 with wall-clock time on ImageNet is able to achieve state-of-the-art results on a few datasets.   ","This paper studies the problem of speeding up the training of neural network training on GPUs and specialized hardware accelerators on GPUs. The authors propose a new training pipeline, called data echoing, where the accelerators are shared across all the pipeline stages. Data echoing is used to reduce the amount of data that needs to be shared across the entire network during training. The paper shows that the benefits of data echoing are demonstrated on a number of different workloads, including disk I/O, data preprocessing, and batch sizes. In addition, the authors show that the data echoing algorithms outperform baseline algorithms on a variety of workloads. They also show that their data echoing algorithm outperforms a baseline that does not use any upstream computation. Finally, they show that ResNet-50 with wall-clock time on ImageNet is able to achieve state-of-the-art results on a few datasets.   "
22383,SP:b4cf56d3fa7d65cacde33f17cd04bd5bbc52dd71,"policy COMPARE policies. policies COMPARE policy. controllable subspace FEATURE-OF Markov decision process. Markov decision process FEATURE-OF behaviors. controllable subspace FEATURE-OF behaviors. Successor features USED-FOR generalization problem. grounded feature space FEATURE-OF reward function. generalization CONJUNCTION task inference. task inference CONJUNCTION generalization. algorithm USED-FOR generalization. algorithm USED-FOR task inference. Variational Intrinsic Successor FeatuRes ( VISR ) USED-FOR task inference. Variational Intrinsic Successor FeatuRes ( VISR ) HYPONYM-OF algorithm. successor features framework USED-FOR task inference. Atari suite EVALUATE-FOR VISR. human - level performance EVALUATE-FOR VISR. Generic are formulation, tasks, techniques, and method. OtherScientificTerm are controllable features, and limited feedback. ","This paper proposes a new formulation of the successor features problem. Successor features is defined as a generalization problem where the goal is to learn behaviors in a controllable subspace of a Markov decision process, where a policy is trained to generalize better than other policies. The paper proposes two techniques to tackle this problem: (1) learn a set of controllative features, (2) learn the reward function in a grounded feature space, and (3) use these learned features to guide the generalization and task inference.  The paper shows that the proposed algorithm, Variational Intrinsic Successor FeatuRes (VISR), is able to improve generalization performance on a number of tasks with limited feedback. The algorithm is based on a successor features framework, and is a simple algorithm that can be applied to any task inference problem. The experimental results on the Atari suite show that VISR improves human-level performance on some of the tasks.  ","This paper proposes a new formulation of the successor features problem. Successor features is defined as a generalization problem where the goal is to learn behaviors in a controllable subspace of a Markov decision process, where a policy is trained to generalize better than other policies. The paper proposes two techniques to tackle this problem: (1) learn a set of controllative features, (2) learn the reward function in a grounded feature space, and (3) use these learned features to guide the generalization and task inference.  The paper shows that the proposed algorithm, Variational Intrinsic Successor FeatuRes (VISR), is able to improve generalization performance on a number of tasks with limited feedback. The algorithm is based on a successor features framework, and is a simple algorithm that can be applied to any task inference problem. The experimental results on the Atari suite show that VISR improves human-level performance on some of the tasks.  "
22392,SP:83500230586a9134f910ad067b7233dc563dc1ba,"functional view USED-FOR networks. functional view USED-FOR them. smoothness of the functional approximation CONJUNCTION flat initial approximation. flat initial approximation CONJUNCTION smoothness of the functional approximation. smoothness of the functional approximation USED-FOR generalization. flat initial approximation USED-FOR generalization. Method are deep neural networks, functional view of these networks, and massively overparamaterized networks. OtherScientificTerm are initializations, loss surface, and smoothness. "," the functional view of deep neural networks. The functional view allows us to view the networks from a functional view, and to understand them in terms of the smoothness of the functional approximation and the flat initial approximation. In this paper, the authors show that the generalization performance of massively overparameterized networks with smooth initializations is a function of the loss surface, and that smoothness is correlated with generalization.   "," the functional view of deep neural networks. The functional view allows us to view the networks from a functional view, and to understand them in terms of the smoothness of the functional approximation and the flat initial approximation. In this paper, the authors show that the generalization performance of massively overparameterized networks with smooth initializations is a function of the loss surface, and that smoothness is correlated with generalization.   "
22401,SP:7225825e353b711a7d023f706fafe5e17e4e2fb2,"Generative Adversarial Network ( GAN ) USED-FOR image - to - image translation problem. imbalance problem FEATURE-OF GAN - based methods. mode collapse CONJUNCTION diminished gradients. diminished gradients CONJUNCTION mode collapse. generator CONJUNCTION discriminator. discriminator CONJUNCTION generator. relative model capacities FEATURE-OF generator. relative model capacities FEATURE-OF discriminator. attention mechanism USED-FOR GuideGAN. it USED-FOR attention map. attention mechanism USED-FOR discriminator. attention map USED-FOR generator. image transfer tasks EVALUATE-FOR GuideGAN framework. Task are supervised and unsupervised manner, and prediction. Generic is approach. "," in supervised and unsupervised manner. This paper tackles the image-to-image translation problem using a Generative Adversarial Network (GAN) to address the imbalance problem in GAN-based methods. The authors propose a new approach, called GuideGAN, to tackle the issue of mode collapse and diminished gradients. The key idea of the proposed approach is to use an attention mechanism in the generator and discriminator to improve the relative model capacities of the generator to the discriminator. In particular, it learns an attention map for the generator, which is then used to train a discriminator that is more robust to mode collapse. Experiments on several image transfer tasks demonstrate the effectiveness of the GuideGAN framework. "," in supervised and unsupervised manner. This paper tackles the image-to-image translation problem using a Generative Adversarial Network (GAN) to address the imbalance problem in GAN-based methods. The authors propose a new approach, called GuideGAN, to tackle the issue of mode collapse and diminished gradients. The key idea of the proposed approach is to use an attention mechanism in the generator and discriminator to improve the relative model capacities of the generator to the discriminator. In particular, it learns an attention map for the generator, which is then used to train a discriminator that is more robust to mode collapse. Experiments on several image transfer tasks demonstrate the effectiveness of the GuideGAN framework. "
22410,SP:41c089ba65393174dae1dc136f79030a0a4fc532,"attention layers CONJUNCTION hypernetworks. hypernetworks CONJUNCTION attention layers. hypernetworks CONJUNCTION dynamic convolutions. dynamic convolutions CONJUNCTION hypernetworks. multiplicative interaction USED-FOR neural network architectural motifs. gating CONJUNCTION attention layers. attention layers CONJUNCTION gating. dynamic convolutions HYPONYM-OF neural network architectural motifs. hypernetworks HYPONYM-OF neural network architectural motifs. attention layers HYPONYM-OF neural network architectural motifs. gating HYPONYM-OF neural network architectural motifs. Multiplicative interaction layers HYPONYM-OF primitive operations. multiplicative interactions USED-FOR inductive bias. multiplicative interactions USED-FOR neural network architectures. them USED-FOR multiplicative interactions. Generic are layers, and they. Method is neural networks. OtherScientificTerm are conditional computation, and concatenation operation. ","This paper studies neural network architectural motifs such as gating, attention layers, hypernetworks, and dynamic convolutions based on multiplicative interaction. Multiplicative interaction layers are primitive operations that are common in modern neural networks. The authors show that these layers are inductive biased, meaning that they can be seen as a special case of conditional computation. They also show that multiplicative interactions in neural network architectures can be viewed as an inductive bias. They further show that the inductive biases of these primitive operations (e.g. gating and attention layers) can be understood as a result of the concatenation operation.   The authors also show how to use them to construct a set of neural network architecture that can be used to construct multiplicative interactions. ","This paper studies neural network architectural motifs such as gating, attention layers, hypernetworks, and dynamic convolutions based on multiplicative interaction. Multiplicative interaction layers are primitive operations that are common in modern neural networks. The authors show that these layers are inductive biased, meaning that they can be seen as a special case of conditional computation. They also show that multiplicative interactions in neural network architectures can be viewed as an inductive bias. They further show that the inductive biases of these primitive operations (e.g. gating and attention layers) can be understood as a result of the concatenation operation.   The authors also show how to use them to construct a set of neural network architecture that can be used to construct multiplicative interactions. "
22419,SP:5144391584e6d3825e12684b7c053e4e282cff2b,"algorithm USED-FOR batch active learning. deep neural network models USED-FOR algorithm. deep neural network models USED-FOR batch active learning. predictive uncertainty CONJUNCTION sample diversity. sample diversity CONJUNCTION predictive uncertainty. algorithm USED-FOR Batch Active learning. predictive uncertainty PART-OF strategy. Diverse Gradient Embeddings ( BADGE ) USED-FOR Batch Active learning. uncertainty CONJUNCTION diversity. diversity CONJUNCTION uncertainty. diversity EVALUATE-FOR BADGE. uncertainty EVALUATE-FOR BADGE. it USED-FOR real world active learning problems. approaches COMPARE BADGE. BADGE COMPARE approaches. approaches USED-FOR batch sizes. BADGE USED-FOR real world active learning problems. OtherScientificTerm are hallucinated gradient space, and hand - tuned hyperparameters. ","This paper proposes a new algorithm for batch active learning with deep neural network models. Batch Active learning with Diverse Gradient Embeddings (BADGE) is an algorithm that combines predictive uncertainty and sample diversity. The key idea is to learn a hallucinated gradient space, where the uncertainty and diversity of each sample are correlated. The authors show that BADGE is able to achieve a trade-off between uncertainty and the diversity of the samples. They also show that it can be applied to real world active learning problems and that it is more robust to hand-tuned hyperparameters. In addition, they show that existing approaches for batch sizes are more robust than BADGE.","This paper proposes a new algorithm for batch active learning with deep neural network models. Batch Active learning with Diverse Gradient Embeddings (BADGE) is an algorithm that combines predictive uncertainty and sample diversity. The key idea is to learn a hallucinated gradient space, where the uncertainty and diversity of each sample are correlated. The authors show that BADGE is able to achieve a trade-off between uncertainty and the diversity of the samples. They also show that it can be applied to real world active learning problems and that it is more robust to hand-tuned hyperparameters. In addition, they show that existing approaches for batch sizes are more robust than BADGE."
22428,SP:ce6023b1e6bf45b071a6f5457b2575425ae03366,"models USED-FOR decision making parameters. obscure feature extraction CONJUNCTION transformation process. transformation process CONJUNCTION obscure feature extraction. complex architectures CONJUNCTION obscure feature extraction. obscure feature extraction CONJUNCTION complex architectures. non - linearity FEATURE-OF activation functions. low level features CONJUNCTION high level features. high level features CONJUNCTION low level features. low level features FEATURE-OF hidden layer. high level features FEATURE-OF hidden layer. feature leveling architecture USED-FOR low level features. low level features CONJUNCTION high level features. high level features CONJUNCTION low level features. GLM layer PART-OF architecture. GLM layer USED-FOR feature leveling architecture. per - layer basis USED-FOR high level features. models COMPARE main - stream architectures. main - stream architectures COMPARE models. Method are Self - explaining models, General Linear Models ( GLMs ), deep neural networks ( DNNs ), and deep architectures. Task is model reasoning process. OtherScientificTerm is model weights. ","Self-explaining models have been a popular topic of interest in recent years due to their ability to explain decision making parameters. However, there are several issues with the current state-of-the-art Self-Explaining models: (1) complex architectures, obscure feature extraction and the transformation process, (2) non-linearity of activation functions, and (3) the model reasoning process.    This paper proposes a feature-leveling architecture based on General Linear Models (GLMs) to address these issues. Specifically, the authors propose a feature leveling architecture that combines the low level features of the hidden layer with the high level features from the previous layer. The architecture consists of a single GLM layer for each layer of the architecture, and a feature leveler layer for the low-level features and a GLM for the high-level ones. The authors show that the proposed feature levelers are able to explain the decision making of deep neural networks (DNNs) in a way that is consistent with the model weights. They also show that their models perform better than other main-stream architectures.  The authors also propose a per-layer basis for explaining the high levels of the model, which can be applied to all layers of the deep architectures.","Self-explaining models have been a popular topic of interest in recent years due to their ability to explain decision making parameters. However, there are several issues with the current state-of-the-art Self-Explaining models: (1) complex architectures, obscure feature extraction and the transformation process, (2) non-linearity of activation functions, and (3) the model reasoning process.    This paper proposes a feature-leveling architecture based on General Linear Models (GLMs) to address these issues. Specifically, the authors propose a feature leveling architecture that combines the low level features of the hidden layer with the high level features from the previous layer. The architecture consists of a single GLM layer for each layer of the architecture, and a feature leveler layer for the low-level features and a GLM for the high-level ones. The authors show that the proposed feature levelers are able to explain the decision making of deep neural networks (DNNs) in a way that is consistent with the model weights. They also show that their models perform better than other main-stream architectures.  The authors also propose a per-layer basis for explaining the high levels of the model, which can be applied to all layers of the deep architectures."
22437,SP:b70ceead1bf6c7dc684c74501716e7012b891022,"gradient cost FEATURE-OF softmax regression. uniform negative sampling USED-FOR scalable softmax approximation. training method USED-FOR gradient signal. adversarial model USED-FOR data distribution. adversarial model USED-FOR negative samples. negative samples USED-FOR training method. adversarial sampling mechanism USED-FOR negative samples. adversarial sampling USED-FOR gradient variance. large scale data sets EVALUATE-FOR competitive baselines. adversarial sampling mechanism USED-FOR gradient updates. training time EVALUATE-FOR competitive baselines. non - uniform sampling USED-FOR bias. Method are classifier, and extreme classification. Task is technology. OtherScientificTerm is slow convergence. Metric is signal - to - noise ratio. ","This paper studies the gradient cost of softmax regression with uniform negative sampling for scalable softmax approximation. The authors propose a new training method to reduce the gradient signal of the classifier by using an adversarial model to sample from the data distribution. The proposed training method uses negative samples generated by the proposed adversarial sampling mechanism as negative samples for the gradient variance. The paper shows competitive results on large scale data sets against competitive baselines in terms of training time and performance. The main contribution of the paper is that the proposed method is able to achieve a signal-to-noise ratio of O(1/\sqrt{T}^T) which is a significant improvement over the state-of-the-art.    The paper also shows that the bias of non-uniform sampling can be alleviated by removing bias due to the use of a non-differentiable classifier, which can be used to improve the performance of the proposed technology.  However, the authors also show that the non-discriminative bias is still present in extreme classification, which is not well-suited for this setting due to slow convergence.  The authors also propose a simple and effective way to mitigate the bias by using the adversarial samplers for gradient updates. ","This paper studies the gradient cost of softmax regression with uniform negative sampling for scalable softmax approximation. The authors propose a new training method to reduce the gradient signal of the classifier by using an adversarial model to sample from the data distribution. The proposed training method uses negative samples generated by the proposed adversarial sampling mechanism as negative samples for the gradient variance. The paper shows competitive results on large scale data sets against competitive baselines in terms of training time and performance. The main contribution of the paper is that the proposed method is able to achieve a signal-to-noise ratio of O(1/\sqrt{T}^T) which is a significant improvement over the state-of-the-art.    The paper also shows that the bias of non-uniform sampling can be alleviated by removing bias due to the use of a non-differentiable classifier, which can be used to improve the performance of the proposed technology.  However, the authors also show that the non-discriminative bias is still present in extreme classification, which is not well-suited for this setting due to slow convergence.  The authors also propose a simple and effective way to mitigate the bias by using the adversarial samplers for gradient updates. "
22446,SP:29b52fee83309268d9864f3b1fc3617948577d41,approach USED-FOR efficient exploration. lowdimensional encoding of the environment USED-FOR approach. modelbased and model - free objectives USED-FOR lowdimensional encoding of the environment. weighted distance of nearest neighbors USED-FOR novelty. intrinsic rewards USED-FOR novelty. low dimensional representational space FEATURE-OF weighted distance of nearest neighbors. weighted distance of nearest neighbors USED-FOR intrinsic rewards. intrinsic rewards USED-FOR approach. intrinsic rewards USED-FOR sample - efficient exploration. intrinsic rewards USED-FOR planning routines. representational space FEATURE-OF planning routines. planning routines USED-FOR sample - efficient exploration. exploration approach COMPARE baselines. baselines COMPARE exploration approach. maze tasks CONJUNCTION control problem. control problem CONJUNCTION maze tasks. control problem EVALUATE-FOR approach. maze tasks EVALUATE-FOR approach. Metric is model accuracy. ,This paper proposes a novel approach to efficient exploration based on a lowdimensional encoding of the environment using both modelbased and model-free objectives. The proposed approach uses intrinsic rewards based on the weighted distance of nearest neighbors in a low dimensional representational space to capture novelty. The intrinsic rewards are used to guide sample-efficient exploration using planning routines in the representational spaces. Experiments on maze tasks and a control problem show that the proposed exploration approach outperforms the baselines in terms of model accuracy. ,This paper proposes a novel approach to efficient exploration based on a lowdimensional encoding of the environment using both modelbased and model-free objectives. The proposed approach uses intrinsic rewards based on the weighted distance of nearest neighbors in a low dimensional representational space to capture novelty. The intrinsic rewards are used to guide sample-efficient exploration using planning routines in the representational spaces. Experiments on maze tasks and a control problem show that the proposed exploration approach outperforms the baselines in terms of model accuracy. 
22455,SP:257c98dc1a9f3efcbf9544d9ee2ff524b000543d,learning model USED-FOR few - shot classification. it USED-FOR out - of - distribution inputs. few - shot classification CONJUNCTION out - of - distribution detection. out - of - distribution detection CONJUNCTION few - shot classification. tasks USED-FOR outof - distribution detection. few - shot setting FEATURE-OF outof - distribution detection. few - shot classification datasets USED-FOR tasks. few - shot classification datasets USED-FOR benchmark datasets. methods USED-FOR task. benchmark datasets EVALUATE-FOR metrics. benchmark datasets EVALUATE-FOR baseline out - of - distribution detection. metrics USED-FOR baseline out - of - distribution detection. ,"This paper proposes a new learning model for few-shot classification and out-of-distribution detection. The authors show that it is possible to learn a good classifier that can be used to detect out of distribution (OOD) samples in the few shot setting, and that it can also be used for few shot classification. The paper also shows that the performance of outof distribution detection in a few shot learning setting can be improved by using the proposed learning model. ","This paper proposes a new learning model for few-shot classification and out-of-distribution detection. The authors show that it is possible to learn a good classifier that can be used to detect out of distribution (OOD) samples in the few shot setting, and that it can also be used for few shot classification. The paper also shows that the performance of outof distribution detection in a few shot learning setting can be improved by using the proposed learning model. "
22464,SP:a3632b773143dfb3a8f104c6b658dfa1167d155b,question - answering CONJUNCTION natural language inference. natural language inference CONJUNCTION question - answering. Undirected neural sequence models USED-FOR discriminative natural language understanding tasks. BERT HYPONYM-OF discriminative natural language understanding tasks. BERT HYPONYM-OF Undirected neural sequence models. natural language inference HYPONYM-OF discriminative natural language understanding tasks. question - answering HYPONYM-OF discriminative natural language understanding tasks. monotonic generation USED-FOR directed sequence models. models USED-FOR generating sequences. decoding PART-OF directed and undirected models. decoding PART-OF generalized model of sequence generation. framework USED-FOR generation. framework USED-FOR neural sequence models. refinement - based non - autoregressive models HYPONYM-OF neural sequence models. autoregressive HYPONYM-OF neural sequence models. decoding algorithms USED-FOR directed sequence models. decoding algorithms USED-FOR undirected models. decoding strategies USED-FOR cross - lingual masked translation model. framework USED-FOR undirected sequence models. approach USED-FOR constant - time translation. approach COMPARE linear - time translation. linear - time translation COMPARE approach. constant - time translation COMPARE linear - time translation. linear - time translation COMPARE constant - time translation. model USED-FOR linear - time translation. Material is WMT’14 English - German translation. Method is autoregressive model. ,"Undirected neural sequence models such as BERT have been widely used in discriminative natural language understanding tasks such as question-answering and natural language inference. The authors propose a framework for the generation of monotonic monotonicity in directed sequence models, which is a generalization of the work of [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,21].   [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14]  [15]  The paper proposes a generalized model of sequence generation that combines decoding in both directed and undirected models.    The authors show that, in the case of directed models, monotony can be achieved by monotonically generating sequences that are monosyllabic, and in the sense that the decoding in a directed model is monotonical.  [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [34] [35] [36] [37] [38] [39] [40] [41] [44] [45] [46] [47] [48] [49] [50] [55] [56] [57] [58] [59] [60] [63] [64] [65]  In this paper, the authors consider the problem of generating sequences from two models, one autoregressive and one non-autoregressive.  In particular, they focus on the case where the two models are trained on the same input sequence (i.e., BERT).   In order to achieve monotone generation, they propose a general framework for both the generation and decoding of a sequence, and they show that this framework can be applied to both neural sequences models (refinement-based non-auto-regressive models, and to the case when the input sequence is generated from a sequence that is generated by a neural sequence model that is non-iid.  The main contribution of this paper is that the authors propose decoding algorithms for directed sequences models that can be used to train undirect","Undirected neural sequence models such as BERT have been widely used in discriminative natural language understanding tasks such as question-answering and natural language inference. The authors propose a framework for the generation of monotonic monotonicity in directed sequence models, which is a generalization of the work of [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,21].   [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14]  [15]  The paper proposes a generalized model of sequence generation that combines decoding in both directed and undirected models.    The authors show that, in the case of directed models, monotony can be achieved by monotonically generating sequences that are monosyllabic, and in the sense that the decoding in a directed model is monotonical.  [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [34] [35] [36] [37] [38] [39] [40] [41] [44] [45] [46] [47] [48] [49] [50] [55] [56] [57] [58] [59] [60] [63] [64] [65]  In this paper, the authors consider the problem of generating sequences from two models, one autoregressive and one non-autoregressive.  In particular, they focus on the case where the two models are trained on the same input sequence (i.e., BERT).   In order to achieve monotone generation, they propose a general framework for both the generation and decoding of a sequence, and they show that this framework can be applied to both neural sequences models (refinement-based non-auto-regressive models, and to the case when the input sequence is generated from a sequence that is generated by a neural sequence model that is non-iid.  The main contribution of this paper is that the authors propose decoding algorithms for directed sequences models that can be used to train undirect"
22473,SP:eca5e2be9831dfb79c4f5e633cbfadcfd2e00eb1,"real scenes USED-FOR MEs recognition. two - stage approach USED-FOR LaTeX sequence. printed mathematical expression image USED-FOR two - stage approach. method USED-FOR math symbols. object detection algorithm USED-FOR method. object detection algorithm USED-FOR math symbols. seq2seq model USED-FOR LaTeX sequences. attention mechanism USED-FOR seq2seq model. position information USED-FOR math symbols. two - stage method COMPARE end - to - end method. end - to - end method COMPARE two - stage method. ExpRate(expression recognition rate ) EVALUATE-FOR model. model COMPARE end - to - end model. end - to - end model COMPARE model. ExpRate(expression recognition rate ) EVALUATE-FOR end - to - end model. Task are mathematical expressions ( MEs ) recognition, and detection of mathematical symbols. Method is neutral network. OtherScientificTerm are mathematical symbols, and mathematical formulas. Metric are recognition accuracy, and generalization ability. ","This paper tackles the problem of mathematical expressions (MEs) recognition from real scenes. The authors propose a two-stage approach to generate a LaTeX sequence from a printed mathematical expression image, which is then used to train a neutral network to predict the presence of mathematical symbols. The proposed method first predicts math symbols using an object detection algorithm, and then trains a seq2seq model to generate LaTeX sequences using an attention mechanism. The seq2sequences are then used for the detection of math symbols based on the position information of the objects in the scene. The paper shows that the proposed two-step method outperforms an end-to-end method in terms of ExpRate(expression recognition rate) and recognition accuracy. The model is also shown to be able to generalize to unseen mathematical formulas.   ","This paper tackles the problem of mathematical expressions (MEs) recognition from real scenes. The authors propose a two-stage approach to generate a LaTeX sequence from a printed mathematical expression image, which is then used to train a neutral network to predict the presence of mathematical symbols. The proposed method first predicts math symbols using an object detection algorithm, and then trains a seq2seq model to generate LaTeX sequences using an attention mechanism. The seq2sequences are then used for the detection of math symbols based on the position information of the objects in the scene. The paper shows that the proposed two-step method outperforms an end-to-end method in terms of ExpRate(expression recognition rate) and recognition accuracy. The model is also shown to be able to generalize to unseen mathematical formulas.   "
22482,SP:923fee8623da1569a7f54a57b4b326f29440b4c0,"memory footprint FEATURE-OF convolutional network architectures. loss reconstruction error FEATURE-OF in - domain inputs. it USED-FOR in - domain inputs. it USED-FOR loss reconstruction error. bytealigned codebooks USED-FOR compressed weights. method USED-FOR inference. unlabelled data USED-FOR quantization time. CPU USED-FOR inference. bytealigned codebooks USED-FOR method. unlabelled data USED-FOR method. bytealigned codebooks USED-FOR inference. top-1 accuracy EVALUATE-FOR approach. Method are vector quantization method, ResNet-50 model, and Mask R - CNN. OtherScientificTerm is memory size. Metric is compression factor. Task is ImageNet object classification. ","This paper proposes a new vector quantization method to reduce the memory footprint of convolutional network architectures. The key idea is to compress the weights of a ResNet-50 model to reduce its memory size, and then use it to quantize in-domain inputs and reduce the loss reconstruction error of in-domains inputs. The proposed method can be used for inference on any CPU with a single CPU, and can be applied to any compression factor. The method uses bytealigned codebooks to compute the compressed weights, and the method can also be applied on unlabelled data to speed up the quantization time. Experiments on ImageNet object classification show that the proposed approach can achieve top-1 accuracy, and is competitive with Mask R-CNN.","This paper proposes a new vector quantization method to reduce the memory footprint of convolutional network architectures. The key idea is to compress the weights of a ResNet-50 model to reduce its memory size, and then use it to quantize in-domain inputs and reduce the loss reconstruction error of in-domains inputs. The proposed method can be used for inference on any CPU with a single CPU, and can be applied to any compression factor. The method uses bytealigned codebooks to compute the compressed weights, and the method can also be applied on unlabelled data to speed up the quantization time. Experiments on ImageNet object classification show that the proposed approach can achieve top-1 accuracy, and is competitive with Mask R-CNN."
22491,SP:74850ad70241948f93fed95ba1f0ac11360437c1,Tensor - Product Representations USED-FOR explicit representation of relation structure. Tensor - Product Representations PART-OF Transformer. free - form math wordproblems FEATURE-OF Mathematics Dataset. Mathematics Dataset EVALUATE-FOR TensorProduct Transformer ( TP - Transformer ). TP - Attention HYPONYM-OF attention mechanism. attention USED-FOR ambiguities. TP - Transformer ’s attention maps USED-FOR it. Generic is model. Task is representation - building. Method is Pretrained models. ,"This paper proposes TensorProduct Transformer (TP-Transformer), a Transformer-based model that uses Tensor-Product Representations to learn an explicit representation of relation structure. TP-Attention is a new attention mechanism that is based on the idea that attention can be used to capture ambiguities in representations. The authors demonstrate the effectiveness of the proposed model on the Mathematics Dataset for solving free-form math wordproblems. Pretrained models have been shown to be effective at representation-building, but it is not clear whether the TP-transformer’s attention maps are as effective. ","This paper proposes TensorProduct Transformer (TP-Transformer), a Transformer-based model that uses Tensor-Product Representations to learn an explicit representation of relation structure. TP-Attention is a new attention mechanism that is based on the idea that attention can be used to capture ambiguities in representations. The authors demonstrate the effectiveness of the proposed model on the Mathematics Dataset for solving free-form math wordproblems. Pretrained models have been shown to be effective at representation-building, but it is not clear whether the TP-transformer’s attention maps are as effective. "
22500,SP:d319df820c6630c409fab32097652a083e8f53ea,"identically distributed training and test sets EVALUATE-FOR Deep artificial neural networks. training and test accuracies EVALUATE-FOR Deep artificial neural networks. training and test sets COMPARE empirical sample set. empirical sample set COMPARE training and test sets. real - world input samples PART-OF empirical sample set. procedure USED-FOR source code. learning algorithm USED-FOR source code. procedure USED-FOR learning algorithm. Kolmogorov complexity USED-FOR universal cognitive similarity metric. information distance HYPONYM-OF universal cognitive similarity metric. optimization problem USED-FOR classification function. condition USED-FOR optimization problem. features USED-FOR classifier. empirical sample set FEATURE-OF training and test sets. model COMPARE model. model COMPARE model. model USED-FOR corruptions. corruptions CONJUNCTION adversarial perturbations. adversarial perturbations CONJUNCTION corruptions. corrupted or perturbed input features PART-OF empirical sample set. model USED-FOR adversarial perturbations. uncoded input features USED-FOR model. uncoded input features USED-FOR model. projected gradient descent USED-FOR adversarial perturbations. encoded input features USED-FOR model. Gaussian and shot noise HYPONYM-OF corruptions. Task are generalization, inference, and image classification. Metric is training and inference accuracies. OtherScientificTerm is channel codes. ","Deep artificial neural networks have been shown to have identically distributed training and test sets, but the empirical sample set of real-world input samples is very different from the training set, and training and inference accuracies can be very different. Deep artificial networks can be seen as a special case of this problem.    Deep artificial neural network can be viewed as a neural network that is trained on a training set that contains a large number of training examples and test examples that are very dissimilar to each other.  The paper proposes a new learning algorithm for learning the source code of a learning algorithm, which is based on a procedure called Kolmogorov complexity. The universal cognitive similarity metric, the information distance, is defined as the number of samples that are sufficiently dissimilar (in terms of their channel codes) to be a sufficient condition for the optimization problem of the classification function. The paper shows that under this condition, the classifier can be trained on features that are dissimilar enough to the ones that are present in the training data, but not dissimilar so far in the test data.  In addition to this, the paper also shows that in the presence of corruptions (e.g. Gaussian and shot noise), the training and testing sets of a training and a test set are very different, and that the training sets of training and the test sets of the empirical set of the training are very similar. In addition, it shows that a model trained on uncoded input features of a model with corrupted or perturbed input features is more robust to corruptions and adversarial perturbations, and is able to generalize better than a model based on encoded input features. Finally, it is shown that the model can generalize to unseen corruptions using projected gradient descent, and generalize well to unseen adversarial examples. ","Deep artificial neural networks have been shown to have identically distributed training and test sets, but the empirical sample set of real-world input samples is very different from the training set, and training and inference accuracies can be very different. Deep artificial networks can be seen as a special case of this problem.    Deep artificial neural network can be viewed as a neural network that is trained on a training set that contains a large number of training examples and test examples that are very dissimilar to each other.  The paper proposes a new learning algorithm for learning the source code of a learning algorithm, which is based on a procedure called Kolmogorov complexity. The universal cognitive similarity metric, the information distance, is defined as the number of samples that are sufficiently dissimilar (in terms of their channel codes) to be a sufficient condition for the optimization problem of the classification function. The paper shows that under this condition, the classifier can be trained on features that are dissimilar enough to the ones that are present in the training data, but not dissimilar so far in the test data.  In addition to this, the paper also shows that in the presence of corruptions (e.g. Gaussian and shot noise), the training and testing sets of a training and a test set are very different, and that the training sets of training and the test sets of the empirical set of the training are very similar. In addition, it shows that a model trained on uncoded input features of a model with corrupted or perturbed input features is more robust to corruptions and adversarial perturbations, and is able to generalize better than a model based on encoded input features. Finally, it is shown that the model can generalize to unseen corruptions using projected gradient descent, and generalize well to unseen adversarial examples. "
22509,SP:b8e86f5e89330d81ba4967a7ed2dbfb56375d8a0,Deep Graph Neural Networks ( GNNs ) USED-FOR graph - based regression tasks. Deep Graph Neural Networks ( GNNs ) USED-FOR graph classification. graph classification CONJUNCTION graph - based regression tasks. graph - based regression tasks CONJUNCTION graph classification. graph pooling USED-FOR GNNs. GNNs USED-FOR graphs. HaarPooling HYPONYM-OF graph pooling operation. compressive Haar transforms USED-FOR graph pooling operation. sequential clusterings USED-FOR HaarPooling. compressive Haar basis USED-FOR clustering. compressive Haar basis USED-FOR pooling layer. HaarPooling USED-FOR fine detail information. compressive Haar transforms USED-FOR HaarPooling. synthesis of nodes USED-FOR HaarPooling. compressive Haar transforms USED-FOR fine detail information. transforms USED-FOR structure information. sparsity of the Haar basis USED-FOR HaarPooling. linear complexity FEATURE-OF HaarPooling. HaarPooling CONJUNCTION graph convolution layers. graph convolution layers CONJUNCTION HaarPooling. diverse graph classification problems EVALUATE-FOR GNN. graph convolution layers USED-FOR GNN. HaarPooling USED-FOR GNN. Generic is tasks. OtherScientificTerm is cluster. ,"This paper proposes a novel graph pooling method, called HaarPooling, for graph classification and graph regression tasks. The proposed method is based on compressive Haar transforms. The authors show that the pooling operation can be applied to graph convolutional networks (GCNs) and graph neural networks (GNNs). The authors also show that it can be used to improve the performance of GNNs on graph classification tasks. ","This paper proposes a novel graph pooling method, called HaarPooling, for graph classification and graph regression tasks. The proposed method is based on compressive Haar transforms. The authors show that the pooling operation can be applied to graph convolutional networks (GCNs) and graph neural networks (GNNs). The authors also show that it can be used to improve the performance of GNNs on graph classification tasks. "
22518,SP:17bea301d6718ef5f28864dd2445552b3cf65eeb,"Point clouds USED-FOR 3D objects. encoder networks USED-FOR semantics of their input point clouds. fully - connected networks USED-FOR shape representations. fully - connected networks USED-FOR point - cloud decoders. decoder architectures USED-FOR semantics of variable sized point clouds. sample - based point - cloud decoders USED-FOR shape representation. Metric is precision. OtherScientificTerm are point feature distribution, and sampled features. Method are sample - based decoder architectures, and feedforward architectures. ","Point clouds for 3D objects are of great interest to the 3D object recognition community. However, the precision of point clouds is not always high-precision. This paper proposes to use encoder networks to capture the semantics of their input point clouds. The paper shows that point-cloud decoders using fully-connected networks are able to capture shape representations, and that sample-based decoder architectures can capture semantics of variable sized point clouds as well. In addition, the paper also shows that the shape representation learned by the sample-by-sample decoder can be improved by using feedforward architectures, where the point feature distribution is sampled from a set of sampled features. ","Point clouds for 3D objects are of great interest to the 3D object recognition community. However, the precision of point clouds is not always high-precision. This paper proposes to use encoder networks to capture the semantics of their input point clouds. The paper shows that point-cloud decoders using fully-connected networks are able to capture shape representations, and that sample-based decoder architectures can capture semantics of variable sized point clouds as well. In addition, the paper also shows that the shape representation learned by the sample-by-sample decoder can be improved by using feedforward architectures, where the point feature distribution is sampled from a set of sampled features. "
22527,SP:51d826ead5d1d9cb89d493ce4c39728651bbc57b,"controlled synthetic noise USED-FOR deep learning. controlled noise levels FEATURE-OF realworld noisy labels. Deep Neural Networks ( DNNs ) USED-FOR real - world noise. noisy data EVALUATE-FOR ImageNet architectures. Robust learning methods USED-FOR synthetic noise. Robust learning methods USED-FOR real - world noise. OtherScientificTerm are noise levels, networks, and Real - world noise. Material are benchmark of realworld noisy labels, and real - world noisy data. Method are DNNs, and robust DNN methods. ","This paper studies the problem of deep learning with controlled synthetic noise in the presence of real-world noisy labels with controlled noise levels. The authors show that Deep Neural Networks (DNNs) can be robust to real world noisy labels, even when the noise levels are controlled. Robust learning methods are shown to be able to deal with synthetic noise, but not real world noise. They also show that robust DNNs are able to handle real world data with different noise levels, and that networks trained on real world labels are robust to synthetic noise. Finally, they show that the robustness of ImageNet architectures to noisy data on noisy data can be improved on ImageNet datasets.    The paper is well-written and well-motivated, and the results are interesting. However, the paper suffers from a lack of comparison to the benchmark of realworld noise labels, and there is no comparison to real-real world noisy data. Real-world noise is not controllable, and it is not clear to me that robust learning methods can handle synthetic noise with controlled levels. ","This paper studies the problem of deep learning with controlled synthetic noise in the presence of real-world noisy labels with controlled noise levels. The authors show that Deep Neural Networks (DNNs) can be robust to real world noisy labels, even when the noise levels are controlled. Robust learning methods are shown to be able to deal with synthetic noise, but not real world noise. They also show that robust DNNs are able to handle real world data with different noise levels, and that networks trained on real world labels are robust to synthetic noise. Finally, they show that the robustness of ImageNet architectures to noisy data on noisy data can be improved on ImageNet datasets.    The paper is well-written and well-motivated, and the results are interesting. However, the paper suffers from a lack of comparison to the benchmark of realworld noise labels, and there is no comparison to real-real world noisy data. Real-world noise is not controllable, and it is not clear to me that robust learning methods can handle synthetic noise with controlled levels. "
22536,SP:9873f78fb2821afdbb5551700e6ab6a0e8bcb9f0,"pain - staking human supervision USED-FOR labeled data. rule - exemplar method USED-FOR collecting human supervision. it USED-FOR learning. training algorithm USED-FOR rules. coverage and label variables FEATURE-OF soft implication loss. latent coverage variables USED-FOR training algorithm. soft implication loss USED-FOR model. latent coverage variables USED-FOR rules. model USED-FOR inference. denoised rules CONJUNCTION model. model CONJUNCTION denoised rules. denoised rules USED-FOR inference. coupled rule - exemplar supervision USED-FOR denoising rules. algorithm COMPARE methods. methods COMPARE algorithm. tasks EVALUATE-FOR algorithm. tasks EVALUATE-FOR methods. clean and noisy supervision USED-FOR algorithm. clean and noisy supervision USED-FOR methods. OtherScientificTerm are human supervision, and supervision. ","This paper proposes a rule-exemplar method for collecting human supervision for the task ofpain-staking human supervision on labeled data. The training algorithm for learning rules is based on soft implication loss on coverage and label variables, and the training algorithm uses the latent coverage variables to train a model to learn a set of rules. Then, the denoised rules and the model are used for inference. The denoising rules are trained with coupled rule-exploitation, where the human supervision is used to train the model. The proposed algorithm is evaluated on a number of tasks, and compared with other methods that use clean and noisy supervision. The results show that the proposed algorithm outperforms the other methods on most of the tasks. ","This paper proposes a rule-exemplar method for collecting human supervision for the task ofpain-staking human supervision on labeled data. The training algorithm for learning rules is based on soft implication loss on coverage and label variables, and the training algorithm uses the latent coverage variables to train a model to learn a set of rules. Then, the denoised rules and the model are used for inference. The denoising rules are trained with coupled rule-exploitation, where the human supervision is used to train the model. The proposed algorithm is evaluated on a number of tasks, and compared with other methods that use clean and noisy supervision. The results show that the proposed algorithm outperforms the other methods on most of the tasks. "
22545,SP:6f2c656dbb7629f652a4291d6971625184d8118b,"Graph neural networks ( GNNs ) HYPONYM-OF deep models. memory layer USED-FOR GNNs. GNNs USED-FOR node representations. memory layer USED-FOR node representations. graph memory network ( GMN ) USED-FOR hierarchical graph representations. memory - based GNN ( MemGNN ) CONJUNCTION graph memory network ( GMN ). graph memory network ( GMN ) CONJUNCTION memory - based GNN ( MemGNN ). networks USED-FOR hierarchical graph representations. graph memory network ( GMN ) HYPONYM-OF networks. memory - based GNN ( MemGNN ) HYPONYM-OF networks. layer USED-FOR networks. graph classification and regression benchmarks EVALUATE-FOR models. chemical features PART-OF molecule data. chemical features FEATURE-OF representations. OtherScientificTerm are graphs, and graph. ","Graph neural networks (GNNs) are a class of deep models that operate on graphs. In this paper, the authors propose a new memory layer for GNNs to improve the performance of node representations. Specifically, they propose two networks: memory-based GNN (MemGNN) and graph memory network (GMCN), which are two networks that use the same layer to learn hierarchical graph representations. The proposed models are evaluated on several graph classification and regression benchmarks. The authors show that the representations learned by the two networks are able to capture chemical features in molecule data, and that the chemical features of the representations are preserved in the graph.","Graph neural networks (GNNs) are a class of deep models that operate on graphs. In this paper, the authors propose a new memory layer for GNNs to improve the performance of node representations. Specifically, they propose two networks: memory-based GNN (MemGNN) and graph memory network (GMCN), which are two networks that use the same layer to learn hierarchical graph representations. The proposed models are evaluated on several graph classification and regression benchmarks. The authors show that the representations learned by the two networks are able to capture chemical features in molecule data, and that the chemical features of the representations are preserved in the graph."
22554,SP:81bc52d734c86975d741b6482d65ca71a9d81620,"initial parameter values USED-FOR gradient - based optimization. convergence times CONJUNCTION model. model CONJUNCTION convergence times. gradient - based optimization USED-FOR deep neural networks. initialization USED-FOR deep linear networks. convergence COMPARE Gaussian initialization. Gaussian initialization COMPARE convergence. iid weights USED-FOR Gaussian initialization. Gaussian initializations USED-FOR convergence. initialization USED-FOR learning. dynamical isometry USED-FOR deep non - linear networks. Method are deep learning systems, initialization schemes, deep networks, and orthogonal initializations. OtherScientificTerm are orthogonal group, and global minimum. ","This paper studies the convergence of gradient-based optimization of the initial parameter values of deep learning systems. The authors show that the convergence times and the model performance of deep neural networks with different initialization schemes are highly correlated. They show that for deep linear networks with orthogonal group, the convergence converges faster than Gaussian initialization with iid weights. For deep non-linear networks, the authors show the convergence is faster than for Gaussian initializations. They also show that deep networks are invariant to the choice of the orthogonality of the initialization, and that the global minimum is a global minimum. Finally, they show that in the case of deep nonlinear networks with dynamical isometry, the learning can be accelerated by using a different initialization for each layer. ","This paper studies the convergence of gradient-based optimization of the initial parameter values of deep learning systems. The authors show that the convergence times and the model performance of deep neural networks with different initialization schemes are highly correlated. They show that for deep linear networks with orthogonal group, the convergence converges faster than Gaussian initialization with iid weights. For deep non-linear networks, the authors show the convergence is faster than for Gaussian initializations. They also show that deep networks are invariant to the choice of the orthogonality of the initialization, and that the global minimum is a global minimum. Finally, they show that in the case of deep nonlinear networks with dynamical isometry, the learning can be accelerated by using a different initialization for each layer. "
22563,SP:9f5d95fc89c2f0d59d04838aa180f3db67997dfa,"quantization methods USED-FOR deep neural networks. coarse quantization USED-FOR layers. 2 - bit quantization HYPONYM-OF high rate compression. additivity property FEATURE-OF deep neural networks. method USED-FOR optimization problem. Lagrangian Formulation USED-FOR method. joint framework USED-FOR optimal bit allocation problem. Lagrangian Formulation USED-FOR optimization problem. deep neural networks EVALUATE-FOR method. It USED-FOR deep CNN ResNet-50. accuracy loss EVALUATE-FOR It. Metric is accuracy. Generic is methods. OtherScientificTerm are equal bit rate, quantization, and additivity of output error. Task is deep CNNs compression. Method is deep CNNs. ","This paper studies quantization methods for deep neural networks. The authors propose two methods: 1) coarse quantization, which quantizes the weights of layers with equal bit rate, and 2) high rate compression, i.e., 2-bit quantization. They show that both methods are optimal for deep CNNs compression. The main contribution of this paper is that the authors show that the quantization of output error has an additivity property, meaning that the accuracy of a layer can be improved if the output of the layer is more sensitive to quantization than that of the previous layer.  2) The authors also show that a method based on Lagrangian Formulation can be used to solve the optimization problem of the optimal bit allocation problem in a joint framework.  The proposed method is evaluated on a number of standard deep CNN neural networks, and is shown to achieve state-of-the-art accuracy loss. It is also shown to be applicable to deep CNN ResNet-50.  ","This paper studies quantization methods for deep neural networks. The authors propose two methods: 1) coarse quantization, which quantizes the weights of layers with equal bit rate, and 2) high rate compression, i.e., 2-bit quantization. They show that both methods are optimal for deep CNNs compression. The main contribution of this paper is that the authors show that the quantization of output error has an additivity property, meaning that the accuracy of a layer can be improved if the output of the layer is more sensitive to quantization than that of the previous layer.  2) The authors also show that a method based on Lagrangian Formulation can be used to solve the optimization problem of the optimal bit allocation problem in a joint framework.  The proposed method is evaluated on a number of standard deep CNN neural networks, and is shown to achieve state-of-the-art accuracy loss. It is also shown to be applicable to deep CNN ResNet-50.  "
22572,SP:7191d7b217a12b1bf9c47d790896a8227c14cc3d,"metric USED-FOR convergence. Wasserstein distance USED-FOR Wasserstein GAN ( WGAN ). auto - encoders CONJUNCTION WGANs. WGANs CONJUNCTION auto - encoders. framework USED-FOR auto - encoders. framework USED-FOR WGANs. encoder network CONJUNCTION generative network. generative network CONJUNCTION encoder network. encoder network PART-OF iWGAN. generative network PART-OF iWGAN. iterative primal dual optimization process USED-FOR generative network. iterative primal dual optimization process USED-FOR iWGAN. generalization error bound FEATURE-OF iWGANs. maximum likelihood estimation USED-FOR model. iWGAN COMPARE autoencoder GANs. autoencoder GANs COMPARE iWGAN. stopping criteria FEATURE-OF iWGAN. model USED-FOR convergence. model USED-FOR mode collapse. measurement of quality check EVALUATE-FOR model. benchmark datasets EVALUATE-FOR state - of - the - art. benchmark datasets EVALUATE-FOR iWGANs. Method are Generative Adversarial Networks ( GANs ), minmax two - player training of GANs, and inference WGAN ( iWGAN ) model. OtherScientificTerm is unstable training. ","This paper proposes a new metric for measuring the convergence of Generative Adversarial Networks (GANs) based on the Wasserstein distance between the output of an encoder network and a generative network. The authors propose a new framework for auto-encoders and WGANs based on this new metric, which they call the WASSERstein GAN (WGAN). The authors show that the minmax two-player training of GANs is unstable in the presence of unstable training, and propose an inference WGAN (iWGAN) model to address this issue. The iWGAN is based on an iterative primal dual optimization process, where the model is trained with maximum likelihood estimation, and the autoencoder and generative networks are trained in parallel.  The authors provide a generalization error bound for iWANs, which shows that the model converges to a mode collapse under certain stopping criteria. They also provide a measurement of quality check for the model, and show that their model is more robust to mode collapse.  Finally, the authors evaluate the performance of their model on a number of benchmark datasets against state-of-the-art GAN models, and demonstrate that iWANNs outperform the state of the art in terms of generalization performance. ","This paper proposes a new metric for measuring the convergence of Generative Adversarial Networks (GANs) based on the Wasserstein distance between the output of an encoder network and a generative network. The authors propose a new framework for auto-encoders and WGANs based on this new metric, which they call the WASSERstein GAN (WGAN). The authors show that the minmax two-player training of GANs is unstable in the presence of unstable training, and propose an inference WGAN (iWGAN) model to address this issue. The iWGAN is based on an iterative primal dual optimization process, where the model is trained with maximum likelihood estimation, and the autoencoder and generative networks are trained in parallel.  The authors provide a generalization error bound for iWANs, which shows that the model converges to a mode collapse under certain stopping criteria. They also provide a measurement of quality check for the model, and show that their model is more robust to mode collapse.  Finally, the authors evaluate the performance of their model on a number of benchmark datasets against state-of-the-art GAN models, and demonstrate that iWANNs outperform the state of the art in terms of generalization performance. "
22581,SP:cca6ae14fd0dd12352855e594acf7f3263bb1f24,coreference CONJUNCTION NLP. NLP CONJUNCTION coreference. Crowdsourcing COMPARE expert annotation. expert annotation COMPARE Crowdsourcing. classification tasks USED-FOR annotation. adjudication USED-FOR crowdsourcing. MPA USED-FOR sparsity. sparsity FEATURE-OF crowdsourcing environments. stick breaking process USED-FOR nonparametric partially pooled structure. large - scale crowdsourced anaphora dataset EVALUATE-FOR model. crowdsourcing setups FEATURE-OF model. annotation tasks USED-FOR classification. model USED-FOR annotation tasks. Task is anaphoric annotation. OtherScientificTerm is coreference chains. Generic is it. ,"This paper studies the problem of anaphoric annotation, i.e. the problem that an annotator should be able to annotate a document in a way that is consistent with its classification tasks. Crowdsourcing has been shown to outperform expert annotation in coreference and NLP, but it is not well-studied in the context of coreference or NLP. This paper proposes to use an adjudication to improve crowdsourcing. The authors propose a nonparametric partially pooled structure based on the stick breaking process. They show that the sparsity of crowdsourcing environments can be controlled by MPA. They evaluate their model on a large-scale crowdsourced anaphora dataset and show that their model outperforms the state-of-the-art crowdsourcing setups. They also demonstrate that the proposed model can be applied to other annotation tasks for classification and demonstrate that it can be used to improve the performance of existing coreference chains. ","This paper studies the problem of anaphoric annotation, i.e. the problem that an annotator should be able to annotate a document in a way that is consistent with its classification tasks. Crowdsourcing has been shown to outperform expert annotation in coreference and NLP, but it is not well-studied in the context of coreference or NLP. This paper proposes to use an adjudication to improve crowdsourcing. The authors propose a nonparametric partially pooled structure based on the stick breaking process. They show that the sparsity of crowdsourcing environments can be controlled by MPA. They evaluate their model on a large-scale crowdsourced anaphora dataset and show that their model outperforms the state-of-the-art crowdsourcing setups. They also demonstrate that the proposed model can be applied to other annotation tasks for classification and demonstrate that it can be used to improve the performance of existing coreference chains. "
22597,SP:4295cae4a56a02eb21c486408c1bf37a7483cb49,"Exploration FEATURE-OF sparse reward reinforcement learning. intrinsic motivation USED-FOR sparse extrinsic reward signal. drives USED-FOR stabilize learning. exploration CONJUNCTION stabilize learning. stabilize learning CONJUNCTION exploration. successor feature control ( SFC ) HYPONYM-OF intrinsic reward. It COMPARE methods. methods COMPARE It. local information USED-FOR intrinsic motivation. statistics over complete trajectories USED-FOR It. local information USED-FOR methods. DeepMind Lab CONJUNCTION DeepMind Control Suite. DeepMind Control Suite CONJUNCTION DeepMind Lab. VizDoom CONJUNCTION DeepMind Lab. DeepMind Lab CONJUNCTION VizDoom. environments EVALUATE-FOR scheduled intrinsic drive ( SID ) agent. pure visual inputs USED-FOR environments. pure visual inputs USED-FOR scheduled intrinsic drive ( SID ) agent. DeepMind Lab HYPONYM-OF pure visual inputs. VizDoom HYPONYM-OF environments. DeepMind Control Suite HYPONYM-OF environments. DeepMind Lab HYPONYM-OF environments. exploration efficiency EVALUATE-FOR SFC. Generic is signals. OtherScientificTerm are bonus rewards, and intrinsic drives. Method are mixture policy, and intrinsic and extrinsic task policies. ","Exploration in sparse reward reinforcement learning is an important problem in which the intrinsic motivation for a sparse extrinsic reward signal can be very different from that of the intrinsic reward signal itself. This is due to the fact that the signals are sparse and the bonus rewards are sparse. This paper proposes a new intrinsic reward, successor feature control (SFC), which is a combination of two types of intrinsic drives: one that encourages exploration and the other that encourages stabilize learning. It is shown to be more efficient than existing methods that rely on local information to learn intrinsic motivation. It uses statistics over complete trajectories to learn a mixture policy, where the mixture policy is the sum of the learned policies of the two intrinsic drives. Experiments on three environments with pure visual inputs (VizDoom, DeepMind Lab and DeepMind Control Suite) show that the proposed scheduled intrinsic drive (SID) agent achieves better exploration efficiency compared to a single intrinsic drive on VizDoom. The paper also shows that SFC can be used to learn both intrinsic and extrinsical task policies.  ","Exploration in sparse reward reinforcement learning is an important problem in which the intrinsic motivation for a sparse extrinsic reward signal can be very different from that of the intrinsic reward signal itself. This is due to the fact that the signals are sparse and the bonus rewards are sparse. This paper proposes a new intrinsic reward, successor feature control (SFC), which is a combination of two types of intrinsic drives: one that encourages exploration and the other that encourages stabilize learning. It is shown to be more efficient than existing methods that rely on local information to learn intrinsic motivation. It uses statistics over complete trajectories to learn a mixture policy, where the mixture policy is the sum of the learned policies of the two intrinsic drives. Experiments on three environments with pure visual inputs (VizDoom, DeepMind Lab and DeepMind Control Suite) show that the proposed scheduled intrinsic drive (SID) agent achieves better exploration efficiency compared to a single intrinsic drive on VizDoom. The paper also shows that SFC can be used to learn both intrinsic and extrinsical task policies.  "
22613,SP:9fa22eb03a79bce0fc1c8e84ae8640e010701eca,"video - sentence pairs USED-FOR model. visual and language representations USED-FOR latent correspondence. multi - level co - attention mechanism USED-FOR multimodal representations. Frame - By - Word interaction module CONJUNCTION Word - Conditioned Visual Graph ( WCVG ). Word - Conditioned Visual Graph ( WCVG ) CONJUNCTION Frame - By - Word interaction module. Word - Conditioned Visual Graph ( WCVG ) PART-OF mechanism. Frame - By - Word interaction module PART-OF mechanism. positional encodings USED-FOR visual - semantic representations. positional encodings USED-FOR Transformers. iterative message - passing USED-FOR positional encodings. positional encodings PART-OF approach. iterative message - passing USED-FOR visual - semantic representations. wMAN model COMPARE weakly - supervised method. weakly - supervised method COMPARE wMAN model. DiDeMo and Charades - STA datasets EVALUATE-FOR representations. Recall@1 accuracy metric EVALUATE-FOR wMAN model. Task is weakly - supervised video moment retrieval. OtherScientificTerm are temporal annotations, and temporal sequence. Material is DiDeMo dataset. ","This paper tackles the problem of weakly-supervised video moment retrieval, where the model is trained on video-sentence pairs that have been annotated with temporal annotations. The authors propose a mechanism that combines visual and language representations to capture the latent correspondence between the two, and a multi-level co-attention mechanism to learn multimodal representations. The proposed mechanism is composed of a Frame-By-Word interaction module and a Word-Conditioned Visual Graph (WCVG). The proposed approach combines positional encodings from Transformers with iterative message-passing to learn visual-semantic representations. Experiments on the DiDeMo and Charades-STA datasets show that the representations learned by the wMAN model outperform the state-of-the-art weakly -supervised method in terms of Recall@1 accuracy metric. ","This paper tackles the problem of weakly-supervised video moment retrieval, where the model is trained on video-sentence pairs that have been annotated with temporal annotations. The authors propose a mechanism that combines visual and language representations to capture the latent correspondence between the two, and a multi-level co-attention mechanism to learn multimodal representations. The proposed mechanism is composed of a Frame-By-Word interaction module and a Word-Conditioned Visual Graph (WCVG). The proposed approach combines positional encodings from Transformers with iterative message-passing to learn visual-semantic representations. Experiments on the DiDeMo and Charades-STA datasets show that the representations learned by the wMAN model outperform the state-of-the-art weakly -supervised method in terms of Recall@1 accuracy metric. "
22629,SP:27ac670353f34ee7a23bb7622f80c1dfbc0985e0,"image - based rendering CONJUNCTION GAN - based image synthesis. GAN - based image synthesis CONJUNCTION image - based rendering. image - based rendering USED-FOR learned image - guided rendering technique. GAN - based image synthesis USED-FOR learned image - guided rendering technique. virtual showrooms CONJUNCTION virtual tours. virtual tours CONJUNCTION virtual showrooms. virtual tours CONJUNCTION digital inspection of historical artifacts. digital inspection of historical artifacts CONJUNCTION virtual tours. digital inspection of historical artifacts HYPONYM-OF virtual and augmented reality applications. virtual tours HYPONYM-OF virtual and augmented reality applications. virtual showrooms HYPONYM-OF virtual and augmented reality applications. handling of view - dependent effects PART-OF work. object - specific deep neural network USED-FOR view - dependent appearance. video USED-FOR proxy geometry. multi - view stereo USED-FOR proxy geometry. diffuse surfaces USED-FOR warping. specular highlights HYPONYM-OF view - dependent effects. deep neural network USED-FOR view - dependent effects. EffectsNet HYPONYM-OF deep neural network. pipeline USED-FOR view - dependent effects. composition network USED-FOR photo - realistic results. image - guided approach USED-FOR network. it USED-FOR appearance of captured images. real data EVALUATE-FOR approach. Generic are method, and estimations. OtherScientificTerm are 3D proxy, appearance, and diffuse images. ","This paper proposes a learned image-guided rendering technique that combines image-based rendering and GAN-based image synthesis. The proposed method is applicable to both virtual and augmented reality applications such as virtual showrooms, virtual tours, and digital inspection of historical artifacts. The work includes the handling of view-dependent effects such as warping and warping on diffuse surfaces, and the use of object-specific deep neural network to learn a view-dependant appearance. The proxy geometry is learned from a video, and a multi-view stereo is used to generate a 3D proxy geometry from a single video. The paper also proposes a pipeline to learn view-independent effects (e.g. specular highlights) from diffuse surfaces. The main contribution of the paper is that the proposed pipeline is able to learn the view- dependent effects from diffuse images, and that it can be used to improve the appearance of captured images.    The proposed approach is evaluated on real data, and it is shown to be able to achieve state-of-the-art results. The authors also show that their proposed pipeline can be applied to a different view of the same object, which is called EffectsNet. The network is trained using an image-driven approach, where a composition network generates photo-realistic results for each object in the scene. The method is also shown to perform well on a synthetic dataset. ","This paper proposes a learned image-guided rendering technique that combines image-based rendering and GAN-based image synthesis. The proposed method is applicable to both virtual and augmented reality applications such as virtual showrooms, virtual tours, and digital inspection of historical artifacts. The work includes the handling of view-dependent effects such as warping and warping on diffuse surfaces, and the use of object-specific deep neural network to learn a view-dependant appearance. The proxy geometry is learned from a video, and a multi-view stereo is used to generate a 3D proxy geometry from a single video. The paper also proposes a pipeline to learn view-independent effects (e.g. specular highlights) from diffuse surfaces. The main contribution of the paper is that the proposed pipeline is able to learn the view- dependent effects from diffuse images, and that it can be used to improve the appearance of captured images.    The proposed approach is evaluated on real data, and it is shown to be able to achieve state-of-the-art results. The authors also show that their proposed pipeline can be applied to a different view of the same object, which is called EffectsNet. The network is trained using an image-driven approach, where a composition network generates photo-realistic results for each object in the scene. The method is also shown to perform well on a synthetic dataset. "
22645,SP:257d124367b1da9a595dc11a9df750d6bade298e,"sparse representation of model uncertainty USED-FOR deep neural networks ( DNNs ). diagonal correction of the Kronecker - factored eigenbasis PART-OF scalable Laplace Approximation scheme. scalable Laplace Approximation scheme USED-FOR model uncertainty. operation USED-FOR full Bayesian analysis. low - rank approximation USED-FOR spectral sparsity. spectral sparsity FEATURE-OF DNNs. low - rank approximation USED-FOR eigenbasis. Methods USED-FOR sparsification. Methods USED-FOR memory - wise tractable sampling computations. approach COMPARE methods. methods COMPARE approach. OtherScientificTerm are information form, Kronecker - factored eigenbasis, and information matrix. Task is inversion of the information matrix. ",This paper proposes a sparse representation of model uncertainty for deep neural networks (DNNs) based on the diagonal correction of the Kronecker-factored eigenbasis in a scalable Laplace Approximation scheme. This operation allows for a full Bayesian analysis of the information form. The authors show that the low-rank approximation of the eigenbais of DNNs with spectral sparsity leads to spectral sparsification. They also show that memory-wise tractable sampling computations can be achieved using this approach.    The authors also propose a novel inversion of the Information matrix to make the information matrix sparsified. They show that this approach is more memory-efficient than existing methods for sparsifying. ,This paper proposes a sparse representation of model uncertainty for deep neural networks (DNNs) based on the diagonal correction of the Kronecker-factored eigenbasis in a scalable Laplace Approximation scheme. This operation allows for a full Bayesian analysis of the information form. The authors show that the low-rank approximation of the eigenbais of DNNs with spectral sparsity leads to spectral sparsification. They also show that memory-wise tractable sampling computations can be achieved using this approach.    The authors also propose a novel inversion of the Information matrix to make the information matrix sparsified. They show that this approach is more memory-efficient than existing methods for sparsifying. 
22661,SP:2e03ceba4004b82f86f8349352a8ee4520e9c35d,"Minwise Hashing ( MinHash ) USED-FOR set similarities. compact high - dimensional data USED-FOR learning and searching. set similarities CONJUNCTION compact high - dimensional data. compact high - dimensional data CONJUNCTION set similarities. Minwise Hashing ( MinHash ) USED-FOR compact high - dimensional data. MinHash USED-FOR MinHash values. permutation ( hash function ) USED-FOR MinHash values. permutation ( hash function ) USED-FOR Permutation Hashing ( OPH ). strategies USED-FOR densification. densification HYPONYM-OF remedial strategy. Amortization Hashing ( AHash ) USED-FOR empty bins. Amortization Hashing ( AHash ) HYPONYM-OF load - balanced hashing. AHash COMPARE densification strategies. densification strategies COMPARE AHash. AHash COMPARE OPH. OPH COMPARE AHash. OPH CONJUNCTION densification strategies. densification strategies CONJUNCTION OPH. runtime efficiency EVALUATE-FOR densification strategies. runtime efficiency EVALUATE-FOR AHash. Material are high - dimensional data, and real datasets. OtherScientificTerm are bins, and unbalanced load. Task is false similarity computation. ","This paper proposes Minwise Hashing (MinHash) for set similarities and compact high-dimensional data for learning and searching. MinHash uses a permutation (hash function) to compute the MinHash values for each pair of bins. Permutation Hashing uses the same permutation as in [1] and [2]. The authors show that MinHash converges to the optimal solution when the number of bins is large enough, but not when there is an unbalanced load. The authors propose a remedial strategy, called densification, to mitigate the issue of false similarity computation. They also propose two strategies for densification. The first one, called Amortization Hashing, is a variant of load-balanced hashing, which is called AHash. The paper shows that AHash outperforms OPH and other densification strategies in terms of runtime efficiency on real datasets. ","This paper proposes Minwise Hashing (MinHash) for set similarities and compact high-dimensional data for learning and searching. MinHash uses a permutation (hash function) to compute the MinHash values for each pair of bins. Permutation Hashing uses the same permutation as in [1] and [2]. The authors show that MinHash converges to the optimal solution when the number of bins is large enough, but not when there is an unbalanced load. The authors propose a remedial strategy, called densification, to mitigate the issue of false similarity computation. They also propose two strategies for densification. The first one, called Amortization Hashing, is a variant of load-balanced hashing, which is called AHash. The paper shows that AHash outperforms OPH and other densification strategies in terms of runtime efficiency on real datasets. "
22677,SP:d73827ab98b0ff6bd92abfefea43a5f88ea40de2,"feature extraction USED-FOR periodic signals. power generation CONJUNCTION industrial machine. industrial machine CONJUNCTION power generation. industrial machine CONJUNCTION robotic system. robotic system CONJUNCTION industrial machine. mechanized transportation vehicle CONJUNCTION power generation. power generation CONJUNCTION mechanized transportation vehicle. rotating shafts PART-OF robotic system. rotating shafts PART-OF mechanized transportation vehicle. multi - layer perceptron HYPONYM-OF methods. robust method USED-FOR features. machine learning architecture USED-FOR graph data. robust method USED-FOR phase shift data. cyclic permutation USED-FOR machine learning architecture. phase shift data USED-FOR features. cyclic permutation FEATURE-OF graph data. graph structure USED-FOR robust method. OtherScientificTerm are periodicity, shaft ’s rotation, Imprecise timing, phase shifts, and phase shift. ","This paper proposes a novel method for feature extraction for periodic signals, where the periodicity of the signal is a function of the shaft’s rotation. This is relevant for mechanized transportation vehicle, power generation, industrial machine, robotic system with rotating shafts, and more. Imprecise timing is critical for phase shifts, and existing methods (e.g., multi-layer perceptron) do not work well due to phase shift. The authors propose a robust method to extract features from phase shift data using a machine learning architecture based on cyclic permutation of the graph data. They show that the proposed robust method is robust to phase shifts in the graph structure. ","This paper proposes a novel method for feature extraction for periodic signals, where the periodicity of the signal is a function of the shaft’s rotation. This is relevant for mechanized transportation vehicle, power generation, industrial machine, robotic system with rotating shafts, and more. Imprecise timing is critical for phase shifts, and existing methods (e.g., multi-layer perceptron) do not work well due to phase shift. The authors propose a robust method to extract features from phase shift data using a machine learning architecture based on cyclic permutation of the graph data. They show that the proposed robust method is robust to phase shifts in the graph structure. "
22693,SP:0df5ad333eb4ff9cca7f2d117909e2ce533a65d8,them USED-FOR real world systems. controllability FEATURE-OF systems. precision USED-FOR real world systems. confidence score FEATURE-OF confidence oriented decoder. calibration technique USED-FOR faithful generation. calibration technique USED-FOR inference time. approach COMPARE state - of - the - art approaches. state - of - the - art approaches COMPARE approach. automatic metrics CONJUNCTION human evaluation. human evaluation CONJUNCTION automatic metrics. structured data - to - text dataset EVALUATE-FOR approach. structured data - to - text dataset EVALUATE-FOR state - of - the - art approaches. WikiBio HYPONYM-OF structured data - to - text dataset. human evaluation EVALUATE-FOR approach. human evaluation EVALUATE-FOR state - of - the - art approaches. automatic metrics EVALUATE-FOR state - of - the - art approaches. automatic metrics EVALUATE-FOR approach. Method is Neural conditional text generation systems. OtherScientificTerm is variational Bayes objective. ,"This paper proposes a new approach to train conditional text generation systems. The authors propose a variational Bayes objective to train the conditional generation system, and use them to train real world systems with high precision and controllability. They also propose a calibration technique to speed up the inference time for faithful generation. The approach is evaluated on a structured data-to-text dataset (e.g. WikiBio) and compares against state-of-the-art approaches on automatic metrics and human evaluation.","This paper proposes a new approach to train conditional text generation systems. The authors propose a variational Bayes objective to train the conditional generation system, and use them to train real world systems with high precision and controllability. They also propose a calibration technique to speed up the inference time for faithful generation. The approach is evaluated on a structured data-to-text dataset (e.g. WikiBio) and compares against state-of-the-art approaches on automatic metrics and human evaluation."
22709,SP:03307deac29173b2968fbd08f95fc77eb1f82410,Magnitude - based pruning USED-FOR pruning neural networks. magnitude - based pruning USED-FOR pruning modern architectures. Frobenius distortion FEATURE-OF linear operator. magnitude - based pruning USED-FOR Frobenius distortion. single layer optimization USED-FOR multi - layer optimization. single layer FEATURE-OF linear operator. lookahead pruning HYPONYM-OF pruning method. single layer optimization USED-FOR pruning method. method COMPARE magnitude - based pruning. magnitude - based pruning COMPARE method. VGG CONJUNCTION ResNet. ResNet CONJUNCTION VGG. networks EVALUATE-FOR magnitude - based pruning. networks EVALUATE-FOR method. VGG HYPONYM-OF networks. ResNet HYPONYM-OF networks. Method is neural networks. OtherScientificTerm is high - sparsity regime. ,"Magnitude-based pruning is a popular technique for pruning neural networks. However, it has been shown that the Frobenius distortion of a linear operator on a single layer of a neural network can be much larger than that of a multi-layer neural network. This paper proposes a new pruning method, called lookahead pruning, to tackle the problem of pruning modern architectures. The main idea is to use magnitude - based pruning to address the Frobonius distortion in pruning the linear operator at each layer of the neural networks, which is known to exist in the high-sparsity regime. The authors propose a novel pruning algorithm based on single layer optimization, which can be used for multi-layered optimization. The proposed method is shown to outperform magnitude-based prior pruning on two networks, VGG and ResNet. ","Magnitude-based pruning is a popular technique for pruning neural networks. However, it has been shown that the Frobenius distortion of a linear operator on a single layer of a neural network can be much larger than that of a multi-layer neural network. This paper proposes a new pruning method, called lookahead pruning, to tackle the problem of pruning modern architectures. The main idea is to use magnitude - based pruning to address the Frobonius distortion in pruning the linear operator at each layer of the neural networks, which is known to exist in the high-sparsity regime. The authors propose a novel pruning algorithm based on single layer optimization, which can be used for multi-layered optimization. The proposed method is shown to outperform magnitude-based prior pruning on two networks, VGG and ResNet. "
22725,SP:dc80fdc75bc14ae19fe4ba9b85c35ce00b12856f,"parallel workers PART-OF graph. technique USED-FOR decentralized SGD. quantized communication USED-FOR technique. quantized communication USED-FOR decentralized SGD. asymptotic rate EVALUATE-FOR algorithm. Moniqua COMPARE algorithm. algorithm COMPARE Moniqua. full - precision communication USED-FOR algorithm. Moniqua COMPARE quantized decentralized algorithms. quantized decentralized algorithms COMPARE Moniqua. wall clock time EVALUATE-FOR Moniqua. bit - budgets FEATURE-OF Moniqua. 4 - bits - per - parameter communication USED-FOR Moniqua. CIFAR10 EVALUATE-FOR VGG16. Method is Decentralized stochastic gradient descent ( SGD ). OtherScientificTerm are memory, biased or linear quantizers, and convergence. Task is non - convex objectives. ","This paper proposes a new technique for decentralized SGD with quantized communication. Decentralized stochastic gradient descent (SGD) is a technique that involves multiple parallel workers in a graph, each with a different memory. The authors provide an asymptotic rate of $O(\log n)$ for the algorithm with full-precision communication, and show that the algorithm Moniqua achieves $O(1/\sqrt{n})$ convergence with a wall clock time. They also show that with 4-bits-per-parameter communication, the algorithm can be faster than the previous algorithm, Monique, which has a memory of $n$ bits. They show that Monique outperforms the previous quantized decentralized algorithms with biased or linear quantizers, and Monique is faster than Monique with non-convex objectives.  The authors also provide an analysis of Monique's performance on non-trivial bit-budgeted algorithms. They find that the VGG16 outperforms Monique on CIFAR10 with a bit budget of $1/n$ and that the performance improves when the number of workers increases.  ","This paper proposes a new technique for decentralized SGD with quantized communication. Decentralized stochastic gradient descent (SGD) is a technique that involves multiple parallel workers in a graph, each with a different memory. The authors provide an asymptotic rate of $O(\log n)$ for the algorithm with full-precision communication, and show that the algorithm Moniqua achieves $O(1/\sqrt{n})$ convergence with a wall clock time. They also show that with 4-bits-per-parameter communication, the algorithm can be faster than the previous algorithm, Monique, which has a memory of $n$ bits. They show that Monique outperforms the previous quantized decentralized algorithms with biased or linear quantizers, and Monique is faster than Monique with non-convex objectives.  The authors also provide an analysis of Monique's performance on non-trivial bit-budgeted algorithms. They find that the VGG16 outperforms Monique on CIFAR10 with a bit budget of $1/n$ and that the performance improves when the number of workers increases.  "
22741,SP:86c61a658d07ab86e2d84cef7e480bf7a06e4ddb,"Method are reinforcement learning, and partial models. Generic are it, and they. Task is jointly modeling future observations. Material is images. ","This paper studies the problem of reinforcement learning in the context of partial models. In particular, it considers the setting of jointly modeling future observations (i.e., jointly modeling the current state of the art) and future observations. The authors show that it is possible to learn partial models in this setting and that they can be trained in an unsupervised way. They also show that partial models can be learned in a supervised way.   ","This paper studies the problem of reinforcement learning in the context of partial models. In particular, it considers the setting of jointly modeling future observations (i.e., jointly modeling the current state of the art) and future observations. The authors show that it is possible to learn partial models in this setting and that they can be trained in an unsupervised way. They also show that partial models can be learned in a supervised way.   "
22757,SP:c70479b2096a52584b242de58272ca8d8565feea,"succinct common representation of two correlated data variables USED-FOR conditional and joint generation tasks. variational autoencoder ( VAE ) model USED-FOR succinct common representation of two correlated data variables. distributed simulation CONJUNCTION channel synthesis. channel synthesis CONJUNCTION distributed simulation. information theoretic problems USED-FOR Wyner VAE model. channel synthesis HYPONYM-OF information theoretic problems. distributed simulation HYPONYM-OF information theoretic problems. common representation CONJUNCTION local representations. local representations CONJUNCTION common representation. Wyner VAE USED-FOR correlated data variables. mutual information FEATURE-OF regularization term. common representation PART-OF correlated data variables. local representations PART-OF Wyner VAE. shared concept HYPONYM-OF common representation. synthetic data CONJUNCTION real images. real images CONJUNCTION synthetic data. approach USED-FOR joint and conditional generation. real images USED-FOR style control. synthetic data USED-FOR style control. style control USED-FOR joint and conditional generation. VAE variants CONJUNCTION variational information bottleneck method. variational information bottleneck method CONJUNCTION VAE variants. model COMPARE VAE variants. VAE variants COMPARE model. model COMPARE variational information bottleneck method. variational information bottleneck method COMPARE model. succinct common representation USED-FOR generative. OtherScientificTerm are Wyner ’s common information, and data variables. ","This paper proposes a variational autoencoder (VAE) model that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed Wyner VAE model is motivated by two information theoretic problems: distributed simulation and channel synthesis. The authors show that the common representation (i.e., the shared concept) of the correlated data variable is a product of a common representation and a set of local representations, and that Wyner’s common information is the sum of the mutual information between the two data variables.  The authors further propose a regularization term that encourages the common information to be the same across all the data variables, and they show that this mutual information improves the performance of joint and conditional generation. The approach is tested on synthetic data and real images for style control for joint and unconditional generation, and it is shown that the proposed model outperforms other VAE variants and the variational information bottleneck method. The paper also shows that this model can be used to learn a more efficient generative model that is able to capture a more concise common representation.   ","This paper proposes a variational autoencoder (VAE) model that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed Wyner VAE model is motivated by two information theoretic problems: distributed simulation and channel synthesis. The authors show that the common representation (i.e., the shared concept) of the correlated data variable is a product of a common representation and a set of local representations, and that Wyner’s common information is the sum of the mutual information between the two data variables.  The authors further propose a regularization term that encourages the common information to be the same across all the data variables, and they show that this mutual information improves the performance of joint and conditional generation. The approach is tested on synthetic data and real images for style control for joint and unconditional generation, and it is shown that the proposed model outperforms other VAE variants and the variational information bottleneck method. The paper also shows that this model can be used to learn a more efficient generative model that is able to capture a more concise common representation.   "
